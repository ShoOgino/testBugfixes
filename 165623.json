{"path":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#_nextBucket().mjava","commits":[{"id":"79759974460bc59933cd169acc94f5c6b16368d5","date":1471318443,"type":1,"author":"David Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#_nextBucket().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorStream[FacetField]#_nextBucket().mjava","sourceNew":"  private SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n\n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getLeafReader().getLiveDocs();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0);\n                }\n              }\n            }\n          }\n\n        }\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        TermQuery bucketQuery = hasSubFacets ? new TermQuery(new Term(freq.field, term)) : null;\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, bucketQuery, termSet);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n    // end of the iteration\n    return null;\n  }\n\n","sourceOld":"  public SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n\n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getLeafReader().getLiveDocs();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0);\n                }\n              }\n            }\n          }\n\n        }\n\n\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        TermQuery bucketQuery = hasSubFacets ? new TermQuery(new Term(freq.field, term)) : null;\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, bucketQuery, termSet);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n\n    // end of the iteration\n    return null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6","date":1471496851,"type":1,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#_nextBucket().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorStream[FacetField]#_nextBucket().mjava","sourceNew":"  private SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n\n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getLeafReader().getLiveDocs();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0);\n                }\n              }\n            }\n          }\n\n        }\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        TermQuery bucketQuery = hasSubFacets ? new TermQuery(new Term(freq.field, term)) : null;\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, bucketQuery, termSet);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n    // end of the iteration\n    return null;\n  }\n\n","sourceOld":"  public SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n\n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getLeafReader().getLiveDocs();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0);\n                }\n              }\n            }\n          }\n\n        }\n\n\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        TermQuery bucketQuery = hasSubFacets ? new TermQuery(new Term(freq.field, term)) : null;\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, bucketQuery, termSet);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n\n    // end of the iteration\n    return null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"403d05f7f8d69b65659157eff1bc1d2717f04c66","date":1471692961,"type":1,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#_nextBucket().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorStream[FacetField]#_nextBucket().mjava","sourceNew":"  private SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n\n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getLeafReader().getLiveDocs();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0);\n                }\n              }\n            }\n          }\n\n        }\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        TermQuery bucketQuery = hasSubFacets ? new TermQuery(new Term(freq.field, term)) : null;\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, bucketQuery, termSet);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n    // end of the iteration\n    return null;\n  }\n\n","sourceOld":"  public SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n\n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getLeafReader().getLiveDocs();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0);\n                }\n              }\n            }\n          }\n\n        }\n\n\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        TermQuery bucketQuery = hasSubFacets ? new TermQuery(new Term(freq.field, term)) : null;\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, bucketQuery, termSet);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n\n    // end of the iteration\n    return null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e07c409cff8701e4dc3d45934b021a949a5a8822","date":1475694629,"type":3,"author":"yonik","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#_nextBucket().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#_nextBucket().mjava","sourceNew":"  private SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n\n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getSlowAtomicReader().getLiveDocs();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0);\n                }\n              }\n            }\n          }\n\n        }\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        TermQuery bucketQuery = hasSubFacets ? new TermQuery(new Term(freq.field, term)) : null;\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, bucketQuery, termSet);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n    // end of the iteration\n    return null;\n  }\n\n","sourceOld":"  private SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n\n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getLeafReader().getLiveDocs();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0);\n                }\n              }\n            }\n          }\n\n        }\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        TermQuery bucketQuery = hasSubFacets ? new TermQuery(new Term(freq.field, term)) : null;\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, bucketQuery, termSet);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n    // end of the iteration\n    return null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#_nextBucket().mjava","pathOld":"/dev/null","sourceNew":"  private SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n\n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getSlowAtomicReader().getLiveDocs();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0);\n                }\n              }\n            }\n          }\n\n        }\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        TermQuery bucketQuery = hasSubFacets ? new TermQuery(new Term(freq.field, term)) : null;\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, bucketQuery, termSet);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n    // end of the iteration\n    return null;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8ec805ca8fedc0166461148c7182f1bcbbd18ee1","date":1489767223,"type":3,"author":"yonik","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#_nextBucket().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#_nextBucket().mjava","sourceNew":"  private SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n\n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getSlowAtomicReader().getLiveDocs();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0);\n                }\n              }\n            }\n          }\n\n        }\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        TermQuery bucketQuery = hasSubFacets ? new TermQuery(new Term(freq.field, term)) : null;\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, bucketQuery, termSet, false);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n    // end of the iteration\n    return null;\n  }\n\n","sourceOld":"  private SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n\n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getSlowAtomicReader().getLiveDocs();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0);\n                }\n              }\n            }\n          }\n\n        }\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        TermQuery bucketQuery = hasSubFacets ? new TermQuery(new Term(freq.field, term)) : null;\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, bucketQuery, termSet);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n    // end of the iteration\n    return null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"58b93c361b4f6fe193e84bfd27ea523366eada52","date":1490100167,"type":3,"author":"yonik","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#_nextBucket().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#_nextBucket().mjava","sourceNew":"  private SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n\n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getSlowAtomicReader().getLiveDocs();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0);\n                }\n              }\n            }\n          }\n\n        }\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        TermQuery bucketQuery = hasSubFacets ? new TermQuery(new Term(freq.field, term)) : null;\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, bucketQuery, termSet, false, null);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n    // end of the iteration\n    return null;\n  }\n\n","sourceOld":"  private SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n\n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getSlowAtomicReader().getLiveDocs();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0);\n                }\n              }\n            }\n          }\n\n        }\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        TermQuery bucketQuery = hasSubFacets ? new TermQuery(new Term(freq.field, term)) : null;\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, bucketQuery, termSet, false);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n    // end of the iteration\n    return null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"11ab475c994c79138885cc8a30b2641d929cdc43","date":1490280010,"type":3,"author":"yonik","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#_nextBucket().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#_nextBucket().mjava","sourceNew":"  private SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n\n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getSlowAtomicReader().getLiveDocs();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0);\n                }\n              }\n            }\n          }\n\n        }\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        TermQuery bucketQuery = hasSubFacets ? new TermQuery(new Term(freq.field, term)) : null;\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, bucketQuery, termSet, false);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n    // end of the iteration\n    return null;\n  }\n\n","sourceOld":"  private SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n\n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getSlowAtomicReader().getLiveDocs();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0);\n                }\n              }\n            }\n          }\n\n        }\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        TermQuery bucketQuery = hasSubFacets ? new TermQuery(new Term(freq.field, term)) : null;\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, bucketQuery, termSet);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n    // end of the iteration\n    return null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8908009aaa8e9318b455c1c22b83e0e87738228a","date":1490280013,"type":3,"author":"yonik","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#_nextBucket().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#_nextBucket().mjava","sourceNew":"  private SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n\n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getSlowAtomicReader().getLiveDocs();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0);\n                }\n              }\n            }\n          }\n\n        }\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        TermQuery bucketQuery = hasSubFacets ? new TermQuery(new Term(freq.field, term)) : null;\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, bucketQuery, termSet, false, null);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n    // end of the iteration\n    return null;\n  }\n\n","sourceOld":"  private SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n\n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getSlowAtomicReader().getLiveDocs();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0);\n                }\n              }\n            }\n          }\n\n        }\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        TermQuery bucketQuery = hasSubFacets ? new TermQuery(new Term(freq.field, term)) : null;\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, bucketQuery, termSet, false);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n    // end of the iteration\n    return null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2c705a0d590cf911e7c942df49563ca2ea176e22","date":1526916174,"type":3,"author":"Chris Hostetter","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#_nextBucket().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#_nextBucket().mjava","sourceNew":"  private SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n    \n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getSlowAtomicReader().getLiveDocs();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0, slotContext);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0, slotContext);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0, slotContext);\n                }\n              }\n            }\n          }\n\n        }\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        TermQuery bucketQuery = hasSubFacets ? new TermQuery(new Term(freq.field, term)) : null;\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, bucketQuery, termSet, false, null);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n    // end of the iteration\n    return null;\n  }\n\n","sourceOld":"  private SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n\n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getSlowAtomicReader().getLiveDocs();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0);\n                }\n              }\n            }\n          }\n\n        }\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        TermQuery bucketQuery = hasSubFacets ? new TermQuery(new Term(freq.field, term)) : null;\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, bucketQuery, termSet, false, null);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n    // end of the iteration\n    return null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bf29cc8a76949bbcbc15b386a9e46a533f5b3332","date":1527778512,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#_nextBucket().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#_nextBucket().mjava","sourceNew":"  private SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n    \n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getLiveDocsBits();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0, slotContext);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0, slotContext);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0, slotContext);\n                }\n              }\n            }\n          }\n\n        }\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        TermQuery bucketQuery = hasSubFacets ? new TermQuery(new Term(freq.field, term)) : null;\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, bucketQuery, termSet, false, null);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n    // end of the iteration\n    return null;\n  }\n\n","sourceOld":"  private SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n    \n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getSlowAtomicReader().getLiveDocs();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0, slotContext);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0, slotContext);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0, slotContext);\n                }\n              }\n            }\n          }\n\n        }\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        TermQuery bucketQuery = hasSubFacets ? new TermQuery(new Term(freq.field, term)) : null;\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, bucketQuery, termSet, false, null);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n    // end of the iteration\n    return null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e8220da23feaeb400771f18161c4965dea5ab4cd","date":1530366342,"type":3,"author":"yonik","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#_nextBucket().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#_nextBucket().mjava","sourceNew":"  private SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n    \n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getLiveDocsBits();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0, slotContext);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0, slotContext);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0, slotContext);\n                }\n              }\n            }\n          }\n\n        }\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          shardHasMoreBuckets.set(true);\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        TermQuery bucketQuery = hasSubFacets ? new TermQuery(new Term(freq.field, term)) : null;\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, bucketQuery, termSet, false, null);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n    // end of the iteration\n    return null;\n  }\n\n","sourceOld":"  private SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n    \n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getLiveDocsBits();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0, slotContext);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0, slotContext);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0, slotContext);\n                }\n              }\n            }\n          }\n\n        }\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        TermQuery bucketQuery = hasSubFacets ? new TermQuery(new Term(freq.field, term)) : null;\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, bucketQuery, termSet, false, null);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n    // end of the iteration\n    return null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26","date":1531589977,"type":3,"author":"Michael Braun","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#_nextBucket().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#_nextBucket().mjava","sourceNew":"  private SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n    \n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getLiveDocsBits();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0, slotContext);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0, slotContext);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0, slotContext);\n                }\n              }\n            }\n          }\n\n        }\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          shardHasMoreBuckets.set(true);\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        TermQuery bucketQuery = hasSubFacets ? new TermQuery(new Term(freq.field, term)) : null;\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, bucketQuery, termSet, false, null);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n    // end of the iteration\n    return null;\n  }\n\n","sourceOld":"  private SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n    \n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getLiveDocsBits();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0, slotContext);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0, slotContext);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0, slotContext);\n                }\n              }\n            }\n          }\n\n        }\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        TermQuery bucketQuery = hasSubFacets ? new TermQuery(new Term(freq.field, term)) : null;\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, bucketQuery, termSet, false, null);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n    // end of the iteration\n    return null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","date":1531905561,"type":3,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#_nextBucket().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#_nextBucket().mjava","sourceNew":"  private SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n    \n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getLiveDocsBits();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0, slotContext);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0, slotContext);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0, slotContext);\n                }\n              }\n            }\n          }\n\n        }\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          shardHasMoreBuckets.set(true);\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        TermQuery bucketQuery = hasSubFacets ? new TermQuery(new Term(freq.field, term)) : null;\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, bucketQuery, termSet, false, null);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n    // end of the iteration\n    return null;\n  }\n\n","sourceOld":"  private SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n    \n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getLiveDocsBits();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0, slotContext);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0, slotContext);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0, slotContext);\n                }\n              }\n            }\n          }\n\n        }\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        TermQuery bucketQuery = hasSubFacets ? new TermQuery(new Term(freq.field, term)) : null;\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, bucketQuery, termSet, false, null);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n    // end of the iteration\n    return null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"90440a2155c2b9ce30bf8fc29b1c978c58dae1e7","date":1582642100,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#_nextBucket().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#_nextBucket().mjava","sourceNew":"  private SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n    \n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getLiveDocsBits();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0, slotContext);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs.getBits();\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.get(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.get(docid + base)) {\n                    c++;\n                    collect(docid, 0, slotContext);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.get(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.get(docid)) {\n                  c++;\n                  collect(docid, 0, slotContext);\n                }\n              }\n            }\n          }\n\n        }\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          shardHasMoreBuckets.set(true);\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        TermQuery bucketQuery = hasSubFacets ? new TermQuery(new Term(freq.field, term)) : null;\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, bucketQuery, termSet, false, null);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n    // end of the iteration\n    return null;\n  }\n\n","sourceOld":"  private SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n    \n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getLiveDocsBits();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0, slotContext);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0, slotContext);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0, slotContext);\n                }\n              }\n            }\n          }\n\n        }\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          shardHasMoreBuckets.set(true);\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        TermQuery bucketQuery = hasSubFacets ? new TermQuery(new Term(freq.field, term)) : null;\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, bucketQuery, termSet, false, null);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n    // end of the iteration\n    return null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"e8220da23feaeb400771f18161c4965dea5ab4cd":["bf29cc8a76949bbcbc15b386a9e46a533f5b3332"],"58b93c361b4f6fe193e84bfd27ea523366eada52":["8ec805ca8fedc0166461148c7182f1bcbbd18ee1"],"403d05f7f8d69b65659157eff1bc1d2717f04c66":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","2c8bedceb91e64a3f0e831450058fc4a76d2c0a6"],"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","79759974460bc59933cd169acc94f5c6b16368d5"],"90440a2155c2b9ce30bf8fc29b1c978c58dae1e7":["e8220da23feaeb400771f18161c4965dea5ab4cd"],"e07c409cff8701e4dc3d45934b021a949a5a8822":["403d05f7f8d69b65659157eff1bc1d2717f04c66"],"8ec805ca8fedc0166461148c7182f1bcbbd18ee1":["e07c409cff8701e4dc3d45934b021a949a5a8822"],"8908009aaa8e9318b455c1c22b83e0e87738228a":["11ab475c994c79138885cc8a30b2641d929cdc43"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","e07c409cff8701e4dc3d45934b021a949a5a8822"],"2c705a0d590cf911e7c942df49563ca2ea176e22":["58b93c361b4f6fe193e84bfd27ea523366eada52"],"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":["bf29cc8a76949bbcbc15b386a9e46a533f5b3332","e8220da23feaeb400771f18161c4965dea5ab4cd"],"bf29cc8a76949bbcbc15b386a9e46a533f5b3332":["2c705a0d590cf911e7c942df49563ca2ea176e22"],"11ab475c994c79138885cc8a30b2641d929cdc43":["e07c409cff8701e4dc3d45934b021a949a5a8822"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"79759974460bc59933cd169acc94f5c6b16368d5":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["90440a2155c2b9ce30bf8fc29b1c978c58dae1e7"],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":["bf29cc8a76949bbcbc15b386a9e46a533f5b3332","e8220da23feaeb400771f18161c4965dea5ab4cd"]},"commit2Childs":{"e8220da23feaeb400771f18161c4965dea5ab4cd":["90440a2155c2b9ce30bf8fc29b1c978c58dae1e7","0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"58b93c361b4f6fe193e84bfd27ea523366eada52":["2c705a0d590cf911e7c942df49563ca2ea176e22"],"403d05f7f8d69b65659157eff1bc1d2717f04c66":["e07c409cff8701e4dc3d45934b021a949a5a8822"],"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6":["403d05f7f8d69b65659157eff1bc1d2717f04c66"],"90440a2155c2b9ce30bf8fc29b1c978c58dae1e7":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"e07c409cff8701e4dc3d45934b021a949a5a8822":["8ec805ca8fedc0166461148c7182f1bcbbd18ee1","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","11ab475c994c79138885cc8a30b2641d929cdc43"],"8ec805ca8fedc0166461148c7182f1bcbbd18ee1":["58b93c361b4f6fe193e84bfd27ea523366eada52"],"8908009aaa8e9318b455c1c22b83e0e87738228a":[],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"2c705a0d590cf911e7c942df49563ca2ea176e22":["bf29cc8a76949bbcbc15b386a9e46a533f5b3332"],"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":[],"bf29cc8a76949bbcbc15b386a9e46a533f5b3332":["e8220da23feaeb400771f18161c4965dea5ab4cd","0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"11ab475c994c79138885cc8a30b2641d929cdc43":["8908009aaa8e9318b455c1c22b83e0e87738228a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["403d05f7f8d69b65659157eff1bc1d2717f04c66","2c8bedceb91e64a3f0e831450058fc4a76d2c0a6","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","79759974460bc59933cd169acc94f5c6b16368d5"],"79759974460bc59933cd169acc94f5c6b16368d5":["2c8bedceb91e64a3f0e831450058fc4a76d2c0a6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":[]},"heads":["8908009aaa8e9318b455c1c22b83e0e87738228a","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","cd5edd1f2b162a5cfa08efd17851a07373a96817","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}