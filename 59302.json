{"path":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#writePoints(SegmentWriteState).mjava","commits":[{"id":"cab7a79353f33d1a94cd307bf33aa5148601ebe6","date":1453391888,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#writePoints(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#writeDimensionalValues(SegmentWriteState).mjava","sourceNew":"  /** Writes all buffered points. */\n  private void writePoints(SegmentWriteState state) throws IOException {\n    PointWriter pointWriter = null;\n    boolean success = false;\n    try {\n      for (int i=0;i<fieldHash.length;i++) {\n        PerField perField = fieldHash[i];\n        while (perField != null) {\n          if (perField.pointValuesWriter != null) {\n            if (perField.fieldInfo.getPointDimensionCount() == 0) {\n              // BUG\n              throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has no points but wrote them\");\n            }\n            if (pointWriter == null) {\n              // lazy init\n              PointFormat fmt = state.segmentInfo.getCodec().pointFormat();\n              if (fmt == null) {\n                throw new IllegalStateException(\"field=\\\"\" + perField.fieldInfo.name + \"\\\" was indexed as points but codec does not support points\");\n              }\n              pointWriter = fmt.fieldsWriter(state);\n            }\n\n            perField.pointValuesWriter.flush(state, pointWriter);\n            perField.pointValuesWriter = null;\n          } else if (perField.fieldInfo.getPointDimensionCount() != 0) {\n            // BUG\n            throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has points but did not write them\");\n          }\n          perField = perField.next;\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(pointWriter);\n      } else {\n        IOUtils.closeWhileHandlingException(pointWriter);\n      }\n    }\n  }\n\n","sourceOld":"  /** Writes all buffered dimensional values. */\n  private void writeDimensionalValues(SegmentWriteState state) throws IOException {\n    DimensionalWriter dimensionalWriter = null;\n    boolean success = false;\n    try {\n      for (int i=0;i<fieldHash.length;i++) {\n        PerField perField = fieldHash[i];\n        while (perField != null) {\n          if (perField.dimensionalValuesWriter != null) {\n            if (perField.fieldInfo.getDimensionCount() == 0) {\n              // BUG\n              throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has no dimensional values but wrote them\");\n            }\n            if (dimensionalWriter == null) {\n              // lazy init\n              DimensionalFormat fmt = state.segmentInfo.getCodec().dimensionalFormat();\n              if (fmt == null) {\n                throw new IllegalStateException(\"field=\\\"\" + perField.fieldInfo.name + \"\\\" was indexed dimensionally but codec does not support dimensional formats\");\n              }\n              dimensionalWriter = fmt.fieldsWriter(state);\n            }\n\n            perField.dimensionalValuesWriter.flush(state, dimensionalWriter);\n            perField.dimensionalValuesWriter = null;\n          } else if (perField.fieldInfo.getDimensionCount() != 0) {\n            // BUG\n            throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has dimensional values but did not write them\");\n          }\n          perField = perField.next;\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(dimensionalWriter);\n      } else {\n        IOUtils.closeWhileHandlingException(dimensionalWriter);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"85ca0e073c286ebb2c89364ada6dd2740fc18880","date":1453996944,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#writePoints(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#writePoints(SegmentWriteState).mjava","sourceNew":"  /** Writes all buffered points. */\n  private void writePoints(SegmentWriteState state) throws IOException {\n    PointWriter pointWriter = null;\n    boolean success = false;\n    try {\n      for (int i=0;i<fieldHash.length;i++) {\n        PerField perField = fieldHash[i];\n        while (perField != null) {\n          if (perField.pointValuesWriter != null) {\n            if (perField.fieldInfo.getPointDimensionCount() == 0) {\n              // BUG\n              throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has no points but wrote them\");\n            }\n            if (pointWriter == null) {\n              // lazy init\n              PointFormat fmt = state.segmentInfo.getCodec().pointFormat();\n              if (fmt == null) {\n                throw new IllegalStateException(\"field=\\\"\" + perField.fieldInfo.name + \"\\\" was indexed as points but codec does not support points\");\n              }\n              pointWriter = fmt.fieldsWriter(state);\n            }\n\n            perField.pointValuesWriter.flush(state, pointWriter);\n            perField.pointValuesWriter = null;\n          } else if (perField.fieldInfo.getPointDimensionCount() != 0) {\n            // BUG\n            throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has points but did not write them\");\n          }\n          perField = perField.next;\n        }\n      }\n      if (pointWriter != null) {\n        pointWriter.finish();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(pointWriter);\n      } else {\n        IOUtils.closeWhileHandlingException(pointWriter);\n      }\n    }\n  }\n\n","sourceOld":"  /** Writes all buffered points. */\n  private void writePoints(SegmentWriteState state) throws IOException {\n    PointWriter pointWriter = null;\n    boolean success = false;\n    try {\n      for (int i=0;i<fieldHash.length;i++) {\n        PerField perField = fieldHash[i];\n        while (perField != null) {\n          if (perField.pointValuesWriter != null) {\n            if (perField.fieldInfo.getPointDimensionCount() == 0) {\n              // BUG\n              throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has no points but wrote them\");\n            }\n            if (pointWriter == null) {\n              // lazy init\n              PointFormat fmt = state.segmentInfo.getCodec().pointFormat();\n              if (fmt == null) {\n                throw new IllegalStateException(\"field=\\\"\" + perField.fieldInfo.name + \"\\\" was indexed as points but codec does not support points\");\n              }\n              pointWriter = fmt.fieldsWriter(state);\n            }\n\n            perField.pointValuesWriter.flush(state, pointWriter);\n            perField.pointValuesWriter = null;\n          } else if (perField.fieldInfo.getPointDimensionCount() != 0) {\n            // BUG\n            throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has points but did not write them\");\n          }\n          perField = perField.next;\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(pointWriter);\n      } else {\n        IOUtils.closeWhileHandlingException(pointWriter);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["4522ffca5a1f420c6a02198c9332d7c596a30ca5"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8d15e34266d75e4e8b95da046cd0afc812367b38","date":1454246129,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#writePoints(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#writePoints(SegmentWriteState).mjava","sourceNew":"  /** Writes all buffered points. */\n  private void writePoints(SegmentWriteState state) throws IOException {\n    PointWriter pointWriter = null;\n    boolean success = false;\n    try {\n      for (int i=0;i<fieldHash.length;i++) {\n        PerField perField = fieldHash[i];\n        while (perField != null) {\n          if (perField.pointValuesWriter != null) {\n            if (perField.fieldInfo.getPointDimensionCount() == 0) {\n              // BUG\n              throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has no points but wrote them\");\n            }\n            if (pointWriter == null) {\n              // lazy init\n              PointFormat fmt = state.segmentInfo.getCodec().pointFormat();\n              if (fmt == null) {\n                throw new IllegalStateException(\"field=\\\"\" + perField.fieldInfo.name + \"\\\" was indexed as points but codec does not support points\");\n              }\n              pointWriter = fmt.fieldsWriter(state);\n            }\n\n            perField.pointValuesWriter.flush(state, pointWriter);\n            perField.pointValuesWriter = null;\n          } else if (perField.fieldInfo.getPointDimensionCount() != 0) {\n            // BUG\n            throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has points but did not write them\");\n          }\n          perField = perField.next;\n        }\n      }\n      if (pointWriter != null) {\n        pointWriter.finish();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(pointWriter);\n      } else {\n        IOUtils.closeWhileHandlingException(pointWriter);\n      }\n    }\n  }\n\n","sourceOld":"  /** Writes all buffered points. */\n  private void writePoints(SegmentWriteState state) throws IOException {\n    PointWriter pointWriter = null;\n    boolean success = false;\n    try {\n      for (int i=0;i<fieldHash.length;i++) {\n        PerField perField = fieldHash[i];\n        while (perField != null) {\n          if (perField.pointValuesWriter != null) {\n            if (perField.fieldInfo.getPointDimensionCount() == 0) {\n              // BUG\n              throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has no points but wrote them\");\n            }\n            if (pointWriter == null) {\n              // lazy init\n              PointFormat fmt = state.segmentInfo.getCodec().pointFormat();\n              if (fmt == null) {\n                throw new IllegalStateException(\"field=\\\"\" + perField.fieldInfo.name + \"\\\" was indexed as points but codec does not support points\");\n              }\n              pointWriter = fmt.fieldsWriter(state);\n            }\n\n            perField.pointValuesWriter.flush(state, pointWriter);\n            perField.pointValuesWriter = null;\n          } else if (perField.fieldInfo.getPointDimensionCount() != 0) {\n            // BUG\n            throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has points but did not write them\");\n          }\n          perField = perField.next;\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(pointWriter);\n      } else {\n        IOUtils.closeWhileHandlingException(pointWriter);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1e6acbaae7af722f17204ceccf0f7db5753eccf3","date":1454775255,"type":3,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#writePoints(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#writePoints(SegmentWriteState).mjava","sourceNew":"  /** Writes all buffered points. */\n  private void writePoints(SegmentWriteState state) throws IOException {\n    PointWriter pointWriter = null;\n    boolean success = false;\n    try {\n      for (int i=0;i<fieldHash.length;i++) {\n        PerField perField = fieldHash[i];\n        while (perField != null) {\n          if (perField.pointValuesWriter != null) {\n            if (perField.fieldInfo.getPointDimensionCount() == 0) {\n              // BUG\n              throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has no points but wrote them\");\n            }\n            if (pointWriter == null) {\n              // lazy init\n              PointFormat fmt = state.segmentInfo.getCodec().pointFormat();\n              if (fmt == null) {\n                throw new IllegalStateException(\"field=\\\"\" + perField.fieldInfo.name + \"\\\" was indexed as points but codec does not support points\");\n              }\n              pointWriter = fmt.fieldsWriter(state);\n            }\n\n            perField.pointValuesWriter.flush(state, pointWriter);\n            perField.pointValuesWriter = null;\n          } else if (perField.fieldInfo.getPointDimensionCount() != 0) {\n            // BUG\n            throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has points but did not write them\");\n          }\n          perField = perField.next;\n        }\n      }\n      if (pointWriter != null) {\n        pointWriter.finish();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(pointWriter);\n      } else {\n        IOUtils.closeWhileHandlingException(pointWriter);\n      }\n    }\n  }\n\n","sourceOld":"  /** Writes all buffered points. */\n  private void writePoints(SegmentWriteState state) throws IOException {\n    PointWriter pointWriter = null;\n    boolean success = false;\n    try {\n      for (int i=0;i<fieldHash.length;i++) {\n        PerField perField = fieldHash[i];\n        while (perField != null) {\n          if (perField.pointValuesWriter != null) {\n            if (perField.fieldInfo.getPointDimensionCount() == 0) {\n              // BUG\n              throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has no points but wrote them\");\n            }\n            if (pointWriter == null) {\n              // lazy init\n              PointFormat fmt = state.segmentInfo.getCodec().pointFormat();\n              if (fmt == null) {\n                throw new IllegalStateException(\"field=\\\"\" + perField.fieldInfo.name + \"\\\" was indexed as points but codec does not support points\");\n              }\n              pointWriter = fmt.fieldsWriter(state);\n            }\n\n            perField.pointValuesWriter.flush(state, pointWriter);\n            perField.pointValuesWriter = null;\n          } else if (perField.fieldInfo.getPointDimensionCount() != 0) {\n            // BUG\n            throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has points but did not write them\");\n          }\n          perField = perField.next;\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(pointWriter);\n      } else {\n        IOUtils.closeWhileHandlingException(pointWriter);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4522ffca5a1f420c6a02198c9332d7c596a30ca5","date":1457270822,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#writePoints(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#writePoints(SegmentWriteState).mjava","sourceNew":"  /** Writes all buffered points. */\n  private void writePoints(SegmentWriteState state) throws IOException {\n    PointsWriter pointsWriter = null;\n    boolean success = false;\n    try {\n      for (int i=0;i<fieldHash.length;i++) {\n        PerField perField = fieldHash[i];\n        while (perField != null) {\n          if (perField.pointValuesWriter != null) {\n            if (perField.fieldInfo.getPointDimensionCount() == 0) {\n              // BUG\n              throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has no points but wrote them\");\n            }\n            if (pointsWriter == null) {\n              // lazy init\n              PointsFormat fmt = state.segmentInfo.getCodec().pointsFormat();\n              if (fmt == null) {\n                throw new IllegalStateException(\"field=\\\"\" + perField.fieldInfo.name + \"\\\" was indexed as points but codec does not support points\");\n              }\n              pointsWriter = fmt.fieldsWriter(state);\n            }\n\n            perField.pointValuesWriter.flush(state, pointsWriter);\n            perField.pointValuesWriter = null;\n          } else if (perField.fieldInfo.getPointDimensionCount() != 0) {\n            // BUG\n            throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has points but did not write them\");\n          }\n          perField = perField.next;\n        }\n      }\n      if (pointsWriter != null) {\n        pointsWriter.finish();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(pointsWriter);\n      } else {\n        IOUtils.closeWhileHandlingException(pointsWriter);\n      }\n    }\n  }\n\n","sourceOld":"  /** Writes all buffered points. */\n  private void writePoints(SegmentWriteState state) throws IOException {\n    PointWriter pointWriter = null;\n    boolean success = false;\n    try {\n      for (int i=0;i<fieldHash.length;i++) {\n        PerField perField = fieldHash[i];\n        while (perField != null) {\n          if (perField.pointValuesWriter != null) {\n            if (perField.fieldInfo.getPointDimensionCount() == 0) {\n              // BUG\n              throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has no points but wrote them\");\n            }\n            if (pointWriter == null) {\n              // lazy init\n              PointFormat fmt = state.segmentInfo.getCodec().pointFormat();\n              if (fmt == null) {\n                throw new IllegalStateException(\"field=\\\"\" + perField.fieldInfo.name + \"\\\" was indexed as points but codec does not support points\");\n              }\n              pointWriter = fmt.fieldsWriter(state);\n            }\n\n            perField.pointValuesWriter.flush(state, pointWriter);\n            perField.pointValuesWriter = null;\n          } else if (perField.fieldInfo.getPointDimensionCount() != 0) {\n            // BUG\n            throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has points but did not write them\");\n          }\n          perField = perField.next;\n        }\n      }\n      if (pointWriter != null) {\n        pointWriter.finish();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(pointWriter);\n      } else {\n        IOUtils.closeWhileHandlingException(pointWriter);\n      }\n    }\n  }\n\n","bugFix":["85ca0e073c286ebb2c89364ada6dd2740fc18880","cab7a79353f33d1a94cd307bf33aa5148601ebe6"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"86a0a50d2d14aaee1e635bbec914468551f7f9a2","date":1482234306,"type":5,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#writePoints(SegmentWriteState,Sorter.DocMap).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#writePoints(SegmentWriteState).mjava","sourceNew":"  /** Writes all buffered points. */\n  private void writePoints(SegmentWriteState state, Sorter.DocMap sortMap) throws IOException {\n    PointsWriter pointsWriter = null;\n    boolean success = false;\n    try {\n      for (int i=0;i<fieldHash.length;i++) {\n        PerField perField = fieldHash[i];\n        while (perField != null) {\n          if (perField.pointValuesWriter != null) {\n            if (perField.fieldInfo.getPointDimensionCount() == 0) {\n              // BUG\n              throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has no points but wrote them\");\n            }\n            if (pointsWriter == null) {\n              // lazy init\n              PointsFormat fmt = state.segmentInfo.getCodec().pointsFormat();\n              if (fmt == null) {\n                throw new IllegalStateException(\"field=\\\"\" + perField.fieldInfo.name + \"\\\" was indexed as points but codec does not support points\");\n              }\n              pointsWriter = fmt.fieldsWriter(state);\n            }\n\n            perField.pointValuesWriter.flush(state, sortMap, pointsWriter);\n            perField.pointValuesWriter = null;\n          } else if (perField.fieldInfo.getPointDimensionCount() != 0) {\n            // BUG\n            throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has points but did not write them\");\n          }\n          perField = perField.next;\n        }\n      }\n      if (pointsWriter != null) {\n        pointsWriter.finish();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(pointsWriter);\n      } else {\n        IOUtils.closeWhileHandlingException(pointsWriter);\n      }\n    }\n  }\n\n","sourceOld":"  /** Writes all buffered points. */\n  private void writePoints(SegmentWriteState state) throws IOException {\n    PointsWriter pointsWriter = null;\n    boolean success = false;\n    try {\n      for (int i=0;i<fieldHash.length;i++) {\n        PerField perField = fieldHash[i];\n        while (perField != null) {\n          if (perField.pointValuesWriter != null) {\n            if (perField.fieldInfo.getPointDimensionCount() == 0) {\n              // BUG\n              throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has no points but wrote them\");\n            }\n            if (pointsWriter == null) {\n              // lazy init\n              PointsFormat fmt = state.segmentInfo.getCodec().pointsFormat();\n              if (fmt == null) {\n                throw new IllegalStateException(\"field=\\\"\" + perField.fieldInfo.name + \"\\\" was indexed as points but codec does not support points\");\n              }\n              pointsWriter = fmt.fieldsWriter(state);\n            }\n\n            perField.pointValuesWriter.flush(state, pointsWriter);\n            perField.pointValuesWriter = null;\n          } else if (perField.fieldInfo.getPointDimensionCount() != 0) {\n            // BUG\n            throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has points but did not write them\");\n          }\n          perField = perField.next;\n        }\n      }\n      if (pointsWriter != null) {\n        pointsWriter.finish();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(pointsWriter);\n      } else {\n        IOUtils.closeWhileHandlingException(pointsWriter);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf","date":1482251961,"type":4,"author":"Kevin Risden","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#writePoints(SegmentWriteState).mjava","sourceNew":null,"sourceOld":"  /** Writes all buffered points. */\n  private void writePoints(SegmentWriteState state) throws IOException {\n    PointsWriter pointsWriter = null;\n    boolean success = false;\n    try {\n      for (int i=0;i<fieldHash.length;i++) {\n        PerField perField = fieldHash[i];\n        while (perField != null) {\n          if (perField.pointValuesWriter != null) {\n            if (perField.fieldInfo.getPointDimensionCount() == 0) {\n              // BUG\n              throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has no points but wrote them\");\n            }\n            if (pointsWriter == null) {\n              // lazy init\n              PointsFormat fmt = state.segmentInfo.getCodec().pointsFormat();\n              if (fmt == null) {\n                throw new IllegalStateException(\"field=\\\"\" + perField.fieldInfo.name + \"\\\" was indexed as points but codec does not support points\");\n              }\n              pointsWriter = fmt.fieldsWriter(state);\n            }\n\n            perField.pointValuesWriter.flush(state, pointsWriter);\n            perField.pointValuesWriter = null;\n          } else if (perField.fieldInfo.getPointDimensionCount() != 0) {\n            // BUG\n            throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has points but did not write them\");\n          }\n          perField = perField.next;\n        }\n      }\n      if (pointsWriter != null) {\n        pointsWriter.finish();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(pointsWriter);\n      } else {\n        IOUtils.closeWhileHandlingException(pointsWriter);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"4522ffca5a1f420c6a02198c9332d7c596a30ca5":["8d15e34266d75e4e8b95da046cd0afc812367b38"],"cab7a79353f33d1a94cd307bf33aa5148601ebe6":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf":["4522ffca5a1f420c6a02198c9332d7c596a30ca5","86a0a50d2d14aaee1e635bbec914468551f7f9a2"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"1e6acbaae7af722f17204ceccf0f7db5753eccf3":["cab7a79353f33d1a94cd307bf33aa5148601ebe6","8d15e34266d75e4e8b95da046cd0afc812367b38"],"85ca0e073c286ebb2c89364ada6dd2740fc18880":["cab7a79353f33d1a94cd307bf33aa5148601ebe6"],"86a0a50d2d14aaee1e635bbec914468551f7f9a2":["4522ffca5a1f420c6a02198c9332d7c596a30ca5"],"8d15e34266d75e4e8b95da046cd0afc812367b38":["cab7a79353f33d1a94cd307bf33aa5148601ebe6","85ca0e073c286ebb2c89364ada6dd2740fc18880"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["86a0a50d2d14aaee1e635bbec914468551f7f9a2"]},"commit2Childs":{"4522ffca5a1f420c6a02198c9332d7c596a30ca5":["5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf","86a0a50d2d14aaee1e635bbec914468551f7f9a2"],"cab7a79353f33d1a94cd307bf33aa5148601ebe6":["1e6acbaae7af722f17204ceccf0f7db5753eccf3","85ca0e073c286ebb2c89364ada6dd2740fc18880","8d15e34266d75e4e8b95da046cd0afc812367b38"],"5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["cab7a79353f33d1a94cd307bf33aa5148601ebe6"],"1e6acbaae7af722f17204ceccf0f7db5753eccf3":[],"85ca0e073c286ebb2c89364ada6dd2740fc18880":["8d15e34266d75e4e8b95da046cd0afc812367b38"],"86a0a50d2d14aaee1e635bbec914468551f7f9a2":["5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"8d15e34266d75e4e8b95da046cd0afc812367b38":["4522ffca5a1f420c6a02198c9332d7c596a30ca5","1e6acbaae7af722f17204ceccf0f7db5753eccf3"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf","1e6acbaae7af722f17204ceccf0f7db5753eccf3","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}