{"path":"backwards/src/java/org/apache/lucene/index/SegmentMerger#copyVectorsWithDeletions(TermVectorsWriter,TermVectorsReader,IndexReader).mjava","commits":[{"id":"480d01e5b0ef8efb136d51670fec297ae5ae2c9c","date":1268821447,"type":0,"author":"Uwe Schindler","isMerge":false,"pathNew":"backwards/src/java/org/apache/lucene/index/SegmentMerger#copyVectorsWithDeletions(TermVectorsWriter,TermVectorsReader,IndexReader).mjava","pathOld":"/dev/null","sourceNew":"  private void copyVectorsWithDeletions(final TermVectorsWriter termVectorsWriter,\n                                        final TermVectorsReader matchingVectorsReader,\n                                        final IndexReader reader)\n    throws IOException, MergeAbortedException {\n    final int maxDoc = reader.maxDoc();\n    if (matchingVectorsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int docNum = 0; docNum < maxDoc;) {\n        if (reader.isDeleted(docNum)) {\n          // skip deleted docs\n          ++docNum;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field \n        // numbers are identical\n        int start = docNum, numDocs = 0;\n        do {\n          docNum++;\n          numDocs++;\n          if (docNum >= maxDoc) break;\n          if (reader.isDeleted(docNum)) {\n            docNum++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n        \n        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, start, numDocs);\n        termVectorsWriter.addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, numDocs);\n        checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int docNum = 0; docNum < maxDoc; docNum++) {\n        if (reader.isDeleted(docNum)) {\n          // skip deleted docs\n          continue;\n        }\n        \n        // NOTE: it's very important to first assign to vectors then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        TermFreqVector[] vectors = reader.getTermFreqVectors(docNum);\n        termVectorsWriter.addAllDocVectors(vectors);\n        checkAbort.work(300);\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/backwards/src/java/org/apache/lucene/index/SegmentMerger#copyVectorsWithDeletions(TermVectorsWriter,TermVectorsReader,IndexReader).mjava","pathOld":"backwards/src/java/org/apache/lucene/index/SegmentMerger#copyVectorsWithDeletions(TermVectorsWriter,TermVectorsReader,IndexReader).mjava","sourceNew":"  private void copyVectorsWithDeletions(final TermVectorsWriter termVectorsWriter,\n                                        final TermVectorsReader matchingVectorsReader,\n                                        final IndexReader reader)\n    throws IOException, MergeAbortedException {\n    final int maxDoc = reader.maxDoc();\n    if (matchingVectorsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int docNum = 0; docNum < maxDoc;) {\n        if (reader.isDeleted(docNum)) {\n          // skip deleted docs\n          ++docNum;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field \n        // numbers are identical\n        int start = docNum, numDocs = 0;\n        do {\n          docNum++;\n          numDocs++;\n          if (docNum >= maxDoc) break;\n          if (reader.isDeleted(docNum)) {\n            docNum++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n        \n        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, start, numDocs);\n        termVectorsWriter.addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, numDocs);\n        checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int docNum = 0; docNum < maxDoc; docNum++) {\n        if (reader.isDeleted(docNum)) {\n          // skip deleted docs\n          continue;\n        }\n        \n        // NOTE: it's very important to first assign to vectors then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        TermFreqVector[] vectors = reader.getTermFreqVectors(docNum);\n        termVectorsWriter.addAllDocVectors(vectors);\n        checkAbort.work(300);\n      }\n    }\n  }\n\n","sourceOld":"  private void copyVectorsWithDeletions(final TermVectorsWriter termVectorsWriter,\n                                        final TermVectorsReader matchingVectorsReader,\n                                        final IndexReader reader)\n    throws IOException, MergeAbortedException {\n    final int maxDoc = reader.maxDoc();\n    if (matchingVectorsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int docNum = 0; docNum < maxDoc;) {\n        if (reader.isDeleted(docNum)) {\n          // skip deleted docs\n          ++docNum;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field \n        // numbers are identical\n        int start = docNum, numDocs = 0;\n        do {\n          docNum++;\n          numDocs++;\n          if (docNum >= maxDoc) break;\n          if (reader.isDeleted(docNum)) {\n            docNum++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n        \n        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, start, numDocs);\n        termVectorsWriter.addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, numDocs);\n        checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int docNum = 0; docNum < maxDoc; docNum++) {\n        if (reader.isDeleted(docNum)) {\n          // skip deleted docs\n          continue;\n        }\n        \n        // NOTE: it's very important to first assign to vectors then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        TermFreqVector[] vectors = reader.getTermFreqVectors(docNum);\n        termVectorsWriter.addAllDocVectors(vectors);\n        checkAbort.work(300);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"480d01e5b0ef8efb136d51670fec297ae5ae2c9c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["480d01e5b0ef8efb136d51670fec297ae5ae2c9c"]},"commit2Childs":{"480d01e5b0ef8efb136d51670fec297ae5ae2c9c":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["480d01e5b0ef8efb136d51670fec297ae5ae2c9c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}