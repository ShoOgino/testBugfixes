{"path":"solr/core/src/test/org/apache/solr/cloud/api/collections/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String,int).mjava","commits":[{"id":"742168028ecb4838c124d27f836df9637be2f769","date":1529417708,"type":1,"author":"Varun Thacker","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/api/collections/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String,int).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/api/collections/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String).mjava","sourceNew":"  private void testBackupAndRestore(String collectionName, int backupReplFactor) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    int restoreReplcationFactor = replFactor;\n    int restoreTlogReplicas = numTlogReplicas;\n    int restorePullReplicas = numPullReplicas;\n    boolean setExternalReplicationFactor = false;\n    if (random().nextBoolean()) { //Override replicationFactor / tLogReplicas / pullReplicas\n      setExternalReplicationFactor = true;\n      restoreTlogReplicas = TestUtil.nextInt(random(), 0, 1);\n      restoreReplcationFactor = TestUtil.nextInt(random(), 1, 2);\n      restorePullReplicas = TestUtil.nextInt(random(), 0, 1);\n    }\n    int numShards = backupCollection.getActiveSlices().size();\n\n    int restoreReplFactor = restoreReplcationFactor + restoreTlogReplicas + restorePullReplicas;\n\n    boolean isMaxShardsPerNodeExternal = false;\n    int restoreMaxShardsPerNode = -1;\n    {\n      CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n      //explicitly specify the replicationFactor/pullReplicas/nrtReplicas/tlogReplicas.\n      if (setExternalReplicationFactor)  {\n        restore.setReplicationFactor(restoreReplcationFactor);\n        restore.setTlogReplicas(restoreTlogReplicas);\n        restore.setPullReplicas(restorePullReplicas);\n      }\n\n\n      if (restoreReplFactor > backupReplFactor) { //else the backup maxShardsPerNode should be enough\n        restoreMaxShardsPerNode = (int)Math.ceil((restoreReplFactor * numShards/(double) cluster.getJettySolrRunners().size()));\n        log.info(\"numShards={} restoreReplFactor={} maxShardsPerNode={} totalNodes={}\",\n            numShards, restoreReplFactor, restoreMaxShardsPerNode, cluster.getJettySolrRunners().size());\n        isMaxShardsPerNodeExternal = true;\n\n        restore.setMaxShardsPerNode(restoreMaxShardsPerNode);\n      }\n\n      if (rarely()) { // Try with createNodeSet configuration\n        int nodeSetSize = cluster.getJettySolrRunners().size() / 2;\n        List<String> nodeStrs = new ArrayList<>(nodeSetSize);\n        Iterator<JettySolrRunner> iter = cluster.getJettySolrRunners().iterator();\n        for (int i = 0; i < nodeSetSize ; i++) {\n          nodeStrs.add(iter.next().getNodeName());\n        }\n        restore.setCreateNodeSet(String.join(\",\", nodeStrs));\n        restore.setCreateNodeSetShuffle(usually());\n        // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n        if (restore.getMaxShardsPerNode() != null) {\n          restore.setMaxShardsPerNode(restore.getMaxShardsPerNode() * 2);\n        } else {\n          restore.setMaxShardsPerNode(origShardToDocCount.size() * 2);\n        }\n      }\n\n      Properties props = new Properties();\n      props.setProperty(\"customKey\", \"customVal\");\n      restore.setProperties(props);\n\n      if (sameConfig==false) {\n        restore.setConfigName(\"customConfigName\");\n      }\n      if (random().nextBoolean()) {\n        assertEquals(0, restore.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n      }\n      AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n          restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n    }\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName, false);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreCollection.getMaxShardsPerNode() ,\n          v <= restoreCollection.getMaxShardsPerNode());\n    });\n\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getReplicationFactor().intValue());\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getNumNrtReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restorePullReplicas, restoreCollection.getNumPullReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restoreTlogReplicas, restoreCollection.getNumTlogReplicas().intValue());\n    if (isMaxShardsPerNodeExternal) {\n      assertEquals(restoreCollectionName, restoreMaxShardsPerNode, restoreCollection.getMaxShardsPerNode());\n    } else {\n      assertEquals(restoreCollectionName, backupCollection.getMaxShardsPerNode(), restoreCollection.getMaxShardsPerNode());\n    }\n\n    assertEquals(\"Restore collection should use stateFormat=2\", 2, restoreCollection.getStateFormat());\n\n    //SOLR-12605: Add more docs after restore is complete to see if they are getting added fine\n    //explicitly querying the leaders. If we use CloudSolrClient there is no guarantee that we'll hit a nrtReplica\n    {\n      Map<String, Integer> restoredCollectionPerShardCount =  getShardToDocCountMap(client, restoreCollection);\n      long restoredCollectionDocCount = restoredCollectionPerShardCount.values().stream().mapToInt(Number::intValue).sum();\n      int numberNewDocsIndexed = indexDocs(restoreCollectionName, true);\n      Map<String, Integer> restoredCollectionPerShardCountAfterIndexing = getShardToDocCountMap(client, restoreCollection);\n      int restoredCollectionFinalDocCount = restoredCollectionPerShardCountAfterIndexing.values().stream().mapToInt(Number::intValue).sum();\n\n      log.info(\"Original doc count in restored collection:\" + restoredCollectionDocCount + \", number of newly added documents to the restored collection: \" + numberNewDocsIndexed + \", after indexing: \" + restoredCollectionFinalDocCount);\n      assertEquals((restoredCollectionDocCount + numberNewDocsIndexed), restoredCollectionFinalDocCount);\n    }\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","sourceOld":"  private void testBackupAndRestore(String collectionName) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    {\n      CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n\n      //explicitly specify the replicationFactor/pullReplicas/nrtReplicas/tlogReplicas .\n      //Value is still the same as the original. maybe test with different values that the original for better test coverage\n      if (random().nextBoolean())  {\n        restore.setReplicationFactor(replFactor);\n      }\n      if (backupCollection.getReplicas().size() > cluster.getJettySolrRunners().size()) {\n        // may need to increase maxShardsPerNode (e.g. if it was shard split, then now we need more)\n        restore.setMaxShardsPerNode((int)Math.ceil(backupCollection.getReplicas().size()/cluster.getJettySolrRunners().size()));\n      }\n\n\n      if (rarely()) { // Try with createNodeSet configuration\n        int nodeSetSize = cluster.getJettySolrRunners().size() / 2;\n        List<String> nodeStrs = new ArrayList<>(nodeSetSize);\n        Iterator<JettySolrRunner> iter = cluster.getJettySolrRunners().iterator();\n        for (int i = 0; i < nodeSetSize ; i++) {\n          nodeStrs.add(iter.next().getNodeName());\n        }\n        restore.setCreateNodeSet(String.join(\",\", nodeStrs));\n        restore.setCreateNodeSetShuffle(usually());\n        // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n        if (restore.getMaxShardsPerNode() != null) {\n          restore.setMaxShardsPerNode(restore.getMaxShardsPerNode() * 2);\n        } else {\n          restore.setMaxShardsPerNode(origShardToDocCount.size() * 2);\n        }\n      }\n\n      Properties props = new Properties();\n      props.setProperty(\"customKey\", \"customVal\");\n      restore.setProperties(props);\n\n      if (sameConfig==false) {\n        restore.setConfigName(\"customConfigName\");\n      }\n      if (random().nextBoolean()) {\n        assertEquals(0, restore.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n      }\n      AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n          restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n    }\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName, false);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getReplicationFactor(), restoreCollection.getReplicationFactor());\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(backupCollection.getActiveSlices().iterator().next().getReplicas().size(),\n        restoreCollection.getActiveSlices().iterator().next().getReplicas().size());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreCollection.getMaxShardsPerNode() ,\n          v <= restoreCollection.getMaxShardsPerNode());\n    });\n\n    assertEquals(\"Different count of nrtReplicas. Backup collection state=\" + backupCollection + \"\\nRestore \" +\n        \"collection state=\" + restoreCollection, replFactor, restoreCollection.getNumNrtReplicas().intValue());\n    assertEquals(\"Different count of pullReplicas. Backup collection state=\" + backupCollection + \"\\nRestore\" +\n        \" collection state=\" + restoreCollection, numPullReplicas, restoreCollection.getNumPullReplicas().intValue());\n    assertEquals(\"Different count of TlogReplica. Backup collection state=\" + backupCollection + \"\\nRestore\" +\n        \" collection state=\" + restoreCollection, numTlogReplicas, restoreCollection.getNumTlogReplicas().intValue());\n\n    assertEquals(\"Restore collection should use stateFormat=2\", 2, restoreCollection.getStateFormat());\n\n    //SOLR-12605: Add more docs after restore is complete to see if they are getting added fine\n    //explicitly querying the leaders. If we use CloudSolrClient there is no guarantee that we'll hit a nrtReplica\n    {\n      Map<String, Integer> restoredCollectionPerShardCount =  getShardToDocCountMap(client, restoreCollection);\n      long restoredCollectionDocCount = restoredCollectionPerShardCount.values().stream().mapToInt(Number::intValue).sum();\n      int numberNewDocsIndexed = indexDocs(restoreCollectionName, true);\n      Map<String, Integer> restoredCollectionPerShardCountAfterIndexing = getShardToDocCountMap(client, restoreCollection);\n      int restoredCollectionFinalDocCount = restoredCollectionPerShardCountAfterIndexing.values().stream().mapToInt(Number::intValue).sum();\n\n      log.info(\"Original doc count in restored collection:\" + restoredCollectionDocCount + \", number of newly added documents to the restored collection: \" + numberNewDocsIndexed + \", after indexing: \" + restoredCollectionFinalDocCount);\n      assertEquals((restoredCollectionDocCount + numberNewDocsIndexed), restoredCollectionFinalDocCount);\n    }\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"63ae38ccbfadf7f763ed165694f6ae139e167f09","date":1529571076,"type":3,"author":"Varun Thacker","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/api/collections/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String,int).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/api/collections/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String,int).mjava","sourceNew":"  private void testBackupAndRestore(String collectionName, int backupReplFactor) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    int restoreReplcationFactor = replFactor;\n    int restoreTlogReplicas = numTlogReplicas;\n    int restorePullReplicas = numPullReplicas;\n    boolean setExternalReplicationFactor = false;\n    if (random().nextBoolean()) { //Override replicationFactor / tLogReplicas / pullReplicas\n      setExternalReplicationFactor = true;\n      restoreTlogReplicas = TestUtil.nextInt(random(), 0, 1);\n      restoreReplcationFactor = TestUtil.nextInt(random(), 1, 2);\n      restorePullReplicas = TestUtil.nextInt(random(), 0, 1);\n    }\n    int numShards = backupCollection.getActiveSlices().size();\n\n    int restoreReplFactor = restoreReplcationFactor + restoreTlogReplicas + restorePullReplicas;\n\n    boolean isMaxShardsPerNodeExternal = false;\n    boolean isMaxShardsUnlimited = false;\n    CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n        .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n    //explicitly specify the replicationFactor/pullReplicas/nrtReplicas/tlogReplicas.\n    if (setExternalReplicationFactor)  {\n      restore.setReplicationFactor(restoreReplcationFactor);\n      restore.setTlogReplicas(restoreTlogReplicas);\n      restore.setPullReplicas(restorePullReplicas);\n    }\n    final int restoreMaxShardsPerNode = (int) Math.ceil((restoreReplFactor * numShards/(double) cluster.getJettySolrRunners().size()));\n\n    if (restoreReplFactor > backupReplFactor) { //else the backup maxShardsPerNode should be enough\n      log.info(\"numShards={} restoreReplFactor={} maxShardsPerNode={} totalNodes={}\",\n          numShards, restoreReplFactor, restoreMaxShardsPerNode, cluster.getJettySolrRunners().size());\n\n      if (random().nextBoolean()) { //set it to -1\n        isMaxShardsUnlimited = true;\n        restore.setMaxShardsPerNode(-1);\n      } else {\n        isMaxShardsPerNodeExternal = true;\n        restore.setMaxShardsPerNode(restoreMaxShardsPerNode);\n      }\n    }\n\n    if (rarely()) { // Try with createNodeSet configuration\n      int nodeSetSize = cluster.getJettySolrRunners().size() / 2;\n      List<String> nodeStrs = new ArrayList<>(nodeSetSize);\n      Iterator<JettySolrRunner> iter = cluster.getJettySolrRunners().iterator();\n      for (int i = 0; i < nodeSetSize ; i++) {\n        nodeStrs.add(iter.next().getNodeName());\n      }\n      restore.setCreateNodeSet(String.join(\",\", nodeStrs));\n      restore.setCreateNodeSetShuffle(usually());\n      // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n      if (restore.getMaxShardsPerNode() != null) {\n        restore.setMaxShardsPerNode(restore.getMaxShardsPerNode() * 2);\n      } else {\n        restore.setMaxShardsPerNode(origShardToDocCount.size() * 2);\n      }\n    }\n\n    Properties props = new Properties();\n    props.setProperty(\"customKey\", \"customVal\");\n    restore.setProperties(props);\n\n    if (sameConfig==false) {\n      restore.setConfigName(\"customConfigName\");\n    }\n    if (random().nextBoolean()) {\n      assertEquals(0, restore.process(client).getStatus());\n    } else {\n      assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n    }\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n        restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName, false);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreMaxShardsPerNode\n              + \" state file \\n\" + restoreCollection, v <= restoreMaxShardsPerNode);\n    });\n\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getReplicationFactor().intValue());\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getNumNrtReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restorePullReplicas, restoreCollection.getNumPullReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restoreTlogReplicas, restoreCollection.getNumTlogReplicas().intValue());\n    if (isMaxShardsPerNodeExternal) {\n      assertEquals(restoreCollectionName, restoreMaxShardsPerNode, restoreCollection.getMaxShardsPerNode());\n    } else if (isMaxShardsUnlimited){\n      assertEquals(restoreCollectionName, -1, restoreCollection.getMaxShardsPerNode());\n    } else {\n      assertEquals(restoreCollectionName, backupCollection.getMaxShardsPerNode(), restoreCollection.getMaxShardsPerNode());\n    }\n\n    assertEquals(\"Restore collection should use stateFormat=2\", 2, restoreCollection.getStateFormat());\n\n    //SOLR-12605: Add more docs after restore is complete to see if they are getting added fine\n    //explicitly querying the leaders. If we use CloudSolrClient there is no guarantee that we'll hit a nrtReplica\n    {\n      Map<String, Integer> restoredCollectionPerShardCount =  getShardToDocCountMap(client, restoreCollection);\n      long restoredCollectionDocCount = restoredCollectionPerShardCount.values().stream().mapToInt(Number::intValue).sum();\n      int numberNewDocsIndexed = indexDocs(restoreCollectionName, true);\n      Map<String, Integer> restoredCollectionPerShardCountAfterIndexing = getShardToDocCountMap(client, restoreCollection);\n      int restoredCollectionFinalDocCount = restoredCollectionPerShardCountAfterIndexing.values().stream().mapToInt(Number::intValue).sum();\n\n      log.info(\"Original doc count in restored collection:\" + restoredCollectionDocCount + \", number of newly added documents to the restored collection: \" + numberNewDocsIndexed + \", after indexing: \" + restoredCollectionFinalDocCount);\n      assertEquals((restoredCollectionDocCount + numberNewDocsIndexed), restoredCollectionFinalDocCount);\n    }\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","sourceOld":"  private void testBackupAndRestore(String collectionName, int backupReplFactor) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    int restoreReplcationFactor = replFactor;\n    int restoreTlogReplicas = numTlogReplicas;\n    int restorePullReplicas = numPullReplicas;\n    boolean setExternalReplicationFactor = false;\n    if (random().nextBoolean()) { //Override replicationFactor / tLogReplicas / pullReplicas\n      setExternalReplicationFactor = true;\n      restoreTlogReplicas = TestUtil.nextInt(random(), 0, 1);\n      restoreReplcationFactor = TestUtil.nextInt(random(), 1, 2);\n      restorePullReplicas = TestUtil.nextInt(random(), 0, 1);\n    }\n    int numShards = backupCollection.getActiveSlices().size();\n\n    int restoreReplFactor = restoreReplcationFactor + restoreTlogReplicas + restorePullReplicas;\n\n    boolean isMaxShardsPerNodeExternal = false;\n    int restoreMaxShardsPerNode = -1;\n    {\n      CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n      //explicitly specify the replicationFactor/pullReplicas/nrtReplicas/tlogReplicas.\n      if (setExternalReplicationFactor)  {\n        restore.setReplicationFactor(restoreReplcationFactor);\n        restore.setTlogReplicas(restoreTlogReplicas);\n        restore.setPullReplicas(restorePullReplicas);\n      }\n\n\n      if (restoreReplFactor > backupReplFactor) { //else the backup maxShardsPerNode should be enough\n        restoreMaxShardsPerNode = (int)Math.ceil((restoreReplFactor * numShards/(double) cluster.getJettySolrRunners().size()));\n        log.info(\"numShards={} restoreReplFactor={} maxShardsPerNode={} totalNodes={}\",\n            numShards, restoreReplFactor, restoreMaxShardsPerNode, cluster.getJettySolrRunners().size());\n        isMaxShardsPerNodeExternal = true;\n\n        restore.setMaxShardsPerNode(restoreMaxShardsPerNode);\n      }\n\n      if (rarely()) { // Try with createNodeSet configuration\n        int nodeSetSize = cluster.getJettySolrRunners().size() / 2;\n        List<String> nodeStrs = new ArrayList<>(nodeSetSize);\n        Iterator<JettySolrRunner> iter = cluster.getJettySolrRunners().iterator();\n        for (int i = 0; i < nodeSetSize ; i++) {\n          nodeStrs.add(iter.next().getNodeName());\n        }\n        restore.setCreateNodeSet(String.join(\",\", nodeStrs));\n        restore.setCreateNodeSetShuffle(usually());\n        // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n        if (restore.getMaxShardsPerNode() != null) {\n          restore.setMaxShardsPerNode(restore.getMaxShardsPerNode() * 2);\n        } else {\n          restore.setMaxShardsPerNode(origShardToDocCount.size() * 2);\n        }\n      }\n\n      Properties props = new Properties();\n      props.setProperty(\"customKey\", \"customVal\");\n      restore.setProperties(props);\n\n      if (sameConfig==false) {\n        restore.setConfigName(\"customConfigName\");\n      }\n      if (random().nextBoolean()) {\n        assertEquals(0, restore.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n      }\n      AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n          restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n    }\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName, false);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreCollection.getMaxShardsPerNode() ,\n          v <= restoreCollection.getMaxShardsPerNode());\n    });\n\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getReplicationFactor().intValue());\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getNumNrtReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restorePullReplicas, restoreCollection.getNumPullReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restoreTlogReplicas, restoreCollection.getNumTlogReplicas().intValue());\n    if (isMaxShardsPerNodeExternal) {\n      assertEquals(restoreCollectionName, restoreMaxShardsPerNode, restoreCollection.getMaxShardsPerNode());\n    } else {\n      assertEquals(restoreCollectionName, backupCollection.getMaxShardsPerNode(), restoreCollection.getMaxShardsPerNode());\n    }\n\n    assertEquals(\"Restore collection should use stateFormat=2\", 2, restoreCollection.getStateFormat());\n\n    //SOLR-12605: Add more docs after restore is complete to see if they are getting added fine\n    //explicitly querying the leaders. If we use CloudSolrClient there is no guarantee that we'll hit a nrtReplica\n    {\n      Map<String, Integer> restoredCollectionPerShardCount =  getShardToDocCountMap(client, restoreCollection);\n      long restoredCollectionDocCount = restoredCollectionPerShardCount.values().stream().mapToInt(Number::intValue).sum();\n      int numberNewDocsIndexed = indexDocs(restoreCollectionName, true);\n      Map<String, Integer> restoredCollectionPerShardCountAfterIndexing = getShardToDocCountMap(client, restoreCollection);\n      int restoredCollectionFinalDocCount = restoredCollectionPerShardCountAfterIndexing.values().stream().mapToInt(Number::intValue).sum();\n\n      log.info(\"Original doc count in restored collection:\" + restoredCollectionDocCount + \", number of newly added documents to the restored collection: \" + numberNewDocsIndexed + \", after indexing: \" + restoredCollectionFinalDocCount);\n      assertEquals((restoredCollectionDocCount + numberNewDocsIndexed), restoredCollectionFinalDocCount);\n    }\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","bugFix":null,"bugIntro":["ab6131420a270c49b653c969cc1dbbaf7d1b36e7","cb9804c0e92c14de7239ee3a6649fe4e5f9e3116"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d516662650185b8ef035b3933dca702cad9ca2c9","date":1530011383,"type":3,"author":"Varun Thacker","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/api/collections/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String,int).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/api/collections/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String,int).mjava","sourceNew":"  private void testBackupAndRestore(String collectionName, int backupReplFactor) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    int restoreReplcationFactor = replFactor;\n    int restoreTlogReplicas = numTlogReplicas;\n    int restorePullReplicas = numPullReplicas;\n    boolean setExternalReplicationFactor = false;\n    if (random().nextBoolean()) { //Override replicationFactor / tLogReplicas / pullReplicas\n      setExternalReplicationFactor = true;\n      restoreTlogReplicas = TestUtil.nextInt(random(), 0, 1);\n      restoreReplcationFactor = TestUtil.nextInt(random(), 1, 2);\n      restorePullReplicas = TestUtil.nextInt(random(), 0, 1);\n    }\n    int numShards = backupCollection.getActiveSlices().size();\n\n    int restoreReplFactor = restoreReplcationFactor + restoreTlogReplicas + restorePullReplicas;\n\n    boolean isMaxShardsPerNodeExternal = false;\n    boolean isMaxShardsUnlimited = false;\n    CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n        .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n    //explicitly specify the replicationFactor/pullReplicas/nrtReplicas/tlogReplicas.\n    if (setExternalReplicationFactor)  {\n      restore.setReplicationFactor(restoreReplcationFactor);\n      restore.setTlogReplicas(restoreTlogReplicas);\n      restore.setPullReplicas(restorePullReplicas);\n    }\n    int computeRestoreMaxShardsPerNode = (int) Math.ceil((restoreReplFactor * numShards/(double) cluster.getJettySolrRunners().size()));\n\n    if (restoreReplFactor > backupReplFactor) { //else the backup maxShardsPerNode should be enough\n      log.info(\"numShards={} restoreReplFactor={} maxShardsPerNode={} totalNodes={}\",\n          numShards, restoreReplFactor, computeRestoreMaxShardsPerNode, cluster.getJettySolrRunners().size());\n\n      if (random().nextBoolean()) { //set it to -1\n        isMaxShardsUnlimited = true;\n        restore.setMaxShardsPerNode(-1);\n      } else {\n        isMaxShardsPerNodeExternal = true;\n        restore.setMaxShardsPerNode(computeRestoreMaxShardsPerNode);\n      }\n    }\n\n    if (rarely()) { // Try with createNodeSet configuration\n      int nodeSetSize = cluster.getJettySolrRunners().size() / 2;\n      List<String> nodeStrs = new ArrayList<>(nodeSetSize);\n      Iterator<JettySolrRunner> iter = cluster.getJettySolrRunners().iterator();\n      for (int i = 0; i < nodeSetSize ; i++) {\n        nodeStrs.add(iter.next().getNodeName());\n      }\n      restore.setCreateNodeSet(String.join(\",\", nodeStrs));\n      restore.setCreateNodeSetShuffle(usually());\n      // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n      isMaxShardsPerNodeExternal = true;\n      if (restore.getMaxShardsPerNode() != null) {\n        computeRestoreMaxShardsPerNode = restore.getMaxShardsPerNode() * 2;\n        restore.setMaxShardsPerNode(computeRestoreMaxShardsPerNode);\n      } else {\n        computeRestoreMaxShardsPerNode = origShardToDocCount.size() * 2;\n        restore.setMaxShardsPerNode(origShardToDocCount.size() * 2);\n      }\n    }\n\n    final int restoreMaxShardsPerNode = computeRestoreMaxShardsPerNode;\n\n    Properties props = new Properties();\n    props.setProperty(\"customKey\", \"customVal\");\n    restore.setProperties(props);\n\n    if (sameConfig==false) {\n      restore.setConfigName(\"customConfigName\");\n    }\n    if (random().nextBoolean()) {\n      assertEquals(0, restore.process(client).getStatus());\n    } else {\n      assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n    }\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n        restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName, false);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreMaxShardsPerNode\n              + \". state: \\n\" + restoreCollection, v <= restoreMaxShardsPerNode);\n    });\n\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getReplicationFactor().intValue());\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getNumNrtReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restorePullReplicas, restoreCollection.getNumPullReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restoreTlogReplicas, restoreCollection.getNumTlogReplicas().intValue());\n    if (isMaxShardsPerNodeExternal) {\n      assertEquals(restoreCollectionName, restoreMaxShardsPerNode, restoreCollection.getMaxShardsPerNode());\n    } else if (isMaxShardsUnlimited){\n      assertEquals(restoreCollectionName, -1, restoreCollection.getMaxShardsPerNode());\n    } else {\n      assertEquals(restoreCollectionName, backupCollection.getMaxShardsPerNode(), restoreCollection.getMaxShardsPerNode());\n    }\n\n    assertEquals(\"Restore collection should use stateFormat=2\", 2, restoreCollection.getStateFormat());\n\n    //SOLR-12605: Add more docs after restore is complete to see if they are getting added fine\n    //explicitly querying the leaders. If we use CloudSolrClient there is no guarantee that we'll hit a nrtReplica\n    {\n      Map<String, Integer> restoredCollectionPerShardCount =  getShardToDocCountMap(client, restoreCollection);\n      long restoredCollectionDocCount = restoredCollectionPerShardCount.values().stream().mapToInt(Number::intValue).sum();\n      int numberNewDocsIndexed = indexDocs(restoreCollectionName, true);\n      Map<String, Integer> restoredCollectionPerShardCountAfterIndexing = getShardToDocCountMap(client, restoreCollection);\n      int restoredCollectionFinalDocCount = restoredCollectionPerShardCountAfterIndexing.values().stream().mapToInt(Number::intValue).sum();\n\n      log.info(\"Original doc count in restored collection:\" + restoredCollectionDocCount + \", number of newly added documents to the restored collection: \" + numberNewDocsIndexed + \", after indexing: \" + restoredCollectionFinalDocCount);\n      assertEquals((restoredCollectionDocCount + numberNewDocsIndexed), restoredCollectionFinalDocCount);\n    }\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","sourceOld":"  private void testBackupAndRestore(String collectionName, int backupReplFactor) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    int restoreReplcationFactor = replFactor;\n    int restoreTlogReplicas = numTlogReplicas;\n    int restorePullReplicas = numPullReplicas;\n    boolean setExternalReplicationFactor = false;\n    if (random().nextBoolean()) { //Override replicationFactor / tLogReplicas / pullReplicas\n      setExternalReplicationFactor = true;\n      restoreTlogReplicas = TestUtil.nextInt(random(), 0, 1);\n      restoreReplcationFactor = TestUtil.nextInt(random(), 1, 2);\n      restorePullReplicas = TestUtil.nextInt(random(), 0, 1);\n    }\n    int numShards = backupCollection.getActiveSlices().size();\n\n    int restoreReplFactor = restoreReplcationFactor + restoreTlogReplicas + restorePullReplicas;\n\n    boolean isMaxShardsPerNodeExternal = false;\n    boolean isMaxShardsUnlimited = false;\n    CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n        .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n    //explicitly specify the replicationFactor/pullReplicas/nrtReplicas/tlogReplicas.\n    if (setExternalReplicationFactor)  {\n      restore.setReplicationFactor(restoreReplcationFactor);\n      restore.setTlogReplicas(restoreTlogReplicas);\n      restore.setPullReplicas(restorePullReplicas);\n    }\n    final int restoreMaxShardsPerNode = (int) Math.ceil((restoreReplFactor * numShards/(double) cluster.getJettySolrRunners().size()));\n\n    if (restoreReplFactor > backupReplFactor) { //else the backup maxShardsPerNode should be enough\n      log.info(\"numShards={} restoreReplFactor={} maxShardsPerNode={} totalNodes={}\",\n          numShards, restoreReplFactor, restoreMaxShardsPerNode, cluster.getJettySolrRunners().size());\n\n      if (random().nextBoolean()) { //set it to -1\n        isMaxShardsUnlimited = true;\n        restore.setMaxShardsPerNode(-1);\n      } else {\n        isMaxShardsPerNodeExternal = true;\n        restore.setMaxShardsPerNode(restoreMaxShardsPerNode);\n      }\n    }\n\n    if (rarely()) { // Try with createNodeSet configuration\n      int nodeSetSize = cluster.getJettySolrRunners().size() / 2;\n      List<String> nodeStrs = new ArrayList<>(nodeSetSize);\n      Iterator<JettySolrRunner> iter = cluster.getJettySolrRunners().iterator();\n      for (int i = 0; i < nodeSetSize ; i++) {\n        nodeStrs.add(iter.next().getNodeName());\n      }\n      restore.setCreateNodeSet(String.join(\",\", nodeStrs));\n      restore.setCreateNodeSetShuffle(usually());\n      // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n      if (restore.getMaxShardsPerNode() != null) {\n        restore.setMaxShardsPerNode(restore.getMaxShardsPerNode() * 2);\n      } else {\n        restore.setMaxShardsPerNode(origShardToDocCount.size() * 2);\n      }\n    }\n\n    Properties props = new Properties();\n    props.setProperty(\"customKey\", \"customVal\");\n    restore.setProperties(props);\n\n    if (sameConfig==false) {\n      restore.setConfigName(\"customConfigName\");\n    }\n    if (random().nextBoolean()) {\n      assertEquals(0, restore.process(client).getStatus());\n    } else {\n      assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n    }\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n        restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName, false);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreMaxShardsPerNode\n              + \" state file \\n\" + restoreCollection, v <= restoreMaxShardsPerNode);\n    });\n\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getReplicationFactor().intValue());\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getNumNrtReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restorePullReplicas, restoreCollection.getNumPullReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restoreTlogReplicas, restoreCollection.getNumTlogReplicas().intValue());\n    if (isMaxShardsPerNodeExternal) {\n      assertEquals(restoreCollectionName, restoreMaxShardsPerNode, restoreCollection.getMaxShardsPerNode());\n    } else if (isMaxShardsUnlimited){\n      assertEquals(restoreCollectionName, -1, restoreCollection.getMaxShardsPerNode());\n    } else {\n      assertEquals(restoreCollectionName, backupCollection.getMaxShardsPerNode(), restoreCollection.getMaxShardsPerNode());\n    }\n\n    assertEquals(\"Restore collection should use stateFormat=2\", 2, restoreCollection.getStateFormat());\n\n    //SOLR-12605: Add more docs after restore is complete to see if they are getting added fine\n    //explicitly querying the leaders. If we use CloudSolrClient there is no guarantee that we'll hit a nrtReplica\n    {\n      Map<String, Integer> restoredCollectionPerShardCount =  getShardToDocCountMap(client, restoreCollection);\n      long restoredCollectionDocCount = restoredCollectionPerShardCount.values().stream().mapToInt(Number::intValue).sum();\n      int numberNewDocsIndexed = indexDocs(restoreCollectionName, true);\n      Map<String, Integer> restoredCollectionPerShardCountAfterIndexing = getShardToDocCountMap(client, restoreCollection);\n      int restoredCollectionFinalDocCount = restoredCollectionPerShardCountAfterIndexing.values().stream().mapToInt(Number::intValue).sum();\n\n      log.info(\"Original doc count in restored collection:\" + restoredCollectionDocCount + \", number of newly added documents to the restored collection: \" + numberNewDocsIndexed + \", after indexing: \" + restoredCollectionFinalDocCount);\n      assertEquals((restoredCollectionDocCount + numberNewDocsIndexed), restoredCollectionFinalDocCount);\n    }\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","bugFix":null,"bugIntro":["cb9804c0e92c14de7239ee3a6649fe4e5f9e3116"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26","date":1531589977,"type":0,"author":"Michael Braun","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/api/collections/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String,int).mjava","pathOld":"/dev/null","sourceNew":"  private void testBackupAndRestore(String collectionName, int backupReplFactor) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    int restoreReplcationFactor = replFactor;\n    int restoreTlogReplicas = numTlogReplicas;\n    int restorePullReplicas = numPullReplicas;\n    boolean setExternalReplicationFactor = false;\n    if (random().nextBoolean()) { //Override replicationFactor / tLogReplicas / pullReplicas\n      setExternalReplicationFactor = true;\n      restoreTlogReplicas = TestUtil.nextInt(random(), 0, 1);\n      restoreReplcationFactor = TestUtil.nextInt(random(), 1, 2);\n      restorePullReplicas = TestUtil.nextInt(random(), 0, 1);\n    }\n    int numShards = backupCollection.getActiveSlices().size();\n\n    int restoreReplFactor = restoreReplcationFactor + restoreTlogReplicas + restorePullReplicas;\n\n    boolean isMaxShardsPerNodeExternal = false;\n    boolean isMaxShardsUnlimited = false;\n    CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n        .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n    //explicitly specify the replicationFactor/pullReplicas/nrtReplicas/tlogReplicas.\n    if (setExternalReplicationFactor)  {\n      restore.setReplicationFactor(restoreReplcationFactor);\n      restore.setTlogReplicas(restoreTlogReplicas);\n      restore.setPullReplicas(restorePullReplicas);\n    }\n    int computeRestoreMaxShardsPerNode = (int) Math.ceil((restoreReplFactor * numShards/(double) cluster.getJettySolrRunners().size()));\n\n    if (restoreReplFactor > backupReplFactor) { //else the backup maxShardsPerNode should be enough\n      log.info(\"numShards={} restoreReplFactor={} maxShardsPerNode={} totalNodes={}\",\n          numShards, restoreReplFactor, computeRestoreMaxShardsPerNode, cluster.getJettySolrRunners().size());\n\n      if (random().nextBoolean()) { //set it to -1\n        isMaxShardsUnlimited = true;\n        restore.setMaxShardsPerNode(-1);\n      } else {\n        isMaxShardsPerNodeExternal = true;\n        restore.setMaxShardsPerNode(computeRestoreMaxShardsPerNode);\n      }\n    }\n\n    if (rarely()) { // Try with createNodeSet configuration\n      int nodeSetSize = cluster.getJettySolrRunners().size() / 2;\n      List<String> nodeStrs = new ArrayList<>(nodeSetSize);\n      Iterator<JettySolrRunner> iter = cluster.getJettySolrRunners().iterator();\n      for (int i = 0; i < nodeSetSize ; i++) {\n        nodeStrs.add(iter.next().getNodeName());\n      }\n      restore.setCreateNodeSet(String.join(\",\", nodeStrs));\n      restore.setCreateNodeSetShuffle(usually());\n      // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n      isMaxShardsPerNodeExternal = true;\n      if (restore.getMaxShardsPerNode() != null) {\n        computeRestoreMaxShardsPerNode = restore.getMaxShardsPerNode() * 2;\n        restore.setMaxShardsPerNode(computeRestoreMaxShardsPerNode);\n      } else {\n        computeRestoreMaxShardsPerNode = origShardToDocCount.size() * 2;\n        restore.setMaxShardsPerNode(origShardToDocCount.size() * 2);\n      }\n    }\n\n    final int restoreMaxShardsPerNode = computeRestoreMaxShardsPerNode;\n\n    Properties props = new Properties();\n    props.setProperty(\"customKey\", \"customVal\");\n    restore.setProperties(props);\n\n    if (sameConfig==false) {\n      restore.setConfigName(\"customConfigName\");\n    }\n    if (random().nextBoolean()) {\n      assertEquals(0, restore.process(client).getStatus());\n    } else {\n      assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n    }\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n        restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName, false);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreMaxShardsPerNode\n              + \". state: \\n\" + restoreCollection, v <= restoreMaxShardsPerNode);\n    });\n\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getReplicationFactor().intValue());\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getNumNrtReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restorePullReplicas, restoreCollection.getNumPullReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restoreTlogReplicas, restoreCollection.getNumTlogReplicas().intValue());\n    if (isMaxShardsPerNodeExternal) {\n      assertEquals(restoreCollectionName, restoreMaxShardsPerNode, restoreCollection.getMaxShardsPerNode());\n    } else if (isMaxShardsUnlimited){\n      assertEquals(restoreCollectionName, -1, restoreCollection.getMaxShardsPerNode());\n    } else {\n      assertEquals(restoreCollectionName, backupCollection.getMaxShardsPerNode(), restoreCollection.getMaxShardsPerNode());\n    }\n\n    assertEquals(\"Restore collection should use stateFormat=2\", 2, restoreCollection.getStateFormat());\n\n    //SOLR-12605: Add more docs after restore is complete to see if they are getting added fine\n    //explicitly querying the leaders. If we use CloudSolrClient there is no guarantee that we'll hit a nrtReplica\n    {\n      Map<String, Integer> restoredCollectionPerShardCount =  getShardToDocCountMap(client, restoreCollection);\n      long restoredCollectionDocCount = restoredCollectionPerShardCount.values().stream().mapToInt(Number::intValue).sum();\n      int numberNewDocsIndexed = indexDocs(restoreCollectionName, true);\n      Map<String, Integer> restoredCollectionPerShardCountAfterIndexing = getShardToDocCountMap(client, restoreCollection);\n      int restoredCollectionFinalDocCount = restoredCollectionPerShardCountAfterIndexing.values().stream().mapToInt(Number::intValue).sum();\n\n      log.info(\"Original doc count in restored collection:\" + restoredCollectionDocCount + \", number of newly added documents to the restored collection: \" + numberNewDocsIndexed + \", after indexing: \" + restoredCollectionFinalDocCount);\n      assertEquals((restoredCollectionDocCount + numberNewDocsIndexed), restoredCollectionFinalDocCount);\n    }\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","date":1531905561,"type":0,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/api/collections/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String,int).mjava","pathOld":"/dev/null","sourceNew":"  private void testBackupAndRestore(String collectionName, int backupReplFactor) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    int restoreReplcationFactor = replFactor;\n    int restoreTlogReplicas = numTlogReplicas;\n    int restorePullReplicas = numPullReplicas;\n    boolean setExternalReplicationFactor = false;\n    if (random().nextBoolean()) { //Override replicationFactor / tLogReplicas / pullReplicas\n      setExternalReplicationFactor = true;\n      restoreTlogReplicas = TestUtil.nextInt(random(), 0, 1);\n      restoreReplcationFactor = TestUtil.nextInt(random(), 1, 2);\n      restorePullReplicas = TestUtil.nextInt(random(), 0, 1);\n    }\n    int numShards = backupCollection.getActiveSlices().size();\n\n    int restoreReplFactor = restoreReplcationFactor + restoreTlogReplicas + restorePullReplicas;\n\n    boolean isMaxShardsPerNodeExternal = false;\n    boolean isMaxShardsUnlimited = false;\n    CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n        .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n    //explicitly specify the replicationFactor/pullReplicas/nrtReplicas/tlogReplicas.\n    if (setExternalReplicationFactor)  {\n      restore.setReplicationFactor(restoreReplcationFactor);\n      restore.setTlogReplicas(restoreTlogReplicas);\n      restore.setPullReplicas(restorePullReplicas);\n    }\n    int computeRestoreMaxShardsPerNode = (int) Math.ceil((restoreReplFactor * numShards/(double) cluster.getJettySolrRunners().size()));\n\n    if (restoreReplFactor > backupReplFactor) { //else the backup maxShardsPerNode should be enough\n      log.info(\"numShards={} restoreReplFactor={} maxShardsPerNode={} totalNodes={}\",\n          numShards, restoreReplFactor, computeRestoreMaxShardsPerNode, cluster.getJettySolrRunners().size());\n\n      if (random().nextBoolean()) { //set it to -1\n        isMaxShardsUnlimited = true;\n        restore.setMaxShardsPerNode(-1);\n      } else {\n        isMaxShardsPerNodeExternal = true;\n        restore.setMaxShardsPerNode(computeRestoreMaxShardsPerNode);\n      }\n    }\n\n    if (rarely()) { // Try with createNodeSet configuration\n      int nodeSetSize = cluster.getJettySolrRunners().size() / 2;\n      List<String> nodeStrs = new ArrayList<>(nodeSetSize);\n      Iterator<JettySolrRunner> iter = cluster.getJettySolrRunners().iterator();\n      for (int i = 0; i < nodeSetSize ; i++) {\n        nodeStrs.add(iter.next().getNodeName());\n      }\n      restore.setCreateNodeSet(String.join(\",\", nodeStrs));\n      restore.setCreateNodeSetShuffle(usually());\n      // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n      isMaxShardsPerNodeExternal = true;\n      if (restore.getMaxShardsPerNode() != null) {\n        computeRestoreMaxShardsPerNode = restore.getMaxShardsPerNode() * 2;\n        restore.setMaxShardsPerNode(computeRestoreMaxShardsPerNode);\n      } else {\n        computeRestoreMaxShardsPerNode = origShardToDocCount.size() * 2;\n        restore.setMaxShardsPerNode(origShardToDocCount.size() * 2);\n      }\n    }\n\n    final int restoreMaxShardsPerNode = computeRestoreMaxShardsPerNode;\n\n    Properties props = new Properties();\n    props.setProperty(\"customKey\", \"customVal\");\n    restore.setProperties(props);\n\n    if (sameConfig==false) {\n      restore.setConfigName(\"customConfigName\");\n    }\n    if (random().nextBoolean()) {\n      assertEquals(0, restore.process(client).getStatus());\n    } else {\n      assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n    }\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n        restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName, false);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreMaxShardsPerNode\n              + \". state: \\n\" + restoreCollection, v <= restoreMaxShardsPerNode);\n    });\n\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getReplicationFactor().intValue());\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getNumNrtReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restorePullReplicas, restoreCollection.getNumPullReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restoreTlogReplicas, restoreCollection.getNumTlogReplicas().intValue());\n    if (isMaxShardsPerNodeExternal) {\n      assertEquals(restoreCollectionName, restoreMaxShardsPerNode, restoreCollection.getMaxShardsPerNode());\n    } else if (isMaxShardsUnlimited){\n      assertEquals(restoreCollectionName, -1, restoreCollection.getMaxShardsPerNode());\n    } else {\n      assertEquals(restoreCollectionName, backupCollection.getMaxShardsPerNode(), restoreCollection.getMaxShardsPerNode());\n    }\n\n    assertEquals(\"Restore collection should use stateFormat=2\", 2, restoreCollection.getStateFormat());\n\n    //SOLR-12605: Add more docs after restore is complete to see if they are getting added fine\n    //explicitly querying the leaders. If we use CloudSolrClient there is no guarantee that we'll hit a nrtReplica\n    {\n      Map<String, Integer> restoredCollectionPerShardCount =  getShardToDocCountMap(client, restoreCollection);\n      long restoredCollectionDocCount = restoredCollectionPerShardCount.values().stream().mapToInt(Number::intValue).sum();\n      int numberNewDocsIndexed = indexDocs(restoreCollectionName, true);\n      Map<String, Integer> restoredCollectionPerShardCountAfterIndexing = getShardToDocCountMap(client, restoreCollection);\n      int restoredCollectionFinalDocCount = restoredCollectionPerShardCountAfterIndexing.values().stream().mapToInt(Number::intValue).sum();\n\n      log.info(\"Original doc count in restored collection:\" + restoredCollectionDocCount + \", number of newly added documents to the restored collection: \" + numberNewDocsIndexed + \", after indexing: \" + restoredCollectionFinalDocCount);\n      assertEquals((restoredCollectionDocCount + numberNewDocsIndexed), restoredCollectionFinalDocCount);\n    }\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a64d56a8e1a34a05fef090cde8892dd4f00971ff","date":1532203507,"type":3,"author":"Varun Thacker","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/api/collections/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String,int).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/api/collections/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String,int).mjava","sourceNew":"  private void testBackupAndRestore(String collectionName, int backupReplFactor) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    int restoreReplcationFactor = replFactor;\n    int restoreTlogReplicas = numTlogReplicas;\n    int restorePullReplicas = numPullReplicas;\n    boolean setExternalReplicationFactor = false;\n    if (random().nextBoolean()) { //Override replicationFactor / tLogReplicas / pullReplicas\n      setExternalReplicationFactor = true;\n      restoreTlogReplicas = TestUtil.nextInt(random(), 0, 1);\n      restoreReplcationFactor = TestUtil.nextInt(random(), 1, 2);\n      restorePullReplicas = TestUtil.nextInt(random(), 0, 1);\n    }\n    int numShards = backupCollection.getActiveSlices().size();\n\n    int restoreReplFactor = restoreReplcationFactor + restoreTlogReplicas + restorePullReplicas;\n\n    boolean isMaxShardsPerNodeExternal = false;\n    boolean isMaxShardsUnlimited = false;\n    CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n        .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n    //explicitly specify the replicationFactor/pullReplicas/nrtReplicas/tlogReplicas.\n    if (setExternalReplicationFactor)  {\n      restore.setReplicationFactor(restoreReplcationFactor);\n      restore.setTlogReplicas(restoreTlogReplicas);\n      restore.setPullReplicas(restorePullReplicas);\n    }\n    int computeRestoreMaxShardsPerNode = (int) Math.ceil((restoreReplFactor * numShards/(double) cluster.getJettySolrRunners().size()));\n\n    if (restoreReplFactor > backupReplFactor) { //else the backup maxShardsPerNode should be enough\n      log.info(\"numShards={} restoreReplFactor={} maxShardsPerNode={} totalNodes={}\",\n          numShards, restoreReplFactor, computeRestoreMaxShardsPerNode, cluster.getJettySolrRunners().size());\n\n      if (random().nextBoolean()) { //set it to -1\n        isMaxShardsUnlimited = true;\n        restore.setMaxShardsPerNode(-1);\n      } else {\n        isMaxShardsPerNodeExternal = true;\n        restore.setMaxShardsPerNode(computeRestoreMaxShardsPerNode);\n      }\n    }\n\n    if (rarely()) { // Try with createNodeSet configuration\n      List<String> nodeStrs = new ArrayList<>(1);//Always 1 as cluster.getJettySolrRunners().size()=NUM_SHARDS=2\n      Iterator<JettySolrRunner> iter = cluster.getJettySolrRunners().iterator();\n      nodeStrs.add(iter.next().getNodeName());\n      restore.setCreateNodeSet(String.join(\",\", nodeStrs));\n      restore.setCreateNodeSetShuffle(usually());\n      // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n      isMaxShardsPerNodeExternal = true;\n      if (restore.getMaxShardsPerNode() != null) {\n        computeRestoreMaxShardsPerNode = restore.getMaxShardsPerNode() * 2;\n        restore.setMaxShardsPerNode(computeRestoreMaxShardsPerNode);\n      } else {\n        computeRestoreMaxShardsPerNode = origShardToDocCount.size() * backupReplFactor;\n        restore.setMaxShardsPerNode(origShardToDocCount.size() * backupReplFactor);\n      }\n    }\n\n    final int restoreMaxShardsPerNode = computeRestoreMaxShardsPerNode;\n\n    Properties props = new Properties();\n    props.setProperty(\"customKey\", \"customVal\");\n    restore.setProperties(props);\n\n    if (sameConfig==false) {\n      restore.setConfigName(\"customConfigName\");\n    }\n    if (random().nextBoolean()) {\n      assertEquals(0, restore.process(client).getStatus());\n    } else {\n      assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n    }\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n        restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName, false);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreMaxShardsPerNode\n              + \". state: \\n\" + restoreCollection, v <= restoreMaxShardsPerNode);\n    });\n\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getReplicationFactor().intValue());\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getNumNrtReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restorePullReplicas, restoreCollection.getNumPullReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restoreTlogReplicas, restoreCollection.getNumTlogReplicas().intValue());\n    if (isMaxShardsPerNodeExternal) {\n      assertEquals(restoreCollectionName, restoreMaxShardsPerNode, restoreCollection.getMaxShardsPerNode());\n    } else if (isMaxShardsUnlimited){\n      assertEquals(restoreCollectionName, -1, restoreCollection.getMaxShardsPerNode());\n    } else {\n      assertEquals(restoreCollectionName, backupCollection.getMaxShardsPerNode(), restoreCollection.getMaxShardsPerNode());\n    }\n\n    assertEquals(\"Restore collection should use stateFormat=2\", 2, restoreCollection.getStateFormat());\n\n    //SOLR-12605: Add more docs after restore is complete to see if they are getting added fine\n    //explicitly querying the leaders. If we use CloudSolrClient there is no guarantee that we'll hit a nrtReplica\n    {\n      Map<String, Integer> restoredCollectionPerShardCount =  getShardToDocCountMap(client, restoreCollection);\n      long restoredCollectionDocCount = restoredCollectionPerShardCount.values().stream().mapToInt(Number::intValue).sum();\n      int numberNewDocsIndexed = indexDocs(restoreCollectionName, true);\n      Map<String, Integer> restoredCollectionPerShardCountAfterIndexing = getShardToDocCountMap(client, restoreCollection);\n      int restoredCollectionFinalDocCount = restoredCollectionPerShardCountAfterIndexing.values().stream().mapToInt(Number::intValue).sum();\n\n      log.info(\"Original doc count in restored collection:\" + restoredCollectionDocCount + \", number of newly added documents to the restored collection: \" + numberNewDocsIndexed + \", after indexing: \" + restoredCollectionFinalDocCount);\n      assertEquals((restoredCollectionDocCount + numberNewDocsIndexed), restoredCollectionFinalDocCount);\n    }\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","sourceOld":"  private void testBackupAndRestore(String collectionName, int backupReplFactor) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    int restoreReplcationFactor = replFactor;\n    int restoreTlogReplicas = numTlogReplicas;\n    int restorePullReplicas = numPullReplicas;\n    boolean setExternalReplicationFactor = false;\n    if (random().nextBoolean()) { //Override replicationFactor / tLogReplicas / pullReplicas\n      setExternalReplicationFactor = true;\n      restoreTlogReplicas = TestUtil.nextInt(random(), 0, 1);\n      restoreReplcationFactor = TestUtil.nextInt(random(), 1, 2);\n      restorePullReplicas = TestUtil.nextInt(random(), 0, 1);\n    }\n    int numShards = backupCollection.getActiveSlices().size();\n\n    int restoreReplFactor = restoreReplcationFactor + restoreTlogReplicas + restorePullReplicas;\n\n    boolean isMaxShardsPerNodeExternal = false;\n    boolean isMaxShardsUnlimited = false;\n    CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n        .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n    //explicitly specify the replicationFactor/pullReplicas/nrtReplicas/tlogReplicas.\n    if (setExternalReplicationFactor)  {\n      restore.setReplicationFactor(restoreReplcationFactor);\n      restore.setTlogReplicas(restoreTlogReplicas);\n      restore.setPullReplicas(restorePullReplicas);\n    }\n    int computeRestoreMaxShardsPerNode = (int) Math.ceil((restoreReplFactor * numShards/(double) cluster.getJettySolrRunners().size()));\n\n    if (restoreReplFactor > backupReplFactor) { //else the backup maxShardsPerNode should be enough\n      log.info(\"numShards={} restoreReplFactor={} maxShardsPerNode={} totalNodes={}\",\n          numShards, restoreReplFactor, computeRestoreMaxShardsPerNode, cluster.getJettySolrRunners().size());\n\n      if (random().nextBoolean()) { //set it to -1\n        isMaxShardsUnlimited = true;\n        restore.setMaxShardsPerNode(-1);\n      } else {\n        isMaxShardsPerNodeExternal = true;\n        restore.setMaxShardsPerNode(computeRestoreMaxShardsPerNode);\n      }\n    }\n\n    if (rarely()) { // Try with createNodeSet configuration\n      int nodeSetSize = cluster.getJettySolrRunners().size() / 2;\n      List<String> nodeStrs = new ArrayList<>(nodeSetSize);\n      Iterator<JettySolrRunner> iter = cluster.getJettySolrRunners().iterator();\n      for (int i = 0; i < nodeSetSize ; i++) {\n        nodeStrs.add(iter.next().getNodeName());\n      }\n      restore.setCreateNodeSet(String.join(\",\", nodeStrs));\n      restore.setCreateNodeSetShuffle(usually());\n      // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n      isMaxShardsPerNodeExternal = true;\n      if (restore.getMaxShardsPerNode() != null) {\n        computeRestoreMaxShardsPerNode = restore.getMaxShardsPerNode() * 2;\n        restore.setMaxShardsPerNode(computeRestoreMaxShardsPerNode);\n      } else {\n        computeRestoreMaxShardsPerNode = origShardToDocCount.size() * 2;\n        restore.setMaxShardsPerNode(origShardToDocCount.size() * 2);\n      }\n    }\n\n    final int restoreMaxShardsPerNode = computeRestoreMaxShardsPerNode;\n\n    Properties props = new Properties();\n    props.setProperty(\"customKey\", \"customVal\");\n    restore.setProperties(props);\n\n    if (sameConfig==false) {\n      restore.setConfigName(\"customConfigName\");\n    }\n    if (random().nextBoolean()) {\n      assertEquals(0, restore.process(client).getStatus());\n    } else {\n      assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n    }\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n        restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName, false);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreMaxShardsPerNode\n              + \". state: \\n\" + restoreCollection, v <= restoreMaxShardsPerNode);\n    });\n\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getReplicationFactor().intValue());\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getNumNrtReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restorePullReplicas, restoreCollection.getNumPullReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restoreTlogReplicas, restoreCollection.getNumTlogReplicas().intValue());\n    if (isMaxShardsPerNodeExternal) {\n      assertEquals(restoreCollectionName, restoreMaxShardsPerNode, restoreCollection.getMaxShardsPerNode());\n    } else if (isMaxShardsUnlimited){\n      assertEquals(restoreCollectionName, -1, restoreCollection.getMaxShardsPerNode());\n    } else {\n      assertEquals(restoreCollectionName, backupCollection.getMaxShardsPerNode(), restoreCollection.getMaxShardsPerNode());\n    }\n\n    assertEquals(\"Restore collection should use stateFormat=2\", 2, restoreCollection.getStateFormat());\n\n    //SOLR-12605: Add more docs after restore is complete to see if they are getting added fine\n    //explicitly querying the leaders. If we use CloudSolrClient there is no guarantee that we'll hit a nrtReplica\n    {\n      Map<String, Integer> restoredCollectionPerShardCount =  getShardToDocCountMap(client, restoreCollection);\n      long restoredCollectionDocCount = restoredCollectionPerShardCount.values().stream().mapToInt(Number::intValue).sum();\n      int numberNewDocsIndexed = indexDocs(restoreCollectionName, true);\n      Map<String, Integer> restoredCollectionPerShardCountAfterIndexing = getShardToDocCountMap(client, restoreCollection);\n      int restoredCollectionFinalDocCount = restoredCollectionPerShardCountAfterIndexing.values().stream().mapToInt(Number::intValue).sum();\n\n      log.info(\"Original doc count in restored collection:\" + restoredCollectionDocCount + \", number of newly added documents to the restored collection: \" + numberNewDocsIndexed + \", after indexing: \" + restoredCollectionFinalDocCount);\n      assertEquals((restoredCollectionDocCount + numberNewDocsIndexed), restoredCollectionFinalDocCount);\n    }\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","bugFix":null,"bugIntro":["cb9804c0e92c14de7239ee3a6649fe4e5f9e3116"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"cb9804c0e92c14de7239ee3a6649fe4e5f9e3116","date":1533075563,"type":3,"author":"Varun Thacker","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/api/collections/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String,int).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/api/collections/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String,int).mjava","sourceNew":"  private void testBackupAndRestore(String collectionName, int backupReplFactor) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    int restoreReplcationFactor = replFactor;\n    int restoreTlogReplicas = numTlogReplicas;\n    int restorePullReplicas = numPullReplicas;\n    boolean setExternalReplicationFactor = false;\n    if (random().nextBoolean()) { //Override replicationFactor / tLogReplicas / pullReplicas\n      setExternalReplicationFactor = true;\n      restoreTlogReplicas = TestUtil.nextInt(random(), 0, 1);\n      restoreReplcationFactor = TestUtil.nextInt(random(), 1, 2);\n      restorePullReplicas = TestUtil.nextInt(random(), 0, 1);\n    }\n    int numShards = backupCollection.getActiveSlices().size();\n\n    int restoreReplFactor = restoreReplcationFactor + restoreTlogReplicas + restorePullReplicas;\n\n    boolean isMaxShardsPerNodeExternal = false;\n    boolean isMaxShardsUnlimited = false;\n    CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n        .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n    //explicitly specify the replicationFactor/pullReplicas/nrtReplicas/tlogReplicas.\n    if (setExternalReplicationFactor)  {\n      restore.setReplicationFactor(restoreReplcationFactor);\n      restore.setTlogReplicas(restoreTlogReplicas);\n      restore.setPullReplicas(restorePullReplicas);\n    }\n    int computeRestoreMaxShardsPerNode = (int) Math.ceil((restoreReplFactor * numShards/(double) cluster.getJettySolrRunners().size()));\n\n    if (restoreReplFactor > backupReplFactor) { //else the backup maxShardsPerNode should be enough\n      log.info(\"numShards={} restoreReplFactor={} maxShardsPerNode={} totalNodes={}\",\n          numShards, restoreReplFactor, computeRestoreMaxShardsPerNode, cluster.getJettySolrRunners().size());\n\n      if (random().nextBoolean()) { //set it to -1\n        isMaxShardsUnlimited = true;\n        restore.setMaxShardsPerNode(-1);\n      } else {\n        isMaxShardsPerNodeExternal = true;\n        restore.setMaxShardsPerNode(computeRestoreMaxShardsPerNode);\n      }\n    }\n\n    if (rarely()) { // Try with createNodeSet configuration\n      //Always 1 as cluster.getJettySolrRunners().size()=NUM_SHARDS=2\n      restore.setCreateNodeSet(cluster.getJettySolrRunners().get(0).getNodeName());\n      // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n      isMaxShardsPerNodeExternal = true;\n      computeRestoreMaxShardsPerNode = origShardToDocCount.size() * restoreReplFactor;\n      restore.setMaxShardsPerNode(computeRestoreMaxShardsPerNode);\n    }\n\n    final int restoreMaxShardsPerNode = computeRestoreMaxShardsPerNode;\n\n    Properties props = new Properties();\n    props.setProperty(\"customKey\", \"customVal\");\n    restore.setProperties(props);\n\n    if (sameConfig==false) {\n      restore.setConfigName(\"customConfigName\");\n    }\n    if (random().nextBoolean()) {\n      assertEquals(0, restore.process(client).getStatus());\n    } else {\n      assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n    }\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n        restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName, false);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreMaxShardsPerNode\n              + \". state: \\n\" + restoreCollection, v <= restoreMaxShardsPerNode);\n    });\n\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getReplicationFactor().intValue());\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getNumNrtReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restorePullReplicas, restoreCollection.getNumPullReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restoreTlogReplicas, restoreCollection.getNumTlogReplicas().intValue());\n    if (isMaxShardsPerNodeExternal) {\n      assertEquals(restoreCollectionName, restoreMaxShardsPerNode, restoreCollection.getMaxShardsPerNode());\n    } else if (isMaxShardsUnlimited){\n      assertEquals(restoreCollectionName, -1, restoreCollection.getMaxShardsPerNode());\n    } else {\n      assertEquals(restoreCollectionName, backupCollection.getMaxShardsPerNode(), restoreCollection.getMaxShardsPerNode());\n    }\n\n    assertEquals(\"Restore collection should use stateFormat=2\", 2, restoreCollection.getStateFormat());\n\n    //SOLR-12605: Add more docs after restore is complete to see if they are getting added fine\n    //explicitly querying the leaders. If we use CloudSolrClient there is no guarantee that we'll hit a nrtReplica\n    {\n      Map<String, Integer> restoredCollectionPerShardCount =  getShardToDocCountMap(client, restoreCollection);\n      long restoredCollectionDocCount = restoredCollectionPerShardCount.values().stream().mapToInt(Number::intValue).sum();\n      int numberNewDocsIndexed = indexDocs(restoreCollectionName, true);\n      Map<String, Integer> restoredCollectionPerShardCountAfterIndexing = getShardToDocCountMap(client, restoreCollection);\n      int restoredCollectionFinalDocCount = restoredCollectionPerShardCountAfterIndexing.values().stream().mapToInt(Number::intValue).sum();\n\n      log.info(\"Original doc count in restored collection:\" + restoredCollectionDocCount + \", number of newly added documents to the restored collection: \" + numberNewDocsIndexed + \", after indexing: \" + restoredCollectionFinalDocCount);\n      assertEquals((restoredCollectionDocCount + numberNewDocsIndexed), restoredCollectionFinalDocCount);\n    }\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","sourceOld":"  private void testBackupAndRestore(String collectionName, int backupReplFactor) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    int restoreReplcationFactor = replFactor;\n    int restoreTlogReplicas = numTlogReplicas;\n    int restorePullReplicas = numPullReplicas;\n    boolean setExternalReplicationFactor = false;\n    if (random().nextBoolean()) { //Override replicationFactor / tLogReplicas / pullReplicas\n      setExternalReplicationFactor = true;\n      restoreTlogReplicas = TestUtil.nextInt(random(), 0, 1);\n      restoreReplcationFactor = TestUtil.nextInt(random(), 1, 2);\n      restorePullReplicas = TestUtil.nextInt(random(), 0, 1);\n    }\n    int numShards = backupCollection.getActiveSlices().size();\n\n    int restoreReplFactor = restoreReplcationFactor + restoreTlogReplicas + restorePullReplicas;\n\n    boolean isMaxShardsPerNodeExternal = false;\n    boolean isMaxShardsUnlimited = false;\n    CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n        .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n    //explicitly specify the replicationFactor/pullReplicas/nrtReplicas/tlogReplicas.\n    if (setExternalReplicationFactor)  {\n      restore.setReplicationFactor(restoreReplcationFactor);\n      restore.setTlogReplicas(restoreTlogReplicas);\n      restore.setPullReplicas(restorePullReplicas);\n    }\n    int computeRestoreMaxShardsPerNode = (int) Math.ceil((restoreReplFactor * numShards/(double) cluster.getJettySolrRunners().size()));\n\n    if (restoreReplFactor > backupReplFactor) { //else the backup maxShardsPerNode should be enough\n      log.info(\"numShards={} restoreReplFactor={} maxShardsPerNode={} totalNodes={}\",\n          numShards, restoreReplFactor, computeRestoreMaxShardsPerNode, cluster.getJettySolrRunners().size());\n\n      if (random().nextBoolean()) { //set it to -1\n        isMaxShardsUnlimited = true;\n        restore.setMaxShardsPerNode(-1);\n      } else {\n        isMaxShardsPerNodeExternal = true;\n        restore.setMaxShardsPerNode(computeRestoreMaxShardsPerNode);\n      }\n    }\n\n    if (rarely()) { // Try with createNodeSet configuration\n      List<String> nodeStrs = new ArrayList<>(1);//Always 1 as cluster.getJettySolrRunners().size()=NUM_SHARDS=2\n      Iterator<JettySolrRunner> iter = cluster.getJettySolrRunners().iterator();\n      nodeStrs.add(iter.next().getNodeName());\n      restore.setCreateNodeSet(String.join(\",\", nodeStrs));\n      restore.setCreateNodeSetShuffle(usually());\n      // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n      isMaxShardsPerNodeExternal = true;\n      if (restore.getMaxShardsPerNode() != null) {\n        computeRestoreMaxShardsPerNode = restore.getMaxShardsPerNode() * 2;\n        restore.setMaxShardsPerNode(computeRestoreMaxShardsPerNode);\n      } else {\n        computeRestoreMaxShardsPerNode = origShardToDocCount.size() * backupReplFactor;\n        restore.setMaxShardsPerNode(origShardToDocCount.size() * backupReplFactor);\n      }\n    }\n\n    final int restoreMaxShardsPerNode = computeRestoreMaxShardsPerNode;\n\n    Properties props = new Properties();\n    props.setProperty(\"customKey\", \"customVal\");\n    restore.setProperties(props);\n\n    if (sameConfig==false) {\n      restore.setConfigName(\"customConfigName\");\n    }\n    if (random().nextBoolean()) {\n      assertEquals(0, restore.process(client).getStatus());\n    } else {\n      assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n    }\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n        restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName, false);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreMaxShardsPerNode\n              + \". state: \\n\" + restoreCollection, v <= restoreMaxShardsPerNode);\n    });\n\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getReplicationFactor().intValue());\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getNumNrtReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restorePullReplicas, restoreCollection.getNumPullReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restoreTlogReplicas, restoreCollection.getNumTlogReplicas().intValue());\n    if (isMaxShardsPerNodeExternal) {\n      assertEquals(restoreCollectionName, restoreMaxShardsPerNode, restoreCollection.getMaxShardsPerNode());\n    } else if (isMaxShardsUnlimited){\n      assertEquals(restoreCollectionName, -1, restoreCollection.getMaxShardsPerNode());\n    } else {\n      assertEquals(restoreCollectionName, backupCollection.getMaxShardsPerNode(), restoreCollection.getMaxShardsPerNode());\n    }\n\n    assertEquals(\"Restore collection should use stateFormat=2\", 2, restoreCollection.getStateFormat());\n\n    //SOLR-12605: Add more docs after restore is complete to see if they are getting added fine\n    //explicitly querying the leaders. If we use CloudSolrClient there is no guarantee that we'll hit a nrtReplica\n    {\n      Map<String, Integer> restoredCollectionPerShardCount =  getShardToDocCountMap(client, restoreCollection);\n      long restoredCollectionDocCount = restoredCollectionPerShardCount.values().stream().mapToInt(Number::intValue).sum();\n      int numberNewDocsIndexed = indexDocs(restoreCollectionName, true);\n      Map<String, Integer> restoredCollectionPerShardCountAfterIndexing = getShardToDocCountMap(client, restoreCollection);\n      int restoredCollectionFinalDocCount = restoredCollectionPerShardCountAfterIndexing.values().stream().mapToInt(Number::intValue).sum();\n\n      log.info(\"Original doc count in restored collection:\" + restoredCollectionDocCount + \", number of newly added documents to the restored collection: \" + numberNewDocsIndexed + \", after indexing: \" + restoredCollectionFinalDocCount);\n      assertEquals((restoredCollectionDocCount + numberNewDocsIndexed), restoredCollectionFinalDocCount);\n    }\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","bugFix":["a64d56a8e1a34a05fef090cde8892dd4f00971ff","d516662650185b8ef035b3933dca702cad9ca2c9","63ae38ccbfadf7f763ed165694f6ae139e167f09"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ab6131420a270c49b653c969cc1dbbaf7d1b36e7","date":1550697886,"type":3,"author":"Tomas Fernandez Lobbe","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/api/collections/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String,int).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/api/collections/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String,int).mjava","sourceNew":"  private void testBackupAndRestore(String collectionName, int backupReplFactor) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = BACKUPNAME_PREFIX + testSuffix;\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    int restoreReplcationFactor = replFactor;\n    int restoreTlogReplicas = numTlogReplicas;\n    int restorePullReplicas = numPullReplicas;\n    boolean setExternalReplicationFactor = false;\n    if (random().nextBoolean()) { //Override replicationFactor / tLogReplicas / pullReplicas\n      setExternalReplicationFactor = true;\n      restoreTlogReplicas = TestUtil.nextInt(random(), 0, 1);\n      restoreReplcationFactor = TestUtil.nextInt(random(), 1, 2);\n      restorePullReplicas = TestUtil.nextInt(random(), 0, 1);\n    }\n    int numShards = backupCollection.getActiveSlices().size();\n\n    int restoreReplFactor = restoreReplcationFactor + restoreTlogReplicas + restorePullReplicas;\n\n    boolean isMaxShardsPerNodeExternal = false;\n    boolean isMaxShardsUnlimited = false;\n    CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n        .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n    //explicitly specify the replicationFactor/pullReplicas/nrtReplicas/tlogReplicas.\n    if (setExternalReplicationFactor)  {\n      restore.setReplicationFactor(restoreReplcationFactor);\n      restore.setTlogReplicas(restoreTlogReplicas);\n      restore.setPullReplicas(restorePullReplicas);\n    }\n    int computeRestoreMaxShardsPerNode = (int) Math.ceil((restoreReplFactor * numShards/(double) cluster.getJettySolrRunners().size()));\n\n    if (restoreReplFactor > backupReplFactor) { //else the backup maxShardsPerNode should be enough\n      log.info(\"numShards={} restoreReplFactor={} maxShardsPerNode={} totalNodes={}\",\n          numShards, restoreReplFactor, computeRestoreMaxShardsPerNode, cluster.getJettySolrRunners().size());\n\n      if (random().nextBoolean()) { //set it to -1\n        isMaxShardsUnlimited = true;\n        restore.setMaxShardsPerNode(-1);\n      } else {\n        isMaxShardsPerNodeExternal = true;\n        restore.setMaxShardsPerNode(computeRestoreMaxShardsPerNode);\n      }\n    }\n\n    if (rarely()) { // Try with createNodeSet configuration\n      //Always 1 as cluster.getJettySolrRunners().size()=NUM_SHARDS=2\n      restore.setCreateNodeSet(cluster.getJettySolrRunners().get(0).getNodeName());\n      // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n      isMaxShardsPerNodeExternal = true;\n      computeRestoreMaxShardsPerNode = origShardToDocCount.size() * restoreReplFactor;\n      restore.setMaxShardsPerNode(computeRestoreMaxShardsPerNode);\n    }\n\n    final int restoreMaxShardsPerNode = computeRestoreMaxShardsPerNode;\n\n    Properties props = new Properties();\n    props.setProperty(\"customKey\", \"customVal\");\n    restore.setProperties(props);\n\n    if (sameConfig==false) {\n      restore.setConfigName(\"customConfigName\");\n    }\n    if (random().nextBoolean()) {\n      assertEquals(0, restore.process(client).getStatus());\n    } else {\n      assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 60));//async\n    }\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n        restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName, false);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreMaxShardsPerNode\n              + \". state: \\n\" + restoreCollection, v <= restoreMaxShardsPerNode);\n    });\n\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getReplicationFactor().intValue());\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getNumNrtReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restorePullReplicas, restoreCollection.getNumPullReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restoreTlogReplicas, restoreCollection.getNumTlogReplicas().intValue());\n    if (isMaxShardsPerNodeExternal) {\n      assertEquals(restoreCollectionName, restoreMaxShardsPerNode, restoreCollection.getMaxShardsPerNode());\n    } else if (isMaxShardsUnlimited){\n      assertEquals(restoreCollectionName, -1, restoreCollection.getMaxShardsPerNode());\n    } else {\n      assertEquals(restoreCollectionName, backupCollection.getMaxShardsPerNode(), restoreCollection.getMaxShardsPerNode());\n    }\n\n    assertEquals(\"Restore collection should use stateFormat=2\", 2, restoreCollection.getStateFormat());\n\n    //SOLR-12605: Add more docs after restore is complete to see if they are getting added fine\n    //explicitly querying the leaders. If we use CloudSolrClient there is no guarantee that we'll hit a nrtReplica\n    {\n      Map<String, Integer> restoredCollectionPerShardCount =  getShardToDocCountMap(client, restoreCollection);\n      long restoredCollectionDocCount = restoredCollectionPerShardCount.values().stream().mapToInt(Number::intValue).sum();\n      int numberNewDocsIndexed = indexDocs(restoreCollectionName, true);\n      Map<String, Integer> restoredCollectionPerShardCountAfterIndexing = getShardToDocCountMap(client, restoreCollection);\n      int restoredCollectionFinalDocCount = restoredCollectionPerShardCountAfterIndexing.values().stream().mapToInt(Number::intValue).sum();\n\n      log.info(\"Original doc count in restored collection:\" + restoredCollectionDocCount + \", number of newly added documents to the restored collection: \" + numberNewDocsIndexed + \", after indexing: \" + restoredCollectionFinalDocCount);\n      assertEquals((restoredCollectionDocCount + numberNewDocsIndexed), restoredCollectionFinalDocCount);\n    }\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","sourceOld":"  private void testBackupAndRestore(String collectionName, int backupReplFactor) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    int restoreReplcationFactor = replFactor;\n    int restoreTlogReplicas = numTlogReplicas;\n    int restorePullReplicas = numPullReplicas;\n    boolean setExternalReplicationFactor = false;\n    if (random().nextBoolean()) { //Override replicationFactor / tLogReplicas / pullReplicas\n      setExternalReplicationFactor = true;\n      restoreTlogReplicas = TestUtil.nextInt(random(), 0, 1);\n      restoreReplcationFactor = TestUtil.nextInt(random(), 1, 2);\n      restorePullReplicas = TestUtil.nextInt(random(), 0, 1);\n    }\n    int numShards = backupCollection.getActiveSlices().size();\n\n    int restoreReplFactor = restoreReplcationFactor + restoreTlogReplicas + restorePullReplicas;\n\n    boolean isMaxShardsPerNodeExternal = false;\n    boolean isMaxShardsUnlimited = false;\n    CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n        .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n    //explicitly specify the replicationFactor/pullReplicas/nrtReplicas/tlogReplicas.\n    if (setExternalReplicationFactor)  {\n      restore.setReplicationFactor(restoreReplcationFactor);\n      restore.setTlogReplicas(restoreTlogReplicas);\n      restore.setPullReplicas(restorePullReplicas);\n    }\n    int computeRestoreMaxShardsPerNode = (int) Math.ceil((restoreReplFactor * numShards/(double) cluster.getJettySolrRunners().size()));\n\n    if (restoreReplFactor > backupReplFactor) { //else the backup maxShardsPerNode should be enough\n      log.info(\"numShards={} restoreReplFactor={} maxShardsPerNode={} totalNodes={}\",\n          numShards, restoreReplFactor, computeRestoreMaxShardsPerNode, cluster.getJettySolrRunners().size());\n\n      if (random().nextBoolean()) { //set it to -1\n        isMaxShardsUnlimited = true;\n        restore.setMaxShardsPerNode(-1);\n      } else {\n        isMaxShardsPerNodeExternal = true;\n        restore.setMaxShardsPerNode(computeRestoreMaxShardsPerNode);\n      }\n    }\n\n    if (rarely()) { // Try with createNodeSet configuration\n      //Always 1 as cluster.getJettySolrRunners().size()=NUM_SHARDS=2\n      restore.setCreateNodeSet(cluster.getJettySolrRunners().get(0).getNodeName());\n      // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n      isMaxShardsPerNodeExternal = true;\n      computeRestoreMaxShardsPerNode = origShardToDocCount.size() * restoreReplFactor;\n      restore.setMaxShardsPerNode(computeRestoreMaxShardsPerNode);\n    }\n\n    final int restoreMaxShardsPerNode = computeRestoreMaxShardsPerNode;\n\n    Properties props = new Properties();\n    props.setProperty(\"customKey\", \"customVal\");\n    restore.setProperties(props);\n\n    if (sameConfig==false) {\n      restore.setConfigName(\"customConfigName\");\n    }\n    if (random().nextBoolean()) {\n      assertEquals(0, restore.process(client).getStatus());\n    } else {\n      assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n    }\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n        restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName, false);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreMaxShardsPerNode\n              + \". state: \\n\" + restoreCollection, v <= restoreMaxShardsPerNode);\n    });\n\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getReplicationFactor().intValue());\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getNumNrtReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restorePullReplicas, restoreCollection.getNumPullReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restoreTlogReplicas, restoreCollection.getNumTlogReplicas().intValue());\n    if (isMaxShardsPerNodeExternal) {\n      assertEquals(restoreCollectionName, restoreMaxShardsPerNode, restoreCollection.getMaxShardsPerNode());\n    } else if (isMaxShardsUnlimited){\n      assertEquals(restoreCollectionName, -1, restoreCollection.getMaxShardsPerNode());\n    } else {\n      assertEquals(restoreCollectionName, backupCollection.getMaxShardsPerNode(), restoreCollection.getMaxShardsPerNode());\n    }\n\n    assertEquals(\"Restore collection should use stateFormat=2\", 2, restoreCollection.getStateFormat());\n\n    //SOLR-12605: Add more docs after restore is complete to see if they are getting added fine\n    //explicitly querying the leaders. If we use CloudSolrClient there is no guarantee that we'll hit a nrtReplica\n    {\n      Map<String, Integer> restoredCollectionPerShardCount =  getShardToDocCountMap(client, restoreCollection);\n      long restoredCollectionDocCount = restoredCollectionPerShardCount.values().stream().mapToInt(Number::intValue).sum();\n      int numberNewDocsIndexed = indexDocs(restoreCollectionName, true);\n      Map<String, Integer> restoredCollectionPerShardCountAfterIndexing = getShardToDocCountMap(client, restoreCollection);\n      int restoredCollectionFinalDocCount = restoredCollectionPerShardCountAfterIndexing.values().stream().mapToInt(Number::intValue).sum();\n\n      log.info(\"Original doc count in restored collection:\" + restoredCollectionDocCount + \", number of newly added documents to the restored collection: \" + numberNewDocsIndexed + \", after indexing: \" + restoredCollectionFinalDocCount);\n      assertEquals((restoredCollectionDocCount + numberNewDocsIndexed), restoredCollectionFinalDocCount);\n    }\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","bugFix":["63ae38ccbfadf7f763ed165694f6ae139e167f09","c5c99ad021f3da085fcb66220598a8f91dc5e453"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fe9f2c4a0d7ac164e4bdd4eee7f87131aec83fd4","date":1588172214,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/api/collections/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String,int).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/api/collections/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String,int).mjava","sourceNew":"  private void testBackupAndRestore(String collectionName, int backupReplFactor) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = BACKUPNAME_PREFIX + testSuffix;\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    int restoreReplcationFactor = replFactor;\n    int restoreTlogReplicas = numTlogReplicas;\n    int restorePullReplicas = numPullReplicas;\n    boolean setExternalReplicationFactor = false;\n    if (random().nextBoolean()) { //Override replicationFactor / tLogReplicas / pullReplicas\n      setExternalReplicationFactor = true;\n      restoreTlogReplicas = TestUtil.nextInt(random(), 0, 1);\n      restoreReplcationFactor = TestUtil.nextInt(random(), 1, 2);\n      restorePullReplicas = TestUtil.nextInt(random(), 0, 1);\n    }\n    int numShards = backupCollection.getActiveSlices().size();\n\n    int restoreReplFactor = restoreReplcationFactor + restoreTlogReplicas + restorePullReplicas;\n\n    boolean isMaxShardsPerNodeExternal = false;\n    boolean isMaxShardsUnlimited = false;\n    CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n        .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n    //explicitly specify the replicationFactor/pullReplicas/nrtReplicas/tlogReplicas.\n    if (setExternalReplicationFactor)  {\n      restore.setReplicationFactor(restoreReplcationFactor);\n      restore.setTlogReplicas(restoreTlogReplicas);\n      restore.setPullReplicas(restorePullReplicas);\n    }\n    int computeRestoreMaxShardsPerNode = (int) Math.ceil((restoreReplFactor * numShards/(double) cluster.getJettySolrRunners().size()));\n\n    if (restoreReplFactor > backupReplFactor) { //else the backup maxShardsPerNode should be enough\n      if (log.isInfoEnabled()) {\n        log.info(\"numShards={} restoreReplFactor={} maxShardsPerNode={} totalNodes={}\",\n            numShards, restoreReplFactor, computeRestoreMaxShardsPerNode, cluster.getJettySolrRunners().size());\n      }\n\n      if (random().nextBoolean()) { //set it to -1\n        isMaxShardsUnlimited = true;\n        restore.setMaxShardsPerNode(-1);\n      } else {\n        isMaxShardsPerNodeExternal = true;\n        restore.setMaxShardsPerNode(computeRestoreMaxShardsPerNode);\n      }\n    }\n\n    if (rarely()) { // Try with createNodeSet configuration\n      //Always 1 as cluster.getJettySolrRunners().size()=NUM_SHARDS=2\n      restore.setCreateNodeSet(cluster.getJettySolrRunners().get(0).getNodeName());\n      // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n      isMaxShardsPerNodeExternal = true;\n      computeRestoreMaxShardsPerNode = origShardToDocCount.size() * restoreReplFactor;\n      restore.setMaxShardsPerNode(computeRestoreMaxShardsPerNode);\n    }\n\n    final int restoreMaxShardsPerNode = computeRestoreMaxShardsPerNode;\n\n    Properties props = new Properties();\n    props.setProperty(\"customKey\", \"customVal\");\n    restore.setProperties(props);\n\n    if (sameConfig==false) {\n      restore.setConfigName(\"customConfigName\");\n    }\n    if (random().nextBoolean()) {\n      assertEquals(0, restore.process(client).getStatus());\n    } else {\n      assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 60));//async\n    }\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n        restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName, false);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreMaxShardsPerNode\n              + \". state: \\n\" + restoreCollection, v <= restoreMaxShardsPerNode);\n    });\n\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getReplicationFactor().intValue());\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getNumNrtReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restorePullReplicas, restoreCollection.getNumPullReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restoreTlogReplicas, restoreCollection.getNumTlogReplicas().intValue());\n    if (isMaxShardsPerNodeExternal) {\n      assertEquals(restoreCollectionName, restoreMaxShardsPerNode, restoreCollection.getMaxShardsPerNode());\n    } else if (isMaxShardsUnlimited){\n      assertEquals(restoreCollectionName, -1, restoreCollection.getMaxShardsPerNode());\n    } else {\n      assertEquals(restoreCollectionName, backupCollection.getMaxShardsPerNode(), restoreCollection.getMaxShardsPerNode());\n    }\n\n    assertEquals(\"Restore collection should use stateFormat=2\", 2, restoreCollection.getStateFormat());\n\n    //SOLR-12605: Add more docs after restore is complete to see if they are getting added fine\n    //explicitly querying the leaders. If we use CloudSolrClient there is no guarantee that we'll hit a nrtReplica\n    {\n      Map<String, Integer> restoredCollectionPerShardCount =  getShardToDocCountMap(client, restoreCollection);\n      long restoredCollectionDocCount = restoredCollectionPerShardCount.values().stream().mapToInt(Number::intValue).sum();\n      int numberNewDocsIndexed = indexDocs(restoreCollectionName, true);\n      Map<String, Integer> restoredCollectionPerShardCountAfterIndexing = getShardToDocCountMap(client, restoreCollection);\n      int restoredCollectionFinalDocCount = restoredCollectionPerShardCountAfterIndexing.values().stream().mapToInt(Number::intValue).sum();\n\n      log.info(\"Original doc count in restored collection:{} , number of newly added documents to the restored collection: {}\"\n          + \", after indexing: {}\"\n          , restoredCollectionDocCount, numberNewDocsIndexed, restoredCollectionFinalDocCount);\n      assertEquals((restoredCollectionDocCount + numberNewDocsIndexed), restoredCollectionFinalDocCount);\n    }\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","sourceOld":"  private void testBackupAndRestore(String collectionName, int backupReplFactor) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = BACKUPNAME_PREFIX + testSuffix;\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    int restoreReplcationFactor = replFactor;\n    int restoreTlogReplicas = numTlogReplicas;\n    int restorePullReplicas = numPullReplicas;\n    boolean setExternalReplicationFactor = false;\n    if (random().nextBoolean()) { //Override replicationFactor / tLogReplicas / pullReplicas\n      setExternalReplicationFactor = true;\n      restoreTlogReplicas = TestUtil.nextInt(random(), 0, 1);\n      restoreReplcationFactor = TestUtil.nextInt(random(), 1, 2);\n      restorePullReplicas = TestUtil.nextInt(random(), 0, 1);\n    }\n    int numShards = backupCollection.getActiveSlices().size();\n\n    int restoreReplFactor = restoreReplcationFactor + restoreTlogReplicas + restorePullReplicas;\n\n    boolean isMaxShardsPerNodeExternal = false;\n    boolean isMaxShardsUnlimited = false;\n    CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n        .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n    //explicitly specify the replicationFactor/pullReplicas/nrtReplicas/tlogReplicas.\n    if (setExternalReplicationFactor)  {\n      restore.setReplicationFactor(restoreReplcationFactor);\n      restore.setTlogReplicas(restoreTlogReplicas);\n      restore.setPullReplicas(restorePullReplicas);\n    }\n    int computeRestoreMaxShardsPerNode = (int) Math.ceil((restoreReplFactor * numShards/(double) cluster.getJettySolrRunners().size()));\n\n    if (restoreReplFactor > backupReplFactor) { //else the backup maxShardsPerNode should be enough\n      log.info(\"numShards={} restoreReplFactor={} maxShardsPerNode={} totalNodes={}\",\n          numShards, restoreReplFactor, computeRestoreMaxShardsPerNode, cluster.getJettySolrRunners().size());\n\n      if (random().nextBoolean()) { //set it to -1\n        isMaxShardsUnlimited = true;\n        restore.setMaxShardsPerNode(-1);\n      } else {\n        isMaxShardsPerNodeExternal = true;\n        restore.setMaxShardsPerNode(computeRestoreMaxShardsPerNode);\n      }\n    }\n\n    if (rarely()) { // Try with createNodeSet configuration\n      //Always 1 as cluster.getJettySolrRunners().size()=NUM_SHARDS=2\n      restore.setCreateNodeSet(cluster.getJettySolrRunners().get(0).getNodeName());\n      // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n      isMaxShardsPerNodeExternal = true;\n      computeRestoreMaxShardsPerNode = origShardToDocCount.size() * restoreReplFactor;\n      restore.setMaxShardsPerNode(computeRestoreMaxShardsPerNode);\n    }\n\n    final int restoreMaxShardsPerNode = computeRestoreMaxShardsPerNode;\n\n    Properties props = new Properties();\n    props.setProperty(\"customKey\", \"customVal\");\n    restore.setProperties(props);\n\n    if (sameConfig==false) {\n      restore.setConfigName(\"customConfigName\");\n    }\n    if (random().nextBoolean()) {\n      assertEquals(0, restore.process(client).getStatus());\n    } else {\n      assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 60));//async\n    }\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n        restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName, false);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreMaxShardsPerNode\n              + \". state: \\n\" + restoreCollection, v <= restoreMaxShardsPerNode);\n    });\n\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getReplicationFactor().intValue());\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getNumNrtReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restorePullReplicas, restoreCollection.getNumPullReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restoreTlogReplicas, restoreCollection.getNumTlogReplicas().intValue());\n    if (isMaxShardsPerNodeExternal) {\n      assertEquals(restoreCollectionName, restoreMaxShardsPerNode, restoreCollection.getMaxShardsPerNode());\n    } else if (isMaxShardsUnlimited){\n      assertEquals(restoreCollectionName, -1, restoreCollection.getMaxShardsPerNode());\n    } else {\n      assertEquals(restoreCollectionName, backupCollection.getMaxShardsPerNode(), restoreCollection.getMaxShardsPerNode());\n    }\n\n    assertEquals(\"Restore collection should use stateFormat=2\", 2, restoreCollection.getStateFormat());\n\n    //SOLR-12605: Add more docs after restore is complete to see if they are getting added fine\n    //explicitly querying the leaders. If we use CloudSolrClient there is no guarantee that we'll hit a nrtReplica\n    {\n      Map<String, Integer> restoredCollectionPerShardCount =  getShardToDocCountMap(client, restoreCollection);\n      long restoredCollectionDocCount = restoredCollectionPerShardCount.values().stream().mapToInt(Number::intValue).sum();\n      int numberNewDocsIndexed = indexDocs(restoreCollectionName, true);\n      Map<String, Integer> restoredCollectionPerShardCountAfterIndexing = getShardToDocCountMap(client, restoreCollection);\n      int restoredCollectionFinalDocCount = restoredCollectionPerShardCountAfterIndexing.values().stream().mapToInt(Number::intValue).sum();\n\n      log.info(\"Original doc count in restored collection:\" + restoredCollectionDocCount + \", number of newly added documents to the restored collection: \" + numberNewDocsIndexed + \", after indexing: \" + restoredCollectionFinalDocCount);\n      assertEquals((restoredCollectionDocCount + numberNewDocsIndexed), restoredCollectionFinalDocCount);\n    }\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5ad9c35f926b4bf8da0336d1300efc709c8d5a56","date":1591729157,"type":3,"author":"murblanc","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/api/collections/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String,int).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/api/collections/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String,int).mjava","sourceNew":"  private void testBackupAndRestore(String collectionName, int backupReplFactor) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = BACKUPNAME_PREFIX + testSuffix;\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    int restoreReplcationFactor = replFactor;\n    int restoreTlogReplicas = numTlogReplicas;\n    int restorePullReplicas = numPullReplicas;\n    boolean setExternalReplicationFactor = false;\n    if (random().nextBoolean()) { //Override replicationFactor / tLogReplicas / pullReplicas\n      setExternalReplicationFactor = true;\n      restoreTlogReplicas = TestUtil.nextInt(random(), 0, 1);\n      restoreReplcationFactor = TestUtil.nextInt(random(), 1, 2);\n      restorePullReplicas = TestUtil.nextInt(random(), 0, 1);\n    }\n    int numShards = backupCollection.getActiveSlices().size();\n\n    int restoreReplFactor = restoreReplcationFactor + restoreTlogReplicas + restorePullReplicas;\n\n    boolean isMaxShardsPerNodeExternal = false;\n    boolean isMaxShardsUnlimited = false;\n    CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n        .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n    //explicitly specify the replicationFactor/pullReplicas/nrtReplicas/tlogReplicas.\n    if (setExternalReplicationFactor)  {\n      restore.setReplicationFactor(restoreReplcationFactor);\n      restore.setTlogReplicas(restoreTlogReplicas);\n      restore.setPullReplicas(restorePullReplicas);\n    }\n    int computeRestoreMaxShardsPerNode = (int) Math.ceil((restoreReplFactor * numShards/(double) cluster.getJettySolrRunners().size()));\n\n    if (restoreReplFactor > backupReplFactor) { //else the backup maxShardsPerNode should be enough\n      if (log.isInfoEnabled()) {\n        log.info(\"numShards={} restoreReplFactor={} maxShardsPerNode={} totalNodes={}\",\n            numShards, restoreReplFactor, computeRestoreMaxShardsPerNode, cluster.getJettySolrRunners().size());\n      }\n\n      if (random().nextBoolean()) { //set it to -1\n        isMaxShardsUnlimited = true;\n        restore.setMaxShardsPerNode(-1);\n      } else {\n        isMaxShardsPerNodeExternal = true;\n        restore.setMaxShardsPerNode(computeRestoreMaxShardsPerNode);\n      }\n    }\n\n    if (rarely()) { // Try with createNodeSet configuration\n      //Always 1 as cluster.getJettySolrRunners().size()=NUM_SHARDS=2\n      restore.setCreateNodeSet(cluster.getJettySolrRunners().get(0).getNodeName());\n      // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n      isMaxShardsPerNodeExternal = true;\n      computeRestoreMaxShardsPerNode = origShardToDocCount.size() * restoreReplFactor;\n      restore.setMaxShardsPerNode(computeRestoreMaxShardsPerNode);\n    }\n\n    final int restoreMaxShardsPerNode = computeRestoreMaxShardsPerNode;\n\n    Properties props = new Properties();\n    props.setProperty(\"customKey\", \"customVal\");\n    restore.setProperties(props);\n\n    if (sameConfig==false) {\n      restore.setConfigName(\"customConfigName\");\n    }\n    if (random().nextBoolean()) {\n      assertEquals(0, restore.process(client).getStatus());\n    } else {\n      assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 60));//async\n    }\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n        restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName, false);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreMaxShardsPerNode\n              + \". state: \\n\" + restoreCollection, v <= restoreMaxShardsPerNode);\n    });\n\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getReplicationFactor().intValue());\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getNumNrtReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restorePullReplicas, restoreCollection.getNumPullReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restoreTlogReplicas, restoreCollection.getNumTlogReplicas().intValue());\n    if (isMaxShardsPerNodeExternal) {\n      assertEquals(restoreCollectionName, restoreMaxShardsPerNode, restoreCollection.getMaxShardsPerNode());\n    } else if (isMaxShardsUnlimited){\n      assertEquals(restoreCollectionName, -1, restoreCollection.getMaxShardsPerNode());\n    } else {\n      assertEquals(restoreCollectionName, backupCollection.getMaxShardsPerNode(), restoreCollection.getMaxShardsPerNode());\n    }\n\n    //SOLR-12605: Add more docs after restore is complete to see if they are getting added fine\n    //explicitly querying the leaders. If we use CloudSolrClient there is no guarantee that we'll hit a nrtReplica\n    {\n      Map<String, Integer> restoredCollectionPerShardCount =  getShardToDocCountMap(client, restoreCollection);\n      long restoredCollectionDocCount = restoredCollectionPerShardCount.values().stream().mapToInt(Number::intValue).sum();\n      int numberNewDocsIndexed = indexDocs(restoreCollectionName, true);\n      Map<String, Integer> restoredCollectionPerShardCountAfterIndexing = getShardToDocCountMap(client, restoreCollection);\n      int restoredCollectionFinalDocCount = restoredCollectionPerShardCountAfterIndexing.values().stream().mapToInt(Number::intValue).sum();\n\n      log.info(\"Original doc count in restored collection:{} , number of newly added documents to the restored collection: {}\"\n          + \", after indexing: {}\"\n          , restoredCollectionDocCount, numberNewDocsIndexed, restoredCollectionFinalDocCount);\n      assertEquals((restoredCollectionDocCount + numberNewDocsIndexed), restoredCollectionFinalDocCount);\n    }\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","sourceOld":"  private void testBackupAndRestore(String collectionName, int backupReplFactor) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = BACKUPNAME_PREFIX + testSuffix;\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    int restoreReplcationFactor = replFactor;\n    int restoreTlogReplicas = numTlogReplicas;\n    int restorePullReplicas = numPullReplicas;\n    boolean setExternalReplicationFactor = false;\n    if (random().nextBoolean()) { //Override replicationFactor / tLogReplicas / pullReplicas\n      setExternalReplicationFactor = true;\n      restoreTlogReplicas = TestUtil.nextInt(random(), 0, 1);\n      restoreReplcationFactor = TestUtil.nextInt(random(), 1, 2);\n      restorePullReplicas = TestUtil.nextInt(random(), 0, 1);\n    }\n    int numShards = backupCollection.getActiveSlices().size();\n\n    int restoreReplFactor = restoreReplcationFactor + restoreTlogReplicas + restorePullReplicas;\n\n    boolean isMaxShardsPerNodeExternal = false;\n    boolean isMaxShardsUnlimited = false;\n    CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n        .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n    //explicitly specify the replicationFactor/pullReplicas/nrtReplicas/tlogReplicas.\n    if (setExternalReplicationFactor)  {\n      restore.setReplicationFactor(restoreReplcationFactor);\n      restore.setTlogReplicas(restoreTlogReplicas);\n      restore.setPullReplicas(restorePullReplicas);\n    }\n    int computeRestoreMaxShardsPerNode = (int) Math.ceil((restoreReplFactor * numShards/(double) cluster.getJettySolrRunners().size()));\n\n    if (restoreReplFactor > backupReplFactor) { //else the backup maxShardsPerNode should be enough\n      if (log.isInfoEnabled()) {\n        log.info(\"numShards={} restoreReplFactor={} maxShardsPerNode={} totalNodes={}\",\n            numShards, restoreReplFactor, computeRestoreMaxShardsPerNode, cluster.getJettySolrRunners().size());\n      }\n\n      if (random().nextBoolean()) { //set it to -1\n        isMaxShardsUnlimited = true;\n        restore.setMaxShardsPerNode(-1);\n      } else {\n        isMaxShardsPerNodeExternal = true;\n        restore.setMaxShardsPerNode(computeRestoreMaxShardsPerNode);\n      }\n    }\n\n    if (rarely()) { // Try with createNodeSet configuration\n      //Always 1 as cluster.getJettySolrRunners().size()=NUM_SHARDS=2\n      restore.setCreateNodeSet(cluster.getJettySolrRunners().get(0).getNodeName());\n      // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n      isMaxShardsPerNodeExternal = true;\n      computeRestoreMaxShardsPerNode = origShardToDocCount.size() * restoreReplFactor;\n      restore.setMaxShardsPerNode(computeRestoreMaxShardsPerNode);\n    }\n\n    final int restoreMaxShardsPerNode = computeRestoreMaxShardsPerNode;\n\n    Properties props = new Properties();\n    props.setProperty(\"customKey\", \"customVal\");\n    restore.setProperties(props);\n\n    if (sameConfig==false) {\n      restore.setConfigName(\"customConfigName\");\n    }\n    if (random().nextBoolean()) {\n      assertEquals(0, restore.process(client).getStatus());\n    } else {\n      assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 60));//async\n    }\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n        restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName, false);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreMaxShardsPerNode\n              + \". state: \\n\" + restoreCollection, v <= restoreMaxShardsPerNode);\n    });\n\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getReplicationFactor().intValue());\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getNumNrtReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restorePullReplicas, restoreCollection.getNumPullReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restoreTlogReplicas, restoreCollection.getNumTlogReplicas().intValue());\n    if (isMaxShardsPerNodeExternal) {\n      assertEquals(restoreCollectionName, restoreMaxShardsPerNode, restoreCollection.getMaxShardsPerNode());\n    } else if (isMaxShardsUnlimited){\n      assertEquals(restoreCollectionName, -1, restoreCollection.getMaxShardsPerNode());\n    } else {\n      assertEquals(restoreCollectionName, backupCollection.getMaxShardsPerNode(), restoreCollection.getMaxShardsPerNode());\n    }\n\n    assertEquals(\"Restore collection should use stateFormat=2\", 2, restoreCollection.getStateFormat());\n\n    //SOLR-12605: Add more docs after restore is complete to see if they are getting added fine\n    //explicitly querying the leaders. If we use CloudSolrClient there is no guarantee that we'll hit a nrtReplica\n    {\n      Map<String, Integer> restoredCollectionPerShardCount =  getShardToDocCountMap(client, restoreCollection);\n      long restoredCollectionDocCount = restoredCollectionPerShardCount.values().stream().mapToInt(Number::intValue).sum();\n      int numberNewDocsIndexed = indexDocs(restoreCollectionName, true);\n      Map<String, Integer> restoredCollectionPerShardCountAfterIndexing = getShardToDocCountMap(client, restoreCollection);\n      int restoredCollectionFinalDocCount = restoredCollectionPerShardCountAfterIndexing.values().stream().mapToInt(Number::intValue).sum();\n\n      log.info(\"Original doc count in restored collection:{} , number of newly added documents to the restored collection: {}\"\n          + \", after indexing: {}\"\n          , restoredCollectionDocCount, numberNewDocsIndexed, restoredCollectionFinalDocCount);\n      assertEquals((restoredCollectionDocCount + numberNewDocsIndexed), restoredCollectionFinalDocCount);\n    }\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","bugFix":["529c69423fc9de95d4d764f4c998f095fead50bd"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e","date":1594223844,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/api/collections/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String,int).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/api/collections/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String,int).mjava","sourceNew":"  private void testBackupAndRestore(String collectionName, int backupReplFactor) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = BACKUPNAME_PREFIX + testSuffix;\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    int restoreReplcationFactor = replFactor;\n    int restoreTlogReplicas = numTlogReplicas;\n    int restorePullReplicas = numPullReplicas;\n    boolean setExternalReplicationFactor = false;\n    if (random().nextBoolean()) { //Override replicationFactor / tLogReplicas / pullReplicas\n      setExternalReplicationFactor = true;\n      restoreTlogReplicas = TestUtil.nextInt(random(), 0, 1);\n      restoreReplcationFactor = TestUtil.nextInt(random(), 1, 2);\n      restorePullReplicas = TestUtil.nextInt(random(), 0, 1);\n    }\n    int numShards = backupCollection.getActiveSlices().size();\n\n    int restoreReplFactor = restoreReplcationFactor + restoreTlogReplicas + restorePullReplicas;\n\n    CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n        .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n    //explicitly specify the replicationFactor/pullReplicas/nrtReplicas/tlogReplicas.\n    if (setExternalReplicationFactor)  {\n      restore.setReplicationFactor(restoreReplcationFactor);\n      restore.setTlogReplicas(restoreTlogReplicas);\n      restore.setPullReplicas(restorePullReplicas);\n    }\n    int computeRestoreMaxShardsPerNode = (int) Math.ceil((restoreReplFactor * numShards/(double) cluster.getJettySolrRunners().size()));\n\n    if (rarely()) { // Try with createNodeSet configuration\n      //Always 1 as cluster.getJettySolrRunners().size()=NUM_SHARDS=2\n      restore.setCreateNodeSet(cluster.getJettySolrRunners().get(0).getNodeName());\n    }\n\n    final int restoreMaxShardsPerNode = computeRestoreMaxShardsPerNode;\n\n    Properties props = new Properties();\n    props.setProperty(\"customKey\", \"customVal\");\n    restore.setProperties(props);\n\n    if (sameConfig==false) {\n      restore.setConfigName(\"customConfigName\");\n    }\n    if (random().nextBoolean()) {\n      assertEquals(0, restore.process(client).getStatus());\n    } else {\n      assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 60));//async\n    }\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n        restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName, false);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreMaxShardsPerNode\n              + \". state: \\n\" + restoreCollection, v <= restoreMaxShardsPerNode);\n    });\n\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getReplicationFactor().intValue());\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getNumNrtReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restorePullReplicas, restoreCollection.getNumPullReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restoreTlogReplicas, restoreCollection.getNumTlogReplicas().intValue());\n\n    //SOLR-12605: Add more docs after restore is complete to see if they are getting added fine\n    //explicitly querying the leaders. If we use CloudSolrClient there is no guarantee that we'll hit a nrtReplica\n    {\n      Map<String, Integer> restoredCollectionPerShardCount =  getShardToDocCountMap(client, restoreCollection);\n      long restoredCollectionDocCount = restoredCollectionPerShardCount.values().stream().mapToInt(Number::intValue).sum();\n      int numberNewDocsIndexed = indexDocs(restoreCollectionName, true);\n      Map<String, Integer> restoredCollectionPerShardCountAfterIndexing = getShardToDocCountMap(client, restoreCollection);\n      int restoredCollectionFinalDocCount = restoredCollectionPerShardCountAfterIndexing.values().stream().mapToInt(Number::intValue).sum();\n\n      log.info(\"Original doc count in restored collection:{} , number of newly added documents to the restored collection: {}\"\n          + \", after indexing: {}\"\n          , restoredCollectionDocCount, numberNewDocsIndexed, restoredCollectionFinalDocCount);\n      assertEquals((restoredCollectionDocCount + numberNewDocsIndexed), restoredCollectionFinalDocCount);\n    }\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","sourceOld":"  private void testBackupAndRestore(String collectionName, int backupReplFactor) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = BACKUPNAME_PREFIX + testSuffix;\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    int restoreReplcationFactor = replFactor;\n    int restoreTlogReplicas = numTlogReplicas;\n    int restorePullReplicas = numPullReplicas;\n    boolean setExternalReplicationFactor = false;\n    if (random().nextBoolean()) { //Override replicationFactor / tLogReplicas / pullReplicas\n      setExternalReplicationFactor = true;\n      restoreTlogReplicas = TestUtil.nextInt(random(), 0, 1);\n      restoreReplcationFactor = TestUtil.nextInt(random(), 1, 2);\n      restorePullReplicas = TestUtil.nextInt(random(), 0, 1);\n    }\n    int numShards = backupCollection.getActiveSlices().size();\n\n    int restoreReplFactor = restoreReplcationFactor + restoreTlogReplicas + restorePullReplicas;\n\n    boolean isMaxShardsPerNodeExternal = false;\n    boolean isMaxShardsUnlimited = false;\n    CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n        .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n    //explicitly specify the replicationFactor/pullReplicas/nrtReplicas/tlogReplicas.\n    if (setExternalReplicationFactor)  {\n      restore.setReplicationFactor(restoreReplcationFactor);\n      restore.setTlogReplicas(restoreTlogReplicas);\n      restore.setPullReplicas(restorePullReplicas);\n    }\n    int computeRestoreMaxShardsPerNode = (int) Math.ceil((restoreReplFactor * numShards/(double) cluster.getJettySolrRunners().size()));\n\n    if (restoreReplFactor > backupReplFactor) { //else the backup maxShardsPerNode should be enough\n      if (log.isInfoEnabled()) {\n        log.info(\"numShards={} restoreReplFactor={} maxShardsPerNode={} totalNodes={}\",\n            numShards, restoreReplFactor, computeRestoreMaxShardsPerNode, cluster.getJettySolrRunners().size());\n      }\n\n      if (random().nextBoolean()) { //set it to -1\n        isMaxShardsUnlimited = true;\n        restore.setMaxShardsPerNode(-1);\n      } else {\n        isMaxShardsPerNodeExternal = true;\n        restore.setMaxShardsPerNode(computeRestoreMaxShardsPerNode);\n      }\n    }\n\n    if (rarely()) { // Try with createNodeSet configuration\n      //Always 1 as cluster.getJettySolrRunners().size()=NUM_SHARDS=2\n      restore.setCreateNodeSet(cluster.getJettySolrRunners().get(0).getNodeName());\n      // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n      isMaxShardsPerNodeExternal = true;\n      computeRestoreMaxShardsPerNode = origShardToDocCount.size() * restoreReplFactor;\n      restore.setMaxShardsPerNode(computeRestoreMaxShardsPerNode);\n    }\n\n    final int restoreMaxShardsPerNode = computeRestoreMaxShardsPerNode;\n\n    Properties props = new Properties();\n    props.setProperty(\"customKey\", \"customVal\");\n    restore.setProperties(props);\n\n    if (sameConfig==false) {\n      restore.setConfigName(\"customConfigName\");\n    }\n    if (random().nextBoolean()) {\n      assertEquals(0, restore.process(client).getStatus());\n    } else {\n      assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 60));//async\n    }\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n        restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName, false);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreMaxShardsPerNode\n              + \". state: \\n\" + restoreCollection, v <= restoreMaxShardsPerNode);\n    });\n\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getReplicationFactor().intValue());\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getNumNrtReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restorePullReplicas, restoreCollection.getNumPullReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restoreTlogReplicas, restoreCollection.getNumTlogReplicas().intValue());\n    if (isMaxShardsPerNodeExternal) {\n      assertEquals(restoreCollectionName, restoreMaxShardsPerNode, restoreCollection.getMaxShardsPerNode());\n    } else if (isMaxShardsUnlimited){\n      assertEquals(restoreCollectionName, -1, restoreCollection.getMaxShardsPerNode());\n    } else {\n      assertEquals(restoreCollectionName, backupCollection.getMaxShardsPerNode(), restoreCollection.getMaxShardsPerNode());\n    }\n\n    //SOLR-12605: Add more docs after restore is complete to see if they are getting added fine\n    //explicitly querying the leaders. If we use CloudSolrClient there is no guarantee that we'll hit a nrtReplica\n    {\n      Map<String, Integer> restoredCollectionPerShardCount =  getShardToDocCountMap(client, restoreCollection);\n      long restoredCollectionDocCount = restoredCollectionPerShardCount.values().stream().mapToInt(Number::intValue).sum();\n      int numberNewDocsIndexed = indexDocs(restoreCollectionName, true);\n      Map<String, Integer> restoredCollectionPerShardCountAfterIndexing = getShardToDocCountMap(client, restoreCollection);\n      int restoredCollectionFinalDocCount = restoredCollectionPerShardCountAfterIndexing.values().stream().mapToInt(Number::intValue).sum();\n\n      log.info(\"Original doc count in restored collection:{} , number of newly added documents to the restored collection: {}\"\n          + \", after indexing: {}\"\n          , restoredCollectionDocCount, numberNewDocsIndexed, restoredCollectionFinalDocCount);\n      assertEquals((restoredCollectionDocCount + numberNewDocsIndexed), restoredCollectionFinalDocCount);\n    }\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3f504512a03d978990cbff30db0522b354e846db","date":1595247421,"type":3,"author":"Ishan Chattopadhyaya","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/api/collections/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String,int).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/api/collections/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String,int).mjava","sourceNew":"  private void testBackupAndRestore(String collectionName, int backupReplFactor) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = BACKUPNAME_PREFIX + testSuffix;\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    int restoreReplcationFactor = replFactor;\n    int restoreTlogReplicas = numTlogReplicas;\n    int restorePullReplicas = numPullReplicas;\n    boolean setExternalReplicationFactor = false;\n    if (random().nextBoolean()) { //Override replicationFactor / tLogReplicas / pullReplicas\n      setExternalReplicationFactor = true;\n      restoreTlogReplicas = TestUtil.nextInt(random(), 0, 1);\n      restoreReplcationFactor = TestUtil.nextInt(random(), 1, 2);\n      restorePullReplicas = TestUtil.nextInt(random(), 0, 1);\n    }\n    int numShards = backupCollection.getActiveSlices().size();\n\n    int restoreReplFactor = restoreReplcationFactor + restoreTlogReplicas + restorePullReplicas;\n\n    CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n        .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n    //explicitly specify the replicationFactor/pullReplicas/nrtReplicas/tlogReplicas.\n    if (setExternalReplicationFactor)  {\n      restore.setReplicationFactor(restoreReplcationFactor);\n      restore.setTlogReplicas(restoreTlogReplicas);\n      restore.setPullReplicas(restorePullReplicas);\n    }\n    int computeRestoreMaxShardsPerNode = (int) Math.ceil((restoreReplFactor * numShards/(double) cluster.getJettySolrRunners().size()));\n\n    if (rarely()) { // Try with createNodeSet configuration\n      //Always 1 as cluster.getJettySolrRunners().size()=NUM_SHARDS=2\n      restore.setCreateNodeSet(cluster.getJettySolrRunners().get(0).getNodeName());\n    }\n\n    final int restoreMaxShardsPerNode = computeRestoreMaxShardsPerNode;\n\n    Properties props = new Properties();\n    props.setProperty(\"customKey\", \"customVal\");\n    restore.setProperties(props);\n\n    if (sameConfig==false) {\n      restore.setConfigName(\"customConfigName\");\n    }\n    if (random().nextBoolean()) {\n      assertEquals(0, restore.process(client).getStatus());\n    } else {\n      assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 60));//async\n    }\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n        restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName, false);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreMaxShardsPerNode\n              + \". state: \\n\" + restoreCollection, v <= restoreMaxShardsPerNode);\n    });\n\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getReplicationFactor().intValue());\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getNumNrtReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restorePullReplicas, restoreCollection.getNumPullReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restoreTlogReplicas, restoreCollection.getNumTlogReplicas().intValue());\n\n    //SOLR-12605: Add more docs after restore is complete to see if they are getting added fine\n    //explicitly querying the leaders. If we use CloudSolrClient there is no guarantee that we'll hit a nrtReplica\n    {\n      Map<String, Integer> restoredCollectionPerShardCount =  getShardToDocCountMap(client, restoreCollection);\n      long restoredCollectionDocCount = restoredCollectionPerShardCount.values().stream().mapToInt(Number::intValue).sum();\n      int numberNewDocsIndexed = indexDocs(restoreCollectionName, true);\n      Map<String, Integer> restoredCollectionPerShardCountAfterIndexing = getShardToDocCountMap(client, restoreCollection);\n      int restoredCollectionFinalDocCount = restoredCollectionPerShardCountAfterIndexing.values().stream().mapToInt(Number::intValue).sum();\n\n      log.info(\"Original doc count in restored collection:{} , number of newly added documents to the restored collection: {}\"\n          + \", after indexing: {}\"\n          , restoredCollectionDocCount, numberNewDocsIndexed, restoredCollectionFinalDocCount);\n      assertEquals((restoredCollectionDocCount + numberNewDocsIndexed), restoredCollectionFinalDocCount);\n    }\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","sourceOld":"  private void testBackupAndRestore(String collectionName, int backupReplFactor) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = BACKUPNAME_PREFIX + testSuffix;\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    int restoreReplcationFactor = replFactor;\n    int restoreTlogReplicas = numTlogReplicas;\n    int restorePullReplicas = numPullReplicas;\n    boolean setExternalReplicationFactor = false;\n    if (random().nextBoolean()) { //Override replicationFactor / tLogReplicas / pullReplicas\n      setExternalReplicationFactor = true;\n      restoreTlogReplicas = TestUtil.nextInt(random(), 0, 1);\n      restoreReplcationFactor = TestUtil.nextInt(random(), 1, 2);\n      restorePullReplicas = TestUtil.nextInt(random(), 0, 1);\n    }\n    int numShards = backupCollection.getActiveSlices().size();\n\n    int restoreReplFactor = restoreReplcationFactor + restoreTlogReplicas + restorePullReplicas;\n\n    CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n        .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n    //explicitly specify the replicationFactor/pullReplicas/nrtReplicas/tlogReplicas.\n    if (setExternalReplicationFactor)  {\n      restore.setReplicationFactor(restoreReplcationFactor);\n      restore.setTlogReplicas(restoreTlogReplicas);\n      restore.setPullReplicas(restorePullReplicas);\n    }\n    int computeRestoreMaxShardsPerNode = (int) Math.ceil((restoreReplFactor * numShards/(double) cluster.getJettySolrRunners().size()));\n\n    if (rarely()) { // Try with createNodeSet configuration\n      //Always 1 as cluster.getJettySolrRunners().size()=NUM_SHARDS=2\n      restore.setCreateNodeSet(cluster.getJettySolrRunners().get(0).getNodeName());\n    }\n\n    final int restoreMaxShardsPerNode = computeRestoreMaxShardsPerNode;\n\n    Properties props = new Properties();\n    props.setProperty(\"customKey\", \"customVal\");\n    restore.setProperties(props);\n\n    if (sameConfig==false) {\n      restore.setConfigName(\"customConfigName\");\n    }\n    if (random().nextBoolean()) {\n      assertEquals(0, restore.process(client).getStatus());\n    } else {\n      assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 60));//async\n    }\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n        restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName, false);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreMaxShardsPerNode\n              + \". state: \\n\" + restoreCollection, v <= restoreMaxShardsPerNode);\n    });\n\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getReplicationFactor().intValue());\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getNumNrtReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restorePullReplicas, restoreCollection.getNumPullReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restoreTlogReplicas, restoreCollection.getNumTlogReplicas().intValue());\n\n    //SOLR-12605: Add more docs after restore is complete to see if they are getting added fine\n    //explicitly querying the leaders. If we use CloudSolrClient there is no guarantee that we'll hit a nrtReplica\n    {\n      Map<String, Integer> restoredCollectionPerShardCount =  getShardToDocCountMap(client, restoreCollection);\n      long restoredCollectionDocCount = restoredCollectionPerShardCount.values().stream().mapToInt(Number::intValue).sum();\n      int numberNewDocsIndexed = indexDocs(restoreCollectionName, true);\n      Map<String, Integer> restoredCollectionPerShardCountAfterIndexing = getShardToDocCountMap(client, restoreCollection);\n      int restoredCollectionFinalDocCount = restoredCollectionPerShardCountAfterIndexing.values().stream().mapToInt(Number::intValue).sum();\n\n      log.info(\"Original doc count in restored collection:{} , number of newly added documents to the restored collection: {}\"\n          + \", after indexing: {}\"\n          , restoredCollectionDocCount, numberNewDocsIndexed, restoredCollectionFinalDocCount);\n      assertEquals((restoredCollectionDocCount + numberNewDocsIndexed), restoredCollectionFinalDocCount);\n    }\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"5ad9c35f926b4bf8da0336d1300efc709c8d5a56":["fe9f2c4a0d7ac164e4bdd4eee7f87131aec83fd4"],"a64d56a8e1a34a05fef090cde8892dd4f00971ff":["d516662650185b8ef035b3933dca702cad9ca2c9"],"3f504512a03d978990cbff30db0522b354e846db":["e46a76bb135597b8bf35930cfdb3702bdd1cbe6e"],"d516662650185b8ef035b3933dca702cad9ca2c9":["63ae38ccbfadf7f763ed165694f6ae139e167f09"],"63ae38ccbfadf7f763ed165694f6ae139e167f09":["742168028ecb4838c124d27f836df9637be2f769"],"742168028ecb4838c124d27f836df9637be2f769":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e":["5ad9c35f926b4bf8da0336d1300efc709c8d5a56"],"ab6131420a270c49b653c969cc1dbbaf7d1b36e7":["cb9804c0e92c14de7239ee3a6649fe4e5f9e3116"],"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","d516662650185b8ef035b3933dca702cad9ca2c9"],"fe9f2c4a0d7ac164e4bdd4eee7f87131aec83fd4":["ab6131420a270c49b653c969cc1dbbaf7d1b36e7"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cb9804c0e92c14de7239ee3a6649fe4e5f9e3116":["a64d56a8e1a34a05fef090cde8892dd4f00971ff"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3f504512a03d978990cbff30db0522b354e846db"],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","d516662650185b8ef035b3933dca702cad9ca2c9"]},"commit2Childs":{"5ad9c35f926b4bf8da0336d1300efc709c8d5a56":["e46a76bb135597b8bf35930cfdb3702bdd1cbe6e"],"a64d56a8e1a34a05fef090cde8892dd4f00971ff":["cb9804c0e92c14de7239ee3a6649fe4e5f9e3116"],"3f504512a03d978990cbff30db0522b354e846db":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d516662650185b8ef035b3933dca702cad9ca2c9":["a64d56a8e1a34a05fef090cde8892dd4f00971ff","0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"63ae38ccbfadf7f763ed165694f6ae139e167f09":["d516662650185b8ef035b3933dca702cad9ca2c9"],"742168028ecb4838c124d27f836df9637be2f769":["63ae38ccbfadf7f763ed165694f6ae139e167f09"],"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e":["3f504512a03d978990cbff30db0522b354e846db"],"ab6131420a270c49b653c969cc1dbbaf7d1b36e7":["fe9f2c4a0d7ac164e4bdd4eee7f87131aec83fd4"],"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":[],"fe9f2c4a0d7ac164e4bdd4eee7f87131aec83fd4":["5ad9c35f926b4bf8da0336d1300efc709c8d5a56"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["742168028ecb4838c124d27f836df9637be2f769","0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"cb9804c0e92c14de7239ee3a6649fe4e5f9e3116":["ab6131420a270c49b653c969cc1dbbaf7d1b36e7"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":[]},"heads":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","cd5edd1f2b162a5cfa08efd17851a07373a96817","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}