{"path":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = j * 10;\n        token.endOffset = j * 10 + testTerms[i].length();\n      }\n    }\n    Arrays.sort(tokens);\n\n    dir = newDirectory();\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MyAnalyzer()).\n            setMaxBufferedDocs(-1).\n            setMergePolicy(newLogMergePolicy(false, 10))\n    );\n\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n      }\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else {\n        customType.setStoreTermVectors(true);\n      }\n      doc.add(new Field(testFields[i], \"\", customType));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++) {\n      writer.addDocument(doc);\n    }\n    writer.commit();\n    seg = writer.newestSegment();\n    writer.close();\n\n    fieldInfos = seg.getFieldInfos(); //new FieldInfos(dir, IndexFileNames.segmentFileName(seg.name, \"\", IndexFileNames.FIELD_INFOS_EXTENSION));\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = j * 10;\n        token.endOffset = j * 10 + testTerms[i].length();\n      }\n    }\n    Arrays.sort(tokens);\n\n    dir = newDirectory();\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MyAnalyzer()).\n            setMaxBufferedDocs(-1).\n            setMergePolicy(newLogMergePolicy(false, 10))\n    );\n\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n      }\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else {\n        customType.setStoreTermVectors(true);\n      }\n      doc.add(new Field(testFields[i], \"\", customType));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++) {\n      writer.addDocument(doc);\n    }\n    writer.commit();\n    seg = writer.newestSegment();\n    writer.close();\n\n    fieldInfos = seg.getFieldInfos(); //new FieldInfos(dir, IndexFileNames.segmentFileName(seg.name, \"\", IndexFileNames.FIELD_INFOS_EXTENSION));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d2dee33619431ada2a7a07f5fe2dbd94bac6a460","date":1337274029,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = j * 10;\n        token.endOffset = j * 10 + testTerms[i].length();\n      }\n    }\n    Arrays.sort(tokens);\n\n    dir = newDirectory();\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MyAnalyzer()).\n            setMaxBufferedDocs(-1).\n            setMergePolicy(newLogMergePolicy(false, 10))\n    );\n\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n      }\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else {\n        customType.setStoreTermVectors(true);\n      }\n      doc.add(new Field(testFields[i], \"\", customType));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++) {\n      writer.addDocument(doc);\n    }\n    writer.commit();\n    seg = writer.newestSegment();\n    writer.close();\n\n    fieldInfos = _TestUtil.getFieldInfos(seg);\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = j * 10;\n        token.endOffset = j * 10 + testTerms[i].length();\n      }\n    }\n    Arrays.sort(tokens);\n\n    dir = newDirectory();\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MyAnalyzer()).\n            setMaxBufferedDocs(-1).\n            setMergePolicy(newLogMergePolicy(false, 10))\n    );\n\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n      }\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else {\n        customType.setStoreTermVectors(true);\n      }\n      doc.add(new Field(testFields[i], \"\", customType));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++) {\n      writer.addDocument(doc);\n    }\n    writer.commit();\n    seg = writer.newestSegment();\n    writer.close();\n\n    fieldInfos = seg.getFieldInfos(); //new FieldInfos(dir, IndexFileNames.segmentFileName(seg.name, \"\", IndexFileNames.FIELD_INFOS_EXTENSION));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9d153abcf92dc5329d98571a8c3035df9bd80648","date":1337702630,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = j * 10;\n        token.endOffset = j * 10 + testTerms[i].length();\n      }\n    }\n    Arrays.sort(tokens);\n\n    dir = newDirectory();\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MyAnalyzer()).\n            setMaxBufferedDocs(-1).\n            setMergePolicy(newLogMergePolicy(false, 10))\n    );\n\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n      }\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else {\n        customType.setStoreTermVectors(true);\n      }\n      doc.add(new Field(testFields[i], \"\", customType));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++) {\n      writer.addDocument(doc);\n    }\n    writer.commit();\n    seg = writer.newestSegment();\n    writer.close();\n\n    fieldInfos = _TestUtil.getFieldInfos(seg.info);\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = j * 10;\n        token.endOffset = j * 10 + testTerms[i].length();\n      }\n    }\n    Arrays.sort(tokens);\n\n    dir = newDirectory();\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MyAnalyzer()).\n            setMaxBufferedDocs(-1).\n            setMergePolicy(newLogMergePolicy(false, 10))\n    );\n\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n      }\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else {\n        customType.setStoreTermVectors(true);\n      }\n      doc.add(new Field(testFields[i], \"\", customType));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++) {\n      writer.addDocument(doc);\n    }\n    writer.commit();\n    seg = writer.newestSegment();\n    writer.close();\n\n    fieldInfos = _TestUtil.getFieldInfos(seg);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"615ddbd81799980d0fdd95e0238e1c498b6f47b0","date":1338233290,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = j * 10;\n        token.endOffset = j * 10 + testTerms[i].length();\n      }\n    }\n    Arrays.sort(tokens);\n\n    dir = newDirectory();\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MyAnalyzer()).\n            setMaxBufferedDocs(-1).\n            setMergePolicy(newLogMergePolicy(false, 10))\n    );\n\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n      }\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else {\n        customType.setStoreTermVectors(true);\n      }\n      doc.add(new Field(testFields[i], \"\", customType));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++) {\n      writer.addDocument(doc);\n    }\n    writer.commit();\n    seg = writer.newestSegment();\n    writer.close();\n\n    fieldInfos = _TestUtil.getFieldInfos(seg.info);\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = j * 10;\n        token.endOffset = j * 10 + testTerms[i].length();\n      }\n    }\n    Arrays.sort(tokens);\n\n    dir = newDirectory();\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MyAnalyzer()).\n            setMaxBufferedDocs(-1).\n            setMergePolicy(newLogMergePolicy(false, 10))\n    );\n\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n      }\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else {\n        customType.setStoreTermVectors(true);\n      }\n      doc.add(new Field(testFields[i], \"\", customType));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++) {\n      writer.addDocument(doc);\n    }\n    writer.commit();\n    seg = writer.newestSegment();\n    writer.close();\n\n    fieldInfos = seg.getFieldInfos(); //new FieldInfos(dir, IndexFileNames.segmentFileName(seg.name, \"\", IndexFileNames.FIELD_INFOS_EXTENSION));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"04f07771a2a7dd3a395700665ed839c3dae2def2","date":1339350139,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = j * 10;\n        token.endOffset = j * 10 + testTerms[i].length();\n      }\n    }\n    Arrays.sort(tokens);\n\n    dir = newDirectory();\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MyAnalyzer()).\n            setMaxBufferedDocs(-1).\n            setMergePolicy(newLogMergePolicy(false, 10))\n    );\n\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n      }\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else {\n        customType.setStoreTermVectors(true);\n      }\n      doc.add(new Field(testFields[i], \"\", customType));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++) {\n      writer.addDocument(doc);\n    }\n    writer.commit();\n    seg = writer.newestSegment();\n    writer.close();\n\n    fieldInfos = _TestUtil.getFieldInfos(seg.info);\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = j * 10;\n        token.endOffset = j * 10 + testTerms[i].length();\n      }\n    }\n    Arrays.sort(tokens);\n\n    dir = newDirectory();\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MyAnalyzer()).\n            setMaxBufferedDocs(-1).\n            setMergePolicy(newLogMergePolicy(false, 10))\n    );\n\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n      }\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else {\n        customType.setStoreTermVectors(true);\n      }\n      doc.add(new Field(testFields[i], \"\", customType));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++) {\n      writer.addDocument(doc);\n    }\n    writer.commit();\n    seg = writer.newestSegment();\n    writer.close();\n\n    fieldInfos = _TestUtil.getFieldInfos(seg.info);\n  }\n\n","bugFix":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"088a7ef694fd43d5d9a4d200c4005865f773d1e7","date":1371136274,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = j * 10;\n        token.endOffset = j * 10 + testTerms[i].length();\n      }\n    }\n    Arrays.sort(tokens);\n\n    dir = newDirectory();\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MyAnalyzer()).\n            setMaxBufferedDocs(-1).\n            setMergePolicy(newLogMergePolicy(false, 10)).setUseCompoundFile(false)\n    );\n\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n      }\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else {\n        customType.setStoreTermVectors(true);\n      }\n      doc.add(new Field(testFields[i], \"\", customType));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++) {\n      writer.addDocument(doc);\n    }\n    writer.commit();\n    seg = writer.newestSegment();\n    writer.close();\n\n    fieldInfos = _TestUtil.getFieldInfos(seg.info);\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = j * 10;\n        token.endOffset = j * 10 + testTerms[i].length();\n      }\n    }\n    Arrays.sort(tokens);\n\n    dir = newDirectory();\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MyAnalyzer()).\n            setMaxBufferedDocs(-1).\n            setMergePolicy(newLogMergePolicy(false, 10))\n    );\n\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n      }\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else {\n        customType.setStoreTermVectors(true);\n      }\n      doc.add(new Field(testFields[i], \"\", customType));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++) {\n      writer.addDocument(doc);\n    }\n    writer.commit();\n    seg = writer.newestSegment();\n    writer.close();\n\n    fieldInfos = _TestUtil.getFieldInfos(seg.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8435160e9702b19398118ddf76b61c846612b6a4","date":1380349140,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = j * 10;\n        token.endOffset = j * 10 + testTerms[i].length();\n      }\n    }\n    Arrays.sort(tokens);\n\n    dir = newDirectory();\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MyAnalyzer()).\n            setMaxBufferedDocs(-1).\n            setMergePolicy(newLogMergePolicy(false, 10)).setUseCompoundFile(false)\n    );\n\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n      }\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else {\n        customType.setStoreTermVectors(true);\n      }\n      doc.add(new Field(testFields[i], \"\", customType));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++) {\n      writer.addDocument(doc);\n    }\n    writer.commit();\n    seg = writer.newestSegment();\n    writer.close();\n\n    fieldInfos = SegmentReader.readFieldInfos(seg);\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = j * 10;\n        token.endOffset = j * 10 + testTerms[i].length();\n      }\n    }\n    Arrays.sort(tokens);\n\n    dir = newDirectory();\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MyAnalyzer()).\n            setMaxBufferedDocs(-1).\n            setMergePolicy(newLogMergePolicy(false, 10)).setUseCompoundFile(false)\n    );\n\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n      }\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else {\n        customType.setStoreTermVectors(true);\n      }\n      doc.add(new Field(testFields[i], \"\", customType));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++) {\n      writer.addDocument(doc);\n    }\n    writer.commit();\n    seg = writer.newestSegment();\n    writer.close();\n\n    fieldInfos = _TestUtil.getFieldInfos(seg.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = j * 10;\n        token.endOffset = j * 10 + testTerms[i].length();\n      }\n    }\n    Arrays.sort(tokens);\n\n    dir = newDirectory();\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MyAnalyzer()).\n            setMaxBufferedDocs(-1).\n            setMergePolicy(newLogMergePolicy(false, 10)).setUseCompoundFile(false)\n    );\n\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n      }\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else {\n        customType.setStoreTermVectors(true);\n      }\n      doc.add(new Field(testFields[i], \"\", customType));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++) {\n      writer.addDocument(doc);\n    }\n    writer.commit();\n    seg = writer.newestSegment();\n    writer.shutdown();\n\n    fieldInfos = SegmentReader.readFieldInfos(seg);\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = j * 10;\n        token.endOffset = j * 10 + testTerms[i].length();\n      }\n    }\n    Arrays.sort(tokens);\n\n    dir = newDirectory();\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MyAnalyzer()).\n            setMaxBufferedDocs(-1).\n            setMergePolicy(newLogMergePolicy(false, 10)).setUseCompoundFile(false)\n    );\n\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n      }\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else {\n        customType.setStoreTermVectors(true);\n      }\n      doc.add(new Field(testFields[i], \"\", customType));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++) {\n      writer.addDocument(doc);\n    }\n    writer.commit();\n    seg = writer.newestSegment();\n    writer.close();\n\n    fieldInfos = SegmentReader.readFieldInfos(seg);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"52c7e49be259508735752fba88085255014a6ecf","date":1398706273,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = j * 10;\n        token.endOffset = j * 10 + testTerms[i].length();\n      }\n    }\n    Arrays.sort(tokens);\n\n    dir = newDirectory();\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MyAnalyzer()).\n            setMaxBufferedDocs(-1).\n            setMergePolicy(newLogMergePolicy(false, 10)).setUseCompoundFile(false)\n    );\n\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n      }\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else {\n        customType.setStoreTermVectors(true);\n      }\n      doc.add(new Field(testFields[i], \"\", customType));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++) {\n      writer.addDocument(doc);\n    }\n    writer.commit();\n    seg = writer.newestSegment();\n    writer.shutdown();\n\n    fieldInfos = SegmentReader.readFieldInfos(seg);\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = j * 10;\n        token.endOffset = j * 10 + testTerms[i].length();\n      }\n    }\n    Arrays.sort(tokens);\n\n    dir = newDirectory();\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MyAnalyzer()).\n            setMaxBufferedDocs(-1).\n            setMergePolicy(newLogMergePolicy(false, 10)).setUseCompoundFile(false)\n    );\n\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n      }\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else {\n        customType.setStoreTermVectors(true);\n      }\n      doc.add(new Field(testFields[i], \"\", customType));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++) {\n      writer.addDocument(doc);\n    }\n    writer.commit();\n    seg = writer.newestSegment();\n    writer.shutdown();\n\n    fieldInfos = SegmentReader.readFieldInfos(seg);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3394716f52b34ab259ad5247e7595d9f9db6e935","date":1398791921,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = j * 10;\n        token.endOffset = j * 10 + testTerms[i].length();\n      }\n    }\n    Arrays.sort(tokens);\n\n    dir = newDirectory();\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MyAnalyzer()).\n            setMaxBufferedDocs(-1).\n            setMergePolicy(newLogMergePolicy(false, 10)).setUseCompoundFile(false)\n    );\n\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n      }\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else {\n        customType.setStoreTermVectors(true);\n      }\n      doc.add(new Field(testFields[i], \"\", customType));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++) {\n      writer.addDocument(doc);\n    }\n    writer.commit();\n    seg = writer.newestSegment();\n    writer.shutdown();\n\n    fieldInfos = SegmentReader.readFieldInfos(seg);\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = j * 10;\n        token.endOffset = j * 10 + testTerms[i].length();\n      }\n    }\n    Arrays.sort(tokens);\n\n    dir = newDirectory();\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MyAnalyzer()).\n            setMaxBufferedDocs(-1).\n            setMergePolicy(newLogMergePolicy(false, 10)).setUseCompoundFile(false)\n    );\n\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n      }\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else {\n        customType.setStoreTermVectors(true);\n      }\n      doc.add(new Field(testFields[i], \"\", customType));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++) {\n      writer.addDocument(doc);\n    }\n    writer.commit();\n    seg = writer.newestSegment();\n    writer.shutdown();\n\n    fieldInfos = SegmentReader.readFieldInfos(seg);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","date":1398844771,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = j * 10;\n        token.endOffset = j * 10 + testTerms[i].length();\n      }\n    }\n    Arrays.sort(tokens);\n\n    dir = newDirectory();\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MyAnalyzer()).\n            setMaxBufferedDocs(-1).\n            setMergePolicy(newLogMergePolicy(false, 10)).setUseCompoundFile(false)\n    );\n\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n      }\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else {\n        customType.setStoreTermVectors(true);\n      }\n      doc.add(new Field(testFields[i], \"\", customType));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++) {\n      writer.addDocument(doc);\n    }\n    writer.commit();\n    seg = writer.newestSegment();\n    writer.shutdown();\n\n    fieldInfos = SegmentReader.readFieldInfos(seg);\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = j * 10;\n        token.endOffset = j * 10 + testTerms[i].length();\n      }\n    }\n    Arrays.sort(tokens);\n\n    dir = newDirectory();\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MyAnalyzer()).\n            setMaxBufferedDocs(-1).\n            setMergePolicy(newLogMergePolicy(false, 10)).setUseCompoundFile(false)\n    );\n\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n      }\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else {\n        customType.setStoreTermVectors(true);\n      }\n      doc.add(new Field(testFields[i], \"\", customType));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++) {\n      writer.addDocument(doc);\n    }\n    writer.commit();\n    seg = writer.newestSegment();\n    writer.shutdown();\n\n    fieldInfos = SegmentReader.readFieldInfos(seg);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = j * 10;\n        token.endOffset = j * 10 + testTerms[i].length();\n      }\n    }\n    Arrays.sort(tokens);\n\n    dir = newDirectory();\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MyAnalyzer()).\n            setMaxBufferedDocs(-1).\n            setMergePolicy(newLogMergePolicy(false, 10))\n            .setUseCompoundFile(false)\n    );\n\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n      }\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else {\n        customType.setStoreTermVectors(true);\n      }\n      doc.add(new Field(testFields[i], \"\", customType));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++) {\n      writer.addDocument(doc);\n    }\n    writer.commit();\n    seg = writer.newestSegment();\n    writer.shutdown();\n\n    fieldInfos = SegmentReader.readFieldInfos(seg);\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = j * 10;\n        token.endOffset = j * 10 + testTerms[i].length();\n      }\n    }\n    Arrays.sort(tokens);\n\n    dir = newDirectory();\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MyAnalyzer()).\n            setMaxBufferedDocs(-1).\n            setMergePolicy(newLogMergePolicy(false, 10)).setUseCompoundFile(false)\n    );\n\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n      }\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else {\n        customType.setStoreTermVectors(true);\n      }\n      doc.add(new Field(testFields[i], \"\", customType));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++) {\n      writer.addDocument(doc);\n    }\n    writer.commit();\n    seg = writer.newestSegment();\n    writer.shutdown();\n\n    fieldInfos = SegmentReader.readFieldInfos(seg);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = j * 10;\n        token.endOffset = j * 10 + testTerms[i].length();\n      }\n    }\n    Arrays.sort(tokens);\n\n    dir = newDirectory();\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MyAnalyzer()).\n            setMaxBufferedDocs(-1).\n            setMergePolicy(newLogMergePolicy(false, 10))\n            .setUseCompoundFile(false)\n    );\n\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n      }\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else {\n        customType.setStoreTermVectors(true);\n      }\n      doc.add(new Field(testFields[i], \"\", customType));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++) {\n      writer.addDocument(doc);\n    }\n    writer.commit();\n    seg = writer.newestSegment();\n    writer.close();\n\n    fieldInfos = SegmentReader.readFieldInfos(seg);\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = j * 10;\n        token.endOffset = j * 10 + testTerms[i].length();\n      }\n    }\n    Arrays.sort(tokens);\n\n    dir = newDirectory();\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MyAnalyzer()).\n            setMaxBufferedDocs(-1).\n            setMergePolicy(newLogMergePolicy(false, 10))\n            .setUseCompoundFile(false)\n    );\n\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n      }\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else {\n        customType.setStoreTermVectors(true);\n      }\n      doc.add(new Field(testFields[i], \"\", customType));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++) {\n      writer.addDocument(doc);\n    }\n    writer.commit();\n    seg = writer.newestSegment();\n    writer.shutdown();\n\n    fieldInfos = SegmentReader.readFieldInfos(seg);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"256a0e54e76f18e115a43e7fe793b54d4e9a3005","date":1412426514,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = j * 10;\n        token.endOffset = j * 10 + testTerms[i].length();\n      }\n    }\n    Arrays.sort(tokens);\n\n    dir = newDirectory();\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MyAnalyzer()).\n            setMaxBufferedDocs(-1).\n            setMergePolicy(newLogMergePolicy(false, 10))\n            .setUseCompoundFile(false)\n    );\n\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n      }\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else {\n        customType.setStoreTermVectors(true);\n      }\n      doc.add(new Field(testFields[i], \"\", customType));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++) {\n      writer.addDocument(doc);\n    }\n    writer.commit();\n    seg = writer.newestSegment();\n    writer.close();\n\n    fieldInfos = IndexWriter.readFieldInfos(seg);\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = j * 10;\n        token.endOffset = j * 10 + testTerms[i].length();\n      }\n    }\n    Arrays.sort(tokens);\n\n    dir = newDirectory();\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MyAnalyzer()).\n            setMaxBufferedDocs(-1).\n            setMergePolicy(newLogMergePolicy(false, 10))\n            .setUseCompoundFile(false)\n    );\n\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n      }\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else {\n        customType.setStoreTermVectors(true);\n      }\n      doc.add(new Field(testFields[i], \"\", customType));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++) {\n      writer.addDocument(doc);\n    }\n    writer.commit();\n    seg = writer.newestSegment();\n    writer.close();\n\n    fieldInfos = SegmentReader.readFieldInfos(seg);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9bb9a29a5e71a90295f175df8919802993142c9a","date":1412517673,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = j * 10;\n        token.endOffset = j * 10 + testTerms[i].length();\n      }\n    }\n    Arrays.sort(tokens);\n\n    dir = newDirectory();\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MyAnalyzer()).\n            setMaxBufferedDocs(-1).\n            setMergePolicy(newLogMergePolicy(false, 10))\n            .setUseCompoundFile(false)\n    );\n\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n      }\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else {\n        customType.setStoreTermVectors(true);\n      }\n      doc.add(new Field(testFields[i], \"\", customType));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++) {\n      writer.addDocument(doc);\n    }\n    writer.commit();\n    seg = writer.newestSegment();\n    writer.close();\n\n    fieldInfos = IndexWriter.readFieldInfos(seg);\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = j * 10;\n        token.endOffset = j * 10 + testTerms[i].length();\n      }\n    }\n    Arrays.sort(tokens);\n\n    dir = newDirectory();\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MyAnalyzer()).\n            setMaxBufferedDocs(-1).\n            setMergePolicy(newLogMergePolicy(false, 10))\n            .setUseCompoundFile(false)\n    );\n\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n      }\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else {\n        customType.setStoreTermVectors(true);\n      }\n      doc.add(new Field(testFields[i], \"\", customType));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++) {\n      writer.addDocument(doc);\n    }\n    writer.commit();\n    seg = writer.newestSegment();\n    writer.close();\n\n    fieldInfos = SegmentReader.readFieldInfos(seg);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"017bea7152e5f8c54b62fc8048a90e95117a626b","date":1440186158,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    Random rnd = random();\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + rnd.nextDouble() * 10);\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = j * 10;\n        token.endOffset = j * 10 + testTerms[i].length();\n      }\n    }\n    Arrays.sort(tokens);\n\n    dir = newDirectory();\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MyAnalyzer()).\n            setMaxBufferedDocs(-1).\n            setMergePolicy(newLogMergePolicy(false, 10))\n            .setUseCompoundFile(false)\n    );\n\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n      }\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else {\n        customType.setStoreTermVectors(true);\n      }\n      doc.add(new Field(testFields[i], \"\", customType));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++) {\n      writer.addDocument(doc);\n    }\n    writer.commit();\n    seg = writer.newestSegment();\n    writer.close();\n\n    fieldInfos = IndexWriter.readFieldInfos(seg);\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = j * 10;\n        token.endOffset = j * 10 + testTerms[i].length();\n      }\n    }\n    Arrays.sort(tokens);\n\n    dir = newDirectory();\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MyAnalyzer()).\n            setMaxBufferedDocs(-1).\n            setMergePolicy(newLogMergePolicy(false, 10))\n            .setUseCompoundFile(false)\n    );\n\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n      }\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i]) {\n        customType.setStoreTermVectors(true);\n        customType.setStoreTermVectorPositions(true);\n        customType.setStoreTermVectorOffsets(true);\n      }\n      else {\n        customType.setStoreTermVectors(true);\n      }\n      doc.add(new Field(testFields[i], \"\", customType));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++) {\n      writer.addDocument(doc);\n    }\n    writer.commit();\n    seg = writer.newestSegment();\n    writer.close();\n\n    fieldInfos = IndexWriter.readFieldInfos(seg);\n  }\n\n","bugFix":["11764865fb318bf86302eab36bdf9cd00c50c110"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["3394716f52b34ab259ad5247e7595d9f9db6e935"],"256a0e54e76f18e115a43e7fe793b54d4e9a3005":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a","3394716f52b34ab259ad5247e7595d9f9db6e935"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"9d153abcf92dc5329d98571a8c3035df9bd80648":["d2dee33619431ada2a7a07f5fe2dbd94bac6a460"],"8435160e9702b19398118ddf76b61c846612b6a4":["088a7ef694fd43d5d9a4d200c4005865f773d1e7"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"d2dee33619431ada2a7a07f5fe2dbd94bac6a460":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"3394716f52b34ab259ad5247e7595d9f9db6e935":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a","52c7e49be259508735752fba88085255014a6ecf"],"088a7ef694fd43d5d9a4d200c4005865f773d1e7":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"615ddbd81799980d0fdd95e0238e1c498b6f47b0":["3a119bbc8703c10faa329ec201c654b3a35a1e3e","9d153abcf92dc5329d98571a8c3035df9bd80648"],"9bb9a29a5e71a90295f175df8919802993142c9a":["d0ef034a4f10871667ae75181537775ddcf8ade4","256a0e54e76f18e115a43e7fe793b54d4e9a3005"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"017bea7152e5f8c54b62fc8048a90e95117a626b":["9bb9a29a5e71a90295f175df8919802993142c9a"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["8435160e9702b19398118ddf76b61c846612b6a4"],"52c7e49be259508735752fba88085255014a6ecf":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["017bea7152e5f8c54b62fc8048a90e95117a626b"]},"commit2Childs":{"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"256a0e54e76f18e115a43e7fe793b54d4e9a3005":["9bb9a29a5e71a90295f175df8919802993142c9a"],"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe":[],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["d2dee33619431ada2a7a07f5fe2dbd94bac6a460","615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"9d153abcf92dc5329d98571a8c3035df9bd80648":["615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"8435160e9702b19398118ddf76b61c846612b6a4":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"3394716f52b34ab259ad5247e7595d9f9db6e935":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","c0cd85fde84cb318b4dc97710dcf15e2959a1bbe"],"d2dee33619431ada2a7a07f5fe2dbd94bac6a460":["9d153abcf92dc5329d98571a8c3035df9bd80648"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["088a7ef694fd43d5d9a4d200c4005865f773d1e7"],"088a7ef694fd43d5d9a4d200c4005865f773d1e7":["8435160e9702b19398118ddf76b61c846612b6a4"],"615ddbd81799980d0fdd95e0238e1c498b6f47b0":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"9bb9a29a5e71a90295f175df8919802993142c9a":["017bea7152e5f8c54b62fc8048a90e95117a626b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["256a0e54e76f18e115a43e7fe793b54d4e9a3005","9bb9a29a5e71a90295f175df8919802993142c9a"],"017bea7152e5f8c54b62fc8048a90e95117a626b":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","3394716f52b34ab259ad5247e7595d9f9db6e935","52c7e49be259508735752fba88085255014a6ecf"],"52c7e49be259508735752fba88085255014a6ecf":["3394716f52b34ab259ad5247e7595d9f9db6e935"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}