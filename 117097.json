{"path":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(FieldsConsumer,SegmentWriteState).mjava","commits":[{"id":"6c18273ea5b3974d2f30117f46f1ae416c28f727","date":1279708040,"type":1,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriter#appendPostings(FreqProxTermsWriterPerField[],FieldsConsumer).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    final boolean currentFieldOmitTermFreqAndPositions = fieldInfo.omitTermFreqAndPositions;\n    \n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.numPostings;\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    \n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n      \n      termsHashPerField.initReader(freq, termID, 0);\n      if (!fieldInfo.omitTermFreqAndPositions) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n  \n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n    \n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n  \n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      int docID = 0;\n      int termFreq = 0;\n      \n      while(true) {\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (!omitTermFreqAndPositions) {\n              termFreq = postings.docFreqs[termID];\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (omitTermFreqAndPositions) {\n            docID += code;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n    \n          assert docID != postings.lastDocIDs[termID];\n        }\n        \n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n        final int termDocFreq = termFreq;\n        postingsConsumer.startDoc(docID, termDocFreq);\n    \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        if (!currentFieldOmitTermFreqAndPositions) {\n          // omitTermFreqAndPositions == false so we do write positions &\n          // payload          \n          int position = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n    \n            final int payloadLength;\n            final BytesRef thisPayload;\n    \n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();  \n              \n              if (payload == null) {\n                payload = new BytesRef();\n                payload.bytes = new byte[payloadLength];\n              } else if (payload.bytes.length < payloadLength) {\n                payload.grow(payloadLength);\n              }\n    \n              prox.readBytes(payload.bytes, 0, payloadLength);\n              payload.length = payloadLength;\n              thisPayload = payload;\n    \n            } else {\n              payloadLength = 0;\n              thisPayload = null;\n            }\n    \n            postingsConsumer.addPosition(position, thisPayload);\n          } \n    \n          postingsConsumer.finishDoc();\n        }\n      } \n      termsConsumer.finishTerm(text, numDocs);\n    }\n  \n    termsConsumer.finish();\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void appendPostings(FreqProxTermsWriterPerField[] fields,\n                      FieldsConsumer consumer)\n    throws CorruptIndexException, IOException {\n\n    int numFields = fields.length;\n\n    final BytesRef text = new BytesRef();\n\n    final FreqProxFieldMergeState[] mergeStates = new FreqProxFieldMergeState[numFields];\n\n    final TermsConsumer termsConsumer = consumer.addField(fields[0].fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    for(int i=0;i<numFields;i++) {\n      FreqProxFieldMergeState fms = mergeStates[i] = new FreqProxFieldMergeState(fields[i], termComp);\n\n      assert fms.field.fieldInfo == fields[0].fieldInfo;\n\n      // Should always be true\n      boolean result = fms.nextTerm();\n      assert result;\n    }\n\n    FreqProxFieldMergeState[] termStates = new FreqProxFieldMergeState[numFields];\n\n    final boolean currentFieldOmitTermFreqAndPositions = fields[0].fieldInfo.omitTermFreqAndPositions;\n    //System.out.println(\"flush terms field=\" + fields[0].fieldInfo.name);\n\n    // TODO: really TermsHashPerField should take over most\n    // of this loop, including merge sort of terms from\n    // multiple threads and interacting with the\n    // TermsConsumer, only calling out to us (passing us the\n    // DocsConsumer) to handle delivery of docs/positions\n    while(numFields > 0) {\n\n      // Get the next term to merge\n      termStates[0] = mergeStates[0];\n      int numToMerge = 1;\n\n      // TODO: pqueue\n      for(int i=1;i<numFields;i++) {\n        final int cmp = termComp.compare(mergeStates[i].text, termStates[0].text);\n        if (cmp < 0) {\n          termStates[0] = mergeStates[i];\n          numToMerge = 1;\n        } else if (cmp == 0) {\n          termStates[numToMerge++] = mergeStates[i];\n        }\n      }\n\n      // Need shallow copy here because termStates[0].text\n      // changes by the time we call finishTerm\n      text.bytes = termStates[0].text.bytes;\n      text.offset = termStates[0].text.offset;\n      text.length = termStates[0].text.length;  \n\n      //System.out.println(\"  term=\" + text.toUnicodeString());\n      //System.out.println(\"  term=\" + text.toString());\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      while(numToMerge > 0) {\n        \n        FreqProxFieldMergeState minState = termStates[0];\n        for(int i=1;i<numToMerge;i++) {\n          if (termStates[i].docID < minState.docID) {\n            minState = termStates[i];\n          }\n        }\n\n        final int termDocFreq = minState.termFreq;\n        numDocs++;\n\n        assert minState.docID < flushedDocCount: \"doc=\" + minState.docID + \" maxDoc=\" + flushedDocCount;\n\n        postingsConsumer.startDoc(minState.docID, termDocFreq);\n\n        final ByteSliceReader prox = minState.prox;\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        if (!currentFieldOmitTermFreqAndPositions) {\n          // omitTermFreqAndPositions == false so we do write positions &\n          // payload          \n          int position = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n            //System.out.println(\"    pos=\" + position);\n\n            final int payloadLength;\n            final BytesRef thisPayload;\n\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();  \n              \n              if (payload == null) {\n                payload = new BytesRef();\n                payload.bytes = new byte[payloadLength];\n              } else if (payload.bytes.length < payloadLength) {\n                payload.grow(payloadLength);\n              }\n\n              prox.readBytes(payload.bytes, 0, payloadLength);\n              payload.length = payloadLength;\n              thisPayload = payload;\n\n            } else {\n              payloadLength = 0;\n              thisPayload = null;\n            }\n\n            postingsConsumer.addPosition(position, thisPayload);\n          } //End for\n\n          postingsConsumer.finishDoc();\n        }\n\n        if (!minState.nextDoc()) {\n\n          // Remove from termStates\n          int upto = 0;\n          // TODO: inefficient O(N) where N = number of\n          // threads that had seen this term:\n          for(int i=0;i<numToMerge;i++) {\n            if (termStates[i] != minState) {\n              termStates[upto++] = termStates[i];\n            }\n          }\n          numToMerge--;\n          assert upto == numToMerge;\n\n          // Advance this state to the next term\n\n          if (!minState.nextTerm()) {\n            // OK, no more terms, so remove from mergeStates\n            // as well\n            upto = 0;\n            for(int i=0;i<numFields;i++)\n              if (mergeStates[i] != minState)\n                mergeStates[upto++] = mergeStates[i];\n            numFields--;\n            assert upto == numFields;\n          }\n        }\n      }\n\n      assert numDocs > 0;\n      termsConsumer.finishTerm(text, numDocs);\n    }\n\n    termsConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    final boolean currentFieldOmitTermFreqAndPositions = fieldInfo.omitTermFreqAndPositions;\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (!fieldInfo.omitTermFreqAndPositions) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      int docID = 0;\n      int termFreq = 0;\n\n      while(true) {\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (!omitTermFreqAndPositions) {\n              termFreq = postings.docFreqs[termID];\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (omitTermFreqAndPositions) {\n            docID += code;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n        final int termDocFreq = termFreq;\n        postingsConsumer.startDoc(docID, termDocFreq);\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        if (!currentFieldOmitTermFreqAndPositions) {\n          // omitTermFreqAndPositions == false so we do write positions &\n          // payload\n          int position = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n\n            final int payloadLength;\n            final BytesRef thisPayload;\n\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();\n\n              if (payload == null) {\n                payload = new BytesRef();\n                payload.bytes = new byte[payloadLength];\n              } else if (payload.bytes.length < payloadLength) {\n                payload.grow(payloadLength);\n              }\n\n              prox.readBytes(payload.bytes, 0, payloadLength);\n              payload.length = payloadLength;\n              thisPayload = payload;\n\n            } else {\n              payloadLength = 0;\n              thisPayload = null;\n            }\n\n            postingsConsumer.addPosition(position, thisPayload);\n          }\n\n          postingsConsumer.finishDoc();\n        }\n      }\n      termsConsumer.finishTerm(text, numDocs);\n    }\n\n    termsConsumer.finish();\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    final boolean currentFieldOmitTermFreqAndPositions = fieldInfo.omitTermFreqAndPositions;\n    \n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.numPostings;\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    \n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n      \n      termsHashPerField.initReader(freq, termID, 0);\n      if (!fieldInfo.omitTermFreqAndPositions) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n  \n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n    \n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n  \n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      int docID = 0;\n      int termFreq = 0;\n      \n      while(true) {\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (!omitTermFreqAndPositions) {\n              termFreq = postings.docFreqs[termID];\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (omitTermFreqAndPositions) {\n            docID += code;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n    \n          assert docID != postings.lastDocIDs[termID];\n        }\n        \n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n        final int termDocFreq = termFreq;\n        postingsConsumer.startDoc(docID, termDocFreq);\n    \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        if (!currentFieldOmitTermFreqAndPositions) {\n          // omitTermFreqAndPositions == false so we do write positions &\n          // payload          \n          int position = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n    \n            final int payloadLength;\n            final BytesRef thisPayload;\n    \n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();  \n              \n              if (payload == null) {\n                payload = new BytesRef();\n                payload.bytes = new byte[payloadLength];\n              } else if (payload.bytes.length < payloadLength) {\n                payload.grow(payloadLength);\n              }\n    \n              prox.readBytes(payload.bytes, 0, payloadLength);\n              payload.length = payloadLength;\n              thisPayload = payload;\n    \n            } else {\n              payloadLength = 0;\n              thisPayload = null;\n            }\n    \n            postingsConsumer.addPosition(position, thisPayload);\n          } \n    \n          postingsConsumer.finishDoc();\n        }\n      } \n      termsConsumer.finishTerm(text, numDocs);\n    }\n  \n    termsConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"16843358872ed92ba92888ab99df297550b9a36a","date":1295144724,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    final boolean currentFieldOmitTermFreqAndPositions = fieldInfo.omitTermFreqAndPositions;\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    long sumTotalTermFreq = 0;\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (!fieldInfo.omitTermFreqAndPositions) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n      int termFreq = 0;\n\n      while(true) {\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (!omitTermFreqAndPositions) {\n              termFreq = postings.docFreqs[termID];\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (omitTermFreqAndPositions) {\n            docID += code;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n        final int termDocFreq = termFreq;\n        postingsConsumer.startDoc(docID, termDocFreq);\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        if (!currentFieldOmitTermFreqAndPositions) {\n          // omitTermFreqAndPositions == false so we do write positions &\n          // payload\n          int position = 0;\n          totTF += termDocFreq;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n\n            final int payloadLength;\n            final BytesRef thisPayload;\n\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();\n\n              if (payload == null) {\n                payload = new BytesRef();\n                payload.bytes = new byte[payloadLength];\n              } else if (payload.bytes.length < payloadLength) {\n                payload.grow(payloadLength);\n              }\n\n              prox.readBytes(payload.bytes, 0, payloadLength);\n              payload.length = payloadLength;\n              thisPayload = payload;\n\n            } else {\n              payloadLength = 0;\n              thisPayload = null;\n            }\n\n            postingsConsumer.addPosition(position, thisPayload);\n          }\n\n          postingsConsumer.finishDoc();\n        }\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq);\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    final boolean currentFieldOmitTermFreqAndPositions = fieldInfo.omitTermFreqAndPositions;\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (!fieldInfo.omitTermFreqAndPositions) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      int docID = 0;\n      int termFreq = 0;\n\n      while(true) {\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (!omitTermFreqAndPositions) {\n              termFreq = postings.docFreqs[termID];\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (omitTermFreqAndPositions) {\n            docID += code;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n        final int termDocFreq = termFreq;\n        postingsConsumer.startDoc(docID, termDocFreq);\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        if (!currentFieldOmitTermFreqAndPositions) {\n          // omitTermFreqAndPositions == false so we do write positions &\n          // payload\n          int position = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n\n            final int payloadLength;\n            final BytesRef thisPayload;\n\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();\n\n              if (payload == null) {\n                payload = new BytesRef();\n                payload.bytes = new byte[payloadLength];\n              } else if (payload.bytes.length < payloadLength) {\n                payload.grow(payloadLength);\n              }\n\n              prox.readBytes(payload.bytes, 0, payloadLength);\n              payload.length = payloadLength;\n              thisPayload = payload;\n\n            } else {\n              payloadLength = 0;\n              thisPayload = null;\n            }\n\n            postingsConsumer.addPosition(position, thisPayload);\n          }\n\n          postingsConsumer.finishDoc();\n        }\n      }\n      termsConsumer.finishTerm(text, numDocs);\n    }\n\n    termsConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bde51b089eb7f86171eb3406e38a274743f9b7ac","date":1298336439,"type":5,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    final Term protoTerm = new Term(fieldName);\n\n    final boolean currentFieldOmitTermFreqAndPositions = fieldInfo.omitTermFreqAndPositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    long sumTotalTermFreq = 0;\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (!fieldInfo.omitTermFreqAndPositions) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(protoTerm.createTerm(text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n      int termFreq = 0;\n\n      while(true) {\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (!omitTermFreqAndPositions) {\n              termFreq = postings.docFreqs[termID];\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (omitTermFreqAndPositions) {\n            docID += code;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n        final int termDocFreq = termFreq;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          if (state.deletedDocs == null) {\n            state.deletedDocs = new BitVector(state.numDocs);\n          }\n          state.deletedDocs.set(docID);\n        }\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        if (!currentFieldOmitTermFreqAndPositions) {\n          // omitTermFreqAndPositions == false so we do write positions &\n          // payload\n          int position = 0;\n          totTF += termDocFreq;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n\n            final int payloadLength;\n            final BytesRef thisPayload;\n\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();\n\n              if (payload == null) {\n                payload = new BytesRef();\n                payload.bytes = new byte[payloadLength];\n              } else if (payload.bytes.length < payloadLength) {\n                payload.grow(payloadLength);\n              }\n\n              prox.readBytes(payload.bytes, 0, payloadLength);\n              payload.length = payloadLength;\n              thisPayload = payload;\n\n            } else {\n              payloadLength = 0;\n              thisPayload = null;\n            }\n\n            postingsConsumer.addPosition(position, thisPayload);\n          }\n\n          postingsConsumer.finishDoc();\n        }\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq);\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    final boolean currentFieldOmitTermFreqAndPositions = fieldInfo.omitTermFreqAndPositions;\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    long sumTotalTermFreq = 0;\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (!fieldInfo.omitTermFreqAndPositions) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n      int termFreq = 0;\n\n      while(true) {\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (!omitTermFreqAndPositions) {\n              termFreq = postings.docFreqs[termID];\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (omitTermFreqAndPositions) {\n            docID += code;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n        final int termDocFreq = termFreq;\n        postingsConsumer.startDoc(docID, termDocFreq);\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        if (!currentFieldOmitTermFreqAndPositions) {\n          // omitTermFreqAndPositions == false so we do write positions &\n          // payload\n          int position = 0;\n          totTF += termDocFreq;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n\n            final int payloadLength;\n            final BytesRef thisPayload;\n\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();\n\n              if (payload == null) {\n                payload = new BytesRef();\n                payload.bytes = new byte[payloadLength];\n              } else if (payload.bytes.length < payloadLength) {\n                payload.grow(payloadLength);\n              }\n\n              prox.readBytes(payload.bytes, 0, payloadLength);\n              payload.length = payloadLength;\n              thisPayload = payload;\n\n            } else {\n              payloadLength = 0;\n              thisPayload = null;\n            }\n\n            postingsConsumer.addPosition(position, thisPayload);\n          }\n\n          postingsConsumer.finishDoc();\n        }\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq);\n  }\n\n","bugFix":null,"bugIntro":["4d3e8520fd031bab31fd0e4d480e55958bc45efe","4d3e8520fd031bab31fd0e4d480e55958bc45efe","29e23e367cc757f42cdfce2bcbf21e68cd209cda","29e23e367cc757f42cdfce2bcbf21e68cd209cda","2c6f93d9250f0d2c776d11fa7fa6ac4ce921052e"],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"6c18273ea5b3974d2f30117f46f1ae416c28f727":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"16843358872ed92ba92888ab99df297550b9a36a":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["6c18273ea5b3974d2f30117f46f1ae416c28f727","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["16843358872ed92ba92888ab99df297550b9a36a","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"6c18273ea5b3974d2f30117f46f1ae416c28f727":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["6c18273ea5b3974d2f30117f46f1ae416c28f727","16843358872ed92ba92888ab99df297550b9a36a","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","bde51b089eb7f86171eb3406e38a274743f9b7ac","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"16843358872ed92ba92888ab99df297550b9a36a":["bde51b089eb7f86171eb3406e38a274743f9b7ac"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["16843358872ed92ba92888ab99df297550b9a36a"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["bde51b089eb7f86171eb3406e38a274743f9b7ac","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}