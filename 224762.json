{"path":"solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkAvroTest#test().mjava","commits":[{"id":"abb23fcc2461782ab204e61213240feb77d355aa","date":1422029612,"type":1,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkAvroTest#test().mjava","pathOld":"solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkAvroTest#doTest().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    Joiner joiner = Joiner.on(File.separator);\n    File file = new File(joiner.join(RESOURCES_DIR, \"test-documents\", \"sample-statuses-20120906-141433-medium.avro\"));\n    \n    waitForRecoveriesToFinish(false);\n    \n    // load avro records via morphline and zk into solr\n    morphline = parse(\"test-morphlines\" + File.separator + \"tutorialReadAvroContainer\");    \n    Record record = new Record();\n    byte[] body = Files.toByteArray(file);    \n    record.put(Fields.ATTACHMENT_BODY, body);\n    startSession();\n    Notifications.notifyBeginTransaction(morphline);\n    assertTrue(morphline.process(record));\n    assertEquals(1, collector.getNumStartEvents());\n    \n    commit();\n    \n    // fetch sorted result set from solr\n    QueryResponse rsp = cloudClient.query(new SolrQuery(\"*:*\").setRows(100000).addSort(\"id\", SolrQuery.ORDER.asc));   \n    assertEquals(2104, collector.getRecords().size());\n    assertEquals(collector.getRecords().size(), rsp.getResults().size());\n    \n    Collections.sort(collector.getRecords(), new Comparator<Record>() {\n      @Override\n      public int compare(Record r1, Record r2) {\n        return r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString());\n      }      \n    });   \n\n    // fetch test input data and sort like solr result set\n    List<GenericData.Record> records = new ArrayList();\n    FileReader<GenericData.Record> reader = new DataFileReader(file, new GenericDatumReader());\n    while (reader.hasNext()) {\n      GenericData.Record expected = reader.next();\n      records.add(expected);\n    }\n    assertEquals(collector.getRecords().size(), records.size());    \n    Collections.sort(records, new Comparator<GenericData.Record>() {\n      @Override\n      public int compare(GenericData.Record r1, GenericData.Record r2) {\n        return r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString());\n      }      \n    });   \n    \n    Object lastId = null;\n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrec\" + i + \":\" + records.get(i));      \n      Object id = records.get(i);\n      if (id != null && id.equals(lastId)) {\n        throw new IllegalStateException(\"Detected duplicate id. Test input data must not contain duplicate ids!\");        \n      }\n      lastId = id;\n    }\n    \n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrsp\" + i + \":\" + rsp.getResults().get(i));      \n    }    \n\n    Iterator<SolrDocument> rspIter = rsp.getResults().iterator();\n    for (int i = 0; i < records.size(); i++) {  \n      // verify morphline spat out expected data\n      Record actual = collector.getRecords().get(i);\n      GenericData.Record expected = records.get(i);\n      Preconditions.checkNotNull(expected);\n      assertTweetEquals(expected, actual, i);\n      \n      // verify Solr result set contains expected data\n      actual = new Record();\n      actual.getFields().putAll(next(rspIter));\n      assertTweetEquals(expected, actual, i);\n    }\n    \n    Notifications.notifyRollbackTransaction(morphline);\n    Notifications.notifyShutdown(morphline);\n    cloudClient.shutdown();\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    Joiner joiner = Joiner.on(File.separator);\n    File file = new File(joiner.join(RESOURCES_DIR, \"test-documents\", \"sample-statuses-20120906-141433-medium.avro\"));\n    \n    waitForRecoveriesToFinish(false);\n    \n    // load avro records via morphline and zk into solr\n    morphline = parse(\"test-morphlines\" + File.separator + \"tutorialReadAvroContainer\");    \n    Record record = new Record();\n    byte[] body = Files.toByteArray(file);    \n    record.put(Fields.ATTACHMENT_BODY, body);\n    startSession();\n    Notifications.notifyBeginTransaction(morphline);\n    assertTrue(morphline.process(record));\n    assertEquals(1, collector.getNumStartEvents());\n    \n    commit();\n    \n    // fetch sorted result set from solr\n    QueryResponse rsp = cloudClient.query(new SolrQuery(\"*:*\").setRows(100000).addSort(\"id\", SolrQuery.ORDER.asc));   \n    assertEquals(2104, collector.getRecords().size());\n    assertEquals(collector.getRecords().size(), rsp.getResults().size());\n    \n    Collections.sort(collector.getRecords(), new Comparator<Record>() {\n      @Override\n      public int compare(Record r1, Record r2) {\n        return r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString());\n      }      \n    });   \n\n    // fetch test input data and sort like solr result set\n    List<GenericData.Record> records = new ArrayList();\n    FileReader<GenericData.Record> reader = new DataFileReader(file, new GenericDatumReader());\n    while (reader.hasNext()) {\n      GenericData.Record expected = reader.next();\n      records.add(expected);\n    }\n    assertEquals(collector.getRecords().size(), records.size());    \n    Collections.sort(records, new Comparator<GenericData.Record>() {\n      @Override\n      public int compare(GenericData.Record r1, GenericData.Record r2) {\n        return r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString());\n      }      \n    });   \n    \n    Object lastId = null;\n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrec\" + i + \":\" + records.get(i));      \n      Object id = records.get(i);\n      if (id != null && id.equals(lastId)) {\n        throw new IllegalStateException(\"Detected duplicate id. Test input data must not contain duplicate ids!\");        \n      }\n      lastId = id;\n    }\n    \n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrsp\" + i + \":\" + rsp.getResults().get(i));      \n    }    \n\n    Iterator<SolrDocument> rspIter = rsp.getResults().iterator();\n    for (int i = 0; i < records.size(); i++) {  \n      // verify morphline spat out expected data\n      Record actual = collector.getRecords().get(i);\n      GenericData.Record expected = records.get(i);\n      Preconditions.checkNotNull(expected);\n      assertTweetEquals(expected, actual, i);\n      \n      // verify Solr result set contains expected data\n      actual = new Record();\n      actual.getFields().putAll(next(rspIter));\n      assertTweetEquals(expected, actual, i);\n    }\n    \n    Notifications.notifyRollbackTransaction(morphline);\n    Notifications.notifyShutdown(morphline);\n    cloudClient.shutdown();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cc3b13b430571c2e169f98fe38e1e7666f88522d","date":1422446157,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkAvroTest#test().mjava","pathOld":"solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkAvroTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    Joiner joiner = Joiner.on(File.separator);\n    File file = new File(joiner.join(RESOURCES_DIR, \"test-documents\", \"sample-statuses-20120906-141433-medium.avro\"));\n    \n    waitForRecoveriesToFinish(false);\n    \n    // load avro records via morphline and zk into solr\n    morphline = parse(\"test-morphlines\" + File.separator + \"tutorialReadAvroContainer\");    \n    Record record = new Record();\n    byte[] body = Files.toByteArray(file);    \n    record.put(Fields.ATTACHMENT_BODY, body);\n    startSession();\n    Notifications.notifyBeginTransaction(morphline);\n    assertTrue(morphline.process(record));\n    assertEquals(1, collector.getNumStartEvents());\n    \n    commit();\n    \n    // fetch sorted result set from solr\n    QueryResponse rsp = cloudClient.query(new SolrQuery(\"*:*\").setRows(100000).addSort(\"id\", SolrQuery.ORDER.asc));   \n    assertEquals(2104, collector.getRecords().size());\n    assertEquals(collector.getRecords().size(), rsp.getResults().size());\n    \n    Collections.sort(collector.getRecords(), new Comparator<Record>() {\n      @Override\n      public int compare(Record r1, Record r2) {\n        return r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString());\n      }      \n    });   \n\n    // fetch test input data and sort like solr result set\n    List<GenericData.Record> records = new ArrayList();\n    FileReader<GenericData.Record> reader = new DataFileReader(file, new GenericDatumReader());\n    while (reader.hasNext()) {\n      GenericData.Record expected = reader.next();\n      records.add(expected);\n    }\n    assertEquals(collector.getRecords().size(), records.size());    \n    Collections.sort(records, new Comparator<GenericData.Record>() {\n      @Override\n      public int compare(GenericData.Record r1, GenericData.Record r2) {\n        return r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString());\n      }      \n    });   \n    \n    Object lastId = null;\n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrec\" + i + \":\" + records.get(i));      \n      Object id = records.get(i);\n      if (id != null && id.equals(lastId)) {\n        throw new IllegalStateException(\"Detected duplicate id. Test input data must not contain duplicate ids!\");        \n      }\n      lastId = id;\n    }\n    \n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrsp\" + i + \":\" + rsp.getResults().get(i));      \n    }    \n\n    Iterator<SolrDocument> rspIter = rsp.getResults().iterator();\n    for (int i = 0; i < records.size(); i++) {  \n      // verify morphline spat out expected data\n      Record actual = collector.getRecords().get(i);\n      GenericData.Record expected = records.get(i);\n      Preconditions.checkNotNull(expected);\n      assertTweetEquals(expected, actual, i);\n      \n      // verify Solr result set contains expected data\n      actual = new Record();\n      actual.getFields().putAll(next(rspIter));\n      assertTweetEquals(expected, actual, i);\n    }\n    \n    Notifications.notifyRollbackTransaction(morphline);\n    Notifications.notifyShutdown(morphline);\n    cloudClient.close();\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    Joiner joiner = Joiner.on(File.separator);\n    File file = new File(joiner.join(RESOURCES_DIR, \"test-documents\", \"sample-statuses-20120906-141433-medium.avro\"));\n    \n    waitForRecoveriesToFinish(false);\n    \n    // load avro records via morphline and zk into solr\n    morphline = parse(\"test-morphlines\" + File.separator + \"tutorialReadAvroContainer\");    \n    Record record = new Record();\n    byte[] body = Files.toByteArray(file);    \n    record.put(Fields.ATTACHMENT_BODY, body);\n    startSession();\n    Notifications.notifyBeginTransaction(morphline);\n    assertTrue(morphline.process(record));\n    assertEquals(1, collector.getNumStartEvents());\n    \n    commit();\n    \n    // fetch sorted result set from solr\n    QueryResponse rsp = cloudClient.query(new SolrQuery(\"*:*\").setRows(100000).addSort(\"id\", SolrQuery.ORDER.asc));   \n    assertEquals(2104, collector.getRecords().size());\n    assertEquals(collector.getRecords().size(), rsp.getResults().size());\n    \n    Collections.sort(collector.getRecords(), new Comparator<Record>() {\n      @Override\n      public int compare(Record r1, Record r2) {\n        return r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString());\n      }      \n    });   \n\n    // fetch test input data and sort like solr result set\n    List<GenericData.Record> records = new ArrayList();\n    FileReader<GenericData.Record> reader = new DataFileReader(file, new GenericDatumReader());\n    while (reader.hasNext()) {\n      GenericData.Record expected = reader.next();\n      records.add(expected);\n    }\n    assertEquals(collector.getRecords().size(), records.size());    \n    Collections.sort(records, new Comparator<GenericData.Record>() {\n      @Override\n      public int compare(GenericData.Record r1, GenericData.Record r2) {\n        return r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString());\n      }      \n    });   \n    \n    Object lastId = null;\n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrec\" + i + \":\" + records.get(i));      \n      Object id = records.get(i);\n      if (id != null && id.equals(lastId)) {\n        throw new IllegalStateException(\"Detected duplicate id. Test input data must not contain duplicate ids!\");        \n      }\n      lastId = id;\n    }\n    \n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrsp\" + i + \":\" + rsp.getResults().get(i));      \n    }    \n\n    Iterator<SolrDocument> rspIter = rsp.getResults().iterator();\n    for (int i = 0; i < records.size(); i++) {  \n      // verify morphline spat out expected data\n      Record actual = collector.getRecords().get(i);\n      GenericData.Record expected = records.get(i);\n      Preconditions.checkNotNull(expected);\n      assertTweetEquals(expected, actual, i);\n      \n      // verify Solr result set contains expected data\n      actual = new Record();\n      actual.getFields().putAll(next(rspIter));\n      assertTweetEquals(expected, actual, i);\n    }\n    \n    Notifications.notifyRollbackTransaction(morphline);\n    Notifications.notifyShutdown(morphline);\n    cloudClient.shutdown();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"043df2e9a841864922c32756a44c939ed768cb89","date":1459876536,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkAvroTest#test().mjava","pathOld":"solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkAvroTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    Joiner joiner = Joiner.on(File.separator);\n    File file = new File(joiner.join(RESOURCES_DIR, \"test-documents\", \"sample-statuses-20120906-141433-medium.avro\"));\n    \n    waitForRecoveriesToFinish(false);\n    \n    // load avro records via morphline and zk into solr\n    morphline = parse(\"test-morphlines\" + File.separator + \"tutorialReadAvroContainer\");    \n    Record record = new Record();\n    byte[] body = Files.toByteArray(file);    \n    record.put(Fields.ATTACHMENT_BODY, body);\n    startSession();\n    Notifications.notifyBeginTransaction(morphline);\n    assertTrue(morphline.process(record));\n    assertEquals(1, collector.getNumStartEvents());\n    \n    commit();\n    \n    // fetch sorted result set from solr\n    QueryResponse rsp = cloudClient.query(new SolrQuery(\"*:*\").setRows(100000).addSort(\"id\", SolrQuery.ORDER.asc));   \n    assertEquals(2104, collector.getRecords().size());\n    assertEquals(collector.getRecords().size(), rsp.getResults().size());\n    \n    Collections.sort(collector.getRecords(), (r1, r2) -> r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString()));\n\n    // fetch test input data and sort like solr result set\n    List<GenericData.Record> records = new ArrayList();\n    FileReader<GenericData.Record> reader = new DataFileReader(file, new GenericDatumReader());\n    while (reader.hasNext()) {\n      GenericData.Record expected = reader.next();\n      records.add(expected);\n    }\n    assertEquals(collector.getRecords().size(), records.size());    \n    Collections.sort(records, (r1, r2) -> r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString()));\n    \n    Object lastId = null;\n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrec\" + i + \":\" + records.get(i));      \n      Object id = records.get(i);\n      if (id != null && id.equals(lastId)) {\n        throw new IllegalStateException(\"Detected duplicate id. Test input data must not contain duplicate ids!\");        \n      }\n      lastId = id;\n    }\n    \n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrsp\" + i + \":\" + rsp.getResults().get(i));      \n    }    \n\n    Iterator<SolrDocument> rspIter = rsp.getResults().iterator();\n    for (int i = 0; i < records.size(); i++) {  \n      // verify morphline spat out expected data\n      Record actual = collector.getRecords().get(i);\n      GenericData.Record expected = records.get(i);\n      Preconditions.checkNotNull(expected);\n      assertTweetEquals(expected, actual, i);\n      \n      // verify Solr result set contains expected data\n      actual = new Record();\n      actual.getFields().putAll(next(rspIter));\n      assertTweetEquals(expected, actual, i);\n    }\n    \n    Notifications.notifyRollbackTransaction(morphline);\n    Notifications.notifyShutdown(morphline);\n    cloudClient.close();\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    Joiner joiner = Joiner.on(File.separator);\n    File file = new File(joiner.join(RESOURCES_DIR, \"test-documents\", \"sample-statuses-20120906-141433-medium.avro\"));\n    \n    waitForRecoveriesToFinish(false);\n    \n    // load avro records via morphline and zk into solr\n    morphline = parse(\"test-morphlines\" + File.separator + \"tutorialReadAvroContainer\");    \n    Record record = new Record();\n    byte[] body = Files.toByteArray(file);    \n    record.put(Fields.ATTACHMENT_BODY, body);\n    startSession();\n    Notifications.notifyBeginTransaction(morphline);\n    assertTrue(morphline.process(record));\n    assertEquals(1, collector.getNumStartEvents());\n    \n    commit();\n    \n    // fetch sorted result set from solr\n    QueryResponse rsp = cloudClient.query(new SolrQuery(\"*:*\").setRows(100000).addSort(\"id\", SolrQuery.ORDER.asc));   \n    assertEquals(2104, collector.getRecords().size());\n    assertEquals(collector.getRecords().size(), rsp.getResults().size());\n    \n    Collections.sort(collector.getRecords(), new Comparator<Record>() {\n      @Override\n      public int compare(Record r1, Record r2) {\n        return r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString());\n      }      \n    });   \n\n    // fetch test input data and sort like solr result set\n    List<GenericData.Record> records = new ArrayList();\n    FileReader<GenericData.Record> reader = new DataFileReader(file, new GenericDatumReader());\n    while (reader.hasNext()) {\n      GenericData.Record expected = reader.next();\n      records.add(expected);\n    }\n    assertEquals(collector.getRecords().size(), records.size());    \n    Collections.sort(records, new Comparator<GenericData.Record>() {\n      @Override\n      public int compare(GenericData.Record r1, GenericData.Record r2) {\n        return r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString());\n      }      \n    });   \n    \n    Object lastId = null;\n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrec\" + i + \":\" + records.get(i));      \n      Object id = records.get(i);\n      if (id != null && id.equals(lastId)) {\n        throw new IllegalStateException(\"Detected duplicate id. Test input data must not contain duplicate ids!\");        \n      }\n      lastId = id;\n    }\n    \n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrsp\" + i + \":\" + rsp.getResults().get(i));      \n    }    \n\n    Iterator<SolrDocument> rspIter = rsp.getResults().iterator();\n    for (int i = 0; i < records.size(); i++) {  \n      // verify morphline spat out expected data\n      Record actual = collector.getRecords().get(i);\n      GenericData.Record expected = records.get(i);\n      Preconditions.checkNotNull(expected);\n      assertTweetEquals(expected, actual, i);\n      \n      // verify Solr result set contains expected data\n      actual = new Record();\n      actual.getFields().putAll(next(rspIter));\n      assertTweetEquals(expected, actual, i);\n    }\n    \n    Notifications.notifyRollbackTransaction(morphline);\n    Notifications.notifyShutdown(morphline);\n    cloudClient.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b6284684320a9808c41a5e43de958b2da22f89bd","date":1459977490,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkAvroTest#test().mjava","pathOld":"solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkAvroTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    Joiner joiner = Joiner.on(File.separator);\n    File file = new File(joiner.join(RESOURCES_DIR, \"test-documents\", \"sample-statuses-20120906-141433-medium.avro\"));\n    \n    waitForRecoveriesToFinish(false);\n    \n    // load avro records via morphline and zk into solr\n    morphline = parse(\"test-morphlines\" + File.separator + \"tutorialReadAvroContainer\");    \n    Record record = new Record();\n    byte[] body = Files.toByteArray(file);    \n    record.put(Fields.ATTACHMENT_BODY, body);\n    startSession();\n    Notifications.notifyBeginTransaction(morphline);\n    assertTrue(morphline.process(record));\n    assertEquals(1, collector.getNumStartEvents());\n    \n    commit();\n    \n    // fetch sorted result set from solr\n    QueryResponse rsp = cloudClient.query(new SolrQuery(\"*:*\").setRows(100000).addSort(\"id\", SolrQuery.ORDER.asc));   \n    assertEquals(2104, collector.getRecords().size());\n    assertEquals(collector.getRecords().size(), rsp.getResults().size());\n    \n    Collections.sort(collector.getRecords(), (r1, r2) -> r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString()));\n\n    // fetch test input data and sort like solr result set\n    List<GenericData.Record> records = new ArrayList();\n    FileReader<GenericData.Record> reader = new DataFileReader(file, new GenericDatumReader());\n    while (reader.hasNext()) {\n      GenericData.Record expected = reader.next();\n      records.add(expected);\n    }\n    assertEquals(collector.getRecords().size(), records.size());    \n    Collections.sort(records, (r1, r2) -> r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString()));\n    \n    Object lastId = null;\n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrec\" + i + \":\" + records.get(i));      \n      Object id = records.get(i);\n      if (id != null && id.equals(lastId)) {\n        throw new IllegalStateException(\"Detected duplicate id. Test input data must not contain duplicate ids!\");        \n      }\n      lastId = id;\n    }\n    \n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrsp\" + i + \":\" + rsp.getResults().get(i));      \n    }    \n\n    Iterator<SolrDocument> rspIter = rsp.getResults().iterator();\n    for (int i = 0; i < records.size(); i++) {  \n      // verify morphline spat out expected data\n      Record actual = collector.getRecords().get(i);\n      GenericData.Record expected = records.get(i);\n      Preconditions.checkNotNull(expected);\n      assertTweetEquals(expected, actual, i);\n      \n      // verify Solr result set contains expected data\n      actual = new Record();\n      actual.getFields().putAll(next(rspIter));\n      assertTweetEquals(expected, actual, i);\n    }\n    \n    Notifications.notifyRollbackTransaction(morphline);\n    Notifications.notifyShutdown(morphline);\n    cloudClient.close();\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    Joiner joiner = Joiner.on(File.separator);\n    File file = new File(joiner.join(RESOURCES_DIR, \"test-documents\", \"sample-statuses-20120906-141433-medium.avro\"));\n    \n    waitForRecoveriesToFinish(false);\n    \n    // load avro records via morphline and zk into solr\n    morphline = parse(\"test-morphlines\" + File.separator + \"tutorialReadAvroContainer\");    \n    Record record = new Record();\n    byte[] body = Files.toByteArray(file);    \n    record.put(Fields.ATTACHMENT_BODY, body);\n    startSession();\n    Notifications.notifyBeginTransaction(morphline);\n    assertTrue(morphline.process(record));\n    assertEquals(1, collector.getNumStartEvents());\n    \n    commit();\n    \n    // fetch sorted result set from solr\n    QueryResponse rsp = cloudClient.query(new SolrQuery(\"*:*\").setRows(100000).addSort(\"id\", SolrQuery.ORDER.asc));   \n    assertEquals(2104, collector.getRecords().size());\n    assertEquals(collector.getRecords().size(), rsp.getResults().size());\n    \n    Collections.sort(collector.getRecords(), new Comparator<Record>() {\n      @Override\n      public int compare(Record r1, Record r2) {\n        return r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString());\n      }      \n    });   \n\n    // fetch test input data and sort like solr result set\n    List<GenericData.Record> records = new ArrayList();\n    FileReader<GenericData.Record> reader = new DataFileReader(file, new GenericDatumReader());\n    while (reader.hasNext()) {\n      GenericData.Record expected = reader.next();\n      records.add(expected);\n    }\n    assertEquals(collector.getRecords().size(), records.size());    \n    Collections.sort(records, new Comparator<GenericData.Record>() {\n      @Override\n      public int compare(GenericData.Record r1, GenericData.Record r2) {\n        return r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString());\n      }      \n    });   \n    \n    Object lastId = null;\n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrec\" + i + \":\" + records.get(i));      \n      Object id = records.get(i);\n      if (id != null && id.equals(lastId)) {\n        throw new IllegalStateException(\"Detected duplicate id. Test input data must not contain duplicate ids!\");        \n      }\n      lastId = id;\n    }\n    \n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrsp\" + i + \":\" + rsp.getResults().get(i));      \n    }    \n\n    Iterator<SolrDocument> rspIter = rsp.getResults().iterator();\n    for (int i = 0; i < records.size(); i++) {  \n      // verify morphline spat out expected data\n      Record actual = collector.getRecords().get(i);\n      GenericData.Record expected = records.get(i);\n      Preconditions.checkNotNull(expected);\n      assertTweetEquals(expected, actual, i);\n      \n      // verify Solr result set contains expected data\n      actual = new Record();\n      actual.getFields().putAll(next(rspIter));\n      assertTweetEquals(expected, actual, i);\n    }\n    \n    Notifications.notifyRollbackTransaction(morphline);\n    Notifications.notifyShutdown(morphline);\n    cloudClient.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"71f7220f5dc230b647fdb53a465d5abf894c4d1e","date":1463146747,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkAvroTest#test().mjava","pathOld":"solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkAvroTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n\n    Path avro = Paths.get(RESOURCES_DIR).resolve(\"test-documents\").resolve(\"sample-statuses-20120906-141433-medium.avro\");\n\n    // load avro records via morphline and zk into solr\n    morphline = parse(\"test-morphlines\" + File.separator + \"tutorialReadAvroContainer\");    \n    Record record = new Record();\n    byte[] body = Files.readAllBytes(avro);\n    record.put(Fields.ATTACHMENT_BODY, body);\n    startSession();\n    Notifications.notifyBeginTransaction(morphline);\n    assertTrue(morphline.process(record));\n    assertEquals(1, collector.getNumStartEvents());\n    \n    Notifications.notifyCommitTransaction(morphline);\n    new UpdateRequest().commit(cluster.getSolrClient(), COLLECTION);\n    \n    // fetch sorted result set from solr\n    QueryResponse rsp = cluster.getSolrClient()\n        .query(COLLECTION, new SolrQuery(\"*:*\").setRows(100000).addSort(\"id\", SolrQuery.ORDER.asc));\n    assertEquals(2104, collector.getRecords().size());\n    assertEquals(collector.getRecords().size(), rsp.getResults().size());\n    \n    Collections.sort(collector.getRecords(), (r1, r2) -> r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString()));\n\n    // fetch test input data and sort like solr result set\n    List<GenericData.Record> records = new ArrayList<>();\n    FileReader<GenericData.Record> reader = new DataFileReader(avro.toFile(), new GenericDatumReader());\n    while (reader.hasNext()) {\n      GenericData.Record expected = reader.next();\n      records.add(expected);\n    }\n    assertEquals(collector.getRecords().size(), records.size());    \n    Collections.sort(records, (r1, r2) -> r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString()));\n    \n    Object lastId = null;\n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrec\" + i + \":\" + records.get(i));      \n      Object id = records.get(i);\n      if (id != null && id.equals(lastId)) {\n        throw new IllegalStateException(\"Detected duplicate id. Test input data must not contain duplicate ids!\");        \n      }\n      lastId = id;\n    }\n    \n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrsp\" + i + \":\" + rsp.getResults().get(i));      \n    }    \n\n    Iterator<SolrDocument> rspIter = rsp.getResults().iterator();\n    for (int i = 0; i < records.size(); i++) {  \n      // verify morphline spat out expected data\n      Record actual = collector.getRecords().get(i);\n      GenericData.Record expected = records.get(i);\n      Preconditions.checkNotNull(expected);\n      assertTweetEquals(expected, actual, i);\n      \n      // verify Solr result set contains expected data\n      actual = new Record();\n      actual.getFields().putAll(next(rspIter));\n      assertTweetEquals(expected, actual, i);\n    }\n    \n    Notifications.notifyRollbackTransaction(morphline);\n    Notifications.notifyShutdown(morphline);\n\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    Joiner joiner = Joiner.on(File.separator);\n    File file = new File(joiner.join(RESOURCES_DIR, \"test-documents\", \"sample-statuses-20120906-141433-medium.avro\"));\n    \n    waitForRecoveriesToFinish(false);\n    \n    // load avro records via morphline and zk into solr\n    morphline = parse(\"test-morphlines\" + File.separator + \"tutorialReadAvroContainer\");    \n    Record record = new Record();\n    byte[] body = Files.toByteArray(file);    \n    record.put(Fields.ATTACHMENT_BODY, body);\n    startSession();\n    Notifications.notifyBeginTransaction(morphline);\n    assertTrue(morphline.process(record));\n    assertEquals(1, collector.getNumStartEvents());\n    \n    commit();\n    \n    // fetch sorted result set from solr\n    QueryResponse rsp = cloudClient.query(new SolrQuery(\"*:*\").setRows(100000).addSort(\"id\", SolrQuery.ORDER.asc));   \n    assertEquals(2104, collector.getRecords().size());\n    assertEquals(collector.getRecords().size(), rsp.getResults().size());\n    \n    Collections.sort(collector.getRecords(), (r1, r2) -> r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString()));\n\n    // fetch test input data and sort like solr result set\n    List<GenericData.Record> records = new ArrayList();\n    FileReader<GenericData.Record> reader = new DataFileReader(file, new GenericDatumReader());\n    while (reader.hasNext()) {\n      GenericData.Record expected = reader.next();\n      records.add(expected);\n    }\n    assertEquals(collector.getRecords().size(), records.size());    \n    Collections.sort(records, (r1, r2) -> r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString()));\n    \n    Object lastId = null;\n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrec\" + i + \":\" + records.get(i));      \n      Object id = records.get(i);\n      if (id != null && id.equals(lastId)) {\n        throw new IllegalStateException(\"Detected duplicate id. Test input data must not contain duplicate ids!\");        \n      }\n      lastId = id;\n    }\n    \n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrsp\" + i + \":\" + rsp.getResults().get(i));      \n    }    \n\n    Iterator<SolrDocument> rspIter = rsp.getResults().iterator();\n    for (int i = 0; i < records.size(); i++) {  \n      // verify morphline spat out expected data\n      Record actual = collector.getRecords().get(i);\n      GenericData.Record expected = records.get(i);\n      Preconditions.checkNotNull(expected);\n      assertTweetEquals(expected, actual, i);\n      \n      // verify Solr result set contains expected data\n      actual = new Record();\n      actual.getFields().putAll(next(rspIter));\n      assertTweetEquals(expected, actual, i);\n    }\n    \n    Notifications.notifyRollbackTransaction(morphline);\n    Notifications.notifyShutdown(morphline);\n    cloudClient.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0ad30c6a479e764150a3316e57263319775f1df2","date":1463395403,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkAvroTest#test().mjava","pathOld":"solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkAvroTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n\n    Path avro = Paths.get(RESOURCES_DIR).resolve(\"test-documents\").resolve(\"sample-statuses-20120906-141433-medium.avro\");\n\n    // load avro records via morphline and zk into solr\n    morphline = parse(\"test-morphlines\" + File.separator + \"tutorialReadAvroContainer\");    \n    Record record = new Record();\n    byte[] body = Files.readAllBytes(avro);\n    record.put(Fields.ATTACHMENT_BODY, body);\n    startSession();\n    Notifications.notifyBeginTransaction(morphline);\n    assertTrue(morphline.process(record));\n    assertEquals(1, collector.getNumStartEvents());\n    \n    Notifications.notifyCommitTransaction(morphline);\n    new UpdateRequest().commit(cluster.getSolrClient(), COLLECTION);\n    \n    // fetch sorted result set from solr\n    QueryResponse rsp = cluster.getSolrClient()\n        .query(COLLECTION, new SolrQuery(\"*:*\").setRows(100000).addSort(\"id\", SolrQuery.ORDER.asc));\n    assertEquals(2104, collector.getRecords().size());\n    assertEquals(collector.getRecords().size(), rsp.getResults().size());\n    \n    Collections.sort(collector.getRecords(), (r1, r2) -> r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString()));\n\n    // fetch test input data and sort like solr result set\n    List<GenericData.Record> records = new ArrayList<>();\n    FileReader<GenericData.Record> reader = new DataFileReader(avro.toFile(), new GenericDatumReader());\n    while (reader.hasNext()) {\n      GenericData.Record expected = reader.next();\n      records.add(expected);\n    }\n    assertEquals(collector.getRecords().size(), records.size());    \n    Collections.sort(records, (r1, r2) -> r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString()));\n    \n    Object lastId = null;\n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrec\" + i + \":\" + records.get(i));      \n      Object id = records.get(i);\n      if (id != null && id.equals(lastId)) {\n        throw new IllegalStateException(\"Detected duplicate id. Test input data must not contain duplicate ids!\");        \n      }\n      lastId = id;\n    }\n    \n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrsp\" + i + \":\" + rsp.getResults().get(i));      \n    }    \n\n    Iterator<SolrDocument> rspIter = rsp.getResults().iterator();\n    for (int i = 0; i < records.size(); i++) {  \n      // verify morphline spat out expected data\n      Record actual = collector.getRecords().get(i);\n      GenericData.Record expected = records.get(i);\n      Preconditions.checkNotNull(expected);\n      assertTweetEquals(expected, actual, i);\n      \n      // verify Solr result set contains expected data\n      actual = new Record();\n      actual.getFields().putAll(next(rspIter));\n      assertTweetEquals(expected, actual, i);\n    }\n    \n    Notifications.notifyRollbackTransaction(morphline);\n    Notifications.notifyShutdown(morphline);\n\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    Joiner joiner = Joiner.on(File.separator);\n    File file = new File(joiner.join(RESOURCES_DIR, \"test-documents\", \"sample-statuses-20120906-141433-medium.avro\"));\n    \n    waitForRecoveriesToFinish(false);\n    \n    // load avro records via morphline and zk into solr\n    morphline = parse(\"test-morphlines\" + File.separator + \"tutorialReadAvroContainer\");    \n    Record record = new Record();\n    byte[] body = Files.toByteArray(file);    \n    record.put(Fields.ATTACHMENT_BODY, body);\n    startSession();\n    Notifications.notifyBeginTransaction(morphline);\n    assertTrue(morphline.process(record));\n    assertEquals(1, collector.getNumStartEvents());\n    \n    commit();\n    \n    // fetch sorted result set from solr\n    QueryResponse rsp = cloudClient.query(new SolrQuery(\"*:*\").setRows(100000).addSort(\"id\", SolrQuery.ORDER.asc));   \n    assertEquals(2104, collector.getRecords().size());\n    assertEquals(collector.getRecords().size(), rsp.getResults().size());\n    \n    Collections.sort(collector.getRecords(), (r1, r2) -> r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString()));\n\n    // fetch test input data and sort like solr result set\n    List<GenericData.Record> records = new ArrayList();\n    FileReader<GenericData.Record> reader = new DataFileReader(file, new GenericDatumReader());\n    while (reader.hasNext()) {\n      GenericData.Record expected = reader.next();\n      records.add(expected);\n    }\n    assertEquals(collector.getRecords().size(), records.size());    \n    Collections.sort(records, (r1, r2) -> r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString()));\n    \n    Object lastId = null;\n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrec\" + i + \":\" + records.get(i));      \n      Object id = records.get(i);\n      if (id != null && id.equals(lastId)) {\n        throw new IllegalStateException(\"Detected duplicate id. Test input data must not contain duplicate ids!\");        \n      }\n      lastId = id;\n    }\n    \n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrsp\" + i + \":\" + rsp.getResults().get(i));      \n    }    \n\n    Iterator<SolrDocument> rspIter = rsp.getResults().iterator();\n    for (int i = 0; i < records.size(); i++) {  \n      // verify morphline spat out expected data\n      Record actual = collector.getRecords().get(i);\n      GenericData.Record expected = records.get(i);\n      Preconditions.checkNotNull(expected);\n      assertTweetEquals(expected, actual, i);\n      \n      // verify Solr result set contains expected data\n      actual = new Record();\n      actual.getFields().putAll(next(rspIter));\n      assertTweetEquals(expected, actual, i);\n    }\n    \n    Notifications.notifyRollbackTransaction(morphline);\n    Notifications.notifyShutdown(morphline);\n    cloudClient.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d470c8182e92b264680e34081b75e70a9f2b3c89","date":1463985353,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkAvroTest#test().mjava","pathOld":"solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkAvroTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n\n    Path avro = Paths.get(RESOURCES_DIR).resolve(\"test-documents\").resolve(\"sample-statuses-20120906-141433-medium.avro\");\n\n    // load avro records via morphline and zk into solr\n    morphline = parse(\"test-morphlines\" + File.separator + \"tutorialReadAvroContainer\");    \n    Record record = new Record();\n    byte[] body = Files.readAllBytes(avro);\n    record.put(Fields.ATTACHMENT_BODY, body);\n    startSession();\n    Notifications.notifyBeginTransaction(morphline);\n    assertTrue(morphline.process(record));\n    assertEquals(1, collector.getNumStartEvents());\n    \n    Notifications.notifyCommitTransaction(morphline);\n    new UpdateRequest().commit(cluster.getSolrClient(), COLLECTION);\n    \n    // fetch sorted result set from solr\n    QueryResponse rsp = cluster.getSolrClient()\n        .query(COLLECTION, new SolrQuery(\"*:*\").setRows(100000).addSort(\"id\", SolrQuery.ORDER.asc));\n    assertEquals(2104, collector.getRecords().size());\n    assertEquals(collector.getRecords().size(), rsp.getResults().size());\n    \n    Collections.sort(collector.getRecords(), (r1, r2) -> r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString()));\n\n    // fetch test input data and sort like solr result set\n    List<GenericData.Record> records = new ArrayList<>();\n    FileReader<GenericData.Record> reader = new DataFileReader(avro.toFile(), new GenericDatumReader());\n    while (reader.hasNext()) {\n      GenericData.Record expected = reader.next();\n      records.add(expected);\n    }\n    assertEquals(collector.getRecords().size(), records.size());    \n    Collections.sort(records, (r1, r2) -> r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString()));\n    \n    Object lastId = null;\n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrec\" + i + \":\" + records.get(i));      \n      Object id = records.get(i);\n      if (id != null && id.equals(lastId)) {\n        throw new IllegalStateException(\"Detected duplicate id. Test input data must not contain duplicate ids!\");        \n      }\n      lastId = id;\n    }\n    \n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrsp\" + i + \":\" + rsp.getResults().get(i));      \n    }    \n\n    Iterator<SolrDocument> rspIter = rsp.getResults().iterator();\n    for (int i = 0; i < records.size(); i++) {  \n      // verify morphline spat out expected data\n      Record actual = collector.getRecords().get(i);\n      GenericData.Record expected = records.get(i);\n      Preconditions.checkNotNull(expected);\n      assertTweetEquals(expected, actual, i);\n      \n      // verify Solr result set contains expected data\n      actual = new Record();\n      actual.getFields().putAll(next(rspIter));\n      assertTweetEquals(expected, actual, i);\n    }\n    \n    Notifications.notifyRollbackTransaction(morphline);\n    Notifications.notifyShutdown(morphline);\n\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    Joiner joiner = Joiner.on(File.separator);\n    File file = new File(joiner.join(RESOURCES_DIR, \"test-documents\", \"sample-statuses-20120906-141433-medium.avro\"));\n    \n    waitForRecoveriesToFinish(false);\n    \n    // load avro records via morphline and zk into solr\n    morphline = parse(\"test-morphlines\" + File.separator + \"tutorialReadAvroContainer\");    \n    Record record = new Record();\n    byte[] body = Files.toByteArray(file);    \n    record.put(Fields.ATTACHMENT_BODY, body);\n    startSession();\n    Notifications.notifyBeginTransaction(morphline);\n    assertTrue(morphline.process(record));\n    assertEquals(1, collector.getNumStartEvents());\n    \n    commit();\n    \n    // fetch sorted result set from solr\n    QueryResponse rsp = cloudClient.query(new SolrQuery(\"*:*\").setRows(100000).addSort(\"id\", SolrQuery.ORDER.asc));   \n    assertEquals(2104, collector.getRecords().size());\n    assertEquals(collector.getRecords().size(), rsp.getResults().size());\n    \n    Collections.sort(collector.getRecords(), (r1, r2) -> r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString()));\n\n    // fetch test input data and sort like solr result set\n    List<GenericData.Record> records = new ArrayList();\n    FileReader<GenericData.Record> reader = new DataFileReader(file, new GenericDatumReader());\n    while (reader.hasNext()) {\n      GenericData.Record expected = reader.next();\n      records.add(expected);\n    }\n    assertEquals(collector.getRecords().size(), records.size());    \n    Collections.sort(records, (r1, r2) -> r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString()));\n    \n    Object lastId = null;\n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrec\" + i + \":\" + records.get(i));      \n      Object id = records.get(i);\n      if (id != null && id.equals(lastId)) {\n        throw new IllegalStateException(\"Detected duplicate id. Test input data must not contain duplicate ids!\");        \n      }\n      lastId = id;\n    }\n    \n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrsp\" + i + \":\" + rsp.getResults().get(i));      \n    }    \n\n    Iterator<SolrDocument> rspIter = rsp.getResults().iterator();\n    for (int i = 0; i < records.size(); i++) {  \n      // verify morphline spat out expected data\n      Record actual = collector.getRecords().get(i);\n      GenericData.Record expected = records.get(i);\n      Preconditions.checkNotNull(expected);\n      assertTweetEquals(expected, actual, i);\n      \n      // verify Solr result set contains expected data\n      actual = new Record();\n      actual.getFields().putAll(next(rspIter));\n      assertTweetEquals(expected, actual, i);\n    }\n    \n    Notifications.notifyRollbackTransaction(morphline);\n    Notifications.notifyShutdown(morphline);\n    cloudClient.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkAvroTest#test().mjava","pathOld":"solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkAvroTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n\n    Path avro = Paths.get(RESOURCES_DIR).resolve(\"test-documents\").resolve(\"sample-statuses-20120906-141433-medium.avro\");\n\n    // load avro records via morphline and zk into solr\n    morphline = parse(\"test-morphlines\" + File.separator + \"tutorialReadAvroContainer\");    \n    Record record = new Record();\n    byte[] body = Files.readAllBytes(avro);\n    record.put(Fields.ATTACHMENT_BODY, body);\n    startSession();\n    Notifications.notifyBeginTransaction(morphline);\n    assertTrue(morphline.process(record));\n    assertEquals(1, collector.getNumStartEvents());\n    \n    Notifications.notifyCommitTransaction(morphline);\n    new UpdateRequest().commit(cluster.getSolrClient(), COLLECTION);\n    \n    // fetch sorted result set from solr\n    QueryResponse rsp = cluster.getSolrClient()\n        .query(COLLECTION, new SolrQuery(\"*:*\").setRows(100000).addSort(\"id\", SolrQuery.ORDER.asc));\n    assertEquals(2104, collector.getRecords().size());\n    assertEquals(collector.getRecords().size(), rsp.getResults().size());\n    \n    Collections.sort(collector.getRecords(), (r1, r2) -> r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString()));\n\n    // fetch test input data and sort like solr result set\n    List<GenericData.Record> records = new ArrayList<>();\n    FileReader<GenericData.Record> reader = new DataFileReader(avro.toFile(), new GenericDatumReader());\n    while (reader.hasNext()) {\n      GenericData.Record expected = reader.next();\n      records.add(expected);\n    }\n    assertEquals(collector.getRecords().size(), records.size());    \n    Collections.sort(records, (r1, r2) -> r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString()));\n    \n    Object lastId = null;\n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrec\" + i + \":\" + records.get(i));      \n      Object id = records.get(i);\n      if (id != null && id.equals(lastId)) {\n        throw new IllegalStateException(\"Detected duplicate id. Test input data must not contain duplicate ids!\");        \n      }\n      lastId = id;\n    }\n    \n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrsp\" + i + \":\" + rsp.getResults().get(i));      \n    }    \n\n    Iterator<SolrDocument> rspIter = rsp.getResults().iterator();\n    for (int i = 0; i < records.size(); i++) {  \n      // verify morphline spat out expected data\n      Record actual = collector.getRecords().get(i);\n      GenericData.Record expected = records.get(i);\n      Preconditions.checkNotNull(expected);\n      assertTweetEquals(expected, actual, i);\n      \n      // verify Solr result set contains expected data\n      actual = new Record();\n      actual.getFields().putAll(next(rspIter));\n      assertTweetEquals(expected, actual, i);\n    }\n    \n    Notifications.notifyRollbackTransaction(morphline);\n    Notifications.notifyShutdown(morphline);\n\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    Joiner joiner = Joiner.on(File.separator);\n    File file = new File(joiner.join(RESOURCES_DIR, \"test-documents\", \"sample-statuses-20120906-141433-medium.avro\"));\n    \n    waitForRecoveriesToFinish(false);\n    \n    // load avro records via morphline and zk into solr\n    morphline = parse(\"test-morphlines\" + File.separator + \"tutorialReadAvroContainer\");    \n    Record record = new Record();\n    byte[] body = Files.toByteArray(file);    \n    record.put(Fields.ATTACHMENT_BODY, body);\n    startSession();\n    Notifications.notifyBeginTransaction(morphline);\n    assertTrue(morphline.process(record));\n    assertEquals(1, collector.getNumStartEvents());\n    \n    commit();\n    \n    // fetch sorted result set from solr\n    QueryResponse rsp = cloudClient.query(new SolrQuery(\"*:*\").setRows(100000).addSort(\"id\", SolrQuery.ORDER.asc));   \n    assertEquals(2104, collector.getRecords().size());\n    assertEquals(collector.getRecords().size(), rsp.getResults().size());\n    \n    Collections.sort(collector.getRecords(), (r1, r2) -> r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString()));\n\n    // fetch test input data and sort like solr result set\n    List<GenericData.Record> records = new ArrayList();\n    FileReader<GenericData.Record> reader = new DataFileReader(file, new GenericDatumReader());\n    while (reader.hasNext()) {\n      GenericData.Record expected = reader.next();\n      records.add(expected);\n    }\n    assertEquals(collector.getRecords().size(), records.size());    \n    Collections.sort(records, (r1, r2) -> r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString()));\n    \n    Object lastId = null;\n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrec\" + i + \":\" + records.get(i));      \n      Object id = records.get(i);\n      if (id != null && id.equals(lastId)) {\n        throw new IllegalStateException(\"Detected duplicate id. Test input data must not contain duplicate ids!\");        \n      }\n      lastId = id;\n    }\n    \n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrsp\" + i + \":\" + rsp.getResults().get(i));      \n    }    \n\n    Iterator<SolrDocument> rspIter = rsp.getResults().iterator();\n    for (int i = 0; i < records.size(); i++) {  \n      // verify morphline spat out expected data\n      Record actual = collector.getRecords().get(i);\n      GenericData.Record expected = records.get(i);\n      Preconditions.checkNotNull(expected);\n      assertTweetEquals(expected, actual, i);\n      \n      // verify Solr result set contains expected data\n      actual = new Record();\n      actual.getFields().putAll(next(rspIter));\n      assertTweetEquals(expected, actual, i);\n    }\n    \n    Notifications.notifyRollbackTransaction(morphline);\n    Notifications.notifyShutdown(morphline);\n    cloudClient.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c609c76a49c17e5c1c8a5d5cd2d685be9f306a47","date":1477689538,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkAvroTest#test().mjava","pathOld":"solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkAvroTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n\n    Path avro = Paths.get(RESOURCES_DIR).resolve(\"test-documents\").resolve(\"sample-statuses-20120906-141433-medium.avro\");\n\n    // load avro records via morphline and zk into solr\n    morphline = parse(\"test-morphlines\" + File.separator + \"tutorialReadAvroContainer\");    \n    Record record = new Record();\n    byte[] body = Files.readAllBytes(avro);\n    record.put(Fields.ATTACHMENT_BODY, body);\n    startSession();\n    Notifications.notifyBeginTransaction(morphline);\n    assertTrue(morphline.process(record));\n    assertEquals(1, collector.getNumStartEvents());\n    \n    Notifications.notifyCommitTransaction(morphline);\n    new UpdateRequest().commit(cluster.getSolrClient(), COLLECTION);\n    \n    // fetch sorted result set from solr\n    QueryResponse rsp = cluster.getSolrClient()\n        .query(COLLECTION, new SolrQuery(\"*:*\").setRows(100000).addSort(\"id\", SolrQuery.ORDER.asc));\n    assertEquals(2104, collector.getRecords().size());\n    assertEquals(collector.getRecords().size(), rsp.getResults().size());\n    \n    Collections.sort(collector.getRecords(), (r1, r2) -> r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString()));\n\n    // fetch test input data and sort like solr result set\n    List<GenericData.Record> records = new ArrayList<>();\n    FileReader<GenericData.Record> reader = new DataFileReader(avro.toFile(), new GenericDatumReader());\n    while (reader.hasNext()) {\n      GenericData.Record expected = reader.next();\n      records.add(expected);\n    }\n    assertEquals(collector.getRecords().size(), records.size());    \n    Collections.sort(records, (r1, r2) -> r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString()));\n    \n    Object lastId = null;\n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrec\" + i + \":\" + records.get(i));      \n      Object id = records.get(i);\n      if (id != null && id.equals(lastId)) {\n        throw new IllegalStateException(\"Detected duplicate id. Test input data must not contain duplicate ids!\");        \n      }\n      lastId = id;\n    }\n    \n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrsp\" + i + \":\" + rsp.getResults().get(i));      \n    }    \n\n    Iterator<SolrDocument> rspIter = rsp.getResults().iterator();\n    for (int i = 0; i < records.size(); i++) {  \n      // verify morphline spat out expected data\n      Record actual = collector.getRecords().get(i);\n      GenericData.Record expected = Objects.requireNonNull(records.get(i));\n      assertTweetEquals(expected, actual, i);\n      \n      // verify Solr result set contains expected data\n      actual = new Record();\n      actual.getFields().putAll(next(rspIter));\n      assertTweetEquals(expected, actual, i);\n    }\n    \n    Notifications.notifyRollbackTransaction(morphline);\n    Notifications.notifyShutdown(morphline);\n\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n\n    Path avro = Paths.get(RESOURCES_DIR).resolve(\"test-documents\").resolve(\"sample-statuses-20120906-141433-medium.avro\");\n\n    // load avro records via morphline and zk into solr\n    morphline = parse(\"test-morphlines\" + File.separator + \"tutorialReadAvroContainer\");    \n    Record record = new Record();\n    byte[] body = Files.readAllBytes(avro);\n    record.put(Fields.ATTACHMENT_BODY, body);\n    startSession();\n    Notifications.notifyBeginTransaction(morphline);\n    assertTrue(morphline.process(record));\n    assertEquals(1, collector.getNumStartEvents());\n    \n    Notifications.notifyCommitTransaction(morphline);\n    new UpdateRequest().commit(cluster.getSolrClient(), COLLECTION);\n    \n    // fetch sorted result set from solr\n    QueryResponse rsp = cluster.getSolrClient()\n        .query(COLLECTION, new SolrQuery(\"*:*\").setRows(100000).addSort(\"id\", SolrQuery.ORDER.asc));\n    assertEquals(2104, collector.getRecords().size());\n    assertEquals(collector.getRecords().size(), rsp.getResults().size());\n    \n    Collections.sort(collector.getRecords(), (r1, r2) -> r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString()));\n\n    // fetch test input data and sort like solr result set\n    List<GenericData.Record> records = new ArrayList<>();\n    FileReader<GenericData.Record> reader = new DataFileReader(avro.toFile(), new GenericDatumReader());\n    while (reader.hasNext()) {\n      GenericData.Record expected = reader.next();\n      records.add(expected);\n    }\n    assertEquals(collector.getRecords().size(), records.size());    \n    Collections.sort(records, (r1, r2) -> r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString()));\n    \n    Object lastId = null;\n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrec\" + i + \":\" + records.get(i));      \n      Object id = records.get(i);\n      if (id != null && id.equals(lastId)) {\n        throw new IllegalStateException(\"Detected duplicate id. Test input data must not contain duplicate ids!\");        \n      }\n      lastId = id;\n    }\n    \n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrsp\" + i + \":\" + rsp.getResults().get(i));      \n    }    \n\n    Iterator<SolrDocument> rspIter = rsp.getResults().iterator();\n    for (int i = 0; i < records.size(); i++) {  \n      // verify morphline spat out expected data\n      Record actual = collector.getRecords().get(i);\n      GenericData.Record expected = records.get(i);\n      Preconditions.checkNotNull(expected);\n      assertTweetEquals(expected, actual, i);\n      \n      // verify Solr result set contains expected data\n      actual = new Record();\n      actual.getFields().putAll(next(rspIter));\n      assertTweetEquals(expected, actual, i);\n    }\n    \n    Notifications.notifyRollbackTransaction(morphline);\n    Notifications.notifyShutdown(morphline);\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"320888923ec13b91f53082558f01f4c9960dd226","date":1477926871,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkAvroTest#test().mjava","pathOld":"solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkAvroTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n\n    Path avro = Paths.get(RESOURCES_DIR).resolve(\"test-documents\").resolve(\"sample-statuses-20120906-141433-medium.avro\");\n\n    // load avro records via morphline and zk into solr\n    morphline = parse(\"test-morphlines\" + File.separator + \"tutorialReadAvroContainer\");    \n    Record record = new Record();\n    byte[] body = Files.readAllBytes(avro);\n    record.put(Fields.ATTACHMENT_BODY, body);\n    startSession();\n    Notifications.notifyBeginTransaction(morphline);\n    assertTrue(morphline.process(record));\n    assertEquals(1, collector.getNumStartEvents());\n    \n    Notifications.notifyCommitTransaction(morphline);\n    new UpdateRequest().commit(cluster.getSolrClient(), COLLECTION);\n    \n    // fetch sorted result set from solr\n    QueryResponse rsp = cluster.getSolrClient()\n        .query(COLLECTION, new SolrQuery(\"*:*\").setRows(100000).addSort(\"id\", SolrQuery.ORDER.asc));\n    assertEquals(2104, collector.getRecords().size());\n    assertEquals(collector.getRecords().size(), rsp.getResults().size());\n    \n    Collections.sort(collector.getRecords(), (r1, r2) -> r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString()));\n\n    // fetch test input data and sort like solr result set\n    List<GenericData.Record> records = new ArrayList<>();\n    FileReader<GenericData.Record> reader = new DataFileReader(avro.toFile(), new GenericDatumReader());\n    while (reader.hasNext()) {\n      GenericData.Record expected = reader.next();\n      records.add(expected);\n    }\n    assertEquals(collector.getRecords().size(), records.size());    \n    Collections.sort(records, (r1, r2) -> r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString()));\n    \n    Object lastId = null;\n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrec\" + i + \":\" + records.get(i));      \n      Object id = records.get(i);\n      if (id != null && id.equals(lastId)) {\n        throw new IllegalStateException(\"Detected duplicate id. Test input data must not contain duplicate ids!\");        \n      }\n      lastId = id;\n    }\n    \n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrsp\" + i + \":\" + rsp.getResults().get(i));      \n    }    \n\n    Iterator<SolrDocument> rspIter = rsp.getResults().iterator();\n    for (int i = 0; i < records.size(); i++) {  \n      // verify morphline spat out expected data\n      Record actual = collector.getRecords().get(i);\n      GenericData.Record expected = Objects.requireNonNull(records.get(i));\n      assertTweetEquals(expected, actual, i);\n      \n      // verify Solr result set contains expected data\n      actual = new Record();\n      actual.getFields().putAll(next(rspIter));\n      assertTweetEquals(expected, actual, i);\n    }\n    \n    Notifications.notifyRollbackTransaction(morphline);\n    Notifications.notifyShutdown(morphline);\n\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n\n    Path avro = Paths.get(RESOURCES_DIR).resolve(\"test-documents\").resolve(\"sample-statuses-20120906-141433-medium.avro\");\n\n    // load avro records via morphline and zk into solr\n    morphline = parse(\"test-morphlines\" + File.separator + \"tutorialReadAvroContainer\");    \n    Record record = new Record();\n    byte[] body = Files.readAllBytes(avro);\n    record.put(Fields.ATTACHMENT_BODY, body);\n    startSession();\n    Notifications.notifyBeginTransaction(morphline);\n    assertTrue(morphline.process(record));\n    assertEquals(1, collector.getNumStartEvents());\n    \n    Notifications.notifyCommitTransaction(morphline);\n    new UpdateRequest().commit(cluster.getSolrClient(), COLLECTION);\n    \n    // fetch sorted result set from solr\n    QueryResponse rsp = cluster.getSolrClient()\n        .query(COLLECTION, new SolrQuery(\"*:*\").setRows(100000).addSort(\"id\", SolrQuery.ORDER.asc));\n    assertEquals(2104, collector.getRecords().size());\n    assertEquals(collector.getRecords().size(), rsp.getResults().size());\n    \n    Collections.sort(collector.getRecords(), (r1, r2) -> r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString()));\n\n    // fetch test input data and sort like solr result set\n    List<GenericData.Record> records = new ArrayList<>();\n    FileReader<GenericData.Record> reader = new DataFileReader(avro.toFile(), new GenericDatumReader());\n    while (reader.hasNext()) {\n      GenericData.Record expected = reader.next();\n      records.add(expected);\n    }\n    assertEquals(collector.getRecords().size(), records.size());    \n    Collections.sort(records, (r1, r2) -> r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString()));\n    \n    Object lastId = null;\n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrec\" + i + \":\" + records.get(i));      \n      Object id = records.get(i);\n      if (id != null && id.equals(lastId)) {\n        throw new IllegalStateException(\"Detected duplicate id. Test input data must not contain duplicate ids!\");        \n      }\n      lastId = id;\n    }\n    \n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrsp\" + i + \":\" + rsp.getResults().get(i));      \n    }    \n\n    Iterator<SolrDocument> rspIter = rsp.getResults().iterator();\n    for (int i = 0; i < records.size(); i++) {  \n      // verify morphline spat out expected data\n      Record actual = collector.getRecords().get(i);\n      GenericData.Record expected = records.get(i);\n      Preconditions.checkNotNull(expected);\n      assertTweetEquals(expected, actual, i);\n      \n      // verify Solr result set contains expected data\n      actual = new Record();\n      actual.getFields().putAll(next(rspIter));\n      assertTweetEquals(expected, actual, i);\n    }\n    \n    Notifications.notifyRollbackTransaction(morphline);\n    Notifications.notifyShutdown(morphline);\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"12109b652e9210b8d58fca47f6c4a725d058a58e","date":1490373076,"type":4,"author":"Steve Rowe","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkAvroTest#test().mjava","sourceNew":null,"sourceOld":"  @Test\n  public void test() throws Exception {\n\n    Path avro = Paths.get(RESOURCES_DIR).resolve(\"test-documents\").resolve(\"sample-statuses-20120906-141433-medium.avro\");\n\n    // load avro records via morphline and zk into solr\n    morphline = parse(\"test-morphlines\" + File.separator + \"tutorialReadAvroContainer\");    \n    Record record = new Record();\n    byte[] body = Files.readAllBytes(avro);\n    record.put(Fields.ATTACHMENT_BODY, body);\n    startSession();\n    Notifications.notifyBeginTransaction(morphline);\n    assertTrue(morphline.process(record));\n    assertEquals(1, collector.getNumStartEvents());\n    \n    Notifications.notifyCommitTransaction(morphline);\n    new UpdateRequest().commit(cluster.getSolrClient(), COLLECTION);\n    \n    // fetch sorted result set from solr\n    QueryResponse rsp = cluster.getSolrClient()\n        .query(COLLECTION, new SolrQuery(\"*:*\").setRows(100000).addSort(\"id\", SolrQuery.ORDER.asc));\n    assertEquals(2104, collector.getRecords().size());\n    assertEquals(collector.getRecords().size(), rsp.getResults().size());\n    \n    Collections.sort(collector.getRecords(), (r1, r2) -> r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString()));\n\n    // fetch test input data and sort like solr result set\n    List<GenericData.Record> records = new ArrayList<>();\n    FileReader<GenericData.Record> reader = new DataFileReader(avro.toFile(), new GenericDatumReader());\n    while (reader.hasNext()) {\n      GenericData.Record expected = reader.next();\n      records.add(expected);\n    }\n    assertEquals(collector.getRecords().size(), records.size());    \n    Collections.sort(records, (r1, r2) -> r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString()));\n    \n    Object lastId = null;\n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrec\" + i + \":\" + records.get(i));      \n      Object id = records.get(i);\n      if (id != null && id.equals(lastId)) {\n        throw new IllegalStateException(\"Detected duplicate id. Test input data must not contain duplicate ids!\");        \n      }\n      lastId = id;\n    }\n    \n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrsp\" + i + \":\" + rsp.getResults().get(i));      \n    }    \n\n    Iterator<SolrDocument> rspIter = rsp.getResults().iterator();\n    for (int i = 0; i < records.size(); i++) {  \n      // verify morphline spat out expected data\n      Record actual = collector.getRecords().get(i);\n      GenericData.Record expected = Objects.requireNonNull(records.get(i));\n      assertTweetEquals(expected, actual, i);\n      \n      // verify Solr result set contains expected data\n      actual = new Record();\n      actual.getFields().putAll(next(rspIter));\n      assertTweetEquals(expected, actual, i);\n    }\n    \n    Notifications.notifyRollbackTransaction(morphline);\n    Notifications.notifyShutdown(morphline);\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe1c4aa9af769a38e878f608070f672efbeac27f","date":1490594650,"type":4,"author":"Steve Rowe","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkAvroTest#test().mjava","sourceNew":null,"sourceOld":"  @Test\n  public void test() throws Exception {\n\n    Path avro = Paths.get(RESOURCES_DIR).resolve(\"test-documents\").resolve(\"sample-statuses-20120906-141433-medium.avro\");\n\n    // load avro records via morphline and zk into solr\n    morphline = parse(\"test-morphlines\" + File.separator + \"tutorialReadAvroContainer\");    \n    Record record = new Record();\n    byte[] body = Files.readAllBytes(avro);\n    record.put(Fields.ATTACHMENT_BODY, body);\n    startSession();\n    Notifications.notifyBeginTransaction(morphline);\n    assertTrue(morphline.process(record));\n    assertEquals(1, collector.getNumStartEvents());\n    \n    Notifications.notifyCommitTransaction(morphline);\n    new UpdateRequest().commit(cluster.getSolrClient(), COLLECTION);\n    \n    // fetch sorted result set from solr\n    QueryResponse rsp = cluster.getSolrClient()\n        .query(COLLECTION, new SolrQuery(\"*:*\").setRows(100000).addSort(\"id\", SolrQuery.ORDER.asc));\n    assertEquals(2104, collector.getRecords().size());\n    assertEquals(collector.getRecords().size(), rsp.getResults().size());\n    \n    Collections.sort(collector.getRecords(), (r1, r2) -> r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString()));\n\n    // fetch test input data and sort like solr result set\n    List<GenericData.Record> records = new ArrayList<>();\n    FileReader<GenericData.Record> reader = new DataFileReader(avro.toFile(), new GenericDatumReader());\n    while (reader.hasNext()) {\n      GenericData.Record expected = reader.next();\n      records.add(expected);\n    }\n    assertEquals(collector.getRecords().size(), records.size());    \n    Collections.sort(records, (r1, r2) -> r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString()));\n    \n    Object lastId = null;\n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrec\" + i + \":\" + records.get(i));      \n      Object id = records.get(i);\n      if (id != null && id.equals(lastId)) {\n        throw new IllegalStateException(\"Detected duplicate id. Test input data must not contain duplicate ids!\");        \n      }\n      lastId = id;\n    }\n    \n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrsp\" + i + \":\" + rsp.getResults().get(i));      \n    }    \n\n    Iterator<SolrDocument> rspIter = rsp.getResults().iterator();\n    for (int i = 0; i < records.size(); i++) {  \n      // verify morphline spat out expected data\n      Record actual = collector.getRecords().get(i);\n      GenericData.Record expected = Objects.requireNonNull(records.get(i));\n      assertTweetEquals(expected, actual, i);\n      \n      // verify Solr result set contains expected data\n      actual = new Record();\n      actual.getFields().putAll(next(rspIter));\n      assertTweetEquals(expected, actual, i);\n    }\n    \n    Notifications.notifyRollbackTransaction(morphline);\n    Notifications.notifyShutdown(morphline);\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"043df2e9a841864922c32756a44c939ed768cb89":["cc3b13b430571c2e169f98fe38e1e7666f88522d"],"12109b652e9210b8d58fca47f6c4a725d058a58e":["c609c76a49c17e5c1c8a5d5cd2d685be9f306a47"],"abb23fcc2461782ab204e61213240feb77d355aa":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"c609c76a49c17e5c1c8a5d5cd2d685be9f306a47":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"0ad30c6a479e764150a3316e57263319775f1df2":["b6284684320a9808c41a5e43de958b2da22f89bd","71f7220f5dc230b647fdb53a465d5abf894c4d1e"],"320888923ec13b91f53082558f01f4c9960dd226":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","c609c76a49c17e5c1c8a5d5cd2d685be9f306a47"],"fe1c4aa9af769a38e878f608070f672efbeac27f":["c609c76a49c17e5c1c8a5d5cd2d685be9f306a47"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["b6284684320a9808c41a5e43de958b2da22f89bd","d470c8182e92b264680e34081b75e70a9f2b3c89"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["b6284684320a9808c41a5e43de958b2da22f89bd","0ad30c6a479e764150a3316e57263319775f1df2"],"b6284684320a9808c41a5e43de958b2da22f89bd":["cc3b13b430571c2e169f98fe38e1e7666f88522d","043df2e9a841864922c32756a44c939ed768cb89"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"71f7220f5dc230b647fdb53a465d5abf894c4d1e":["b6284684320a9808c41a5e43de958b2da22f89bd"],"cc3b13b430571c2e169f98fe38e1e7666f88522d":["abb23fcc2461782ab204e61213240feb77d355aa"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["12109b652e9210b8d58fca47f6c4a725d058a58e"]},"commit2Childs":{"043df2e9a841864922c32756a44c939ed768cb89":["b6284684320a9808c41a5e43de958b2da22f89bd"],"12109b652e9210b8d58fca47f6c4a725d058a58e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"abb23fcc2461782ab204e61213240feb77d355aa":["cc3b13b430571c2e169f98fe38e1e7666f88522d"],"c609c76a49c17e5c1c8a5d5cd2d685be9f306a47":["12109b652e9210b8d58fca47f6c4a725d058a58e","320888923ec13b91f53082558f01f4c9960dd226","fe1c4aa9af769a38e878f608070f672efbeac27f"],"0ad30c6a479e764150a3316e57263319775f1df2":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"320888923ec13b91f53082558f01f4c9960dd226":[],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["320888923ec13b91f53082558f01f4c9960dd226"],"fe1c4aa9af769a38e878f608070f672efbeac27f":[],"d470c8182e92b264680e34081b75e70a9f2b3c89":["c609c76a49c17e5c1c8a5d5cd2d685be9f306a47","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"b6284684320a9808c41a5e43de958b2da22f89bd":["0ad30c6a479e764150a3316e57263319775f1df2","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","d470c8182e92b264680e34081b75e70a9f2b3c89","71f7220f5dc230b647fdb53a465d5abf894c4d1e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["abb23fcc2461782ab204e61213240feb77d355aa"],"71f7220f5dc230b647fdb53a465d5abf894c4d1e":["0ad30c6a479e764150a3316e57263319775f1df2"],"cc3b13b430571c2e169f98fe38e1e7666f88522d":["043df2e9a841864922c32756a44c939ed768cb89","b6284684320a9808c41a5e43de958b2da22f89bd"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["320888923ec13b91f53082558f01f4c9960dd226","fe1c4aa9af769a38e878f608070f672efbeac27f","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}