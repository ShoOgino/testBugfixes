{"path":"lucene/luke/src/test/org/apache/lucene/luke/models/analysis/AnalysisImplTest#testAnalyzeStepByStep_custom().mjava","commits":[{"id":"7b7dac0d1d148a67a2728aa772cf93b6a3ef6e77","date":1561188146,"type":0,"author":"Tomoko Uchida","isMerge":false,"pathNew":"lucene/luke/src/test/org/apache/lucene/luke/models/analysis/AnalysisImplTest#testAnalyzeStepByStep_custom().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  public void testAnalyzeStepByStep_custom() {\n    AnalysisImpl analysis = new AnalysisImpl();\n    Map<String, String> tkParams = new HashMap<>();\n    tkParams.put(\"maxTokenLen\", \"128\");\n    CustomAnalyzerConfig.Builder builder = new CustomAnalyzerConfig.Builder(\"keyword\", tkParams)\n        .addTokenFilterConfig(\"lowercase\", Collections.emptyMap())\n        .addCharFilterConfig(\"htmlstrip\", Collections.emptyMap());\n    CustomAnalyzer analyzer = (CustomAnalyzer) analysis.buildCustomAnalyzer(builder.build());\n    assertEquals(\"org.apache.lucene.analysis.custom.CustomAnalyzer\", analyzer.getClass().getName());\n    assertEquals(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilterFactory\",\n        analyzer.getCharFilterFactories().get(0).getClass().getName());\n    assertEquals(\"org.apache.lucene.analysis.core.KeywordTokenizerFactory\",\n        analyzer.getTokenizerFactory().getClass().getName());\n    assertEquals(\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\",\n        analyzer.getTokenFilterFactories().get(0).getClass().getName());\n\n    String text = \"Apache Lucene\";\n    Analysis.StepByStepResult result = analysis.analyzeStepByStep(text);\n    assertNotNull(result);\n    assertNotNull(result.getCharfilteredTexts());\n    assertEquals(1,result.getCharfilteredTexts().size());\n    assertEquals(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\", result.getCharfilteredTexts().get(0).getName());\n\n    assertNotNull(result.getNamedTokens());\n    assertEquals(2, result.getNamedTokens().size());\n    //FIXME check each namedTokensList\n    assertEquals(\"org.apache.lucene.analysis.core.KeywordTokenizer\", result.getNamedTokens().get(0).getName());\n    assertEquals(\"org.apache.lucene.analysis.core.LowerCaseFilter\", result.getNamedTokens().get(1).getName());\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"dde00f8ce3ea6870a348e607a273123f0895ec87","date":1561189287,"type":0,"author":"Tomoko Uchida","isMerge":true,"pathNew":"lucene/luke/src/test/org/apache/lucene/luke/models/analysis/AnalysisImplTest#testAnalyzeStepByStep_custom().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  public void testAnalyzeStepByStep_custom() {\n    AnalysisImpl analysis = new AnalysisImpl();\n    Map<String, String> tkParams = new HashMap<>();\n    tkParams.put(\"maxTokenLen\", \"128\");\n    CustomAnalyzerConfig.Builder builder = new CustomAnalyzerConfig.Builder(\"keyword\", tkParams)\n        .addTokenFilterConfig(\"lowercase\", Collections.emptyMap())\n        .addCharFilterConfig(\"htmlstrip\", Collections.emptyMap());\n    CustomAnalyzer analyzer = (CustomAnalyzer) analysis.buildCustomAnalyzer(builder.build());\n    assertEquals(\"org.apache.lucene.analysis.custom.CustomAnalyzer\", analyzer.getClass().getName());\n    assertEquals(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilterFactory\",\n        analyzer.getCharFilterFactories().get(0).getClass().getName());\n    assertEquals(\"org.apache.lucene.analysis.core.KeywordTokenizerFactory\",\n        analyzer.getTokenizerFactory().getClass().getName());\n    assertEquals(\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\",\n        analyzer.getTokenFilterFactories().get(0).getClass().getName());\n\n    String text = \"Apache Lucene\";\n    Analysis.StepByStepResult result = analysis.analyzeStepByStep(text);\n    assertNotNull(result);\n    assertNotNull(result.getCharfilteredTexts());\n    assertEquals(1,result.getCharfilteredTexts().size());\n    assertEquals(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\", result.getCharfilteredTexts().get(0).getName());\n\n    assertNotNull(result.getNamedTokens());\n    assertEquals(2, result.getNamedTokens().size());\n    //FIXME check each namedTokensList\n    assertEquals(\"org.apache.lucene.analysis.core.KeywordTokenizer\", result.getNamedTokens().get(0).getName());\n    assertEquals(\"org.apache.lucene.analysis.core.LowerCaseFilter\", result.getNamedTokens().get(1).getName());\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4a1b9d0d0992557a1e5ce918e02fe32aeb267468","date":1561868844,"type":3,"author":"Tomoko Uchida","isMerge":false,"pathNew":"lucene/luke/src/test/org/apache/lucene/luke/models/analysis/AnalysisImplTest#testAnalyzeStepByStep_custom().mjava","pathOld":"lucene/luke/src/test/org/apache/lucene/luke/models/analysis/AnalysisImplTest#testAnalyzeStepByStep_custom().mjava","sourceNew":"  @Test\n  public void testAnalyzeStepByStep_custom() {\n    AnalysisImpl analysis = new AnalysisImpl();\n    Map<String, String> tkParams = new HashMap<>();\n    tkParams.put(\"maxTokenLen\", \"128\");\n    CustomAnalyzerConfig.Builder builder = new CustomAnalyzerConfig.Builder(\"keyword\", tkParams)\n        .addTokenFilterConfig(\"lowercase\", Collections.emptyMap())\n        .addCharFilterConfig(\"htmlstrip\", Collections.emptyMap());\n    CustomAnalyzer analyzer = (CustomAnalyzer) analysis.buildCustomAnalyzer(builder.build());\n    assertEquals(\"org.apache.lucene.analysis.custom.CustomAnalyzer\", analyzer.getClass().getName());\n    assertEquals(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilterFactory\",\n        analyzer.getCharFilterFactories().get(0).getClass().getName());\n    assertEquals(\"org.apache.lucene.analysis.core.KeywordTokenizerFactory\",\n        analyzer.getTokenizerFactory().getClass().getName());\n    assertEquals(\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\",\n        analyzer.getTokenFilterFactories().get(0).getClass().getName());\n\n    String text = \"Apache Lucene\";\n    Analysis.StepByStepResult result = analysis.analyzeStepByStep(text);\n    assertNotNull(result);\n    assertNotNull(result.getCharfilteredTexts());\n    assertEquals(1,result.getCharfilteredTexts().size());\n    assertEquals(\"htmlStrip\", result.getCharfilteredTexts().get(0).getName());\n\n    assertNotNull(result.getNamedTokens());\n    assertEquals(2, result.getNamedTokens().size());\n    //FIXME check each namedTokensList\n    assertEquals(\"keyword\", result.getNamedTokens().get(0).getName());\n    assertEquals(\"lowercase\", result.getNamedTokens().get(1).getName());\n  }\n\n","sourceOld":"  @Test\n  public void testAnalyzeStepByStep_custom() {\n    AnalysisImpl analysis = new AnalysisImpl();\n    Map<String, String> tkParams = new HashMap<>();\n    tkParams.put(\"maxTokenLen\", \"128\");\n    CustomAnalyzerConfig.Builder builder = new CustomAnalyzerConfig.Builder(\"keyword\", tkParams)\n        .addTokenFilterConfig(\"lowercase\", Collections.emptyMap())\n        .addCharFilterConfig(\"htmlstrip\", Collections.emptyMap());\n    CustomAnalyzer analyzer = (CustomAnalyzer) analysis.buildCustomAnalyzer(builder.build());\n    assertEquals(\"org.apache.lucene.analysis.custom.CustomAnalyzer\", analyzer.getClass().getName());\n    assertEquals(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilterFactory\",\n        analyzer.getCharFilterFactories().get(0).getClass().getName());\n    assertEquals(\"org.apache.lucene.analysis.core.KeywordTokenizerFactory\",\n        analyzer.getTokenizerFactory().getClass().getName());\n    assertEquals(\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\",\n        analyzer.getTokenFilterFactories().get(0).getClass().getName());\n\n    String text = \"Apache Lucene\";\n    Analysis.StepByStepResult result = analysis.analyzeStepByStep(text);\n    assertNotNull(result);\n    assertNotNull(result.getCharfilteredTexts());\n    assertEquals(1,result.getCharfilteredTexts().size());\n    assertEquals(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\", result.getCharfilteredTexts().get(0).getName());\n\n    assertNotNull(result.getNamedTokens());\n    assertEquals(2, result.getNamedTokens().size());\n    //FIXME check each namedTokensList\n    assertEquals(\"org.apache.lucene.analysis.core.KeywordTokenizer\", result.getNamedTokens().get(0).getName());\n    assertEquals(\"org.apache.lucene.analysis.core.LowerCaseFilter\", result.getNamedTokens().get(1).getName());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"4a1b9d0d0992557a1e5ce918e02fe32aeb267468":["dde00f8ce3ea6870a348e607a273123f0895ec87"],"dde00f8ce3ea6870a348e607a273123f0895ec87":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","7b7dac0d1d148a67a2728aa772cf93b6a3ef6e77"],"7b7dac0d1d148a67a2728aa772cf93b6a3ef6e77":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["4a1b9d0d0992557a1e5ce918e02fe32aeb267468"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["dde00f8ce3ea6870a348e607a273123f0895ec87","7b7dac0d1d148a67a2728aa772cf93b6a3ef6e77"],"4a1b9d0d0992557a1e5ce918e02fe32aeb267468":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"dde00f8ce3ea6870a348e607a273123f0895ec87":["4a1b9d0d0992557a1e5ce918e02fe32aeb267468"],"7b7dac0d1d148a67a2728aa772cf93b6a3ef6e77":["dde00f8ce3ea6870a348e607a273123f0895ec87"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}