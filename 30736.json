{"path":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","commits":[{"id":"02e175abd2c4c1611c5a9647486ae8ba249a94c1","date":1468327116,"type":1,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public final Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n        PointValues values = reader.getPointValues();\n        if (values == null) {\n          // No docs in this segment indexed any points\n          return null;\n        }\n        FieldInfo fieldInfo = reader.getFieldInfos().fieldInfo(field);\n        if (fieldInfo == null) {\n          // No docs in this segment indexed this field at all\n          return null;\n        }\n        if (fieldInfo.getPointDimensionCount() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + fieldInfo.getPointDimensionCount() + \" but this query has numDims=\" + numDims);\n        }\n        if (fieldInfo.getPointNumBytes() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + fieldInfo.getPointNumBytes() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc(), values, field);\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(field, new MergePointVisitor(sortedPackedPoints, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(field, visitor);\n          }\n        }\n\n        return new ConstantScoreScorer(this, score(), result.build().iterator());\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public final Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n        PointValues values = reader.getPointValues();\n        if (values == null) {\n          // No docs in this segment indexed any points\n          return null;\n        }\n        FieldInfo fieldInfo = reader.getFieldInfos().fieldInfo(field);\n        if (fieldInfo == null) {\n          // No docs in this segment indexed this field at all\n          return null;\n        }\n        if (fieldInfo.getPointDimensionCount() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + fieldInfo.getPointDimensionCount() + \" but this query has numDims=\" + numDims);\n        }\n        if (fieldInfo.getPointNumBytes() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + fieldInfo.getPointNumBytes() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc(), values, field);\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(field, new MergePointVisitor(sortedPackedPoints, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(field, visitor);\n          }\n        }\n\n        return new ConstantScoreScorer(this, score(), result.build().iterator());\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"367f57e2ee85b7f7e28cfe73370a22cf67624f65","date":1476778467,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","sourceNew":"  @Override\n  public final Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n\n        PointValues values = reader.getPointValues(field);\n        if (values == null) {\n          // No docs in this segment/field indexed any points\n          return null;\n        }\n\n        if (values.getNumDimensions() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + values.getNumDimensions() + \" but this query has numDims=\" + numDims);\n        }\n        if (values.getBytesPerDimension() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + values.getBytesPerDimension() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc(), values, field);\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(new MergePointVisitor(sortedPackedPoints, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(visitor);\n          }\n        }\n\n        return new ConstantScoreScorer(this, score(), result.build().iterator());\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public final Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n        PointValues values = reader.getPointValues();\n        if (values == null) {\n          // No docs in this segment indexed any points\n          return null;\n        }\n        FieldInfo fieldInfo = reader.getFieldInfos().fieldInfo(field);\n        if (fieldInfo == null) {\n          // No docs in this segment indexed this field at all\n          return null;\n        }\n        if (fieldInfo.getPointDimensionCount() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + fieldInfo.getPointDimensionCount() + \" but this query has numDims=\" + numDims);\n        }\n        if (fieldInfo.getPointNumBytes() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + fieldInfo.getPointNumBytes() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc(), values, field);\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(field, new MergePointVisitor(sortedPackedPoints, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(field, visitor);\n          }\n        }\n\n        return new ConstantScoreScorer(this, score(), result.build().iterator());\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public final Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n\n        PointValues values = reader.getPointValues(field);\n        if (values == null) {\n          // No docs in this segment/field indexed any points\n          return null;\n        }\n\n        if (values.getNumDimensions() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + values.getNumDimensions() + \" but this query has numDims=\" + numDims);\n        }\n        if (values.getBytesPerDimension() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + values.getBytesPerDimension() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc(), values, field);\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(new MergePointVisitor(sortedPackedPoints, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(visitor);\n          }\n        }\n\n        return new ConstantScoreScorer(this, score(), result.build().iterator());\n      }\n    };\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"706a7a3396c030cc66dda92a0492eb492131c4c0","date":1509705614,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","sourceNew":"  @Override\n  public final Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n\n        PointValues values = reader.getPointValues(field);\n        if (values == null) {\n          // No docs in this segment/field indexed any points\n          return null;\n        }\n\n        if (values.getNumDimensions() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + values.getNumDimensions() + \" but this query has numDims=\" + numDims);\n        }\n        if (values.getBytesPerDimension() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + values.getBytesPerDimension() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc(), values, field);\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(new MergePointVisitor(sortedPackedPoints, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(visitor);\n          }\n        }\n\n        return new ConstantScoreScorer(this, score(), result.build().iterator());\n      }\n\n      @Override\n      public IndexReader.CacheHelper getCacheHelper(LeafReaderContext context) {\n        return context.reader().getCoreCacheHelper();\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public final Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n\n        PointValues values = reader.getPointValues(field);\n        if (values == null) {\n          // No docs in this segment/field indexed any points\n          return null;\n        }\n\n        if (values.getNumDimensions() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + values.getNumDimensions() + \" but this query has numDims=\" + numDims);\n        }\n        if (values.getBytesPerDimension() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + values.getBytesPerDimension() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc(), values, field);\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(new MergePointVisitor(sortedPackedPoints, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(visitor);\n          }\n        }\n\n        return new ConstantScoreScorer(this, score(), result.build().iterator());\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d523b8189b211dd1630166aa77b8c88bb48b3fcc","date":1510144168,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","sourceNew":"  @Override\n  public final Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n\n        PointValues values = reader.getPointValues(field);\n        if (values == null) {\n          // No docs in this segment/field indexed any points\n          return null;\n        }\n\n        if (values.getNumDimensions() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + values.getNumDimensions() + \" but this query has numDims=\" + numDims);\n        }\n        if (values.getBytesPerDimension() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + values.getBytesPerDimension() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc(), values, field);\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(new MergePointVisitor(sortedPackedPoints, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(visitor);\n          }\n        }\n\n        return new ConstantScoreScorer(this, score(), result.build().iterator());\n      }\n\n      @Override\n      public IndexReader.CacheHelper getCacheHelper(LeafReaderContext context) {\n        return context.reader().getCoreCacheHelper();\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public final Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n\n        PointValues values = reader.getPointValues(field);\n        if (values == null) {\n          // No docs in this segment/field indexed any points\n          return null;\n        }\n\n        if (values.getNumDimensions() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + values.getNumDimensions() + \" but this query has numDims=\" + numDims);\n        }\n        if (values.getBytesPerDimension() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + values.getBytesPerDimension() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc(), values, field);\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(new MergePointVisitor(sortedPackedPoints, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(visitor);\n          }\n        }\n\n        return new ConstantScoreScorer(this, score(), result.build().iterator());\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"da1460d7a5dea2658e7b8e4f6e632e53ade440ac","date":1510316270,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","sourceNew":"  @Override\n  public final Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n\n        PointValues values = reader.getPointValues(field);\n        if (values == null) {\n          // No docs in this segment/field indexed any points\n          return null;\n        }\n\n        if (values.getNumDimensions() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + values.getNumDimensions() + \" but this query has numDims=\" + numDims);\n        }\n        if (values.getBytesPerDimension() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + values.getBytesPerDimension() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc(), values, field);\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(new MergePointVisitor(sortedPackedPoints, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(visitor);\n          }\n        }\n\n        return new ConstantScoreScorer(this, score(), result.build().iterator());\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        return true;\n      }\n\n    };\n  }\n\n","sourceOld":"  @Override\n  public final Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n\n        PointValues values = reader.getPointValues(field);\n        if (values == null) {\n          // No docs in this segment/field indexed any points\n          return null;\n        }\n\n        if (values.getNumDimensions() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + values.getNumDimensions() + \" but this query has numDims=\" + numDims);\n        }\n        if (values.getBytesPerDimension() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + values.getBytesPerDimension() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc(), values, field);\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(new MergePointVisitor(sortedPackedPoints, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(visitor);\n          }\n        }\n\n        return new ConstantScoreScorer(this, score(), result.build().iterator());\n      }\n\n      @Override\n      public IndexReader.CacheHelper getCacheHelper(LeafReaderContext context) {\n        return context.reader().getCoreCacheHelper();\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9fc47cb7b4346802411bb432f501ed0673d7119e","date":1512640179,"type":5,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,ScoreMode,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","sourceNew":"  @Override\n  public final Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n\n        PointValues values = reader.getPointValues(field);\n        if (values == null) {\n          // No docs in this segment/field indexed any points\n          return null;\n        }\n\n        if (values.getNumDimensions() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + values.getNumDimensions() + \" but this query has numDims=\" + numDims);\n        }\n        if (values.getBytesPerDimension() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + values.getBytesPerDimension() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc(), values, field);\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(new MergePointVisitor(sortedPackedPoints, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(visitor);\n          }\n        }\n\n        return new ConstantScoreScorer(this, score(), result.build().iterator());\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        return true;\n      }\n\n    };\n  }\n\n","sourceOld":"  @Override\n  public final Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n\n        PointValues values = reader.getPointValues(field);\n        if (values == null) {\n          // No docs in this segment/field indexed any points\n          return null;\n        }\n\n        if (values.getNumDimensions() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + values.getNumDimensions() + \" but this query has numDims=\" + numDims);\n        }\n        if (values.getBytesPerDimension() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + values.getBytesPerDimension() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc(), values, field);\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(new MergePointVisitor(sortedPackedPoints, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(visitor);\n          }\n        }\n\n        return new ConstantScoreScorer(this, score(), result.build().iterator());\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        return true;\n      }\n\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"417142ff08fda9cf0b72d5133e63097a166c6458","date":1512729693,"type":5,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,ScoreMode,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","sourceNew":"  @Override\n  public final Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n\n        PointValues values = reader.getPointValues(field);\n        if (values == null) {\n          // No docs in this segment/field indexed any points\n          return null;\n        }\n\n        if (values.getNumDimensions() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + values.getNumDimensions() + \" but this query has numDims=\" + numDims);\n        }\n        if (values.getBytesPerDimension() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + values.getBytesPerDimension() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc(), values, field);\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(new MergePointVisitor(sortedPackedPoints, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(visitor);\n          }\n        }\n\n        return new ConstantScoreScorer(this, score(), result.build().iterator());\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        return true;\n      }\n\n    };\n  }\n\n","sourceOld":"  @Override\n  public final Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n\n        PointValues values = reader.getPointValues(field);\n        if (values == null) {\n          // No docs in this segment/field indexed any points\n          return null;\n        }\n\n        if (values.getNumDimensions() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + values.getNumDimensions() + \" but this query has numDims=\" + numDims);\n        }\n        if (values.getBytesPerDimension() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + values.getBytesPerDimension() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc(), values, field);\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(new MergePointVisitor(sortedPackedPoints, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(visitor);\n          }\n        }\n\n        return new ConstantScoreScorer(this, score(), result.build().iterator());\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        return true;\n      }\n\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"706a7a3396c030cc66dda92a0492eb492131c4c0":["367f57e2ee85b7f7e28cfe73370a22cf67624f65"],"da1460d7a5dea2658e7b8e4f6e632e53ade440ac":["d523b8189b211dd1630166aa77b8c88bb48b3fcc"],"d523b8189b211dd1630166aa77b8c88bb48b3fcc":["367f57e2ee85b7f7e28cfe73370a22cf67624f65","706a7a3396c030cc66dda92a0492eb492131c4c0"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"02e175abd2c4c1611c5a9647486ae8ba249a94c1":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"367f57e2ee85b7f7e28cfe73370a22cf67624f65":["02e175abd2c4c1611c5a9647486ae8ba249a94c1"],"9fc47cb7b4346802411bb432f501ed0673d7119e":["da1460d7a5dea2658e7b8e4f6e632e53ade440ac"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","367f57e2ee85b7f7e28cfe73370a22cf67624f65"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["417142ff08fda9cf0b72d5133e63097a166c6458"],"417142ff08fda9cf0b72d5133e63097a166c6458":["da1460d7a5dea2658e7b8e4f6e632e53ade440ac","9fc47cb7b4346802411bb432f501ed0673d7119e"]},"commit2Childs":{"706a7a3396c030cc66dda92a0492eb492131c4c0":["d523b8189b211dd1630166aa77b8c88bb48b3fcc"],"da1460d7a5dea2658e7b8e4f6e632e53ade440ac":["9fc47cb7b4346802411bb432f501ed0673d7119e","417142ff08fda9cf0b72d5133e63097a166c6458"],"d523b8189b211dd1630166aa77b8c88bb48b3fcc":["da1460d7a5dea2658e7b8e4f6e632e53ade440ac"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["02e175abd2c4c1611c5a9647486ae8ba249a94c1","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"02e175abd2c4c1611c5a9647486ae8ba249a94c1":["367f57e2ee85b7f7e28cfe73370a22cf67624f65"],"367f57e2ee85b7f7e28cfe73370a22cf67624f65":["706a7a3396c030cc66dda92a0492eb492131c4c0","d523b8189b211dd1630166aa77b8c88bb48b3fcc","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"9fc47cb7b4346802411bb432f501ed0673d7119e":["417142ff08fda9cf0b72d5133e63097a166c6458"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"417142ff08fda9cf0b72d5133e63097a166c6458":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}