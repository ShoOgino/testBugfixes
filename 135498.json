{"path":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","pathOld":"contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","sourceNew":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Fieldable field : document.getDocument().getFields()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = StringHelper.intern(field.name());\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.isIndexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.isTokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.isStored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.isBinary()) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.isTermVectorStored()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Fieldable, LinkedList<Token>> tokensByField = new LinkedHashMap<Fieldable, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Fieldable> it = document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Fieldable field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.tokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          while (tokenStream.incrementToken()) {\n            // TODO: this is a simple workaround to still work with tokens, not very effective, but as far as I know, this writer should get removed soon:\n            final Token token = new Token();\n            for (Iterator<AttributeImpl> atts = tokenStream.getAttributeImplsIterator(); atts.hasNext();) {\n              final AttributeImpl att = atts.next();\n              try {\n                att.copyTo(token);\n              } catch (Exception e) {\n                // ignore unsupported attributes,\n                // this may fail to copy some attributes, if a special combined AttributeImpl is used, that\n                // implements basic attributes supported by Token and also other customized ones in one class.\n              }\n            }\n            tokens.add(token); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n            if (fieldSetting.fieldLength > maxFieldLength) {\n              break;\n            }\n          }\n          tokenStream.end();\n          tokenStream.close();\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setTermBuffer(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Fieldable, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.term());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.term(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","sourceOld":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Fieldable field : document.getDocument().getFields()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = StringHelper.intern(field.name());\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.isIndexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.isTokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.isStored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.isBinary()) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.isTermVectorStored()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Fieldable, LinkedList<Token>> tokensByField = new LinkedHashMap<Fieldable, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Fieldable> it = document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Fieldable field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.tokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          while (tokenStream.incrementToken()) {\n            // TODO: this is a simple workaround to still work with tokens, not very effective, but as far as I know, this writer should get removed soon:\n            final Token token = new Token();\n            for (Iterator<AttributeImpl> atts = tokenStream.getAttributeImplsIterator(); atts.hasNext();) {\n              final AttributeImpl att = atts.next();\n              try {\n                att.copyTo(token);\n              } catch (Exception e) {\n                // ignore unsupported attributes,\n                // this may fail to copy some attributes, if a special combined AttributeImpl is used, that\n                // implements basic attributes supported by Token and also other customized ones in one class.\n              }\n            }\n            tokens.add(token); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n            if (fieldSetting.fieldLength > maxFieldLength) {\n              break;\n            }\n          }\n          tokenStream.end();\n          tokenStream.close();\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setTermBuffer(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Fieldable, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.term());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.term(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a7347509fad0711ac30cb15a746e9a3830a38ebd","date":1275388513,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","pathOld":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","sourceNew":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Fieldable field : document.getDocument().getFields()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = StringHelper.intern(field.name());\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.isIndexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.isTokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.isStored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.isBinary()) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.isTermVectorStored()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Fieldable, LinkedList<Token>> tokensByField = new LinkedHashMap<Fieldable, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Fieldable> it = document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Fieldable field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.tokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          while (tokenStream.incrementToken()) {\n            // TODO: this is a simple workaround to still work with tokens, not very effective, but as far as I know, this writer should get removed soon:\n            final Token token = new Token();\n            for (Iterator<AttributeImpl> atts = tokenStream.getAttributeImplsIterator(); atts.hasNext();) {\n              final AttributeImpl att = atts.next();\n              try {\n                att.copyTo(token);\n              } catch (Exception e) {\n                // ignore unsupported attributes,\n                // this may fail to copy some attributes, if a special combined AttributeImpl is used, that\n                // implements basic attributes supported by Token and also other customized ones in one class.\n              }\n            }\n            tokens.add(token); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n            if (fieldSetting.fieldLength > maxFieldLength) {\n              break;\n            }\n          }\n          tokenStream.end();\n          tokenStream.close();\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setEmpty().append(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Fieldable, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.toString());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.toString(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","sourceOld":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Fieldable field : document.getDocument().getFields()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = StringHelper.intern(field.name());\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.isIndexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.isTokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.isStored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.isBinary()) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.isTermVectorStored()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Fieldable, LinkedList<Token>> tokensByField = new LinkedHashMap<Fieldable, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Fieldable> it = document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Fieldable field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.tokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          while (tokenStream.incrementToken()) {\n            // TODO: this is a simple workaround to still work with tokens, not very effective, but as far as I know, this writer should get removed soon:\n            final Token token = new Token();\n            for (Iterator<AttributeImpl> atts = tokenStream.getAttributeImplsIterator(); atts.hasNext();) {\n              final AttributeImpl att = atts.next();\n              try {\n                att.copyTo(token);\n              } catch (Exception e) {\n                // ignore unsupported attributes,\n                // this may fail to copy some attributes, if a special combined AttributeImpl is used, that\n                // implements basic attributes supported by Token and also other customized ones in one class.\n              }\n            }\n            tokens.add(token); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n            if (fieldSetting.fieldLength > maxFieldLength) {\n              break;\n            }\n          }\n          tokenStream.end();\n          tokenStream.close();\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setTermBuffer(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Fieldable, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.term());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.term(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4e8cc373c801e54cec75daf9f52792cb4b17f536","date":1291116159,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","pathOld":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","sourceNew":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Fieldable field : document.getDocument().getFields()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = StringHelper.intern(field.name());\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.isIndexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.isTokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.isStored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.isBinary()) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.isTermVectorStored()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Fieldable, LinkedList<Token>> tokensByField = new LinkedHashMap<Fieldable, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Fieldable> it = document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Fieldable field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.tokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          while (tokenStream.incrementToken()) {\n            // TODO: this is a simple workaround to still work with tokens, not very effective, but as far as I know, this writer should get removed soon:\n            final Token token = new Token();\n            for (Iterator<AttributeImpl> atts = tokenStream.getAttributeImplsIterator(); atts.hasNext();) {\n              final AttributeImpl att = atts.next();\n              try {\n                att.copyTo(token);\n              } catch (Exception e) {\n                // ignore unsupported attributes,\n                // this may fail to copy some attributes, if a special combined AttributeImpl is used, that\n                // implements basic attributes supported by Token and also other customized ones in one class.\n              }\n            }\n            tokens.add(token); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n          }\n          tokenStream.end();\n          tokenStream.close();\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setEmpty().append(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Fieldable, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.toString());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.toString(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","sourceOld":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Fieldable field : document.getDocument().getFields()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = StringHelper.intern(field.name());\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.isIndexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.isTokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.isStored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.isBinary()) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.isTermVectorStored()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Fieldable, LinkedList<Token>> tokensByField = new LinkedHashMap<Fieldable, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Fieldable> it = document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Fieldable field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.tokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          while (tokenStream.incrementToken()) {\n            // TODO: this is a simple workaround to still work with tokens, not very effective, but as far as I know, this writer should get removed soon:\n            final Token token = new Token();\n            for (Iterator<AttributeImpl> atts = tokenStream.getAttributeImplsIterator(); atts.hasNext();) {\n              final AttributeImpl att = atts.next();\n              try {\n                att.copyTo(token);\n              } catch (Exception e) {\n                // ignore unsupported attributes,\n                // this may fail to copy some attributes, if a special combined AttributeImpl is used, that\n                // implements basic attributes supported by Token and also other customized ones in one class.\n              }\n            }\n            tokens.add(token); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n            if (fieldSetting.fieldLength > maxFieldLength) {\n              break;\n            }\n          }\n          tokenStream.end();\n          tokenStream.close();\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setEmpty().append(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Fieldable, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.toString());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.toString(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3bb13258feba31ab676502787ab2e1779f129b7a","date":1291596436,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","pathOld":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","sourceNew":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Fieldable field : document.getDocument().getFields()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = StringHelper.intern(field.name());\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.isIndexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.isTokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.isStored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.isBinary()) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.isTermVectorStored()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Fieldable, LinkedList<Token>> tokensByField = new LinkedHashMap<Fieldable, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Fieldable> it = document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Fieldable field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.tokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          while (tokenStream.incrementToken()) {\n            // TODO: this is a simple workaround to still work with tokens, not very effective, but as far as I know, this writer should get removed soon:\n            final Token token = new Token();\n            for (Iterator<AttributeImpl> atts = tokenStream.getAttributeImplsIterator(); atts.hasNext();) {\n              final AttributeImpl att = atts.next();\n              try {\n                att.copyTo(token);\n              } catch (Exception e) {\n                // ignore unsupported attributes,\n                // this may fail to copy some attributes, if a special combined AttributeImpl is used, that\n                // implements basic attributes supported by Token and also other customized ones in one class.\n              }\n            }\n            tokens.add(token); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n          }\n          tokenStream.end();\n          tokenStream.close();\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setEmpty().append(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Fieldable, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.toString());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.toString(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","sourceOld":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Fieldable field : document.getDocument().getFields()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = StringHelper.intern(field.name());\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.isIndexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.isTokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.isStored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.isBinary()) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.isTermVectorStored()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Fieldable, LinkedList<Token>> tokensByField = new LinkedHashMap<Fieldable, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Fieldable> it = document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Fieldable field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.tokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          while (tokenStream.incrementToken()) {\n            // TODO: this is a simple workaround to still work with tokens, not very effective, but as far as I know, this writer should get removed soon:\n            final Token token = new Token();\n            for (Iterator<AttributeImpl> atts = tokenStream.getAttributeImplsIterator(); atts.hasNext();) {\n              final AttributeImpl att = atts.next();\n              try {\n                att.copyTo(token);\n              } catch (Exception e) {\n                // ignore unsupported attributes,\n                // this may fail to copy some attributes, if a special combined AttributeImpl is used, that\n                // implements basic attributes supported by Token and also other customized ones in one class.\n              }\n            }\n            tokens.add(token); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n            if (fieldSetting.fieldLength > maxFieldLength) {\n              break;\n            }\n          }\n          tokenStream.end();\n          tokenStream.close();\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setEmpty().append(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Fieldable, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.toString());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.toString(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","pathOld":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","sourceNew":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Fieldable field : document.getDocument().getFields()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = StringHelper.intern(field.name());\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.isIndexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.isTokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.isStored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.isBinary()) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.isTermVectorStored()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Fieldable, LinkedList<Token>> tokensByField = new LinkedHashMap<Fieldable, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Fieldable> it = document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Fieldable field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.tokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          while (tokenStream.incrementToken()) {\n            // TODO: this is a simple workaround to still work with tokens, not very effective, but as far as I know, this writer should get removed soon:\n            final Token token = new Token();\n            for (Iterator<AttributeImpl> atts = tokenStream.getAttributeImplsIterator(); atts.hasNext();) {\n              final AttributeImpl att = atts.next();\n              try {\n                att.copyTo(token);\n              } catch (Exception e) {\n                // ignore unsupported attributes,\n                // this may fail to copy some attributes, if a special combined AttributeImpl is used, that\n                // implements basic attributes supported by Token and also other customized ones in one class.\n              }\n            }\n            tokens.add(token); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n          }\n          tokenStream.end();\n          tokenStream.close();\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setEmpty().append(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Fieldable, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.toString());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.toString(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","sourceOld":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Fieldable field : document.getDocument().getFields()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = StringHelper.intern(field.name());\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.isIndexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.isTokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.isStored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.isBinary()) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.isTermVectorStored()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Fieldable, LinkedList<Token>> tokensByField = new LinkedHashMap<Fieldable, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Fieldable> it = document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Fieldable field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.tokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          while (tokenStream.incrementToken()) {\n            // TODO: this is a simple workaround to still work with tokens, not very effective, but as far as I know, this writer should get removed soon:\n            final Token token = new Token();\n            for (Iterator<AttributeImpl> atts = tokenStream.getAttributeImplsIterator(); atts.hasNext();) {\n              final AttributeImpl att = atts.next();\n              try {\n                att.copyTo(token);\n              } catch (Exception e) {\n                // ignore unsupported attributes,\n                // this may fail to copy some attributes, if a special combined AttributeImpl is used, that\n                // implements basic attributes supported by Token and also other customized ones in one class.\n              }\n            }\n            tokens.add(token); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n            if (fieldSetting.fieldLength > maxFieldLength) {\n              break;\n            }\n          }\n          tokenStream.end();\n          tokenStream.close();\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setEmpty().append(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Fieldable, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.toString());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.toString(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"00746ad002a54281629e3b6f3eb39833a33f093e","date":1305306799,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","pathOld":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","sourceNew":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Fieldable field : document.getDocument().getFields()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = StringHelper.intern(field.name());\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.isIndexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.isTokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.isStored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.isBinary()) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.isTermVectorStored()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Fieldable, LinkedList<Token>> tokensByField = new LinkedHashMap<Fieldable, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Fieldable> it = document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Fieldable field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.reusableTokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          while (tokenStream.incrementToken()) {\n            // TODO: this is a simple workaround to still work with tokens, not very effective, but as far as I know, this writer should get removed soon:\n            final Token token = new Token();\n            for (Iterator<AttributeImpl> atts = tokenStream.getAttributeImplsIterator(); atts.hasNext();) {\n              final AttributeImpl att = atts.next();\n              try {\n                att.copyTo(token);\n              } catch (Exception e) {\n                // ignore unsupported attributes,\n                // this may fail to copy some attributes, if a special combined AttributeImpl is used, that\n                // implements basic attributes supported by Token and also other customized ones in one class.\n              }\n            }\n            tokens.add(token); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n          }\n          tokenStream.end();\n          tokenStream.close();\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setEmpty().append(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Fieldable, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.toString());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.toString(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","sourceOld":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Fieldable field : document.getDocument().getFields()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = StringHelper.intern(field.name());\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.isIndexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.isTokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.isStored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.isBinary()) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.isTermVectorStored()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Fieldable, LinkedList<Token>> tokensByField = new LinkedHashMap<Fieldable, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Fieldable> it = document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Fieldable field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.tokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          while (tokenStream.incrementToken()) {\n            // TODO: this is a simple workaround to still work with tokens, not very effective, but as far as I know, this writer should get removed soon:\n            final Token token = new Token();\n            for (Iterator<AttributeImpl> atts = tokenStream.getAttributeImplsIterator(); atts.hasNext();) {\n              final AttributeImpl att = atts.next();\n              try {\n                att.copyTo(token);\n              } catch (Exception e) {\n                // ignore unsupported attributes,\n                // this may fail to copy some attributes, if a special combined AttributeImpl is used, that\n                // implements basic attributes supported by Token and also other customized ones in one class.\n              }\n            }\n            tokens.add(token); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n          }\n          tokenStream.end();\n          tokenStream.close();\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setEmpty().append(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Fieldable, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.toString());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.toString(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c3a8a449466c1ff7ce2274fe73dab487256964b4","date":1305735867,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","pathOld":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","sourceNew":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Fieldable field : document.getDocument().getFields()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = StringHelper.intern(field.name());\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.isIndexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.isTokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.isStored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.isBinary()) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.isTermVectorStored()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Fieldable, LinkedList<Token>> tokensByField = new LinkedHashMap<Fieldable, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Fieldable> it = document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Fieldable field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.reusableTokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          while (tokenStream.incrementToken()) {\n            // TODO: this is a simple workaround to still work with tokens, not very effective, but as far as I know, this writer should get removed soon:\n            final Token token = new Token();\n            for (Iterator<AttributeImpl> atts = tokenStream.getAttributeImplsIterator(); atts.hasNext();) {\n              final AttributeImpl att = atts.next();\n              try {\n                att.copyTo(token);\n              } catch (Exception e) {\n                // ignore unsupported attributes,\n                // this may fail to copy some attributes, if a special combined AttributeImpl is used, that\n                // implements basic attributes supported by Token and also other customized ones in one class.\n              }\n            }\n            tokens.add(token); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n          }\n          tokenStream.end();\n          tokenStream.close();\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setEmpty().append(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Fieldable, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.toString());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.toString(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","sourceOld":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Fieldable field : document.getDocument().getFields()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = StringHelper.intern(field.name());\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.isIndexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.isTokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.isStored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.isBinary()) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.isTermVectorStored()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Fieldable, LinkedList<Token>> tokensByField = new LinkedHashMap<Fieldable, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Fieldable> it = document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Fieldable field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.tokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          while (tokenStream.incrementToken()) {\n            // TODO: this is a simple workaround to still work with tokens, not very effective, but as far as I know, this writer should get removed soon:\n            final Token token = new Token();\n            for (Iterator<AttributeImpl> atts = tokenStream.getAttributeImplsIterator(); atts.hasNext();) {\n              final AttributeImpl att = atts.next();\n              try {\n                att.copyTo(token);\n              } catch (Exception e) {\n                // ignore unsupported attributes,\n                // this may fail to copy some attributes, if a special combined AttributeImpl is used, that\n                // implements basic attributes supported by Token and also other customized ones in one class.\n              }\n            }\n            tokens.add(token); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n          }\n          tokenStream.end();\n          tokenStream.close();\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setEmpty().append(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Fieldable, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.toString());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.toString(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","pathOld":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","sourceNew":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Fieldable field : document.getDocument().getFields()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = StringHelper.intern(field.name());\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.isIndexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.isTokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.isStored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.isBinary()) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.isTermVectorStored()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Fieldable, LinkedList<Token>> tokensByField = new LinkedHashMap<Fieldable, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Fieldable> it = document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Fieldable field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.reusableTokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          while (tokenStream.incrementToken()) {\n            // TODO: this is a simple workaround to still work with tokens, not very effective, but as far as I know, this writer should get removed soon:\n            final Token token = new Token();\n            for (Iterator<AttributeImpl> atts = tokenStream.getAttributeImplsIterator(); atts.hasNext();) {\n              final AttributeImpl att = atts.next();\n              try {\n                att.copyTo(token);\n              } catch (Exception e) {\n                // ignore unsupported attributes,\n                // this may fail to copy some attributes, if a special combined AttributeImpl is used, that\n                // implements basic attributes supported by Token and also other customized ones in one class.\n              }\n            }\n            tokens.add(token); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n          }\n          tokenStream.end();\n          tokenStream.close();\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setEmpty().append(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Fieldable, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.toString());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.toString(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","sourceOld":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Fieldable field : document.getDocument().getFields()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = StringHelper.intern(field.name());\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.isIndexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.isTokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.isStored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.isBinary()) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.isTermVectorStored()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Fieldable, LinkedList<Token>> tokensByField = new LinkedHashMap<Fieldable, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Fieldable> it = document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Fieldable field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.tokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          while (tokenStream.incrementToken()) {\n            // TODO: this is a simple workaround to still work with tokens, not very effective, but as far as I know, this writer should get removed soon:\n            final Token token = new Token();\n            for (Iterator<AttributeImpl> atts = tokenStream.getAttributeImplsIterator(); atts.hasNext();) {\n              final AttributeImpl att = atts.next();\n              try {\n                att.copyTo(token);\n              } catch (Exception e) {\n                // ignore unsupported attributes,\n                // this may fail to copy some attributes, if a special combined AttributeImpl is used, that\n                // implements basic attributes supported by Token and also other customized ones in one class.\n              }\n            }\n            tokens.add(token); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n          }\n          tokenStream.end();\n          tokenStream.close();\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setEmpty().append(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Fieldable, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.toString());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.toString(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fafef7c83fe8e0b3ca9298d5d75d6b943dc28153","date":1308670974,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","pathOld":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","sourceNew":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Fieldable field : document.getDocument().getFields()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = field.name();\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.isIndexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.isTokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.isStored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.isBinary()) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.isTermVectorStored()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Fieldable, LinkedList<Token>> tokensByField = new LinkedHashMap<Fieldable, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Fieldable> it = document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Fieldable field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.reusableTokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          while (tokenStream.incrementToken()) {\n            // TODO: this is a simple workaround to still work with tokens, not very effective, but as far as I know, this writer should get removed soon:\n            final Token token = new Token();\n            for (Iterator<AttributeImpl> atts = tokenStream.getAttributeImplsIterator(); atts.hasNext();) {\n              final AttributeImpl att = atts.next();\n              try {\n                att.copyTo(token);\n              } catch (Exception e) {\n                // ignore unsupported attributes,\n                // this may fail to copy some attributes, if a special combined AttributeImpl is used, that\n                // implements basic attributes supported by Token and also other customized ones in one class.\n              }\n            }\n            tokens.add(token); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n          }\n          tokenStream.end();\n          tokenStream.close();\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setEmpty().append(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Fieldable, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.toString());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.toString(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","sourceOld":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Fieldable field : document.getDocument().getFields()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = StringHelper.intern(field.name());\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.isIndexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.isTokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.isStored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.isBinary()) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.isTermVectorStored()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Fieldable, LinkedList<Token>> tokensByField = new LinkedHashMap<Fieldable, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Fieldable> it = document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Fieldable field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.reusableTokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          while (tokenStream.incrementToken()) {\n            // TODO: this is a simple workaround to still work with tokens, not very effective, but as far as I know, this writer should get removed soon:\n            final Token token = new Token();\n            for (Iterator<AttributeImpl> atts = tokenStream.getAttributeImplsIterator(); atts.hasNext();) {\n              final AttributeImpl att = atts.next();\n              try {\n                att.copyTo(token);\n              } catch (Exception e) {\n                // ignore unsupported attributes,\n                // this may fail to copy some attributes, if a special combined AttributeImpl is used, that\n                // implements basic attributes supported by Token and also other customized ones in one class.\n              }\n            }\n            tokens.add(token); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n          }\n          tokenStream.end();\n          tokenStream.close();\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setEmpty().append(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Fieldable, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.toString());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.toString(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2553b00f699380c64959ccb27991289aae87be2e","date":1309290151,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","pathOld":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","sourceNew":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Fieldable field : document.getDocument().getFields()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = field.name();\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.isIndexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.isTokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.isStored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.isBinary()) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.isTermVectorStored()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Fieldable, LinkedList<Token>> tokensByField = new LinkedHashMap<Fieldable, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Fieldable> it = document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Fieldable field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.reusableTokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          while (tokenStream.incrementToken()) {\n            // TODO: this is a simple workaround to still work with tokens, not very effective, but as far as I know, this writer should get removed soon:\n            final Token token = new Token();\n            for (Iterator<AttributeImpl> atts = tokenStream.getAttributeImplsIterator(); atts.hasNext();) {\n              final AttributeImpl att = atts.next();\n              try {\n                att.copyTo(token);\n              } catch (Exception e) {\n                // ignore unsupported attributes,\n                // this may fail to copy some attributes, if a special combined AttributeImpl is used, that\n                // implements basic attributes supported by Token and also other customized ones in one class.\n              }\n            }\n            tokens.add(token); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n          }\n          tokenStream.end();\n          tokenStream.close();\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setEmpty().append(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Fieldable, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.toString());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.toString(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","sourceOld":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Fieldable field : document.getDocument().getFields()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = StringHelper.intern(field.name());\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.isIndexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.isTokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.isStored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.isBinary()) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.isTermVectorStored()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Fieldable, LinkedList<Token>> tokensByField = new LinkedHashMap<Fieldable, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Fieldable> it = document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Fieldable field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.reusableTokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          while (tokenStream.incrementToken()) {\n            // TODO: this is a simple workaround to still work with tokens, not very effective, but as far as I know, this writer should get removed soon:\n            final Token token = new Token();\n            for (Iterator<AttributeImpl> atts = tokenStream.getAttributeImplsIterator(); atts.hasNext();) {\n              final AttributeImpl att = atts.next();\n              try {\n                att.copyTo(token);\n              } catch (Exception e) {\n                // ignore unsupported attributes,\n                // this may fail to copy some attributes, if a special combined AttributeImpl is used, that\n                // implements basic attributes supported by Token and also other customized ones in one class.\n              }\n            }\n            tokens.add(token); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n          }\n          tokenStream.end();\n          tokenStream.close();\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setEmpty().append(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Fieldable, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.toString());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.toString(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","pathOld":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","sourceNew":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Fieldable field : document.getDocument().getFields()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = field.name();\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.isIndexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.isTokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.isStored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.isBinary()) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.isTermVectorStored()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Fieldable, LinkedList<Token>> tokensByField = new LinkedHashMap<Fieldable, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Fieldable> it = document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Fieldable field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.reusableTokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          while (tokenStream.incrementToken()) {\n            // TODO: this is a simple workaround to still work with tokens, not very effective, but as far as I know, this writer should get removed soon:\n            final Token token = new Token();\n            for (Iterator<AttributeImpl> atts = tokenStream.getAttributeImplsIterator(); atts.hasNext();) {\n              final AttributeImpl att = atts.next();\n              try {\n                att.copyTo(token);\n              } catch (Exception e) {\n                // ignore unsupported attributes,\n                // this may fail to copy some attributes, if a special combined AttributeImpl is used, that\n                // implements basic attributes supported by Token and also other customized ones in one class.\n              }\n            }\n            tokens.add(token); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n          }\n          tokenStream.end();\n          tokenStream.close();\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setEmpty().append(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Fieldable, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.toString());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.toString(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","sourceOld":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Fieldable field : document.getDocument().getFields()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = StringHelper.intern(field.name());\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.isIndexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.isTokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.isStored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.isBinary()) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.isTermVectorStored()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Fieldable, LinkedList<Token>> tokensByField = new LinkedHashMap<Fieldable, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Fieldable> it = document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Fieldable field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.reusableTokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          while (tokenStream.incrementToken()) {\n            // TODO: this is a simple workaround to still work with tokens, not very effective, but as far as I know, this writer should get removed soon:\n            final Token token = new Token();\n            for (Iterator<AttributeImpl> atts = tokenStream.getAttributeImplsIterator(); atts.hasNext();) {\n              final AttributeImpl att = atts.next();\n              try {\n                att.copyTo(token);\n              } catch (Exception e) {\n                // ignore unsupported attributes,\n                // this may fail to copy some attributes, if a special combined AttributeImpl is used, that\n                // implements basic attributes supported by Token and also other customized ones in one class.\n              }\n            }\n            tokens.add(token); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n          }\n          tokenStream.end();\n          tokenStream.close();\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setEmpty().append(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Fieldable, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.toString());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.toString(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1509f151d7692d84fae414b2b799ac06ba60fcb4","date":1314451621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","pathOld":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","sourceNew":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (IndexableField field : document.getDocument()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = field.name();\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.boost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.omitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.indexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.tokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.stored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.binaryValue() != null) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.storeTermVectors()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.storeTermVectorPositions()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.storeTermVectorOffsets()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<IndexableField, LinkedList<Token>> tokensByField = new LinkedHashMap<IndexableField, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<IndexableField> it = document.getDocument().iterator(); it.hasNext();) {\n\n      IndexableField field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.indexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.tokenized()) {\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.reusableTokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          while (tokenStream.incrementToken()) {\n            // TODO: this is a simple workaround to still work with tokens, not very effective, but as far as I know, this writer should get removed soon:\n            final Token token = new Token();\n            for (Iterator<AttributeImpl> atts = tokenStream.getAttributeImplsIterator(); atts.hasNext();) {\n              final AttributeImpl att = atts.next();\n              try {\n                att.copyTo(token);\n              } catch (Exception e) {\n                // ignore unsupported attributes,\n                // this may fail to copy some attributes, if a special combined AttributeImpl is used, that\n                // implements basic attributes supported by Token and also other customized ones in one class.\n              }\n            }\n            tokens.add(token); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n          }\n          tokenStream.end();\n          tokenStream.close();\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setEmpty().append(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.stored()) {\n        //it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<IndexableField, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.toString());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.toString(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().storeTermVectorOffsets()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().storeTermVectorOffsets()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","sourceOld":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Fieldable field : document.getDocument().getFields()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = field.name();\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.isIndexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.isTokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.isStored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.isBinary()) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.isTermVectorStored()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Fieldable, LinkedList<Token>> tokensByField = new LinkedHashMap<Fieldable, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Fieldable> it = document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Fieldable field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.reusableTokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          while (tokenStream.incrementToken()) {\n            // TODO: this is a simple workaround to still work with tokens, not very effective, but as far as I know, this writer should get removed soon:\n            final Token token = new Token();\n            for (Iterator<AttributeImpl> atts = tokenStream.getAttributeImplsIterator(); atts.hasNext();) {\n              final AttributeImpl att = atts.next();\n              try {\n                att.copyTo(token);\n              } catch (Exception e) {\n                // ignore unsupported attributes,\n                // this may fail to copy some attributes, if a special combined AttributeImpl is used, that\n                // implements basic attributes supported by Token and also other customized ones in one class.\n              }\n            }\n            tokens.add(token); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n          }\n          tokenStream.end();\n          tokenStream.close();\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setEmpty().append(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Fieldable, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.toString());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.toString(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6eb141f80638abdb6ffaa5149877f36ea39b6ad5","date":1315714072,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","pathOld":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","sourceNew":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (IndexableField field : document.getDocument()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = field.name();\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.boost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.fieldType().omitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.fieldType().indexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.fieldType().tokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.fieldType().stored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.binaryValue() != null) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.fieldType().storeTermVectors()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.fieldType().storeTermVectorPositions()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.fieldType().storeTermVectorOffsets()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<IndexableField, LinkedList<Token>> tokensByField = new LinkedHashMap<IndexableField, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<IndexableField> it = document.getDocument().iterator(); it.hasNext();) {\n\n      IndexableField field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.fieldType().indexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.fieldType().tokenized()) {\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.reusableTokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          while (tokenStream.incrementToken()) {\n            // TODO: this is a simple workaround to still work with tokens, not very effective, but as far as I know, this writer should get removed soon:\n            final Token token = new Token();\n            for (Iterator<AttributeImpl> atts = tokenStream.getAttributeImplsIterator(); atts.hasNext();) {\n              final AttributeImpl att = atts.next();\n              try {\n                att.copyTo(token);\n              } catch (Exception e) {\n                // ignore unsupported attributes,\n                // this may fail to copy some attributes, if a special combined AttributeImpl is used, that\n                // implements basic attributes supported by Token and also other customized ones in one class.\n              }\n            }\n            tokens.add(token); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n          }\n          tokenStream.end();\n          tokenStream.close();\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setEmpty().append(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.fieldType().stored()) {\n        //it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<IndexableField, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.toString());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.toString(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().fieldType().storeTermVectorOffsets()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().fieldType().storeTermVectorOffsets()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","sourceOld":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (IndexableField field : document.getDocument()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = field.name();\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.boost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.omitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.indexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.tokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.stored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.binaryValue() != null) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.storeTermVectors()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.storeTermVectorPositions()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.storeTermVectorOffsets()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<IndexableField, LinkedList<Token>> tokensByField = new LinkedHashMap<IndexableField, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<IndexableField> it = document.getDocument().iterator(); it.hasNext();) {\n\n      IndexableField field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.indexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.tokenized()) {\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.reusableTokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          while (tokenStream.incrementToken()) {\n            // TODO: this is a simple workaround to still work with tokens, not very effective, but as far as I know, this writer should get removed soon:\n            final Token token = new Token();\n            for (Iterator<AttributeImpl> atts = tokenStream.getAttributeImplsIterator(); atts.hasNext();) {\n              final AttributeImpl att = atts.next();\n              try {\n                att.copyTo(token);\n              } catch (Exception e) {\n                // ignore unsupported attributes,\n                // this may fail to copy some attributes, if a special combined AttributeImpl is used, that\n                // implements basic attributes supported by Token and also other customized ones in one class.\n              }\n            }\n            tokens.add(token); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n          }\n          tokenStream.end();\n          tokenStream.close();\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setEmpty().append(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.stored()) {\n        //it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<IndexableField, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.toString());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.toString(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().storeTermVectorOffsets()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().storeTermVectorOffsets()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8b3bdb938a073ccc28d7ed813f6e8c4cb58e04c5","date":1316747797,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","pathOld":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","sourceNew":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (IndexableField field : document.getDocument()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = field.name();\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.boost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.fieldType().omitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.fieldType().indexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.fieldType().tokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.fieldType().stored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.binaryValue() != null) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.fieldType().storeTermVectors()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.fieldType().storeTermVectorPositions()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.fieldType().storeTermVectorOffsets()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<IndexableField, LinkedList<Token>> tokensByField = new LinkedHashMap<IndexableField, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<IndexableField> it = document.getDocument().iterator(); it.hasNext();) {\n\n      IndexableField field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.fieldType().indexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.fieldType().tokenized()) {\n          final TokenStream tokenStream = field.tokenStream(analyzer);\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          while (tokenStream.incrementToken()) {\n            // TODO: this is a simple workaround to still work with tokens, not very effective, but as far as I know, this writer should get removed soon:\n            final Token token = new Token();\n            for (Iterator<AttributeImpl> atts = tokenStream.getAttributeImplsIterator(); atts.hasNext();) {\n              final AttributeImpl att = atts.next();\n              try {\n                att.copyTo(token);\n              } catch (Exception e) {\n                // ignore unsupported attributes,\n                // this may fail to copy some attributes, if a special combined AttributeImpl is used, that\n                // implements basic attributes supported by Token and also other customized ones in one class.\n              }\n            }\n            tokens.add(token); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n          }\n          tokenStream.end();\n          tokenStream.close();\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setEmpty().append(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.fieldType().stored()) {\n        //it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<IndexableField, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.toString());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.toString(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().fieldType().storeTermVectorOffsets()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().fieldType().storeTermVectorOffsets()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","sourceOld":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (IndexableField field : document.getDocument()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = field.name();\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.boost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.fieldType().omitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.fieldType().indexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.fieldType().tokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.fieldType().stored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.binaryValue() != null) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.fieldType().storeTermVectors()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.fieldType().storeTermVectorPositions()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.fieldType().storeTermVectorOffsets()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<IndexableField, LinkedList<Token>> tokensByField = new LinkedHashMap<IndexableField, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<IndexableField> it = document.getDocument().iterator(); it.hasNext();) {\n\n      IndexableField field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.fieldType().indexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.fieldType().tokenized()) {\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.reusableTokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          while (tokenStream.incrementToken()) {\n            // TODO: this is a simple workaround to still work with tokens, not very effective, but as far as I know, this writer should get removed soon:\n            final Token token = new Token();\n            for (Iterator<AttributeImpl> atts = tokenStream.getAttributeImplsIterator(); atts.hasNext();) {\n              final AttributeImpl att = atts.next();\n              try {\n                att.copyTo(token);\n              } catch (Exception e) {\n                // ignore unsupported attributes,\n                // this may fail to copy some attributes, if a special combined AttributeImpl is used, that\n                // implements basic attributes supported by Token and also other customized ones in one class.\n              }\n            }\n            tokens.add(token); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n          }\n          tokenStream.end();\n          tokenStream.close();\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setEmpty().append(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.fieldType().stored()) {\n        //it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<IndexableField, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.toString());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.toString(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().fieldType().storeTermVectorOffsets()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().fieldType().storeTermVectorOffsets()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3cc749c053615f5871f3b95715fe292f34e70a53","date":1321470575,"type":4,"author":"Robert Muir","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","sourceNew":null,"sourceOld":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (IndexableField field : document.getDocument()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = field.name();\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.boost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.fieldType().omitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.fieldType().indexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.fieldType().tokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.fieldType().stored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.binaryValue() != null) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.fieldType().storeTermVectors()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.fieldType().storeTermVectorPositions()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.fieldType().storeTermVectorOffsets()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<IndexableField, LinkedList<Token>> tokensByField = new LinkedHashMap<IndexableField, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<IndexableField> it = document.getDocument().iterator(); it.hasNext();) {\n\n      IndexableField field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.fieldType().indexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.fieldType().tokenized()) {\n          final TokenStream tokenStream = field.tokenStream(analyzer);\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          while (tokenStream.incrementToken()) {\n            // TODO: this is a simple workaround to still work with tokens, not very effective, but as far as I know, this writer should get removed soon:\n            final Token token = new Token();\n            for (Iterator<AttributeImpl> atts = tokenStream.getAttributeImplsIterator(); atts.hasNext();) {\n              final AttributeImpl att = atts.next();\n              try {\n                att.copyTo(token);\n              } catch (Exception e) {\n                // ignore unsupported attributes,\n                // this may fail to copy some attributes, if a special combined AttributeImpl is used, that\n                // implements basic attributes supported by Token and also other customized ones in one class.\n              }\n            }\n            tokens.add(token); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n          }\n          tokenStream.end();\n          tokenStream.close();\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setEmpty().append(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.fieldType().stored()) {\n        //it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<IndexableField, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.toString());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.toString(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().fieldType().storeTermVectorOffsets()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().fieldType().storeTermVectorOffsets()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"fafef7c83fe8e0b3ca9298d5d75d6b943dc28153":["00746ad002a54281629e3b6f3eb39833a33f093e"],"6eb141f80638abdb6ffaa5149877f36ea39b6ad5":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"3cc749c053615f5871f3b95715fe292f34e70a53":["8b3bdb938a073ccc28d7ed813f6e8c4cb58e04c5"],"2553b00f699380c64959ccb27991289aae87be2e":["a3776dccca01c11e7046323cfad46a3b4a471233","fafef7c83fe8e0b3ca9298d5d75d6b943dc28153"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["a7347509fad0711ac30cb15a746e9a3830a38ebd","4e8cc373c801e54cec75daf9f52792cb4b17f536"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["00746ad002a54281629e3b6f3eb39833a33f093e","fafef7c83fe8e0b3ca9298d5d75d6b943dc28153"],"c3a8a449466c1ff7ce2274fe73dab487256964b4":["3bb13258feba31ab676502787ab2e1779f129b7a","00746ad002a54281629e3b6f3eb39833a33f093e"],"a3776dccca01c11e7046323cfad46a3b4a471233":["4e8cc373c801e54cec75daf9f52792cb4b17f536","00746ad002a54281629e3b6f3eb39833a33f093e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["fafef7c83fe8e0b3ca9298d5d75d6b943dc28153"],"8b3bdb938a073ccc28d7ed813f6e8c4cb58e04c5":["6eb141f80638abdb6ffaa5149877f36ea39b6ad5"],"00746ad002a54281629e3b6f3eb39833a33f093e":["4e8cc373c801e54cec75daf9f52792cb4b17f536"],"a7347509fad0711ac30cb15a746e9a3830a38ebd":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"3bb13258feba31ab676502787ab2e1779f129b7a":["a7347509fad0711ac30cb15a746e9a3830a38ebd","4e8cc373c801e54cec75daf9f52792cb4b17f536"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3cc749c053615f5871f3b95715fe292f34e70a53"],"4e8cc373c801e54cec75daf9f52792cb4b17f536":["a7347509fad0711ac30cb15a746e9a3830a38ebd"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"fafef7c83fe8e0b3ca9298d5d75d6b943dc28153":["2553b00f699380c64959ccb27991289aae87be2e","d083e83f225b11e5fdd900e83d26ddb385b6955c","1509f151d7692d84fae414b2b799ac06ba60fcb4"],"6eb141f80638abdb6ffaa5149877f36ea39b6ad5":["8b3bdb938a073ccc28d7ed813f6e8c4cb58e04c5"],"3cc749c053615f5871f3b95715fe292f34e70a53":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"2553b00f699380c64959ccb27991289aae87be2e":[],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":[],"d083e83f225b11e5fdd900e83d26ddb385b6955c":[],"c3a8a449466c1ff7ce2274fe73dab487256964b4":[],"a3776dccca01c11e7046323cfad46a3b4a471233":["2553b00f699380c64959ccb27991289aae87be2e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["6eb141f80638abdb6ffaa5149877f36ea39b6ad5"],"00746ad002a54281629e3b6f3eb39833a33f093e":["fafef7c83fe8e0b3ca9298d5d75d6b943dc28153","d083e83f225b11e5fdd900e83d26ddb385b6955c","c3a8a449466c1ff7ce2274fe73dab487256964b4","a3776dccca01c11e7046323cfad46a3b4a471233"],"8b3bdb938a073ccc28d7ed813f6e8c4cb58e04c5":["3cc749c053615f5871f3b95715fe292f34e70a53"],"a7347509fad0711ac30cb15a746e9a3830a38ebd":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","3bb13258feba31ab676502787ab2e1779f129b7a","4e8cc373c801e54cec75daf9f52792cb4b17f536"],"3bb13258feba31ab676502787ab2e1779f129b7a":["c3a8a449466c1ff7ce2274fe73dab487256964b4"],"4e8cc373c801e54cec75daf9f52792cb4b17f536":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","a3776dccca01c11e7046323cfad46a3b4a471233","00746ad002a54281629e3b6f3eb39833a33f093e","3bb13258feba31ab676502787ab2e1779f129b7a"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a7347509fad0711ac30cb15a746e9a3830a38ebd"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["2553b00f699380c64959ccb27991289aae87be2e","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","d083e83f225b11e5fdd900e83d26ddb385b6955c","c3a8a449466c1ff7ce2274fe73dab487256964b4","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}