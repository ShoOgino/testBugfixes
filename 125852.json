{"path":"lucene/src/test/org/apache/lucene/search/TestBoolean2#setUp().mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestBoolean2#setUp().mjava","pathOld":"src/test/org/apache/lucene/search/TestBoolean2#setUp().mjava","sourceNew":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    RAMDirectory directory = new RAMDirectory();\n    IndexWriter writer= new IndexWriter(directory, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(new Field(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockRAMDirectory(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new RAMDirectory(dir2);\n      IndexWriter w = new IndexWriter(dir2, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));\n      w.addIndexesNoOptimize(new Directory[] {copy});\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    IndexWriter w = new IndexWriter(dir2, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));\n    Document doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    // optimize to 1 segment\n    w.optimize();\n    reader = w.getReader();\n    w.close();\n    bigSearcher = new IndexSearcher(reader);\n  }\n\n","sourceOld":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    RAMDirectory directory = new RAMDirectory();\n    IndexWriter writer= new IndexWriter(directory, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(new Field(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockRAMDirectory(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new RAMDirectory(dir2);\n      IndexWriter w = new IndexWriter(dir2, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));\n      w.addIndexesNoOptimize(new Directory[] {copy});\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    IndexWriter w = new IndexWriter(dir2, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));\n    Document doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    // optimize to 1 segment\n    w.optimize();\n    reader = w.getReader();\n    w.close();\n    bigSearcher = new IndexSearcher(reader);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d572389229127c297dd1fa5ce4758e1cec41e799","date":1273610938,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestBoolean2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestBoolean2#setUp().mjava","sourceNew":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    RAMDirectory directory = new RAMDirectory();\n    IndexWriter writer= new IndexWriter(directory, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(new Field(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockRAMDirectory(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new RAMDirectory(dir2);\n      IndexWriter w = new IndexWriter(dir2, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n      w.addIndexesNoOptimize(new Directory[] {copy});\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    IndexWriter w = new IndexWriter(dir2, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n    Document doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    // optimize to 1 segment\n    w.optimize();\n    reader = w.getReader();\n    w.close();\n    bigSearcher = new IndexSearcher(reader);\n  }\n\n","sourceOld":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    RAMDirectory directory = new RAMDirectory();\n    IndexWriter writer= new IndexWriter(directory, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(new Field(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockRAMDirectory(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new RAMDirectory(dir2);\n      IndexWriter w = new IndexWriter(dir2, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));\n      w.addIndexesNoOptimize(new Directory[] {copy});\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    IndexWriter w = new IndexWriter(dir2, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));\n    Document doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    // optimize to 1 segment\n    w.optimize();\n    reader = w.getReader();\n    w.close();\n    bigSearcher = new IndexSearcher(reader);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fb10b6bcde550b87d8f10e5f010bd8f3021023b6","date":1274974592,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestBoolean2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestBoolean2#setUp().mjava","sourceNew":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    RAMDirectory directory = new RAMDirectory();\n    IndexWriter writer= new IndexWriter(directory, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(new Field(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockRAMDirectory(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new RAMDirectory(dir2);\n      IndexWriter w = new IndexWriter(dir2, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n      w.addIndexes(new Directory[] {copy});\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    IndexWriter w = new IndexWriter(dir2, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n    Document doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    // optimize to 1 segment\n    w.optimize();\n    reader = w.getReader();\n    w.close();\n    bigSearcher = new IndexSearcher(reader);\n  }\n\n","sourceOld":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    RAMDirectory directory = new RAMDirectory();\n    IndexWriter writer= new IndexWriter(directory, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(new Field(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockRAMDirectory(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new RAMDirectory(dir2);\n      IndexWriter w = new IndexWriter(dir2, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n      w.addIndexesNoOptimize(new Directory[] {copy});\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    IndexWriter w = new IndexWriter(dir2, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n    Document doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    // optimize to 1 segment\n    w.optimize();\n    reader = w.getReader();\n    w.close();\n    bigSearcher = new IndexSearcher(reader);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c084e47df29de3330311d69dabf515ceaa989512","date":1279030906,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestBoolean2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestBoolean2#setUp().mjava","sourceNew":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    rnd = newRandom();\n    RAMDirectory directory = new RAMDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(rnd, directory, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(new Field(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockRAMDirectory(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new RAMDirectory(dir2);\n      RandomIndexWriter w = new RandomIndexWriter(rnd, dir2, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n      w.addIndexes(new Directory[] {copy});\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(rnd, dir2, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n    Document doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = new IndexSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    RAMDirectory directory = new RAMDirectory();\n    IndexWriter writer= new IndexWriter(directory, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(new Field(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockRAMDirectory(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new RAMDirectory(dir2);\n      IndexWriter w = new IndexWriter(dir2, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n      w.addIndexes(new Directory[] {copy});\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    IndexWriter w = new IndexWriter(dir2, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n    Document doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    // optimize to 1 segment\n    w.optimize();\n    reader = w.getReader();\n    w.close();\n    bigSearcher = new IndexSearcher(reader);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5f4e87790277826a2aea119328600dfb07761f32","date":1279827275,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestBoolean2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestBoolean2#setUp().mjava","sourceNew":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    rnd = newRandom();\n    RAMDirectory directory = new RAMDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(rnd, directory, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(new Field(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockRAMDirectory(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new RAMDirectory(dir2);\n      RandomIndexWriter w = new RandomIndexWriter(rnd, dir2, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n      w.addIndexes(new Directory[] {copy});\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(rnd, dir2, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n    Document doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = new IndexSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    RAMDirectory directory = new RAMDirectory();\n    IndexWriter writer= new IndexWriter(directory, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(new Field(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockRAMDirectory(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new RAMDirectory(dir2);\n      IndexWriter w = new IndexWriter(dir2, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n      w.addIndexes(new Directory[] {copy});\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    IndexWriter w = new IndexWriter(dir2, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n    Document doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    // optimize to 1 segment\n    w.optimize();\n    reader = w.getReader();\n    w.close();\n    bigSearcher = new IndexSearcher(reader);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"15bbd254c1506df5299c4df8c148262c7bd6301e","date":1279913113,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestBoolean2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestBoolean2#setUp().mjava","sourceNew":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    rnd = newRandom();\n    RAMDirectory directory = new RAMDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(rnd, directory);\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(new Field(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockRAMDirectory(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new RAMDirectory(dir2);\n      RandomIndexWriter w = new RandomIndexWriter(rnd, dir2);\n      w.addIndexes(new Directory[] {copy});\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(rnd, dir2);\n    Document doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = new IndexSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    rnd = newRandom();\n    RAMDirectory directory = new RAMDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(rnd, directory, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(new Field(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockRAMDirectory(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new RAMDirectory(dir2);\n      RandomIndexWriter w = new RandomIndexWriter(rnd, dir2, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n      w.addIndexes(new Directory[] {copy});\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(rnd, dir2, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n    Document doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = new IndexSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4b103252dee6afa1b6d7a622c773d178788eb85a","date":1280180143,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestBoolean2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestBoolean2#setUp().mjava","sourceNew":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    rnd = newRandom();\n    RAMDirectory directory = new RAMDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(rnd, directory);\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(new Field(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockRAMDirectory(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new RAMDirectory(dir2);\n      RandomIndexWriter w = new RandomIndexWriter(rnd, dir2);\n      w.addIndexes(new Directory[] {copy});\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(rnd, dir2);\n    Document doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = new IndexSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    rnd = newRandom();\n    RAMDirectory directory = new RAMDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(rnd, directory, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(new Field(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockRAMDirectory(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new RAMDirectory(dir2);\n      RandomIndexWriter w = new RandomIndexWriter(rnd, dir2, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n      w.addIndexes(new Directory[] {copy});\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(rnd, dir2, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n    Document doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = new IndexSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3242a09f703274d3b9283f2064a1a33064b53a1b","date":1280263474,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestBoolean2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestBoolean2#setUp().mjava","sourceNew":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    rnd = newRandom();\n    RAMDirectory directory = new RAMDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(rnd, directory);\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(new Field(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockRAMDirectory(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new RAMDirectory(dir2);\n      RandomIndexWriter w = new RandomIndexWriter(rnd, dir2);\n      w.addIndexes(new Directory[] {copy});\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(rnd, dir2);\n    Document doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = new IndexSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    rnd = newRandom();\n    RAMDirectory directory = new RAMDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(rnd, directory, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(new Field(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockRAMDirectory(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new RAMDirectory(dir2);\n      RandomIndexWriter w = new RandomIndexWriter(rnd, dir2, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n      w.addIndexes(new Directory[] {copy});\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(rnd, dir2, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n    Document doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = new IndexSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1a2e46fd1b7cbc52d7d6461a6ef99e7107ae2a9c","date":1281477834,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestBoolean2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestBoolean2#setUp().mjava","sourceNew":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    rnd = newRandom();\n    MockRAMDirectory directory = new MockRAMDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(rnd, directory);\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(new Field(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockRAMDirectory(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockRAMDirectory(dir2);\n      RandomIndexWriter w = new RandomIndexWriter(rnd, dir2);\n      w.addIndexes(new Directory[] {copy});\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(rnd, dir2);\n    Document doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = new IndexSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    rnd = newRandom();\n    RAMDirectory directory = new RAMDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(rnd, directory);\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(new Field(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockRAMDirectory(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new RAMDirectory(dir2);\n      RandomIndexWriter w = new RandomIndexWriter(rnd, dir2);\n      w.addIndexes(new Directory[] {copy});\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(rnd, dir2);\n    Document doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = new IndexSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c","date":1281646583,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestBoolean2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestBoolean2#setUp().mjava","sourceNew":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    rnd = newRandom();\n    directory = newDirectory(rnd);\n    RandomIndexWriter writer= new RandomIndexWriter(rnd, directory);\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(new Field(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockRAMDirectory(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockRAMDirectory(dir2);\n      RandomIndexWriter w = new RandomIndexWriter(rnd, dir2);\n      w.addIndexes(new Directory[] {copy});\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(rnd, dir2);\n    Document doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = new IndexSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    rnd = newRandom();\n    MockRAMDirectory directory = new MockRAMDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(rnd, directory);\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(new Field(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockRAMDirectory(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockRAMDirectory(dir2);\n      RandomIndexWriter w = new RandomIndexWriter(rnd, dir2);\n      w.addIndexes(new Directory[] {copy});\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(rnd, dir2);\n    Document doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = new IndexSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a05409176bd65129d67a785ee70e881e238a9aef","date":1282582843,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestBoolean2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestBoolean2#setUp().mjava","sourceNew":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    rnd = newRandom();\n    directory = newDirectory(rnd);\n    RandomIndexWriter writer= new RandomIndexWriter(rnd, directory);\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(new Field(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(rnd, dir2);\n      w.addIndexes(new Directory[] {copy});\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(rnd, dir2);\n    Document doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = new IndexSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    rnd = newRandom();\n    directory = newDirectory(rnd);\n    RandomIndexWriter writer= new RandomIndexWriter(rnd, directory);\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(new Field(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockRAMDirectory(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockRAMDirectory(dir2);\n      RandomIndexWriter w = new RandomIndexWriter(rnd, dir2);\n      w.addIndexes(new Directory[] {copy});\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(rnd, dir2);\n    Document doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = new IndexSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1f653cfcf159baeaafe5d01682a911e95bba4012","date":1284122058,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestBoolean2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestBoolean2#setUp().mjava","sourceNew":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory);\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(new Field(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(new Directory[] {copy});\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n    Document doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = new IndexSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    rnd = newRandom();\n    directory = newDirectory(rnd);\n    RandomIndexWriter writer= new RandomIndexWriter(rnd, directory);\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(new Field(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(rnd, dir2);\n      w.addIndexes(new Directory[] {copy});\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(rnd, dir2);\n    Document doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = new IndexSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"132903c28af3aa6f67284b78de91c0f0a99488c2","date":1284282129,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestBoolean2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestBoolean2#setUp().mjava","sourceNew":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory);\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(new Directory[] {copy});\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = new IndexSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory);\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(new Field(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(new Directory[] {copy});\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n    Document doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = new IndexSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"150488c1317972164a9a824be05b1ba2ba0fc68c","date":1284316090,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestBoolean2#setUp().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    Random random = newStaticRandom(TestBoolean2.class);\n    directory = newDirectory(random);\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory);\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(random, field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(new Directory[] {copy});\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(random, \"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(random, \"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = new IndexSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory);\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(new Directory[] {copy});\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = new IndexSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":4,"author":"Michael Busch","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/src/test/org/apache/lucene/search/TestBoolean2#setUp().mjava","sourceNew":null,"sourceOld":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    rnd = newRandom();\n    RAMDirectory directory = new RAMDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(rnd, directory);\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(new Field(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockRAMDirectory(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new RAMDirectory(dir2);\n      RandomIndexWriter w = new RandomIndexWriter(rnd, dir2);\n      w.addIndexes(new Directory[] {copy});\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(rnd, dir2);\n    Document doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = new IndexSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"3242a09f703274d3b9283f2064a1a33064b53a1b":["5f4e87790277826a2aea119328600dfb07761f32","4b103252dee6afa1b6d7a622c773d178788eb85a"],"4b103252dee6afa1b6d7a622c773d178788eb85a":["c084e47df29de3330311d69dabf515ceaa989512","15bbd254c1506df5299c4df8c148262c7bd6301e"],"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c":["1a2e46fd1b7cbc52d7d6461a6ef99e7107ae2a9c"],"15bbd254c1506df5299c4df8c148262c7bd6301e":["c084e47df29de3330311d69dabf515ceaa989512"],"d572389229127c297dd1fa5ce4758e1cec41e799":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"132903c28af3aa6f67284b78de91c0f0a99488c2":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"5f4e87790277826a2aea119328600dfb07761f32":["fb10b6bcde550b87d8f10e5f010bd8f3021023b6","c084e47df29de3330311d69dabf515ceaa989512"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["3242a09f703274d3b9283f2064a1a33064b53a1b","150488c1317972164a9a824be05b1ba2ba0fc68c"],"fb10b6bcde550b87d8f10e5f010bd8f3021023b6":["d572389229127c297dd1fa5ce4758e1cec41e799"],"c084e47df29de3330311d69dabf515ceaa989512":["fb10b6bcde550b87d8f10e5f010bd8f3021023b6"],"1f653cfcf159baeaafe5d01682a911e95bba4012":["a05409176bd65129d67a785ee70e881e238a9aef"],"1a2e46fd1b7cbc52d7d6461a6ef99e7107ae2a9c":["4b103252dee6afa1b6d7a622c773d178788eb85a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a05409176bd65129d67a785ee70e881e238a9aef":["ab9633cb67e3c0aec3c066147a23a957d6e7ad8c"],"150488c1317972164a9a824be05b1ba2ba0fc68c":["132903c28af3aa6f67284b78de91c0f0a99488c2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["150488c1317972164a9a824be05b1ba2ba0fc68c"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"3242a09f703274d3b9283f2064a1a33064b53a1b":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"4b103252dee6afa1b6d7a622c773d178788eb85a":["3242a09f703274d3b9283f2064a1a33064b53a1b","1a2e46fd1b7cbc52d7d6461a6ef99e7107ae2a9c"],"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c":["a05409176bd65129d67a785ee70e881e238a9aef"],"15bbd254c1506df5299c4df8c148262c7bd6301e":["4b103252dee6afa1b6d7a622c773d178788eb85a"],"d572389229127c297dd1fa5ce4758e1cec41e799":["fb10b6bcde550b87d8f10e5f010bd8f3021023b6"],"5f4e87790277826a2aea119328600dfb07761f32":["3242a09f703274d3b9283f2064a1a33064b53a1b"],"132903c28af3aa6f67284b78de91c0f0a99488c2":["150488c1317972164a9a824be05b1ba2ba0fc68c"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":[],"fb10b6bcde550b87d8f10e5f010bd8f3021023b6":["5f4e87790277826a2aea119328600dfb07761f32","c084e47df29de3330311d69dabf515ceaa989512"],"c084e47df29de3330311d69dabf515ceaa989512":["4b103252dee6afa1b6d7a622c773d178788eb85a","15bbd254c1506df5299c4df8c148262c7bd6301e","5f4e87790277826a2aea119328600dfb07761f32"],"1f653cfcf159baeaafe5d01682a911e95bba4012":["132903c28af3aa6f67284b78de91c0f0a99488c2"],"1a2e46fd1b7cbc52d7d6461a6ef99e7107ae2a9c":["ab9633cb67e3c0aec3c066147a23a957d6e7ad8c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"a05409176bd65129d67a785ee70e881e238a9aef":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"150488c1317972164a9a824be05b1ba2ba0fc68c":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["d572389229127c297dd1fa5ce4758e1cec41e799"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}