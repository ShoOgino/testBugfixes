{"path":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/MapReduceIndexerTool#renameTreeMergeShardDirs(Path,Job,FileSystem).mjava","commits":[{"id":"42d384b06aa87eae925b668b65f3246154f0b0fa","date":1386181725,"type":0,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/MapReduceIndexerTool#renameTreeMergeShardDirs(Path,Job,FileSystem).mjava","pathOld":"/dev/null","sourceNew":"  /*\n   * You can run MapReduceIndexerTool in Solrcloud mode, and once the MR job completes, you can use\n   * the standard solrj Solrcloud API to send doc updates and deletes to SolrCloud, and those updates\n   * and deletes will go to the right Solr shards, and it will work just fine.\n   * \n   * The MapReduce framework doesn't guarantee that input split N goes to the map task with the\n   * taskId = N. The job tracker and Yarn schedule and assign tasks, considering data locality\n   * aspects, but without regard of the input split# withing the overall list of input splits. In\n   * other words, split# != taskId can be true.\n   * \n   * To deal with this issue, our mapper tasks write a little auxiliary metadata file (per task)\n   * that tells the job driver which taskId processed which split#. Once the mapper-only job is\n   * completed, the job driver renames the output dirs such that the dir name contains the true solr\n   * shard id, based on these auxiliary files.\n   * \n   * This way each doc gets assigned to the right Solr shard even with #reducers > #solrshards\n   * \n   * Example for a merge with two shards:\n   * \n   * part-m-00000 and part-m-00001 goes to outputShardNum = 0 and will end up in merged part-m-00000\n   * part-m-00002 and part-m-00003 goes to outputShardNum = 1 and will end up in merged part-m-00001\n   * part-m-00004 and part-m-00005 goes to outputShardNum = 2 and will end up in merged part-m-00002\n   * ... and so on\n   * \n   * Also see run() method above where it uses NLineInputFormat.setNumLinesPerSplit(job,\n   * options.fanout)\n   * \n   * Also see TreeMergeOutputFormat.TreeMergeRecordWriter.writeShardNumberFile()\n   */\n  private boolean renameTreeMergeShardDirs(Path outputTreeMergeStep, Job job, FileSystem fs) throws IOException {\n    final String dirPrefix = SolrOutputFormat.getOutputName(job);\n    FileStatus[] dirs = fs.listStatus(outputTreeMergeStep, new PathFilter() {      \n      @Override\n      public boolean accept(Path path) {\n        return path.getName().startsWith(dirPrefix);\n      }\n    });\n    \n    for (FileStatus dir : dirs) {\n      if (!dir.isDirectory()) {\n        throw new IllegalStateException(\"Not a directory: \" + dir.getPath());\n      }\n    }\n\n    // Example: rename part-m-00004 to _part-m-00004\n    for (FileStatus dir : dirs) {\n      Path path = dir.getPath();\n      Path renamedPath = new Path(path.getParent(), \"_\" + path.getName());\n      if (!rename(path, renamedPath, fs)) {\n        return false;\n      }\n    }\n    \n    // Example: rename _part-m-00004 to part-m-00002\n    for (FileStatus dir : dirs) {\n      Path path = dir.getPath();\n      Path renamedPath = new Path(path.getParent(), \"_\" + path.getName());\n      \n      // read auxiliary metadata file (per task) that tells which taskId \n      // processed which split# aka solrShard\n      Path solrShardNumberFile = new Path(renamedPath, TreeMergeMapper.SOLR_SHARD_NUMBER);\n      InputStream in = fs.open(solrShardNumberFile);\n      byte[] bytes = ByteStreams.toByteArray(in);\n      in.close();\n      Preconditions.checkArgument(bytes.length > 0);\n      int solrShard = Integer.parseInt(new String(bytes, Charsets.UTF_8));\n      if (!delete(solrShardNumberFile, false, fs)) {\n        return false;\n      }\n      \n      // same as FileOutputFormat.NUMBER_FORMAT\n      NumberFormat numberFormat = NumberFormat.getInstance(Locale.ENGLISH);\n      numberFormat.setMinimumIntegerDigits(5);\n      numberFormat.setGroupingUsed(false);\n      Path finalPath = new Path(renamedPath.getParent(), dirPrefix + \"-m-\" + numberFormat.format(solrShard));\n      \n      LOG.info(\"MTree merge renaming solr shard: \" + solrShard + \" from dir: \" + dir.getPath() + \" to dir: \" + finalPath);\n      if (!rename(renamedPath, finalPath, fs)) {\n        return false;\n      }\n    }\n    return true;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"74f45af4339b0daf7a95c820ab88c1aea74fbce0","date":1387475327,"type":0,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/MapReduceIndexerTool#renameTreeMergeShardDirs(Path,Job,FileSystem).mjava","pathOld":"/dev/null","sourceNew":"  /*\n   * You can run MapReduceIndexerTool in Solrcloud mode, and once the MR job completes, you can use\n   * the standard solrj Solrcloud API to send doc updates and deletes to SolrCloud, and those updates\n   * and deletes will go to the right Solr shards, and it will work just fine.\n   * \n   * The MapReduce framework doesn't guarantee that input split N goes to the map task with the\n   * taskId = N. The job tracker and Yarn schedule and assign tasks, considering data locality\n   * aspects, but without regard of the input split# withing the overall list of input splits. In\n   * other words, split# != taskId can be true.\n   * \n   * To deal with this issue, our mapper tasks write a little auxiliary metadata file (per task)\n   * that tells the job driver which taskId processed which split#. Once the mapper-only job is\n   * completed, the job driver renames the output dirs such that the dir name contains the true solr\n   * shard id, based on these auxiliary files.\n   * \n   * This way each doc gets assigned to the right Solr shard even with #reducers > #solrshards\n   * \n   * Example for a merge with two shards:\n   * \n   * part-m-00000 and part-m-00001 goes to outputShardNum = 0 and will end up in merged part-m-00000\n   * part-m-00002 and part-m-00003 goes to outputShardNum = 1 and will end up in merged part-m-00001\n   * part-m-00004 and part-m-00005 goes to outputShardNum = 2 and will end up in merged part-m-00002\n   * ... and so on\n   * \n   * Also see run() method above where it uses NLineInputFormat.setNumLinesPerSplit(job,\n   * options.fanout)\n   * \n   * Also see TreeMergeOutputFormat.TreeMergeRecordWriter.writeShardNumberFile()\n   */\n  private boolean renameTreeMergeShardDirs(Path outputTreeMergeStep, Job job, FileSystem fs) throws IOException {\n    final String dirPrefix = SolrOutputFormat.getOutputName(job);\n    FileStatus[] dirs = fs.listStatus(outputTreeMergeStep, new PathFilter() {      \n      @Override\n      public boolean accept(Path path) {\n        return path.getName().startsWith(dirPrefix);\n      }\n    });\n    \n    for (FileStatus dir : dirs) {\n      if (!dir.isDirectory()) {\n        throw new IllegalStateException(\"Not a directory: \" + dir.getPath());\n      }\n    }\n\n    // Example: rename part-m-00004 to _part-m-00004\n    for (FileStatus dir : dirs) {\n      Path path = dir.getPath();\n      Path renamedPath = new Path(path.getParent(), \"_\" + path.getName());\n      if (!rename(path, renamedPath, fs)) {\n        return false;\n      }\n    }\n    \n    // Example: rename _part-m-00004 to part-m-00002\n    for (FileStatus dir : dirs) {\n      Path path = dir.getPath();\n      Path renamedPath = new Path(path.getParent(), \"_\" + path.getName());\n      \n      // read auxiliary metadata file (per task) that tells which taskId \n      // processed which split# aka solrShard\n      Path solrShardNumberFile = new Path(renamedPath, TreeMergeMapper.SOLR_SHARD_NUMBER);\n      InputStream in = fs.open(solrShardNumberFile);\n      byte[] bytes = ByteStreams.toByteArray(in);\n      in.close();\n      Preconditions.checkArgument(bytes.length > 0);\n      int solrShard = Integer.parseInt(new String(bytes, Charsets.UTF_8));\n      if (!delete(solrShardNumberFile, false, fs)) {\n        return false;\n      }\n      \n      // same as FileOutputFormat.NUMBER_FORMAT\n      NumberFormat numberFormat = NumberFormat.getInstance(Locale.ENGLISH);\n      numberFormat.setMinimumIntegerDigits(5);\n      numberFormat.setGroupingUsed(false);\n      Path finalPath = new Path(renamedPath.getParent(), dirPrefix + \"-m-\" + numberFormat.format(solrShard));\n      \n      LOG.info(\"MTree merge renaming solr shard: \" + solrShard + \" from dir: \" + dir.getPath() + \" to dir: \" + finalPath);\n      if (!rename(renamedPath, finalPath, fs)) {\n        return false;\n      }\n    }\n    return true;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6e1c1455e91339e72d6cdc7518defd3e05a43957","date":1396304204,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/MapReduceIndexerTool#renameTreeMergeShardDirs(Path,Job,FileSystem).mjava","pathOld":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/MapReduceIndexerTool#renameTreeMergeShardDirs(Path,Job,FileSystem).mjava","sourceNew":"  /*\n   * You can run MapReduceIndexerTool in Solrcloud mode, and once the MR job completes, you can use\n   * the standard solrj Solrcloud API to send doc updates and deletes to SolrCloud, and those updates\n   * and deletes will go to the right Solr shards, and it will work just fine.\n   * \n   * The MapReduce framework doesn't guarantee that input split N goes to the map task with the\n   * taskId = N. The job tracker and Yarn schedule and assign tasks, considering data locality\n   * aspects, but without regard of the input split# withing the overall list of input splits. In\n   * other words, split# != taskId can be true.\n   * \n   * To deal with this issue, our mapper tasks write a little auxiliary metadata file (per task)\n   * that tells the job driver which taskId processed which split#. Once the mapper-only job is\n   * completed, the job driver renames the output dirs such that the dir name contains the true solr\n   * shard id, based on these auxiliary files.\n   * \n   * This way each doc gets assigned to the right Solr shard even with #reducers > #solrshards\n   * \n   * Example for a merge with two shards:\n   * \n   * part-m-00000 and part-m-00001 goes to outputShardNum = 0 and will end up in merged part-m-00000\n   * part-m-00002 and part-m-00003 goes to outputShardNum = 1 and will end up in merged part-m-00001\n   * part-m-00004 and part-m-00005 goes to outputShardNum = 2 and will end up in merged part-m-00002\n   * ... and so on\n   * \n   * Also see run() method above where it uses NLineInputFormat.setNumLinesPerSplit(job,\n   * options.fanout)\n   * \n   * Also see TreeMergeOutputFormat.TreeMergeRecordWriter.writeShardNumberFile()\n   */\n  private boolean renameTreeMergeShardDirs(Path outputTreeMergeStep, Job job, FileSystem fs) throws IOException {\n    final String dirPrefix = SolrOutputFormat.getOutputName(job);\n    FileStatus[] dirs = fs.listStatus(outputTreeMergeStep, new PathFilter() {      \n      @Override\n      public boolean accept(Path path) {\n        return path.getName().startsWith(dirPrefix);\n      }\n    });\n    \n    for (FileStatus dir : dirs) {\n      if (!dir.isDirectory()) {\n        throw new IllegalStateException(\"Not a directory: \" + dir.getPath());\n      }\n    }\n\n    // Example: rename part-m-00004 to _part-m-00004\n    for (FileStatus dir : dirs) {\n      Path path = dir.getPath();\n      Path renamedPath = new Path(path.getParent(), \"_\" + path.getName());\n      if (!rename(path, renamedPath, fs)) {\n        return false;\n      }\n    }\n    \n    // Example: rename _part-m-00004 to part-m-00002\n    for (FileStatus dir : dirs) {\n      Path path = dir.getPath();\n      Path renamedPath = new Path(path.getParent(), \"_\" + path.getName());\n      \n      // read auxiliary metadata file (per task) that tells which taskId \n      // processed which split# aka solrShard\n      Path solrShardNumberFile = new Path(renamedPath, TreeMergeMapper.SOLR_SHARD_NUMBER);\n      InputStream in = fs.open(solrShardNumberFile);\n      byte[] bytes = ByteStreams.toByteArray(in);\n      in.close();\n      Preconditions.checkArgument(bytes.length > 0);\n      int solrShard = Integer.parseInt(new String(bytes, StandardCharsets.UTF_8));\n      if (!delete(solrShardNumberFile, false, fs)) {\n        return false;\n      }\n      \n      // same as FileOutputFormat.NUMBER_FORMAT\n      NumberFormat numberFormat = NumberFormat.getInstance(Locale.ENGLISH);\n      numberFormat.setMinimumIntegerDigits(5);\n      numberFormat.setGroupingUsed(false);\n      Path finalPath = new Path(renamedPath.getParent(), dirPrefix + \"-m-\" + numberFormat.format(solrShard));\n      \n      LOG.info(\"MTree merge renaming solr shard: \" + solrShard + \" from dir: \" + dir.getPath() + \" to dir: \" + finalPath);\n      if (!rename(renamedPath, finalPath, fs)) {\n        return false;\n      }\n    }\n    return true;\n  }\n\n","sourceOld":"  /*\n   * You can run MapReduceIndexerTool in Solrcloud mode, and once the MR job completes, you can use\n   * the standard solrj Solrcloud API to send doc updates and deletes to SolrCloud, and those updates\n   * and deletes will go to the right Solr shards, and it will work just fine.\n   * \n   * The MapReduce framework doesn't guarantee that input split N goes to the map task with the\n   * taskId = N. The job tracker and Yarn schedule and assign tasks, considering data locality\n   * aspects, but without regard of the input split# withing the overall list of input splits. In\n   * other words, split# != taskId can be true.\n   * \n   * To deal with this issue, our mapper tasks write a little auxiliary metadata file (per task)\n   * that tells the job driver which taskId processed which split#. Once the mapper-only job is\n   * completed, the job driver renames the output dirs such that the dir name contains the true solr\n   * shard id, based on these auxiliary files.\n   * \n   * This way each doc gets assigned to the right Solr shard even with #reducers > #solrshards\n   * \n   * Example for a merge with two shards:\n   * \n   * part-m-00000 and part-m-00001 goes to outputShardNum = 0 and will end up in merged part-m-00000\n   * part-m-00002 and part-m-00003 goes to outputShardNum = 1 and will end up in merged part-m-00001\n   * part-m-00004 and part-m-00005 goes to outputShardNum = 2 and will end up in merged part-m-00002\n   * ... and so on\n   * \n   * Also see run() method above where it uses NLineInputFormat.setNumLinesPerSplit(job,\n   * options.fanout)\n   * \n   * Also see TreeMergeOutputFormat.TreeMergeRecordWriter.writeShardNumberFile()\n   */\n  private boolean renameTreeMergeShardDirs(Path outputTreeMergeStep, Job job, FileSystem fs) throws IOException {\n    final String dirPrefix = SolrOutputFormat.getOutputName(job);\n    FileStatus[] dirs = fs.listStatus(outputTreeMergeStep, new PathFilter() {      \n      @Override\n      public boolean accept(Path path) {\n        return path.getName().startsWith(dirPrefix);\n      }\n    });\n    \n    for (FileStatus dir : dirs) {\n      if (!dir.isDirectory()) {\n        throw new IllegalStateException(\"Not a directory: \" + dir.getPath());\n      }\n    }\n\n    // Example: rename part-m-00004 to _part-m-00004\n    for (FileStatus dir : dirs) {\n      Path path = dir.getPath();\n      Path renamedPath = new Path(path.getParent(), \"_\" + path.getName());\n      if (!rename(path, renamedPath, fs)) {\n        return false;\n      }\n    }\n    \n    // Example: rename _part-m-00004 to part-m-00002\n    for (FileStatus dir : dirs) {\n      Path path = dir.getPath();\n      Path renamedPath = new Path(path.getParent(), \"_\" + path.getName());\n      \n      // read auxiliary metadata file (per task) that tells which taskId \n      // processed which split# aka solrShard\n      Path solrShardNumberFile = new Path(renamedPath, TreeMergeMapper.SOLR_SHARD_NUMBER);\n      InputStream in = fs.open(solrShardNumberFile);\n      byte[] bytes = ByteStreams.toByteArray(in);\n      in.close();\n      Preconditions.checkArgument(bytes.length > 0);\n      int solrShard = Integer.parseInt(new String(bytes, Charsets.UTF_8));\n      if (!delete(solrShardNumberFile, false, fs)) {\n        return false;\n      }\n      \n      // same as FileOutputFormat.NUMBER_FORMAT\n      NumberFormat numberFormat = NumberFormat.getInstance(Locale.ENGLISH);\n      numberFormat.setMinimumIntegerDigits(5);\n      numberFormat.setGroupingUsed(false);\n      Path finalPath = new Path(renamedPath.getParent(), dirPrefix + \"-m-\" + numberFormat.format(solrShard));\n      \n      LOG.info(\"MTree merge renaming solr shard: \" + solrShard + \" from dir: \" + dir.getPath() + \" to dir: \" + finalPath);\n      if (!rename(renamedPath, finalPath, fs)) {\n        return false;\n      }\n    }\n    return true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5eb2511ababf862ea11e10761c70ee560cd84510","date":1396607225,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/MapReduceIndexerTool#renameTreeMergeShardDirs(Path,Job,FileSystem).mjava","pathOld":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/MapReduceIndexerTool#renameTreeMergeShardDirs(Path,Job,FileSystem).mjava","sourceNew":"  /*\n   * You can run MapReduceIndexerTool in Solrcloud mode, and once the MR job completes, you can use\n   * the standard solrj Solrcloud API to send doc updates and deletes to SolrCloud, and those updates\n   * and deletes will go to the right Solr shards, and it will work just fine.\n   * \n   * The MapReduce framework doesn't guarantee that input split N goes to the map task with the\n   * taskId = N. The job tracker and Yarn schedule and assign tasks, considering data locality\n   * aspects, but without regard of the input split# withing the overall list of input splits. In\n   * other words, split# != taskId can be true.\n   * \n   * To deal with this issue, our mapper tasks write a little auxiliary metadata file (per task)\n   * that tells the job driver which taskId processed which split#. Once the mapper-only job is\n   * completed, the job driver renames the output dirs such that the dir name contains the true solr\n   * shard id, based on these auxiliary files.\n   * \n   * This way each doc gets assigned to the right Solr shard even with #reducers > #solrshards\n   * \n   * Example for a merge with two shards:\n   * \n   * part-m-00000 and part-m-00001 goes to outputShardNum = 0 and will end up in merged part-m-00000\n   * part-m-00002 and part-m-00003 goes to outputShardNum = 1 and will end up in merged part-m-00001\n   * part-m-00004 and part-m-00005 goes to outputShardNum = 2 and will end up in merged part-m-00002\n   * ... and so on\n   * \n   * Also see run() method above where it uses NLineInputFormat.setNumLinesPerSplit(job,\n   * options.fanout)\n   * \n   * Also see TreeMergeOutputFormat.TreeMergeRecordWriter.writeShardNumberFile()\n   */\n  private boolean renameTreeMergeShardDirs(Path outputTreeMergeStep, Job job, FileSystem fs) throws IOException {\n    final String dirPrefix = SolrOutputFormat.getOutputName(job);\n    FileStatus[] dirs = fs.listStatus(outputTreeMergeStep, new PathFilter() {      \n      @Override\n      public boolean accept(Path path) {\n        return path.getName().startsWith(dirPrefix);\n      }\n    });\n    \n    for (FileStatus dir : dirs) {\n      if (!dir.isDirectory()) {\n        throw new IllegalStateException(\"Not a directory: \" + dir.getPath());\n      }\n    }\n\n    // Example: rename part-m-00004 to _part-m-00004\n    for (FileStatus dir : dirs) {\n      Path path = dir.getPath();\n      Path renamedPath = new Path(path.getParent(), \"_\" + path.getName());\n      if (!rename(path, renamedPath, fs)) {\n        return false;\n      }\n    }\n    \n    // Example: rename _part-m-00004 to part-m-00002\n    for (FileStatus dir : dirs) {\n      Path path = dir.getPath();\n      Path renamedPath = new Path(path.getParent(), \"_\" + path.getName());\n      \n      // read auxiliary metadata file (per task) that tells which taskId \n      // processed which split# aka solrShard\n      Path solrShardNumberFile = new Path(renamedPath, TreeMergeMapper.SOLR_SHARD_NUMBER);\n      InputStream in = fs.open(solrShardNumberFile);\n      byte[] bytes = ByteStreams.toByteArray(in);\n      in.close();\n      Preconditions.checkArgument(bytes.length > 0);\n      int solrShard = Integer.parseInt(new String(bytes, StandardCharsets.UTF_8));\n      if (!delete(solrShardNumberFile, false, fs)) {\n        return false;\n      }\n      \n      // same as FileOutputFormat.NUMBER_FORMAT\n      NumberFormat numberFormat = NumberFormat.getInstance(Locale.ENGLISH);\n      numberFormat.setMinimumIntegerDigits(5);\n      numberFormat.setGroupingUsed(false);\n      Path finalPath = new Path(renamedPath.getParent(), dirPrefix + \"-m-\" + numberFormat.format(solrShard));\n      \n      LOG.info(\"MTree merge renaming solr shard: \" + solrShard + \" from dir: \" + dir.getPath() + \" to dir: \" + finalPath);\n      if (!rename(renamedPath, finalPath, fs)) {\n        return false;\n      }\n    }\n    return true;\n  }\n\n","sourceOld":"  /*\n   * You can run MapReduceIndexerTool in Solrcloud mode, and once the MR job completes, you can use\n   * the standard solrj Solrcloud API to send doc updates and deletes to SolrCloud, and those updates\n   * and deletes will go to the right Solr shards, and it will work just fine.\n   * \n   * The MapReduce framework doesn't guarantee that input split N goes to the map task with the\n   * taskId = N. The job tracker and Yarn schedule and assign tasks, considering data locality\n   * aspects, but without regard of the input split# withing the overall list of input splits. In\n   * other words, split# != taskId can be true.\n   * \n   * To deal with this issue, our mapper tasks write a little auxiliary metadata file (per task)\n   * that tells the job driver which taskId processed which split#. Once the mapper-only job is\n   * completed, the job driver renames the output dirs such that the dir name contains the true solr\n   * shard id, based on these auxiliary files.\n   * \n   * This way each doc gets assigned to the right Solr shard even with #reducers > #solrshards\n   * \n   * Example for a merge with two shards:\n   * \n   * part-m-00000 and part-m-00001 goes to outputShardNum = 0 and will end up in merged part-m-00000\n   * part-m-00002 and part-m-00003 goes to outputShardNum = 1 and will end up in merged part-m-00001\n   * part-m-00004 and part-m-00005 goes to outputShardNum = 2 and will end up in merged part-m-00002\n   * ... and so on\n   * \n   * Also see run() method above where it uses NLineInputFormat.setNumLinesPerSplit(job,\n   * options.fanout)\n   * \n   * Also see TreeMergeOutputFormat.TreeMergeRecordWriter.writeShardNumberFile()\n   */\n  private boolean renameTreeMergeShardDirs(Path outputTreeMergeStep, Job job, FileSystem fs) throws IOException {\n    final String dirPrefix = SolrOutputFormat.getOutputName(job);\n    FileStatus[] dirs = fs.listStatus(outputTreeMergeStep, new PathFilter() {      \n      @Override\n      public boolean accept(Path path) {\n        return path.getName().startsWith(dirPrefix);\n      }\n    });\n    \n    for (FileStatus dir : dirs) {\n      if (!dir.isDirectory()) {\n        throw new IllegalStateException(\"Not a directory: \" + dir.getPath());\n      }\n    }\n\n    // Example: rename part-m-00004 to _part-m-00004\n    for (FileStatus dir : dirs) {\n      Path path = dir.getPath();\n      Path renamedPath = new Path(path.getParent(), \"_\" + path.getName());\n      if (!rename(path, renamedPath, fs)) {\n        return false;\n      }\n    }\n    \n    // Example: rename _part-m-00004 to part-m-00002\n    for (FileStatus dir : dirs) {\n      Path path = dir.getPath();\n      Path renamedPath = new Path(path.getParent(), \"_\" + path.getName());\n      \n      // read auxiliary metadata file (per task) that tells which taskId \n      // processed which split# aka solrShard\n      Path solrShardNumberFile = new Path(renamedPath, TreeMergeMapper.SOLR_SHARD_NUMBER);\n      InputStream in = fs.open(solrShardNumberFile);\n      byte[] bytes = ByteStreams.toByteArray(in);\n      in.close();\n      Preconditions.checkArgument(bytes.length > 0);\n      int solrShard = Integer.parseInt(new String(bytes, Charsets.UTF_8));\n      if (!delete(solrShardNumberFile, false, fs)) {\n        return false;\n      }\n      \n      // same as FileOutputFormat.NUMBER_FORMAT\n      NumberFormat numberFormat = NumberFormat.getInstance(Locale.ENGLISH);\n      numberFormat.setMinimumIntegerDigits(5);\n      numberFormat.setGroupingUsed(false);\n      Path finalPath = new Path(renamedPath.getParent(), dirPrefix + \"-m-\" + numberFormat.format(solrShard));\n      \n      LOG.info(\"MTree merge renaming solr shard: \" + solrShard + \" from dir: \" + dir.getPath() + \" to dir: \" + finalPath);\n      if (!rename(renamedPath, finalPath, fs)) {\n        return false;\n      }\n    }\n    return true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"12109b652e9210b8d58fca47f6c4a725d058a58e","date":1490373076,"type":4,"author":"Steve Rowe","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/MapReduceIndexerTool#renameTreeMergeShardDirs(Path,Job,FileSystem).mjava","sourceNew":null,"sourceOld":"  /*\n   * You can run MapReduceIndexerTool in Solrcloud mode, and once the MR job completes, you can use\n   * the standard solrj Solrcloud API to send doc updates and deletes to SolrCloud, and those updates\n   * and deletes will go to the right Solr shards, and it will work just fine.\n   * \n   * The MapReduce framework doesn't guarantee that input split N goes to the map task with the\n   * taskId = N. The job tracker and Yarn schedule and assign tasks, considering data locality\n   * aspects, but without regard of the input split# withing the overall list of input splits. In\n   * other words, split# != taskId can be true.\n   * \n   * To deal with this issue, our mapper tasks write a little auxiliary metadata file (per task)\n   * that tells the job driver which taskId processed which split#. Once the mapper-only job is\n   * completed, the job driver renames the output dirs such that the dir name contains the true solr\n   * shard id, based on these auxiliary files.\n   * \n   * This way each doc gets assigned to the right Solr shard even with #reducers > #solrshards\n   * \n   * Example for a merge with two shards:\n   * \n   * part-m-00000 and part-m-00001 goes to outputShardNum = 0 and will end up in merged part-m-00000\n   * part-m-00002 and part-m-00003 goes to outputShardNum = 1 and will end up in merged part-m-00001\n   * part-m-00004 and part-m-00005 goes to outputShardNum = 2 and will end up in merged part-m-00002\n   * ... and so on\n   * \n   * Also see run() method above where it uses NLineInputFormat.setNumLinesPerSplit(job,\n   * options.fanout)\n   * \n   * Also see TreeMergeOutputFormat.TreeMergeRecordWriter.writeShardNumberFile()\n   */\n  private boolean renameTreeMergeShardDirs(Path outputTreeMergeStep, Job job, FileSystem fs) throws IOException {\n    final String dirPrefix = SolrOutputFormat.getOutputName(job);\n    FileStatus[] dirs = fs.listStatus(outputTreeMergeStep, new PathFilter() {      \n      @Override\n      public boolean accept(Path path) {\n        return path.getName().startsWith(dirPrefix);\n      }\n    });\n    \n    for (FileStatus dir : dirs) {\n      if (!dir.isDirectory()) {\n        throw new IllegalStateException(\"Not a directory: \" + dir.getPath());\n      }\n    }\n\n    // Example: rename part-m-00004 to _part-m-00004\n    for (FileStatus dir : dirs) {\n      Path path = dir.getPath();\n      Path renamedPath = new Path(path.getParent(), \"_\" + path.getName());\n      if (!rename(path, renamedPath, fs)) {\n        return false;\n      }\n    }\n    \n    // Example: rename _part-m-00004 to part-m-00002\n    for (FileStatus dir : dirs) {\n      Path path = dir.getPath();\n      Path renamedPath = new Path(path.getParent(), \"_\" + path.getName());\n      \n      // read auxiliary metadata file (per task) that tells which taskId \n      // processed which split# aka solrShard\n      Path solrShardNumberFile = new Path(renamedPath, TreeMergeMapper.SOLR_SHARD_NUMBER);\n      InputStream in = fs.open(solrShardNumberFile);\n      byte[] bytes = ByteStreams.toByteArray(in);\n      in.close();\n      Preconditions.checkArgument(bytes.length > 0);\n      int solrShard = Integer.parseInt(new String(bytes, StandardCharsets.UTF_8));\n      if (!delete(solrShardNumberFile, false, fs)) {\n        return false;\n      }\n      \n      // same as FileOutputFormat.NUMBER_FORMAT\n      NumberFormat numberFormat = NumberFormat.getInstance(Locale.ENGLISH);\n      numberFormat.setMinimumIntegerDigits(5);\n      numberFormat.setGroupingUsed(false);\n      Path finalPath = new Path(renamedPath.getParent(), dirPrefix + \"-m-\" + numberFormat.format(solrShard));\n      \n      LOG.info(\"MTree merge renaming solr shard: \" + solrShard + \" from dir: \" + dir.getPath() + \" to dir: \" + finalPath);\n      if (!rename(renamedPath, finalPath, fs)) {\n        return false;\n      }\n    }\n    return true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe1c4aa9af769a38e878f608070f672efbeac27f","date":1490594650,"type":4,"author":"Steve Rowe","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/MapReduceIndexerTool#renameTreeMergeShardDirs(Path,Job,FileSystem).mjava","sourceNew":null,"sourceOld":"  /*\n   * You can run MapReduceIndexerTool in Solrcloud mode, and once the MR job completes, you can use\n   * the standard solrj Solrcloud API to send doc updates and deletes to SolrCloud, and those updates\n   * and deletes will go to the right Solr shards, and it will work just fine.\n   * \n   * The MapReduce framework doesn't guarantee that input split N goes to the map task with the\n   * taskId = N. The job tracker and Yarn schedule and assign tasks, considering data locality\n   * aspects, but without regard of the input split# withing the overall list of input splits. In\n   * other words, split# != taskId can be true.\n   * \n   * To deal with this issue, our mapper tasks write a little auxiliary metadata file (per task)\n   * that tells the job driver which taskId processed which split#. Once the mapper-only job is\n   * completed, the job driver renames the output dirs such that the dir name contains the true solr\n   * shard id, based on these auxiliary files.\n   * \n   * This way each doc gets assigned to the right Solr shard even with #reducers > #solrshards\n   * \n   * Example for a merge with two shards:\n   * \n   * part-m-00000 and part-m-00001 goes to outputShardNum = 0 and will end up in merged part-m-00000\n   * part-m-00002 and part-m-00003 goes to outputShardNum = 1 and will end up in merged part-m-00001\n   * part-m-00004 and part-m-00005 goes to outputShardNum = 2 and will end up in merged part-m-00002\n   * ... and so on\n   * \n   * Also see run() method above where it uses NLineInputFormat.setNumLinesPerSplit(job,\n   * options.fanout)\n   * \n   * Also see TreeMergeOutputFormat.TreeMergeRecordWriter.writeShardNumberFile()\n   */\n  private boolean renameTreeMergeShardDirs(Path outputTreeMergeStep, Job job, FileSystem fs) throws IOException {\n    final String dirPrefix = SolrOutputFormat.getOutputName(job);\n    FileStatus[] dirs = fs.listStatus(outputTreeMergeStep, new PathFilter() {      \n      @Override\n      public boolean accept(Path path) {\n        return path.getName().startsWith(dirPrefix);\n      }\n    });\n    \n    for (FileStatus dir : dirs) {\n      if (!dir.isDirectory()) {\n        throw new IllegalStateException(\"Not a directory: \" + dir.getPath());\n      }\n    }\n\n    // Example: rename part-m-00004 to _part-m-00004\n    for (FileStatus dir : dirs) {\n      Path path = dir.getPath();\n      Path renamedPath = new Path(path.getParent(), \"_\" + path.getName());\n      if (!rename(path, renamedPath, fs)) {\n        return false;\n      }\n    }\n    \n    // Example: rename _part-m-00004 to part-m-00002\n    for (FileStatus dir : dirs) {\n      Path path = dir.getPath();\n      Path renamedPath = new Path(path.getParent(), \"_\" + path.getName());\n      \n      // read auxiliary metadata file (per task) that tells which taskId \n      // processed which split# aka solrShard\n      Path solrShardNumberFile = new Path(renamedPath, TreeMergeMapper.SOLR_SHARD_NUMBER);\n      InputStream in = fs.open(solrShardNumberFile);\n      byte[] bytes = ByteStreams.toByteArray(in);\n      in.close();\n      Preconditions.checkArgument(bytes.length > 0);\n      int solrShard = Integer.parseInt(new String(bytes, StandardCharsets.UTF_8));\n      if (!delete(solrShardNumberFile, false, fs)) {\n        return false;\n      }\n      \n      // same as FileOutputFormat.NUMBER_FORMAT\n      NumberFormat numberFormat = NumberFormat.getInstance(Locale.ENGLISH);\n      numberFormat.setMinimumIntegerDigits(5);\n      numberFormat.setGroupingUsed(false);\n      Path finalPath = new Path(renamedPath.getParent(), dirPrefix + \"-m-\" + numberFormat.format(solrShard));\n      \n      LOG.info(\"MTree merge renaming solr shard: \" + solrShard + \" from dir: \" + dir.getPath() + \" to dir: \" + finalPath);\n      if (!rename(renamedPath, finalPath, fs)) {\n        return false;\n      }\n    }\n    return true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"5eb2511ababf862ea11e10761c70ee560cd84510":["42d384b06aa87eae925b668b65f3246154f0b0fa","6e1c1455e91339e72d6cdc7518defd3e05a43957"],"12109b652e9210b8d58fca47f6c4a725d058a58e":["6e1c1455e91339e72d6cdc7518defd3e05a43957"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"6e1c1455e91339e72d6cdc7518defd3e05a43957":["42d384b06aa87eae925b668b65f3246154f0b0fa"],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","42d384b06aa87eae925b668b65f3246154f0b0fa"],"fe1c4aa9af769a38e878f608070f672efbeac27f":["6e1c1455e91339e72d6cdc7518defd3e05a43957"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["12109b652e9210b8d58fca47f6c4a725d058a58e"],"42d384b06aa87eae925b668b65f3246154f0b0fa":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"5eb2511ababf862ea11e10761c70ee560cd84510":[],"12109b652e9210b8d58fca47f6c4a725d058a58e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["74f45af4339b0daf7a95c820ab88c1aea74fbce0","42d384b06aa87eae925b668b65f3246154f0b0fa"],"6e1c1455e91339e72d6cdc7518defd3e05a43957":["5eb2511ababf862ea11e10761c70ee560cd84510","12109b652e9210b8d58fca47f6c4a725d058a58e","fe1c4aa9af769a38e878f608070f672efbeac27f"],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":[],"fe1c4aa9af769a38e878f608070f672efbeac27f":[],"42d384b06aa87eae925b668b65f3246154f0b0fa":["5eb2511ababf862ea11e10761c70ee560cd84510","6e1c1455e91339e72d6cdc7518defd3e05a43957","74f45af4339b0daf7a95c820ab88c1aea74fbce0"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["5eb2511ababf862ea11e10761c70ee560cd84510","74f45af4339b0daf7a95c820ab88c1aea74fbce0","fe1c4aa9af769a38e878f608070f672efbeac27f","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}