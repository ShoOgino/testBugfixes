{"path":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#build(int,int,PathSlice[],LongBitSet,IndexOutput,byte[],byte[],byte[],long[],List[Closeable]).mjava","commits":[{"id":"51b2a4c0f6c28a8ba7c41911b421cea2ede8ef40","date":1458553787,"type":1,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#build(int,int,PathSlice[],LongBitSet,IndexOutput,byte[],byte[],byte[],long[],List[Closeable]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#build(int,int,PathSlice[],LongBitSet,IndexOutput,byte[],byte[],byte[],long[]).mjava","sourceNew":"  /** The array (sized numDims) of PathSlice describe the cell we have currently recursed to. */\n  private void build(int nodeID, int leafNodeOffset,\n                     PathSlice[] slices,\n                     LongBitSet ordBitSet,\n                     IndexOutput out,\n                     byte[] minPackedValue, byte[] maxPackedValue,\n                     byte[] splitPackedValues,\n                     long[] leafBlockFPs,\n                     List<Closeable> toCloseHeroically) throws IOException {\n\n    for(PathSlice slice : slices) {\n      assert slice.count == slices[0].count;\n    }\n\n    if (numDims == 1 && slices[0].writer instanceof OfflinePointWriter && slices[0].count <= maxPointsSortInHeap) {\n      // Special case for 1D, to cutover to heap once we recurse deeply enough:\n      slices[0] = switchToHeap(slices[0], toCloseHeroically);\n    }\n\n    if (nodeID >= leafNodeOffset) {\n\n      // Leaf node: write block\n      for (int dim=0;dim<numDims;dim++) {\n        if (slices[dim].writer instanceof HeapPointWriter == false) {\n          // Adversarial cases can cause this, e.g. very lopsided data, all equal points, such that we started\n          // offline, but then kept splitting only in one dimension, and so never had to rewrite into heap writer\n          slices[dim] = switchToHeap(slices[dim], toCloseHeroically);\n        }\n\n        PathSlice source = slices[dim];\n\n        HeapPointWriter heapSource = (HeapPointWriter) source.writer;\n\n        // Find common prefix by comparing first and last values, already sorted in this dimension:\n        heapSource.readPackedValue(Math.toIntExact(source.start), scratch1);\n        heapSource.readPackedValue(Math.toIntExact(source.start + source.count - 1), scratch2);\n\n        int offset = dim * bytesPerDim;\n        commonPrefixLengths[dim] = bytesPerDim;\n        for(int j=0;j<bytesPerDim;j++) {\n          if (scratch1[offset+j] != scratch2[offset+j]) {\n            commonPrefixLengths[dim] = j;\n            break;\n          }\n        }\n      }\n\n      PathSlice source = slices[0];\n\n      // We ensured that maxPointsSortInHeap was >= maxPointsInLeafNode, so we better be in heap at this point:\n      HeapPointWriter heapSource = (HeapPointWriter) source.writer;\n\n      // Save the block file pointer:\n      leafBlockFPs[nodeID - leafNodeOffset] = out.getFilePointer();\n      //System.out.println(\"  write leaf block @ fp=\" + out.getFilePointer());\n\n      // Write docIDs first, as their own chunk, so that at intersect time we can add all docIDs w/o\n      // loading the values:\n      int count = Math.toIntExact(source.count);\n      assert count > 0: \"nodeID=\" + nodeID + \" leafNodeOffset=\" + leafNodeOffset;\n      writeLeafBlockDocs(out, heapSource.docIDs, Math.toIntExact(source.start), count);\n\n      // TODO: minor opto: we don't really have to write the actual common prefixes, because BKDReader on recursing can regenerate it for us\n      // from the index, much like how terms dict does so from the FST:\n\n      // Write the common prefixes:\n      writeCommonPrefixes(out, commonPrefixLengths, scratch1);\n\n      // Write the full values:\n      byte[] lastPackedValue = new byte[bytesPerDim];\n      for (int i=0;i<count;i++) {\n        heapSource.getPackedValueSlice(Math.toIntExact(source.start + i), scratchBytesRef);\n        assert numDims != 1 || valueInOrder(i, lastPackedValue, scratchBytesRef.bytes, scratchBytesRef.offset);\n\n        // Make sure this value does in fact fall within this leaf cell:\n        assert valueInBounds(scratchBytesRef, minPackedValue, maxPackedValue);\n        writeLeafBlockPackedValue(out, commonPrefixLengths, scratchBytesRef.bytes, scratchBytesRef.offset);\n      }\n\n    } else {\n      // Inner node: partition/recurse\n\n      int splitDim;\n      if (numDims > 1) {\n        splitDim = split(minPackedValue, maxPackedValue);\n      } else {\n        splitDim = 0;\n      }\n\n      PathSlice source = slices[splitDim];\n\n      assert nodeID < splitPackedValues.length: \"nodeID=\" + nodeID + \" splitValues.length=\" + splitPackedValues.length;\n\n      // How many points will be in the left tree:\n      long rightCount = source.count / 2;\n      long leftCount = source.count - rightCount;\n\n      byte[] splitValue = markRightTree(rightCount, splitDim, source, ordBitSet);\n      int address = nodeID * (1+bytesPerDim);\n      splitPackedValues[address] = (byte) splitDim;\n      System.arraycopy(splitValue, 0, splitPackedValues, address + 1, bytesPerDim);\n\n      // Partition all PathSlice that are not the split dim into sorted left and right sets, so we can recurse:\n\n      PathSlice[] leftSlices = new PathSlice[numDims];\n      PathSlice[] rightSlices = new PathSlice[numDims];\n\n      byte[] minSplitPackedValue = new byte[packedBytesLength];\n      System.arraycopy(minPackedValue, 0, minSplitPackedValue, 0, packedBytesLength);\n\n      byte[] maxSplitPackedValue = new byte[packedBytesLength];\n      System.arraycopy(maxPackedValue, 0, maxSplitPackedValue, 0, packedBytesLength);\n\n      // When we are on this dim, below, we clear the ordBitSet:\n      int dimToClear;\n      if (numDims - 1 == splitDim) {\n        dimToClear = numDims - 2;\n      } else {\n        dimToClear = numDims - 1;\n      }\n\n      for(int dim=0;dim<numDims;dim++) {\n\n        if (dim == splitDim) {\n          // No need to partition on this dim since it's a simple slice of the incoming already sorted slice, and we\n          // will re-use its shared reader when visiting it as we recurse:\n          leftSlices[dim] = new PathSlice(source.writer, source.start, leftCount);\n          rightSlices[dim] = new PathSlice(source.writer, source.start + leftCount, rightCount);\n          System.arraycopy(splitValue, 0, minSplitPackedValue, dim*bytesPerDim, bytesPerDim);\n          System.arraycopy(splitValue, 0, maxSplitPackedValue, dim*bytesPerDim, bytesPerDim);\n          continue;\n        }\n\n        // Not inside the try because we don't want to close this one now, so that after recursion is done,\n        // we will have done a singel full sweep of the file:\n        PointReader reader = slices[dim].writer.getSharedReader(slices[dim].start, slices[dim].count, toCloseHeroically);\n\n        try (PointWriter leftPointWriter = getPointWriter(leftCount, \"left\" + dim);\n             PointWriter rightPointWriter = getPointWriter(source.count - leftCount, \"right\" + dim)) {\n\n          long nextRightCount = reader.split(source.count, ordBitSet, leftPointWriter, rightPointWriter, dim == dimToClear);\n          if (rightCount != nextRightCount) {\n            throw new IllegalStateException(\"wrong number of points in split: expected=\" + rightCount + \" but actual=\" + nextRightCount);\n          }\n\n          leftSlices[dim] = new PathSlice(leftPointWriter, 0, leftCount);\n          rightSlices[dim] = new PathSlice(rightPointWriter, 0, rightCount);\n        } catch (Throwable t) {\n          verifyChecksum(t, slices[dim].writer);\n        }\n      }\n\n      // Recurse on left tree:\n      build(2*nodeID, leafNodeOffset, leftSlices,\n            ordBitSet, out,\n            minPackedValue, maxSplitPackedValue,\n            splitPackedValues, leafBlockFPs, toCloseHeroically);\n      for(int dim=0;dim<numDims;dim++) {\n        // Don't destroy the dim we split on because we just re-used what our caller above gave us for that dim:\n        if (dim != splitDim) {\n          leftSlices[dim].writer.destroy();\n        }\n      }\n\n      // TODO: we could \"tail recurse\" here?  have our parent discard its refs as we recurse right?\n      // Recurse on right tree:\n      build(2*nodeID+1, leafNodeOffset, rightSlices,\n            ordBitSet, out,\n            minSplitPackedValue, maxPackedValue,\n            splitPackedValues, leafBlockFPs, toCloseHeroically);\n      for(int dim=0;dim<numDims;dim++) {\n        // Don't destroy the dim we split on because we just re-used what our caller above gave us for that dim:\n        if (dim != splitDim) {\n          rightSlices[dim].writer.destroy();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /** The array (sized numDims) of PathSlice describe the cell we have currently recursed to. */\n  private void build(int nodeID, int leafNodeOffset,\n                     PathSlice[] slices,\n                     LongBitSet ordBitSet,\n                     IndexOutput out,\n                     byte[] minPackedValue, byte[] maxPackedValue,\n                     byte[] splitPackedValues,\n                     long[] leafBlockFPs) throws IOException {\n\n    for(PathSlice slice : slices) {\n      assert slice.count == slices[0].count;\n    }\n\n    if (numDims == 1 && slices[0].writer instanceof OfflinePointWriter && slices[0].count <= maxPointsSortInHeap) {\n      // Special case for 1D, to cutover to heap once we recurse deeply enough:\n      slices[0] = switchToHeap(slices[0]);\n    }\n\n    if (nodeID >= leafNodeOffset) {\n\n      // Leaf node: write block\n      for (int dim=0;dim<numDims;dim++) {\n        if (slices[dim].writer instanceof HeapPointWriter == false) {\n          // Adversarial cases can cause this, e.g. very lopsided data, all equal points, such that we started\n          // offline, but then kept splitting only in one dimension, and so never had to rewrite into heap writer\n          slices[dim] = switchToHeap(slices[dim]);\n        }\n\n        PathSlice source = slices[dim];\n\n        HeapPointWriter heapSource = (HeapPointWriter) source.writer;\n\n        // Find common prefix by comparing first and last values, already sorted in this dimension:\n        heapSource.readPackedValue(Math.toIntExact(source.start), scratch1);\n        heapSource.readPackedValue(Math.toIntExact(source.start + source.count - 1), scratch2);\n\n        int offset = dim * bytesPerDim;\n        commonPrefixLengths[dim] = bytesPerDim;\n        for(int j=0;j<bytesPerDim;j++) {\n          if (scratch1[offset+j] != scratch2[offset+j]) {\n            commonPrefixLengths[dim] = j;\n            break;\n          }\n        }\n      }\n\n      PathSlice source = slices[0];\n\n      // We ensured that maxPointsSortInHeap was >= maxPointsInLeafNode, so we better be in heap at this point:\n      HeapPointWriter heapSource = (HeapPointWriter) source.writer;\n\n      // Save the block file pointer:\n      leafBlockFPs[nodeID - leafNodeOffset] = out.getFilePointer();\n      //System.out.println(\"  write leaf block @ fp=\" + out.getFilePointer());\n\n      // Write docIDs first, as their own chunk, so that at intersect time we can add all docIDs w/o\n      // loading the values:\n      int count = Math.toIntExact(source.count);\n      assert count > 0: \"nodeID=\" + nodeID + \" leafNodeOffset=\" + leafNodeOffset;\n      writeLeafBlockDocs(out, heapSource.docIDs, Math.toIntExact(source.start), count);\n\n      // TODO: minor opto: we don't really have to write the actual common prefixes, because BKDReader on recursing can regenerate it for us\n      // from the index, much like how terms dict does so from the FST:\n\n      // Write the common prefixes:\n      writeCommonPrefixes(out, commonPrefixLengths, scratch1);\n\n      // Write the full values:\n      byte[] lastPackedValue = new byte[bytesPerDim];\n      for (int i=0;i<count;i++) {\n        heapSource.getPackedValueSlice(Math.toIntExact(source.start + i), scratchBytesRef);\n        assert numDims != 1 || valueInOrder(i, lastPackedValue, scratchBytesRef.bytes, scratchBytesRef.offset);\n\n        // Make sure this value does in fact fall within this leaf cell:\n        assert valueInBounds(scratchBytesRef, minPackedValue, maxPackedValue);\n        writeLeafBlockPackedValue(out, commonPrefixLengths, scratchBytesRef.bytes, scratchBytesRef.offset);\n      }\n\n    } else {\n      // Inner node: partition/recurse\n\n      int splitDim;\n      if (numDims > 1) {\n        splitDim = split(minPackedValue, maxPackedValue);\n      } else {\n        splitDim = 0;\n      }\n\n      PathSlice source = slices[splitDim];\n\n      assert nodeID < splitPackedValues.length: \"nodeID=\" + nodeID + \" splitValues.length=\" + splitPackedValues.length;\n\n      // How many points will be in the left tree:\n      long rightCount = source.count / 2;\n      long leftCount = source.count - rightCount;\n\n      byte[] splitValue = markRightTree(rightCount, splitDim, source, ordBitSet);\n      int address = nodeID * (1+bytesPerDim);\n      splitPackedValues[address] = (byte) splitDim;\n      System.arraycopy(splitValue, 0, splitPackedValues, address + 1, bytesPerDim);\n\n      // Partition all PathSlice that are not the split dim into sorted left and right sets, so we can recurse:\n\n      PathSlice[] leftSlices = new PathSlice[numDims];\n      PathSlice[] rightSlices = new PathSlice[numDims];\n\n      byte[] minSplitPackedValue = new byte[packedBytesLength];\n      System.arraycopy(minPackedValue, 0, minSplitPackedValue, 0, packedBytesLength);\n\n      byte[] maxSplitPackedValue = new byte[packedBytesLength];\n      System.arraycopy(maxPackedValue, 0, maxSplitPackedValue, 0, packedBytesLength);\n\n      // When we are on this dim, below, we clear the ordBitSet:\n      int dimToClear;\n      if (numDims - 1 == splitDim) {\n        dimToClear = numDims - 2;\n      } else {\n        dimToClear = numDims - 1;\n      }\n\n      for(int dim=0;dim<numDims;dim++) {\n\n        if (dim == splitDim) {\n          // No need to partition on this dim since it's a simple slice of the incoming already sorted slice.\n          leftSlices[dim] = new PathSlice(source.writer, source.start, leftCount);\n          rightSlices[dim] = new PathSlice(source.writer, source.start + leftCount, rightCount);\n          System.arraycopy(splitValue, 0, minSplitPackedValue, dim*bytesPerDim, bytesPerDim);\n          System.arraycopy(splitValue, 0, maxSplitPackedValue, dim*bytesPerDim, bytesPerDim);\n          continue;\n        }\n\n        try (PointWriter leftPointWriter = getPointWriter(leftCount, \"left\" + dim);\n             PointWriter rightPointWriter = getPointWriter(source.count - leftCount, \"right\" + dim);\n             PointReader reader = slices[dim].writer.getReader(slices[dim].start, slices[dim].count);) {\n\n          long nextRightCount = reader.split(source.count, ordBitSet, leftPointWriter, rightPointWriter, dim == dimToClear);\n          if (rightCount != nextRightCount) {\n            throw new IllegalStateException(\"wrong number of points in split: expected=\" + rightCount + \" but actual=\" + nextRightCount);\n          }\n\n          leftSlices[dim] = new PathSlice(leftPointWriter, 0, leftCount);\n          rightSlices[dim] = new PathSlice(rightPointWriter, 0, rightCount);\n        } catch (Throwable t) {\n          verifyChecksum(t, slices[dim].writer);\n        }\n      }\n\n      // Recurse on left tree:\n      build(2*nodeID, leafNodeOffset, leftSlices,\n            ordBitSet, out,\n            minPackedValue, maxSplitPackedValue,\n            splitPackedValues, leafBlockFPs);\n      for(int dim=0;dim<numDims;dim++) {\n        // Don't destroy the dim we split on because we just re-used what our caller above gave us for that dim:\n        if (dim != splitDim) {\n          leftSlices[dim].writer.destroy();\n        }\n      }\n\n      // TODO: we could \"tail recurse\" here?  have our parent discard its refs as we recurse right?\n      // Recurse on right tree:\n      build(2*nodeID+1, leafNodeOffset, rightSlices,\n            ordBitSet, out,\n            minSplitPackedValue, maxPackedValue,\n            splitPackedValues, leafBlockFPs);\n      for(int dim=0;dim<numDims;dim++) {\n        // Don't destroy the dim we split on because we just re-used what our caller above gave us for that dim:\n        if (dim != splitDim) {\n          rightSlices[dim].writer.destroy();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3ca40baa99f9578eb8408ee5b9177f7ffe6f65d6","date":1468339076,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#build(int,int,PathSlice[],LongBitSet,IndexOutput,byte[],byte[],byte[],long[],List[Closeable]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#build(int,int,PathSlice[],LongBitSet,IndexOutput,byte[],byte[],byte[],long[],List[Closeable]).mjava","sourceNew":"  /** The array (sized numDims) of PathSlice describe the cell we have currently recursed to. */\n  private void build(int nodeID, int leafNodeOffset,\n                     PathSlice[] slices,\n                     LongBitSet ordBitSet,\n                     IndexOutput out,\n                     byte[] minPackedValue, byte[] maxPackedValue,\n                     byte[] splitPackedValues,\n                     long[] leafBlockFPs,\n                     List<Closeable> toCloseHeroically) throws IOException {\n\n    for(PathSlice slice : slices) {\n      assert slice.count == slices[0].count;\n    }\n\n    if (numDims == 1 && slices[0].writer instanceof OfflinePointWriter && slices[0].count <= maxPointsSortInHeap) {\n      // Special case for 1D, to cutover to heap once we recurse deeply enough:\n      slices[0] = switchToHeap(slices[0], toCloseHeroically);\n    }\n\n    if (nodeID >= leafNodeOffset) {\n\n      // Leaf node: write block\n      // We can write the block in any order so by default we write it sorted by the dimension that has the\n      // least number of unique bytes at commonPrefixLengths[dim], which makes compression more efficient\n      int sortedDim = 0;\n      int sortedDimCardinality = Integer.MAX_VALUE;\n\n      for (int dim=0;dim<numDims;dim++) {\n        if (slices[dim].writer instanceof HeapPointWriter == false) {\n          // Adversarial cases can cause this, e.g. very lopsided data, all equal points, such that we started\n          // offline, but then kept splitting only in one dimension, and so never had to rewrite into heap writer\n          slices[dim] = switchToHeap(slices[dim], toCloseHeroically);\n        }\n\n        PathSlice source = slices[dim];\n\n        HeapPointWriter heapSource = (HeapPointWriter) source.writer;\n\n        // Find common prefix by comparing first and last values, already sorted in this dimension:\n        heapSource.readPackedValue(Math.toIntExact(source.start), scratch1);\n        heapSource.readPackedValue(Math.toIntExact(source.start + source.count - 1), scratch2);\n\n        int offset = dim * bytesPerDim;\n        commonPrefixLengths[dim] = bytesPerDim;\n        for(int j=0;j<bytesPerDim;j++) {\n          if (scratch1[offset+j] != scratch2[offset+j]) {\n            commonPrefixLengths[dim] = j;\n            break;\n          }\n        }\n\n        int prefix = commonPrefixLengths[dim];\n        if (prefix < bytesPerDim) {\n          int cardinality = 1;\n          byte previous = scratch1[offset + prefix];\n          for (long i = 1; i < source.count; ++i) {\n            heapSource.readPackedValue(Math.toIntExact(source.start + i), scratch2);\n            byte b = scratch2[offset + prefix];\n            assert Byte.toUnsignedInt(previous) <= Byte.toUnsignedInt(b);\n            if (b != previous) {\n              cardinality++;\n              previous = b;\n            }\n          }\n          assert cardinality <= 256;\n          if (cardinality < sortedDimCardinality) {\n            sortedDim = dim;\n            sortedDimCardinality = cardinality;\n          }\n        }\n      }\n\n      PathSlice source = slices[sortedDim];\n\n      // We ensured that maxPointsSortInHeap was >= maxPointsInLeafNode, so we better be in heap at this point:\n      HeapPointWriter heapSource = (HeapPointWriter) source.writer;\n\n      // Save the block file pointer:\n      leafBlockFPs[nodeID - leafNodeOffset] = out.getFilePointer();\n      //System.out.println(\"  write leaf block @ fp=\" + out.getFilePointer());\n\n      // Write docIDs first, as their own chunk, so that at intersect time we can add all docIDs w/o\n      // loading the values:\n      int count = Math.toIntExact(source.count);\n      assert count > 0: \"nodeID=\" + nodeID + \" leafNodeOffset=\" + leafNodeOffset;\n      writeLeafBlockDocs(out, heapSource.docIDs, Math.toIntExact(source.start), count);\n\n      // TODO: minor opto: we don't really have to write the actual common prefixes, because BKDReader on recursing can regenerate it for us\n      // from the index, much like how terms dict does so from the FST:\n\n      // Write the common prefixes:\n      writeCommonPrefixes(out, commonPrefixLengths, scratch1);\n\n      // Write the full values:\n      IntFunction<BytesRef> packedValues = new IntFunction<BytesRef>() {\n        final BytesRef scratch = new BytesRef();\n\n        {\n          scratch.length = packedBytesLength;\n        }\n\n        @Override\n        public BytesRef apply(int i) {\n          heapSource.getPackedValueSlice(Math.toIntExact(source.start + i), scratch);\n          return scratch;\n        }\n      };\n      assert valuesInOrderAndBounds(count, minPackedValue, maxPackedValue, packedValues);\n      writeLeafBlockPackedValues(out, commonPrefixLengths, count, sortedDim, packedValues);\n\n    } else {\n      // Inner node: partition/recurse\n\n      int splitDim;\n      if (numDims > 1) {\n        splitDim = split(minPackedValue, maxPackedValue);\n      } else {\n        splitDim = 0;\n      }\n\n      PathSlice source = slices[splitDim];\n\n      assert nodeID < splitPackedValues.length: \"nodeID=\" + nodeID + \" splitValues.length=\" + splitPackedValues.length;\n\n      // How many points will be in the left tree:\n      long rightCount = source.count / 2;\n      long leftCount = source.count - rightCount;\n\n      byte[] splitValue = markRightTree(rightCount, splitDim, source, ordBitSet);\n      int address = nodeID * (1+bytesPerDim);\n      splitPackedValues[address] = (byte) splitDim;\n      System.arraycopy(splitValue, 0, splitPackedValues, address + 1, bytesPerDim);\n\n      // Partition all PathSlice that are not the split dim into sorted left and right sets, so we can recurse:\n\n      PathSlice[] leftSlices = new PathSlice[numDims];\n      PathSlice[] rightSlices = new PathSlice[numDims];\n\n      byte[] minSplitPackedValue = new byte[packedBytesLength];\n      System.arraycopy(minPackedValue, 0, minSplitPackedValue, 0, packedBytesLength);\n\n      byte[] maxSplitPackedValue = new byte[packedBytesLength];\n      System.arraycopy(maxPackedValue, 0, maxSplitPackedValue, 0, packedBytesLength);\n\n      // When we are on this dim, below, we clear the ordBitSet:\n      int dimToClear;\n      if (numDims - 1 == splitDim) {\n        dimToClear = numDims - 2;\n      } else {\n        dimToClear = numDims - 1;\n      }\n\n      for(int dim=0;dim<numDims;dim++) {\n\n        if (dim == splitDim) {\n          // No need to partition on this dim since it's a simple slice of the incoming already sorted slice, and we\n          // will re-use its shared reader when visiting it as we recurse:\n          leftSlices[dim] = new PathSlice(source.writer, source.start, leftCount);\n          rightSlices[dim] = new PathSlice(source.writer, source.start + leftCount, rightCount);\n          System.arraycopy(splitValue, 0, minSplitPackedValue, dim*bytesPerDim, bytesPerDim);\n          System.arraycopy(splitValue, 0, maxSplitPackedValue, dim*bytesPerDim, bytesPerDim);\n          continue;\n        }\n\n        // Not inside the try because we don't want to close this one now, so that after recursion is done,\n        // we will have done a singel full sweep of the file:\n        PointReader reader = slices[dim].writer.getSharedReader(slices[dim].start, slices[dim].count, toCloseHeroically);\n\n        try (PointWriter leftPointWriter = getPointWriter(leftCount, \"left\" + dim);\n             PointWriter rightPointWriter = getPointWriter(source.count - leftCount, \"right\" + dim)) {\n\n          long nextRightCount = reader.split(source.count, ordBitSet, leftPointWriter, rightPointWriter, dim == dimToClear);\n          if (rightCount != nextRightCount) {\n            throw new IllegalStateException(\"wrong number of points in split: expected=\" + rightCount + \" but actual=\" + nextRightCount);\n          }\n\n          leftSlices[dim] = new PathSlice(leftPointWriter, 0, leftCount);\n          rightSlices[dim] = new PathSlice(rightPointWriter, 0, rightCount);\n        } catch (Throwable t) {\n          verifyChecksum(t, slices[dim].writer);\n        }\n      }\n\n      // Recurse on left tree:\n      build(2*nodeID, leafNodeOffset, leftSlices,\n            ordBitSet, out,\n            minPackedValue, maxSplitPackedValue,\n            splitPackedValues, leafBlockFPs, toCloseHeroically);\n      for(int dim=0;dim<numDims;dim++) {\n        // Don't destroy the dim we split on because we just re-used what our caller above gave us for that dim:\n        if (dim != splitDim) {\n          leftSlices[dim].writer.destroy();\n        }\n      }\n\n      // TODO: we could \"tail recurse\" here?  have our parent discard its refs as we recurse right?\n      // Recurse on right tree:\n      build(2*nodeID+1, leafNodeOffset, rightSlices,\n            ordBitSet, out,\n            minSplitPackedValue, maxPackedValue,\n            splitPackedValues, leafBlockFPs, toCloseHeroically);\n      for(int dim=0;dim<numDims;dim++) {\n        // Don't destroy the dim we split on because we just re-used what our caller above gave us for that dim:\n        if (dim != splitDim) {\n          rightSlices[dim].writer.destroy();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /** The array (sized numDims) of PathSlice describe the cell we have currently recursed to. */\n  private void build(int nodeID, int leafNodeOffset,\n                     PathSlice[] slices,\n                     LongBitSet ordBitSet,\n                     IndexOutput out,\n                     byte[] minPackedValue, byte[] maxPackedValue,\n                     byte[] splitPackedValues,\n                     long[] leafBlockFPs,\n                     List<Closeable> toCloseHeroically) throws IOException {\n\n    for(PathSlice slice : slices) {\n      assert slice.count == slices[0].count;\n    }\n\n    if (numDims == 1 && slices[0].writer instanceof OfflinePointWriter && slices[0].count <= maxPointsSortInHeap) {\n      // Special case for 1D, to cutover to heap once we recurse deeply enough:\n      slices[0] = switchToHeap(slices[0], toCloseHeroically);\n    }\n\n    if (nodeID >= leafNodeOffset) {\n\n      // Leaf node: write block\n      for (int dim=0;dim<numDims;dim++) {\n        if (slices[dim].writer instanceof HeapPointWriter == false) {\n          // Adversarial cases can cause this, e.g. very lopsided data, all equal points, such that we started\n          // offline, but then kept splitting only in one dimension, and so never had to rewrite into heap writer\n          slices[dim] = switchToHeap(slices[dim], toCloseHeroically);\n        }\n\n        PathSlice source = slices[dim];\n\n        HeapPointWriter heapSource = (HeapPointWriter) source.writer;\n\n        // Find common prefix by comparing first and last values, already sorted in this dimension:\n        heapSource.readPackedValue(Math.toIntExact(source.start), scratch1);\n        heapSource.readPackedValue(Math.toIntExact(source.start + source.count - 1), scratch2);\n\n        int offset = dim * bytesPerDim;\n        commonPrefixLengths[dim] = bytesPerDim;\n        for(int j=0;j<bytesPerDim;j++) {\n          if (scratch1[offset+j] != scratch2[offset+j]) {\n            commonPrefixLengths[dim] = j;\n            break;\n          }\n        }\n      }\n\n      PathSlice source = slices[0];\n\n      // We ensured that maxPointsSortInHeap was >= maxPointsInLeafNode, so we better be in heap at this point:\n      HeapPointWriter heapSource = (HeapPointWriter) source.writer;\n\n      // Save the block file pointer:\n      leafBlockFPs[nodeID - leafNodeOffset] = out.getFilePointer();\n      //System.out.println(\"  write leaf block @ fp=\" + out.getFilePointer());\n\n      // Write docIDs first, as their own chunk, so that at intersect time we can add all docIDs w/o\n      // loading the values:\n      int count = Math.toIntExact(source.count);\n      assert count > 0: \"nodeID=\" + nodeID + \" leafNodeOffset=\" + leafNodeOffset;\n      writeLeafBlockDocs(out, heapSource.docIDs, Math.toIntExact(source.start), count);\n\n      // TODO: minor opto: we don't really have to write the actual common prefixes, because BKDReader on recursing can regenerate it for us\n      // from the index, much like how terms dict does so from the FST:\n\n      // Write the common prefixes:\n      writeCommonPrefixes(out, commonPrefixLengths, scratch1);\n\n      // Write the full values:\n      byte[] lastPackedValue = new byte[bytesPerDim];\n      for (int i=0;i<count;i++) {\n        heapSource.getPackedValueSlice(Math.toIntExact(source.start + i), scratchBytesRef);\n        assert numDims != 1 || valueInOrder(i, lastPackedValue, scratchBytesRef.bytes, scratchBytesRef.offset);\n\n        // Make sure this value does in fact fall within this leaf cell:\n        assert valueInBounds(scratchBytesRef, minPackedValue, maxPackedValue);\n        writeLeafBlockPackedValue(out, commonPrefixLengths, scratchBytesRef.bytes, scratchBytesRef.offset);\n      }\n\n    } else {\n      // Inner node: partition/recurse\n\n      int splitDim;\n      if (numDims > 1) {\n        splitDim = split(minPackedValue, maxPackedValue);\n      } else {\n        splitDim = 0;\n      }\n\n      PathSlice source = slices[splitDim];\n\n      assert nodeID < splitPackedValues.length: \"nodeID=\" + nodeID + \" splitValues.length=\" + splitPackedValues.length;\n\n      // How many points will be in the left tree:\n      long rightCount = source.count / 2;\n      long leftCount = source.count - rightCount;\n\n      byte[] splitValue = markRightTree(rightCount, splitDim, source, ordBitSet);\n      int address = nodeID * (1+bytesPerDim);\n      splitPackedValues[address] = (byte) splitDim;\n      System.arraycopy(splitValue, 0, splitPackedValues, address + 1, bytesPerDim);\n\n      // Partition all PathSlice that are not the split dim into sorted left and right sets, so we can recurse:\n\n      PathSlice[] leftSlices = new PathSlice[numDims];\n      PathSlice[] rightSlices = new PathSlice[numDims];\n\n      byte[] minSplitPackedValue = new byte[packedBytesLength];\n      System.arraycopy(minPackedValue, 0, minSplitPackedValue, 0, packedBytesLength);\n\n      byte[] maxSplitPackedValue = new byte[packedBytesLength];\n      System.arraycopy(maxPackedValue, 0, maxSplitPackedValue, 0, packedBytesLength);\n\n      // When we are on this dim, below, we clear the ordBitSet:\n      int dimToClear;\n      if (numDims - 1 == splitDim) {\n        dimToClear = numDims - 2;\n      } else {\n        dimToClear = numDims - 1;\n      }\n\n      for(int dim=0;dim<numDims;dim++) {\n\n        if (dim == splitDim) {\n          // No need to partition on this dim since it's a simple slice of the incoming already sorted slice, and we\n          // will re-use its shared reader when visiting it as we recurse:\n          leftSlices[dim] = new PathSlice(source.writer, source.start, leftCount);\n          rightSlices[dim] = new PathSlice(source.writer, source.start + leftCount, rightCount);\n          System.arraycopy(splitValue, 0, minSplitPackedValue, dim*bytesPerDim, bytesPerDim);\n          System.arraycopy(splitValue, 0, maxSplitPackedValue, dim*bytesPerDim, bytesPerDim);\n          continue;\n        }\n\n        // Not inside the try because we don't want to close this one now, so that after recursion is done,\n        // we will have done a singel full sweep of the file:\n        PointReader reader = slices[dim].writer.getSharedReader(slices[dim].start, slices[dim].count, toCloseHeroically);\n\n        try (PointWriter leftPointWriter = getPointWriter(leftCount, \"left\" + dim);\n             PointWriter rightPointWriter = getPointWriter(source.count - leftCount, \"right\" + dim)) {\n\n          long nextRightCount = reader.split(source.count, ordBitSet, leftPointWriter, rightPointWriter, dim == dimToClear);\n          if (rightCount != nextRightCount) {\n            throw new IllegalStateException(\"wrong number of points in split: expected=\" + rightCount + \" but actual=\" + nextRightCount);\n          }\n\n          leftSlices[dim] = new PathSlice(leftPointWriter, 0, leftCount);\n          rightSlices[dim] = new PathSlice(rightPointWriter, 0, rightCount);\n        } catch (Throwable t) {\n          verifyChecksum(t, slices[dim].writer);\n        }\n      }\n\n      // Recurse on left tree:\n      build(2*nodeID, leafNodeOffset, leftSlices,\n            ordBitSet, out,\n            minPackedValue, maxSplitPackedValue,\n            splitPackedValues, leafBlockFPs, toCloseHeroically);\n      for(int dim=0;dim<numDims;dim++) {\n        // Don't destroy the dim we split on because we just re-used what our caller above gave us for that dim:\n        if (dim != splitDim) {\n          leftSlices[dim].writer.destroy();\n        }\n      }\n\n      // TODO: we could \"tail recurse\" here?  have our parent discard its refs as we recurse right?\n      // Recurse on right tree:\n      build(2*nodeID+1, leafNodeOffset, rightSlices,\n            ordBitSet, out,\n            minSplitPackedValue, maxPackedValue,\n            splitPackedValues, leafBlockFPs, toCloseHeroically);\n      for(int dim=0;dim<numDims;dim++) {\n        // Don't destroy the dim we split on because we just re-used what our caller above gave us for that dim:\n        if (dim != splitDim) {\n          rightSlices[dim].writer.destroy();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d561885e9bb6238af1ff8afe8630dcfe49b66ac7","date":1469780634,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#build(int,int,PathSlice[],LongBitSet,IndexOutput,byte[],byte[],byte[],long[],List[Closeable]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#build(int,int,PathSlice[],LongBitSet,IndexOutput,byte[],byte[],byte[],long[],List[Closeable]).mjava","sourceNew":"  /** The array (sized numDims) of PathSlice describe the cell we have currently recursed to. */\n  private void build(int nodeID, int leafNodeOffset,\n                     PathSlice[] slices,\n                     LongBitSet ordBitSet,\n                     IndexOutput out,\n                     byte[] minPackedValue, byte[] maxPackedValue,\n                     byte[] splitPackedValues,\n                     long[] leafBlockFPs,\n                     List<Closeable> toCloseHeroically) throws IOException {\n\n    for(PathSlice slice : slices) {\n      assert slice.count == slices[0].count;\n    }\n\n    if (numDims == 1 && slices[0].writer instanceof OfflinePointWriter && slices[0].count <= maxPointsSortInHeap) {\n      // Special case for 1D, to cutover to heap once we recurse deeply enough:\n      slices[0] = switchToHeap(slices[0], toCloseHeroically);\n    }\n\n    if (nodeID >= leafNodeOffset) {\n\n      // Leaf node: write block\n      // We can write the block in any order so by default we write it sorted by the dimension that has the\n      // least number of unique bytes at commonPrefixLengths[dim], which makes compression more efficient\n      int sortedDim = 0;\n      int sortedDimCardinality = Integer.MAX_VALUE;\n\n      for (int dim=0;dim<numDims;dim++) {\n        if (slices[dim].writer instanceof HeapPointWriter == false) {\n          // Adversarial cases can cause this, e.g. very lopsided data, all equal points, such that we started\n          // offline, but then kept splitting only in one dimension, and so never had to rewrite into heap writer\n          slices[dim] = switchToHeap(slices[dim], toCloseHeroically);\n        }\n\n        PathSlice source = slices[dim];\n\n        HeapPointWriter heapSource = (HeapPointWriter) source.writer;\n\n        // Find common prefix by comparing first and last values, already sorted in this dimension:\n        heapSource.readPackedValue(Math.toIntExact(source.start), scratch1);\n        heapSource.readPackedValue(Math.toIntExact(source.start + source.count - 1), scratch2);\n\n        int offset = dim * bytesPerDim;\n        commonPrefixLengths[dim] = bytesPerDim;\n        for(int j=0;j<bytesPerDim;j++) {\n          if (scratch1[offset+j] != scratch2[offset+j]) {\n            commonPrefixLengths[dim] = j;\n            break;\n          }\n        }\n\n        int prefix = commonPrefixLengths[dim];\n        if (prefix < bytesPerDim) {\n          int cardinality = 1;\n          byte previous = scratch1[offset + prefix];\n          for (long i = 1; i < source.count; ++i) {\n            heapSource.readPackedValue(Math.toIntExact(source.start + i), scratch2);\n            byte b = scratch2[offset + prefix];\n            assert Byte.toUnsignedInt(previous) <= Byte.toUnsignedInt(b);\n            if (b != previous) {\n              cardinality++;\n              previous = b;\n            }\n          }\n          assert cardinality <= 256;\n          if (cardinality < sortedDimCardinality) {\n            sortedDim = dim;\n            sortedDimCardinality = cardinality;\n          }\n        }\n      }\n\n      PathSlice source = slices[sortedDim];\n\n      // We ensured that maxPointsSortInHeap was >= maxPointsInLeafNode, so we better be in heap at this point:\n      HeapPointWriter heapSource = (HeapPointWriter) source.writer;\n\n      // Save the block file pointer:\n      leafBlockFPs[nodeID - leafNodeOffset] = out.getFilePointer();\n      //System.out.println(\"  write leaf block @ fp=\" + out.getFilePointer());\n\n      // Write docIDs first, as their own chunk, so that at intersect time we can add all docIDs w/o\n      // loading the values:\n      int count = Math.toIntExact(source.count);\n      assert count > 0: \"nodeID=\" + nodeID + \" leafNodeOffset=\" + leafNodeOffset;\n      writeLeafBlockDocs(out, heapSource.docIDs, Math.toIntExact(source.start), count);\n\n      // TODO: minor opto: we don't really have to write the actual common prefixes, because BKDReader on recursing can regenerate it for us\n      // from the index, much like how terms dict does so from the FST:\n\n      // Write the common prefixes:\n      writeCommonPrefixes(out, commonPrefixLengths, scratch1);\n\n      // Write the full values:\n      IntFunction<BytesRef> packedValues = new IntFunction<BytesRef>() {\n        final BytesRef scratch = new BytesRef();\n\n        {\n          scratch.length = packedBytesLength;\n        }\n\n        @Override\n        public BytesRef apply(int i) {\n          heapSource.getPackedValueSlice(Math.toIntExact(source.start + i), scratch);\n          return scratch;\n        }\n      };\n      assert valuesInOrderAndBounds(count, sortedDim, minPackedValue, maxPackedValue, packedValues,\n          heapSource.docIDs, Math.toIntExact(source.start));\n      writeLeafBlockPackedValues(out, commonPrefixLengths, count, sortedDim, packedValues);\n\n    } else {\n      // Inner node: partition/recurse\n\n      int splitDim;\n      if (numDims > 1) {\n        splitDim = split(minPackedValue, maxPackedValue);\n      } else {\n        splitDim = 0;\n      }\n\n      PathSlice source = slices[splitDim];\n\n      assert nodeID < splitPackedValues.length: \"nodeID=\" + nodeID + \" splitValues.length=\" + splitPackedValues.length;\n\n      // How many points will be in the left tree:\n      long rightCount = source.count / 2;\n      long leftCount = source.count - rightCount;\n\n      byte[] splitValue = markRightTree(rightCount, splitDim, source, ordBitSet);\n      int address = nodeID * (1+bytesPerDim);\n      splitPackedValues[address] = (byte) splitDim;\n      System.arraycopy(splitValue, 0, splitPackedValues, address + 1, bytesPerDim);\n\n      // Partition all PathSlice that are not the split dim into sorted left and right sets, so we can recurse:\n\n      PathSlice[] leftSlices = new PathSlice[numDims];\n      PathSlice[] rightSlices = new PathSlice[numDims];\n\n      byte[] minSplitPackedValue = new byte[packedBytesLength];\n      System.arraycopy(minPackedValue, 0, minSplitPackedValue, 0, packedBytesLength);\n\n      byte[] maxSplitPackedValue = new byte[packedBytesLength];\n      System.arraycopy(maxPackedValue, 0, maxSplitPackedValue, 0, packedBytesLength);\n\n      // When we are on this dim, below, we clear the ordBitSet:\n      int dimToClear;\n      if (numDims - 1 == splitDim) {\n        dimToClear = numDims - 2;\n      } else {\n        dimToClear = numDims - 1;\n      }\n\n      for(int dim=0;dim<numDims;dim++) {\n\n        if (dim == splitDim) {\n          // No need to partition on this dim since it's a simple slice of the incoming already sorted slice, and we\n          // will re-use its shared reader when visiting it as we recurse:\n          leftSlices[dim] = new PathSlice(source.writer, source.start, leftCount);\n          rightSlices[dim] = new PathSlice(source.writer, source.start + leftCount, rightCount);\n          System.arraycopy(splitValue, 0, minSplitPackedValue, dim*bytesPerDim, bytesPerDim);\n          System.arraycopy(splitValue, 0, maxSplitPackedValue, dim*bytesPerDim, bytesPerDim);\n          continue;\n        }\n\n        // Not inside the try because we don't want to close this one now, so that after recursion is done,\n        // we will have done a singel full sweep of the file:\n        PointReader reader = slices[dim].writer.getSharedReader(slices[dim].start, slices[dim].count, toCloseHeroically);\n\n        try (PointWriter leftPointWriter = getPointWriter(leftCount, \"left\" + dim);\n             PointWriter rightPointWriter = getPointWriter(source.count - leftCount, \"right\" + dim)) {\n\n          long nextRightCount = reader.split(source.count, ordBitSet, leftPointWriter, rightPointWriter, dim == dimToClear);\n          if (rightCount != nextRightCount) {\n            throw new IllegalStateException(\"wrong number of points in split: expected=\" + rightCount + \" but actual=\" + nextRightCount);\n          }\n\n          leftSlices[dim] = new PathSlice(leftPointWriter, 0, leftCount);\n          rightSlices[dim] = new PathSlice(rightPointWriter, 0, rightCount);\n        } catch (Throwable t) {\n          verifyChecksum(t, slices[dim].writer);\n        }\n      }\n\n      // Recurse on left tree:\n      build(2*nodeID, leafNodeOffset, leftSlices,\n            ordBitSet, out,\n            minPackedValue, maxSplitPackedValue,\n            splitPackedValues, leafBlockFPs, toCloseHeroically);\n      for(int dim=0;dim<numDims;dim++) {\n        // Don't destroy the dim we split on because we just re-used what our caller above gave us for that dim:\n        if (dim != splitDim) {\n          leftSlices[dim].writer.destroy();\n        }\n      }\n\n      // TODO: we could \"tail recurse\" here?  have our parent discard its refs as we recurse right?\n      // Recurse on right tree:\n      build(2*nodeID+1, leafNodeOffset, rightSlices,\n            ordBitSet, out,\n            minSplitPackedValue, maxPackedValue,\n            splitPackedValues, leafBlockFPs, toCloseHeroically);\n      for(int dim=0;dim<numDims;dim++) {\n        // Don't destroy the dim we split on because we just re-used what our caller above gave us for that dim:\n        if (dim != splitDim) {\n          rightSlices[dim].writer.destroy();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /** The array (sized numDims) of PathSlice describe the cell we have currently recursed to. */\n  private void build(int nodeID, int leafNodeOffset,\n                     PathSlice[] slices,\n                     LongBitSet ordBitSet,\n                     IndexOutput out,\n                     byte[] minPackedValue, byte[] maxPackedValue,\n                     byte[] splitPackedValues,\n                     long[] leafBlockFPs,\n                     List<Closeable> toCloseHeroically) throws IOException {\n\n    for(PathSlice slice : slices) {\n      assert slice.count == slices[0].count;\n    }\n\n    if (numDims == 1 && slices[0].writer instanceof OfflinePointWriter && slices[0].count <= maxPointsSortInHeap) {\n      // Special case for 1D, to cutover to heap once we recurse deeply enough:\n      slices[0] = switchToHeap(slices[0], toCloseHeroically);\n    }\n\n    if (nodeID >= leafNodeOffset) {\n\n      // Leaf node: write block\n      // We can write the block in any order so by default we write it sorted by the dimension that has the\n      // least number of unique bytes at commonPrefixLengths[dim], which makes compression more efficient\n      int sortedDim = 0;\n      int sortedDimCardinality = Integer.MAX_VALUE;\n\n      for (int dim=0;dim<numDims;dim++) {\n        if (slices[dim].writer instanceof HeapPointWriter == false) {\n          // Adversarial cases can cause this, e.g. very lopsided data, all equal points, such that we started\n          // offline, but then kept splitting only in one dimension, and so never had to rewrite into heap writer\n          slices[dim] = switchToHeap(slices[dim], toCloseHeroically);\n        }\n\n        PathSlice source = slices[dim];\n\n        HeapPointWriter heapSource = (HeapPointWriter) source.writer;\n\n        // Find common prefix by comparing first and last values, already sorted in this dimension:\n        heapSource.readPackedValue(Math.toIntExact(source.start), scratch1);\n        heapSource.readPackedValue(Math.toIntExact(source.start + source.count - 1), scratch2);\n\n        int offset = dim * bytesPerDim;\n        commonPrefixLengths[dim] = bytesPerDim;\n        for(int j=0;j<bytesPerDim;j++) {\n          if (scratch1[offset+j] != scratch2[offset+j]) {\n            commonPrefixLengths[dim] = j;\n            break;\n          }\n        }\n\n        int prefix = commonPrefixLengths[dim];\n        if (prefix < bytesPerDim) {\n          int cardinality = 1;\n          byte previous = scratch1[offset + prefix];\n          for (long i = 1; i < source.count; ++i) {\n            heapSource.readPackedValue(Math.toIntExact(source.start + i), scratch2);\n            byte b = scratch2[offset + prefix];\n            assert Byte.toUnsignedInt(previous) <= Byte.toUnsignedInt(b);\n            if (b != previous) {\n              cardinality++;\n              previous = b;\n            }\n          }\n          assert cardinality <= 256;\n          if (cardinality < sortedDimCardinality) {\n            sortedDim = dim;\n            sortedDimCardinality = cardinality;\n          }\n        }\n      }\n\n      PathSlice source = slices[sortedDim];\n\n      // We ensured that maxPointsSortInHeap was >= maxPointsInLeafNode, so we better be in heap at this point:\n      HeapPointWriter heapSource = (HeapPointWriter) source.writer;\n\n      // Save the block file pointer:\n      leafBlockFPs[nodeID - leafNodeOffset] = out.getFilePointer();\n      //System.out.println(\"  write leaf block @ fp=\" + out.getFilePointer());\n\n      // Write docIDs first, as their own chunk, so that at intersect time we can add all docIDs w/o\n      // loading the values:\n      int count = Math.toIntExact(source.count);\n      assert count > 0: \"nodeID=\" + nodeID + \" leafNodeOffset=\" + leafNodeOffset;\n      writeLeafBlockDocs(out, heapSource.docIDs, Math.toIntExact(source.start), count);\n\n      // TODO: minor opto: we don't really have to write the actual common prefixes, because BKDReader on recursing can regenerate it for us\n      // from the index, much like how terms dict does so from the FST:\n\n      // Write the common prefixes:\n      writeCommonPrefixes(out, commonPrefixLengths, scratch1);\n\n      // Write the full values:\n      IntFunction<BytesRef> packedValues = new IntFunction<BytesRef>() {\n        final BytesRef scratch = new BytesRef();\n\n        {\n          scratch.length = packedBytesLength;\n        }\n\n        @Override\n        public BytesRef apply(int i) {\n          heapSource.getPackedValueSlice(Math.toIntExact(source.start + i), scratch);\n          return scratch;\n        }\n      };\n      assert valuesInOrderAndBounds(count, minPackedValue, maxPackedValue, packedValues);\n      writeLeafBlockPackedValues(out, commonPrefixLengths, count, sortedDim, packedValues);\n\n    } else {\n      // Inner node: partition/recurse\n\n      int splitDim;\n      if (numDims > 1) {\n        splitDim = split(minPackedValue, maxPackedValue);\n      } else {\n        splitDim = 0;\n      }\n\n      PathSlice source = slices[splitDim];\n\n      assert nodeID < splitPackedValues.length: \"nodeID=\" + nodeID + \" splitValues.length=\" + splitPackedValues.length;\n\n      // How many points will be in the left tree:\n      long rightCount = source.count / 2;\n      long leftCount = source.count - rightCount;\n\n      byte[] splitValue = markRightTree(rightCount, splitDim, source, ordBitSet);\n      int address = nodeID * (1+bytesPerDim);\n      splitPackedValues[address] = (byte) splitDim;\n      System.arraycopy(splitValue, 0, splitPackedValues, address + 1, bytesPerDim);\n\n      // Partition all PathSlice that are not the split dim into sorted left and right sets, so we can recurse:\n\n      PathSlice[] leftSlices = new PathSlice[numDims];\n      PathSlice[] rightSlices = new PathSlice[numDims];\n\n      byte[] minSplitPackedValue = new byte[packedBytesLength];\n      System.arraycopy(minPackedValue, 0, minSplitPackedValue, 0, packedBytesLength);\n\n      byte[] maxSplitPackedValue = new byte[packedBytesLength];\n      System.arraycopy(maxPackedValue, 0, maxSplitPackedValue, 0, packedBytesLength);\n\n      // When we are on this dim, below, we clear the ordBitSet:\n      int dimToClear;\n      if (numDims - 1 == splitDim) {\n        dimToClear = numDims - 2;\n      } else {\n        dimToClear = numDims - 1;\n      }\n\n      for(int dim=0;dim<numDims;dim++) {\n\n        if (dim == splitDim) {\n          // No need to partition on this dim since it's a simple slice of the incoming already sorted slice, and we\n          // will re-use its shared reader when visiting it as we recurse:\n          leftSlices[dim] = new PathSlice(source.writer, source.start, leftCount);\n          rightSlices[dim] = new PathSlice(source.writer, source.start + leftCount, rightCount);\n          System.arraycopy(splitValue, 0, minSplitPackedValue, dim*bytesPerDim, bytesPerDim);\n          System.arraycopy(splitValue, 0, maxSplitPackedValue, dim*bytesPerDim, bytesPerDim);\n          continue;\n        }\n\n        // Not inside the try because we don't want to close this one now, so that after recursion is done,\n        // we will have done a singel full sweep of the file:\n        PointReader reader = slices[dim].writer.getSharedReader(slices[dim].start, slices[dim].count, toCloseHeroically);\n\n        try (PointWriter leftPointWriter = getPointWriter(leftCount, \"left\" + dim);\n             PointWriter rightPointWriter = getPointWriter(source.count - leftCount, \"right\" + dim)) {\n\n          long nextRightCount = reader.split(source.count, ordBitSet, leftPointWriter, rightPointWriter, dim == dimToClear);\n          if (rightCount != nextRightCount) {\n            throw new IllegalStateException(\"wrong number of points in split: expected=\" + rightCount + \" but actual=\" + nextRightCount);\n          }\n\n          leftSlices[dim] = new PathSlice(leftPointWriter, 0, leftCount);\n          rightSlices[dim] = new PathSlice(rightPointWriter, 0, rightCount);\n        } catch (Throwable t) {\n          verifyChecksum(t, slices[dim].writer);\n        }\n      }\n\n      // Recurse on left tree:\n      build(2*nodeID, leafNodeOffset, leftSlices,\n            ordBitSet, out,\n            minPackedValue, maxSplitPackedValue,\n            splitPackedValues, leafBlockFPs, toCloseHeroically);\n      for(int dim=0;dim<numDims;dim++) {\n        // Don't destroy the dim we split on because we just re-used what our caller above gave us for that dim:\n        if (dim != splitDim) {\n          leftSlices[dim].writer.destroy();\n        }\n      }\n\n      // TODO: we could \"tail recurse\" here?  have our parent discard its refs as we recurse right?\n      // Recurse on right tree:\n      build(2*nodeID+1, leafNodeOffset, rightSlices,\n            ordBitSet, out,\n            minSplitPackedValue, maxPackedValue,\n            splitPackedValues, leafBlockFPs, toCloseHeroically);\n      for(int dim=0;dim<numDims;dim++) {\n        // Don't destroy the dim we split on because we just re-used what our caller above gave us for that dim:\n        if (dim != splitDim) {\n          rightSlices[dim].writer.destroy();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3b013574eedcdbac35dc7e35b0ee616ffc38895d","date":1470897818,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#build(int,int,PathSlice[],LongBitSet,IndexOutput,byte[],byte[],byte[],long[],List[Closeable]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#build(int,int,PathSlice[],LongBitSet,IndexOutput,byte[],byte[],byte[],long[],List[Closeable]).mjava","sourceNew":"  /** The array (sized numDims) of PathSlice describe the cell we have currently recursed to. */\n  private void build(int nodeID, int leafNodeOffset,\n                     PathSlice[] slices,\n                     LongBitSet ordBitSet,\n                     IndexOutput out,\n                     byte[] minPackedValue, byte[] maxPackedValue,\n                     byte[] splitPackedValues,\n                     long[] leafBlockFPs,\n                     List<Closeable> toCloseHeroically) throws IOException {\n\n    for(PathSlice slice : slices) {\n      assert slice.count == slices[0].count;\n    }\n\n    if (numDims == 1 && slices[0].writer instanceof OfflinePointWriter && slices[0].count <= maxPointsSortInHeap) {\n      // Special case for 1D, to cutover to heap once we recurse deeply enough:\n      slices[0] = switchToHeap(slices[0], toCloseHeroically);\n    }\n\n    if (nodeID >= leafNodeOffset) {\n\n      // Leaf node: write block\n      // We can write the block in any order so by default we write it sorted by the dimension that has the\n      // least number of unique bytes at commonPrefixLengths[dim], which makes compression more efficient\n      int sortedDim = 0;\n      int sortedDimCardinality = Integer.MAX_VALUE;\n\n      for (int dim=0;dim<numDims;dim++) {\n        if (slices[dim].writer instanceof HeapPointWriter == false) {\n          // Adversarial cases can cause this, e.g. very lopsided data, all equal points, such that we started\n          // offline, but then kept splitting only in one dimension, and so never had to rewrite into heap writer\n          slices[dim] = switchToHeap(slices[dim], toCloseHeroically);\n        }\n\n        PathSlice source = slices[dim];\n\n        HeapPointWriter heapSource = (HeapPointWriter) source.writer;\n\n        // Find common prefix by comparing first and last values, already sorted in this dimension:\n        heapSource.readPackedValue(Math.toIntExact(source.start), scratch1);\n        heapSource.readPackedValue(Math.toIntExact(source.start + source.count - 1), scratch2);\n\n        int offset = dim * bytesPerDim;\n        commonPrefixLengths[dim] = bytesPerDim;\n        for(int j=0;j<bytesPerDim;j++) {\n          if (scratch1[offset+j] != scratch2[offset+j]) {\n            commonPrefixLengths[dim] = j;\n            break;\n          }\n        }\n\n        int prefix = commonPrefixLengths[dim];\n        if (prefix < bytesPerDim) {\n          int cardinality = 1;\n          byte previous = scratch1[offset + prefix];\n          for (long i = 1; i < source.count; ++i) {\n            heapSource.readPackedValue(Math.toIntExact(source.start + i), scratch2);\n            byte b = scratch2[offset + prefix];\n            assert Byte.toUnsignedInt(previous) <= Byte.toUnsignedInt(b);\n            if (b != previous) {\n              cardinality++;\n              previous = b;\n            }\n          }\n          assert cardinality <= 256;\n          if (cardinality < sortedDimCardinality) {\n            sortedDim = dim;\n            sortedDimCardinality = cardinality;\n          }\n        }\n      }\n\n      PathSlice source = slices[sortedDim];\n\n      // We ensured that maxPointsSortInHeap was >= maxPointsInLeafNode, so we better be in heap at this point:\n      HeapPointWriter heapSource = (HeapPointWriter) source.writer;\n\n      // Save the block file pointer:\n      leafBlockFPs[nodeID - leafNodeOffset] = out.getFilePointer();\n      //System.out.println(\"  write leaf block @ fp=\" + out.getFilePointer());\n\n      // Write docIDs first, as their own chunk, so that at intersect time we can add all docIDs w/o\n      // loading the values:\n      int count = Math.toIntExact(source.count);\n      assert count > 0: \"nodeID=\" + nodeID + \" leafNodeOffset=\" + leafNodeOffset;\n      writeLeafBlockDocs(out, heapSource.docIDs, Math.toIntExact(source.start), count);\n\n      // TODO: minor opto: we don't really have to write the actual common prefixes, because BKDReader on recursing can regenerate it for us\n      // from the index, much like how terms dict does so from the FST:\n\n      // Write the common prefixes:\n      writeCommonPrefixes(out, commonPrefixLengths, scratch1);\n\n      // Write the full values:\n      IntFunction<BytesRef> packedValues = new IntFunction<BytesRef>() {\n        final BytesRef scratch = new BytesRef();\n\n        {\n          scratch.length = packedBytesLength;\n        }\n\n        @Override\n        public BytesRef apply(int i) {\n          heapSource.getPackedValueSlice(Math.toIntExact(source.start + i), scratch);\n          return scratch;\n        }\n      };\n      assert valuesInOrderAndBounds(count, sortedDim, minPackedValue, maxPackedValue, packedValues,\n          heapSource.docIDs, Math.toIntExact(source.start));\n      writeLeafBlockPackedValues(out, commonPrefixLengths, count, sortedDim, packedValues);\n\n    } else {\n      // Inner node: partition/recurse\n\n      int splitDim;\n      if (numDims > 1) {\n        splitDim = split(minPackedValue, maxPackedValue);\n      } else {\n        splitDim = 0;\n      }\n\n      PathSlice source = slices[splitDim];\n\n      assert nodeID < splitPackedValues.length: \"nodeID=\" + nodeID + \" splitValues.length=\" + splitPackedValues.length;\n\n      // How many points will be in the left tree:\n      long rightCount = source.count / 2;\n      long leftCount = source.count - rightCount;\n\n      byte[] splitValue = markRightTree(rightCount, splitDim, source, ordBitSet);\n      int address = nodeID * (1+bytesPerDim);\n      splitPackedValues[address] = (byte) splitDim;\n      System.arraycopy(splitValue, 0, splitPackedValues, address + 1, bytesPerDim);\n\n      // Partition all PathSlice that are not the split dim into sorted left and right sets, so we can recurse:\n\n      PathSlice[] leftSlices = new PathSlice[numDims];\n      PathSlice[] rightSlices = new PathSlice[numDims];\n\n      byte[] minSplitPackedValue = new byte[packedBytesLength];\n      System.arraycopy(minPackedValue, 0, minSplitPackedValue, 0, packedBytesLength);\n\n      byte[] maxSplitPackedValue = new byte[packedBytesLength];\n      System.arraycopy(maxPackedValue, 0, maxSplitPackedValue, 0, packedBytesLength);\n\n      // When we are on this dim, below, we clear the ordBitSet:\n      int dimToClear;\n      if (numDims - 1 == splitDim) {\n        dimToClear = numDims - 2;\n      } else {\n        dimToClear = numDims - 1;\n      }\n\n      for(int dim=0;dim<numDims;dim++) {\n\n        if (dim == splitDim) {\n          // No need to partition on this dim since it's a simple slice of the incoming already sorted slice, and we\n          // will re-use its shared reader when visiting it as we recurse:\n          leftSlices[dim] = new PathSlice(source.writer, source.start, leftCount);\n          rightSlices[dim] = new PathSlice(source.writer, source.start + leftCount, rightCount);\n          System.arraycopy(splitValue, 0, minSplitPackedValue, dim*bytesPerDim, bytesPerDim);\n          System.arraycopy(splitValue, 0, maxSplitPackedValue, dim*bytesPerDim, bytesPerDim);\n          continue;\n        }\n\n        // Not inside the try because we don't want to close this one now, so that after recursion is done,\n        // we will have done a singel full sweep of the file:\n        PointReader reader = slices[dim].writer.getSharedReader(slices[dim].start, slices[dim].count, toCloseHeroically);\n\n        try (PointWriter leftPointWriter = getPointWriter(leftCount, \"left\" + dim);\n             PointWriter rightPointWriter = getPointWriter(source.count - leftCount, \"right\" + dim)) {\n\n          long nextRightCount = reader.split(source.count, ordBitSet, leftPointWriter, rightPointWriter, dim == dimToClear);\n          if (rightCount != nextRightCount) {\n            throw new IllegalStateException(\"wrong number of points in split: expected=\" + rightCount + \" but actual=\" + nextRightCount);\n          }\n\n          leftSlices[dim] = new PathSlice(leftPointWriter, 0, leftCount);\n          rightSlices[dim] = new PathSlice(rightPointWriter, 0, rightCount);\n        } catch (Throwable t) {\n          verifyChecksum(t, slices[dim].writer);\n        }\n      }\n\n      // Recurse on left tree:\n      build(2*nodeID, leafNodeOffset, leftSlices,\n            ordBitSet, out,\n            minPackedValue, maxSplitPackedValue,\n            splitPackedValues, leafBlockFPs, toCloseHeroically);\n      for(int dim=0;dim<numDims;dim++) {\n        // Don't destroy the dim we split on because we just re-used what our caller above gave us for that dim:\n        if (dim != splitDim) {\n          leftSlices[dim].writer.destroy();\n        }\n      }\n\n      // TODO: we could \"tail recurse\" here?  have our parent discard its refs as we recurse right?\n      // Recurse on right tree:\n      build(2*nodeID+1, leafNodeOffset, rightSlices,\n            ordBitSet, out,\n            minSplitPackedValue, maxPackedValue,\n            splitPackedValues, leafBlockFPs, toCloseHeroically);\n      for(int dim=0;dim<numDims;dim++) {\n        // Don't destroy the dim we split on because we just re-used what our caller above gave us for that dim:\n        if (dim != splitDim) {\n          rightSlices[dim].writer.destroy();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /** The array (sized numDims) of PathSlice describe the cell we have currently recursed to. */\n  private void build(int nodeID, int leafNodeOffset,\n                     PathSlice[] slices,\n                     LongBitSet ordBitSet,\n                     IndexOutput out,\n                     byte[] minPackedValue, byte[] maxPackedValue,\n                     byte[] splitPackedValues,\n                     long[] leafBlockFPs,\n                     List<Closeable> toCloseHeroically) throws IOException {\n\n    for(PathSlice slice : slices) {\n      assert slice.count == slices[0].count;\n    }\n\n    if (numDims == 1 && slices[0].writer instanceof OfflinePointWriter && slices[0].count <= maxPointsSortInHeap) {\n      // Special case for 1D, to cutover to heap once we recurse deeply enough:\n      slices[0] = switchToHeap(slices[0], toCloseHeroically);\n    }\n\n    if (nodeID >= leafNodeOffset) {\n\n      // Leaf node: write block\n      // We can write the block in any order so by default we write it sorted by the dimension that has the\n      // least number of unique bytes at commonPrefixLengths[dim], which makes compression more efficient\n      int sortedDim = 0;\n      int sortedDimCardinality = Integer.MAX_VALUE;\n\n      for (int dim=0;dim<numDims;dim++) {\n        if (slices[dim].writer instanceof HeapPointWriter == false) {\n          // Adversarial cases can cause this, e.g. very lopsided data, all equal points, such that we started\n          // offline, but then kept splitting only in one dimension, and so never had to rewrite into heap writer\n          slices[dim] = switchToHeap(slices[dim], toCloseHeroically);\n        }\n\n        PathSlice source = slices[dim];\n\n        HeapPointWriter heapSource = (HeapPointWriter) source.writer;\n\n        // Find common prefix by comparing first and last values, already sorted in this dimension:\n        heapSource.readPackedValue(Math.toIntExact(source.start), scratch1);\n        heapSource.readPackedValue(Math.toIntExact(source.start + source.count - 1), scratch2);\n\n        int offset = dim * bytesPerDim;\n        commonPrefixLengths[dim] = bytesPerDim;\n        for(int j=0;j<bytesPerDim;j++) {\n          if (scratch1[offset+j] != scratch2[offset+j]) {\n            commonPrefixLengths[dim] = j;\n            break;\n          }\n        }\n\n        int prefix = commonPrefixLengths[dim];\n        if (prefix < bytesPerDim) {\n          int cardinality = 1;\n          byte previous = scratch1[offset + prefix];\n          for (long i = 1; i < source.count; ++i) {\n            heapSource.readPackedValue(Math.toIntExact(source.start + i), scratch2);\n            byte b = scratch2[offset + prefix];\n            assert Byte.toUnsignedInt(previous) <= Byte.toUnsignedInt(b);\n            if (b != previous) {\n              cardinality++;\n              previous = b;\n            }\n          }\n          assert cardinality <= 256;\n          if (cardinality < sortedDimCardinality) {\n            sortedDim = dim;\n            sortedDimCardinality = cardinality;\n          }\n        }\n      }\n\n      PathSlice source = slices[sortedDim];\n\n      // We ensured that maxPointsSortInHeap was >= maxPointsInLeafNode, so we better be in heap at this point:\n      HeapPointWriter heapSource = (HeapPointWriter) source.writer;\n\n      // Save the block file pointer:\n      leafBlockFPs[nodeID - leafNodeOffset] = out.getFilePointer();\n      //System.out.println(\"  write leaf block @ fp=\" + out.getFilePointer());\n\n      // Write docIDs first, as their own chunk, so that at intersect time we can add all docIDs w/o\n      // loading the values:\n      int count = Math.toIntExact(source.count);\n      assert count > 0: \"nodeID=\" + nodeID + \" leafNodeOffset=\" + leafNodeOffset;\n      writeLeafBlockDocs(out, heapSource.docIDs, Math.toIntExact(source.start), count);\n\n      // TODO: minor opto: we don't really have to write the actual common prefixes, because BKDReader on recursing can regenerate it for us\n      // from the index, much like how terms dict does so from the FST:\n\n      // Write the common prefixes:\n      writeCommonPrefixes(out, commonPrefixLengths, scratch1);\n\n      // Write the full values:\n      IntFunction<BytesRef> packedValues = new IntFunction<BytesRef>() {\n        final BytesRef scratch = new BytesRef();\n\n        {\n          scratch.length = packedBytesLength;\n        }\n\n        @Override\n        public BytesRef apply(int i) {\n          heapSource.getPackedValueSlice(Math.toIntExact(source.start + i), scratch);\n          return scratch;\n        }\n      };\n      assert valuesInOrderAndBounds(count, minPackedValue, maxPackedValue, packedValues);\n      writeLeafBlockPackedValues(out, commonPrefixLengths, count, sortedDim, packedValues);\n\n    } else {\n      // Inner node: partition/recurse\n\n      int splitDim;\n      if (numDims > 1) {\n        splitDim = split(minPackedValue, maxPackedValue);\n      } else {\n        splitDim = 0;\n      }\n\n      PathSlice source = slices[splitDim];\n\n      assert nodeID < splitPackedValues.length: \"nodeID=\" + nodeID + \" splitValues.length=\" + splitPackedValues.length;\n\n      // How many points will be in the left tree:\n      long rightCount = source.count / 2;\n      long leftCount = source.count - rightCount;\n\n      byte[] splitValue = markRightTree(rightCount, splitDim, source, ordBitSet);\n      int address = nodeID * (1+bytesPerDim);\n      splitPackedValues[address] = (byte) splitDim;\n      System.arraycopy(splitValue, 0, splitPackedValues, address + 1, bytesPerDim);\n\n      // Partition all PathSlice that are not the split dim into sorted left and right sets, so we can recurse:\n\n      PathSlice[] leftSlices = new PathSlice[numDims];\n      PathSlice[] rightSlices = new PathSlice[numDims];\n\n      byte[] minSplitPackedValue = new byte[packedBytesLength];\n      System.arraycopy(minPackedValue, 0, minSplitPackedValue, 0, packedBytesLength);\n\n      byte[] maxSplitPackedValue = new byte[packedBytesLength];\n      System.arraycopy(maxPackedValue, 0, maxSplitPackedValue, 0, packedBytesLength);\n\n      // When we are on this dim, below, we clear the ordBitSet:\n      int dimToClear;\n      if (numDims - 1 == splitDim) {\n        dimToClear = numDims - 2;\n      } else {\n        dimToClear = numDims - 1;\n      }\n\n      for(int dim=0;dim<numDims;dim++) {\n\n        if (dim == splitDim) {\n          // No need to partition on this dim since it's a simple slice of the incoming already sorted slice, and we\n          // will re-use its shared reader when visiting it as we recurse:\n          leftSlices[dim] = new PathSlice(source.writer, source.start, leftCount);\n          rightSlices[dim] = new PathSlice(source.writer, source.start + leftCount, rightCount);\n          System.arraycopy(splitValue, 0, minSplitPackedValue, dim*bytesPerDim, bytesPerDim);\n          System.arraycopy(splitValue, 0, maxSplitPackedValue, dim*bytesPerDim, bytesPerDim);\n          continue;\n        }\n\n        // Not inside the try because we don't want to close this one now, so that after recursion is done,\n        // we will have done a singel full sweep of the file:\n        PointReader reader = slices[dim].writer.getSharedReader(slices[dim].start, slices[dim].count, toCloseHeroically);\n\n        try (PointWriter leftPointWriter = getPointWriter(leftCount, \"left\" + dim);\n             PointWriter rightPointWriter = getPointWriter(source.count - leftCount, \"right\" + dim)) {\n\n          long nextRightCount = reader.split(source.count, ordBitSet, leftPointWriter, rightPointWriter, dim == dimToClear);\n          if (rightCount != nextRightCount) {\n            throw new IllegalStateException(\"wrong number of points in split: expected=\" + rightCount + \" but actual=\" + nextRightCount);\n          }\n\n          leftSlices[dim] = new PathSlice(leftPointWriter, 0, leftCount);\n          rightSlices[dim] = new PathSlice(rightPointWriter, 0, rightCount);\n        } catch (Throwable t) {\n          verifyChecksum(t, slices[dim].writer);\n        }\n      }\n\n      // Recurse on left tree:\n      build(2*nodeID, leafNodeOffset, leftSlices,\n            ordBitSet, out,\n            minPackedValue, maxSplitPackedValue,\n            splitPackedValues, leafBlockFPs, toCloseHeroically);\n      for(int dim=0;dim<numDims;dim++) {\n        // Don't destroy the dim we split on because we just re-used what our caller above gave us for that dim:\n        if (dim != splitDim) {\n          leftSlices[dim].writer.destroy();\n        }\n      }\n\n      // TODO: we could \"tail recurse\" here?  have our parent discard its refs as we recurse right?\n      // Recurse on right tree:\n      build(2*nodeID+1, leafNodeOffset, rightSlices,\n            ordBitSet, out,\n            minSplitPackedValue, maxPackedValue,\n            splitPackedValues, leafBlockFPs, toCloseHeroically);\n      for(int dim=0;dim<numDims;dim++) {\n        // Don't destroy the dim we split on because we just re-used what our caller above gave us for that dim:\n        if (dim != splitDim) {\n          rightSlices[dim].writer.destroy();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#build(int,int,PathSlice[],LongBitSet,IndexOutput,byte[],byte[],byte[],long[],List[Closeable]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#build(int,int,PathSlice[],LongBitSet,IndexOutput,byte[],byte[],byte[],long[],List[Closeable]).mjava","sourceNew":"  /** The array (sized numDims) of PathSlice describe the cell we have currently recursed to. */\n  private void build(int nodeID, int leafNodeOffset,\n                     PathSlice[] slices,\n                     LongBitSet ordBitSet,\n                     IndexOutput out,\n                     byte[] minPackedValue, byte[] maxPackedValue,\n                     byte[] splitPackedValues,\n                     long[] leafBlockFPs,\n                     List<Closeable> toCloseHeroically) throws IOException {\n\n    for(PathSlice slice : slices) {\n      assert slice.count == slices[0].count;\n    }\n\n    if (numDims == 1 && slices[0].writer instanceof OfflinePointWriter && slices[0].count <= maxPointsSortInHeap) {\n      // Special case for 1D, to cutover to heap once we recurse deeply enough:\n      slices[0] = switchToHeap(slices[0], toCloseHeroically);\n    }\n\n    if (nodeID >= leafNodeOffset) {\n\n      // Leaf node: write block\n      // We can write the block in any order so by default we write it sorted by the dimension that has the\n      // least number of unique bytes at commonPrefixLengths[dim], which makes compression more efficient\n      int sortedDim = 0;\n      int sortedDimCardinality = Integer.MAX_VALUE;\n\n      for (int dim=0;dim<numDims;dim++) {\n        if (slices[dim].writer instanceof HeapPointWriter == false) {\n          // Adversarial cases can cause this, e.g. very lopsided data, all equal points, such that we started\n          // offline, but then kept splitting only in one dimension, and so never had to rewrite into heap writer\n          slices[dim] = switchToHeap(slices[dim], toCloseHeroically);\n        }\n\n        PathSlice source = slices[dim];\n\n        HeapPointWriter heapSource = (HeapPointWriter) source.writer;\n\n        // Find common prefix by comparing first and last values, already sorted in this dimension:\n        heapSource.readPackedValue(Math.toIntExact(source.start), scratch1);\n        heapSource.readPackedValue(Math.toIntExact(source.start + source.count - 1), scratch2);\n\n        int offset = dim * bytesPerDim;\n        commonPrefixLengths[dim] = bytesPerDim;\n        for(int j=0;j<bytesPerDim;j++) {\n          if (scratch1[offset+j] != scratch2[offset+j]) {\n            commonPrefixLengths[dim] = j;\n            break;\n          }\n        }\n\n        int prefix = commonPrefixLengths[dim];\n        if (prefix < bytesPerDim) {\n          int cardinality = 1;\n          byte previous = scratch1[offset + prefix];\n          for (long i = 1; i < source.count; ++i) {\n            heapSource.readPackedValue(Math.toIntExact(source.start + i), scratch2);\n            byte b = scratch2[offset + prefix];\n            assert Byte.toUnsignedInt(previous) <= Byte.toUnsignedInt(b);\n            if (b != previous) {\n              cardinality++;\n              previous = b;\n            }\n          }\n          assert cardinality <= 256;\n          if (cardinality < sortedDimCardinality) {\n            sortedDim = dim;\n            sortedDimCardinality = cardinality;\n          }\n        }\n      }\n\n      PathSlice source = slices[sortedDim];\n\n      // We ensured that maxPointsSortInHeap was >= maxPointsInLeafNode, so we better be in heap at this point:\n      HeapPointWriter heapSource = (HeapPointWriter) source.writer;\n\n      // Save the block file pointer:\n      leafBlockFPs[nodeID - leafNodeOffset] = out.getFilePointer();\n      //System.out.println(\"  write leaf block @ fp=\" + out.getFilePointer());\n\n      // Write docIDs first, as their own chunk, so that at intersect time we can add all docIDs w/o\n      // loading the values:\n      int count = Math.toIntExact(source.count);\n      assert count > 0: \"nodeID=\" + nodeID + \" leafNodeOffset=\" + leafNodeOffset;\n      writeLeafBlockDocs(out, heapSource.docIDs, Math.toIntExact(source.start), count);\n\n      // TODO: minor opto: we don't really have to write the actual common prefixes, because BKDReader on recursing can regenerate it for us\n      // from the index, much like how terms dict does so from the FST:\n\n      // Write the common prefixes:\n      writeCommonPrefixes(out, commonPrefixLengths, scratch1);\n\n      // Write the full values:\n      IntFunction<BytesRef> packedValues = new IntFunction<BytesRef>() {\n        final BytesRef scratch = new BytesRef();\n\n        {\n          scratch.length = packedBytesLength;\n        }\n\n        @Override\n        public BytesRef apply(int i) {\n          heapSource.getPackedValueSlice(Math.toIntExact(source.start + i), scratch);\n          return scratch;\n        }\n      };\n      assert valuesInOrderAndBounds(count, sortedDim, minPackedValue, maxPackedValue, packedValues,\n          heapSource.docIDs, Math.toIntExact(source.start));\n      writeLeafBlockPackedValues(out, commonPrefixLengths, count, sortedDim, packedValues);\n\n    } else {\n      // Inner node: partition/recurse\n\n      int splitDim;\n      if (numDims > 1) {\n        splitDim = split(minPackedValue, maxPackedValue);\n      } else {\n        splitDim = 0;\n      }\n\n      PathSlice source = slices[splitDim];\n\n      assert nodeID < splitPackedValues.length: \"nodeID=\" + nodeID + \" splitValues.length=\" + splitPackedValues.length;\n\n      // How many points will be in the left tree:\n      long rightCount = source.count / 2;\n      long leftCount = source.count - rightCount;\n\n      byte[] splitValue = markRightTree(rightCount, splitDim, source, ordBitSet);\n      int address = nodeID * (1+bytesPerDim);\n      splitPackedValues[address] = (byte) splitDim;\n      System.arraycopy(splitValue, 0, splitPackedValues, address + 1, bytesPerDim);\n\n      // Partition all PathSlice that are not the split dim into sorted left and right sets, so we can recurse:\n\n      PathSlice[] leftSlices = new PathSlice[numDims];\n      PathSlice[] rightSlices = new PathSlice[numDims];\n\n      byte[] minSplitPackedValue = new byte[packedBytesLength];\n      System.arraycopy(minPackedValue, 0, minSplitPackedValue, 0, packedBytesLength);\n\n      byte[] maxSplitPackedValue = new byte[packedBytesLength];\n      System.arraycopy(maxPackedValue, 0, maxSplitPackedValue, 0, packedBytesLength);\n\n      // When we are on this dim, below, we clear the ordBitSet:\n      int dimToClear;\n      if (numDims - 1 == splitDim) {\n        dimToClear = numDims - 2;\n      } else {\n        dimToClear = numDims - 1;\n      }\n\n      for(int dim=0;dim<numDims;dim++) {\n\n        if (dim == splitDim) {\n          // No need to partition on this dim since it's a simple slice of the incoming already sorted slice, and we\n          // will re-use its shared reader when visiting it as we recurse:\n          leftSlices[dim] = new PathSlice(source.writer, source.start, leftCount);\n          rightSlices[dim] = new PathSlice(source.writer, source.start + leftCount, rightCount);\n          System.arraycopy(splitValue, 0, minSplitPackedValue, dim*bytesPerDim, bytesPerDim);\n          System.arraycopy(splitValue, 0, maxSplitPackedValue, dim*bytesPerDim, bytesPerDim);\n          continue;\n        }\n\n        // Not inside the try because we don't want to close this one now, so that after recursion is done,\n        // we will have done a singel full sweep of the file:\n        PointReader reader = slices[dim].writer.getSharedReader(slices[dim].start, slices[dim].count, toCloseHeroically);\n\n        try (PointWriter leftPointWriter = getPointWriter(leftCount, \"left\" + dim);\n             PointWriter rightPointWriter = getPointWriter(source.count - leftCount, \"right\" + dim)) {\n\n          long nextRightCount = reader.split(source.count, ordBitSet, leftPointWriter, rightPointWriter, dim == dimToClear);\n          if (rightCount != nextRightCount) {\n            throw new IllegalStateException(\"wrong number of points in split: expected=\" + rightCount + \" but actual=\" + nextRightCount);\n          }\n\n          leftSlices[dim] = new PathSlice(leftPointWriter, 0, leftCount);\n          rightSlices[dim] = new PathSlice(rightPointWriter, 0, rightCount);\n        } catch (Throwable t) {\n          verifyChecksum(t, slices[dim].writer);\n        }\n      }\n\n      // Recurse on left tree:\n      build(2*nodeID, leafNodeOffset, leftSlices,\n            ordBitSet, out,\n            minPackedValue, maxSplitPackedValue,\n            splitPackedValues, leafBlockFPs, toCloseHeroically);\n      for(int dim=0;dim<numDims;dim++) {\n        // Don't destroy the dim we split on because we just re-used what our caller above gave us for that dim:\n        if (dim != splitDim) {\n          leftSlices[dim].writer.destroy();\n        }\n      }\n\n      // TODO: we could \"tail recurse\" here?  have our parent discard its refs as we recurse right?\n      // Recurse on right tree:\n      build(2*nodeID+1, leafNodeOffset, rightSlices,\n            ordBitSet, out,\n            minSplitPackedValue, maxPackedValue,\n            splitPackedValues, leafBlockFPs, toCloseHeroically);\n      for(int dim=0;dim<numDims;dim++) {\n        // Don't destroy the dim we split on because we just re-used what our caller above gave us for that dim:\n        if (dim != splitDim) {\n          rightSlices[dim].writer.destroy();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /** The array (sized numDims) of PathSlice describe the cell we have currently recursed to. */\n  private void build(int nodeID, int leafNodeOffset,\n                     PathSlice[] slices,\n                     LongBitSet ordBitSet,\n                     IndexOutput out,\n                     byte[] minPackedValue, byte[] maxPackedValue,\n                     byte[] splitPackedValues,\n                     long[] leafBlockFPs,\n                     List<Closeable> toCloseHeroically) throws IOException {\n\n    for(PathSlice slice : slices) {\n      assert slice.count == slices[0].count;\n    }\n\n    if (numDims == 1 && slices[0].writer instanceof OfflinePointWriter && slices[0].count <= maxPointsSortInHeap) {\n      // Special case for 1D, to cutover to heap once we recurse deeply enough:\n      slices[0] = switchToHeap(slices[0], toCloseHeroically);\n    }\n\n    if (nodeID >= leafNodeOffset) {\n\n      // Leaf node: write block\n      for (int dim=0;dim<numDims;dim++) {\n        if (slices[dim].writer instanceof HeapPointWriter == false) {\n          // Adversarial cases can cause this, e.g. very lopsided data, all equal points, such that we started\n          // offline, but then kept splitting only in one dimension, and so never had to rewrite into heap writer\n          slices[dim] = switchToHeap(slices[dim], toCloseHeroically);\n        }\n\n        PathSlice source = slices[dim];\n\n        HeapPointWriter heapSource = (HeapPointWriter) source.writer;\n\n        // Find common prefix by comparing first and last values, already sorted in this dimension:\n        heapSource.readPackedValue(Math.toIntExact(source.start), scratch1);\n        heapSource.readPackedValue(Math.toIntExact(source.start + source.count - 1), scratch2);\n\n        int offset = dim * bytesPerDim;\n        commonPrefixLengths[dim] = bytesPerDim;\n        for(int j=0;j<bytesPerDim;j++) {\n          if (scratch1[offset+j] != scratch2[offset+j]) {\n            commonPrefixLengths[dim] = j;\n            break;\n          }\n        }\n      }\n\n      PathSlice source = slices[0];\n\n      // We ensured that maxPointsSortInHeap was >= maxPointsInLeafNode, so we better be in heap at this point:\n      HeapPointWriter heapSource = (HeapPointWriter) source.writer;\n\n      // Save the block file pointer:\n      leafBlockFPs[nodeID - leafNodeOffset] = out.getFilePointer();\n      //System.out.println(\"  write leaf block @ fp=\" + out.getFilePointer());\n\n      // Write docIDs first, as their own chunk, so that at intersect time we can add all docIDs w/o\n      // loading the values:\n      int count = Math.toIntExact(source.count);\n      assert count > 0: \"nodeID=\" + nodeID + \" leafNodeOffset=\" + leafNodeOffset;\n      writeLeafBlockDocs(out, heapSource.docIDs, Math.toIntExact(source.start), count);\n\n      // TODO: minor opto: we don't really have to write the actual common prefixes, because BKDReader on recursing can regenerate it for us\n      // from the index, much like how terms dict does so from the FST:\n\n      // Write the common prefixes:\n      writeCommonPrefixes(out, commonPrefixLengths, scratch1);\n\n      // Write the full values:\n      byte[] lastPackedValue = new byte[bytesPerDim];\n      for (int i=0;i<count;i++) {\n        heapSource.getPackedValueSlice(Math.toIntExact(source.start + i), scratchBytesRef);\n        assert numDims != 1 || valueInOrder(i, lastPackedValue, scratchBytesRef.bytes, scratchBytesRef.offset);\n\n        // Make sure this value does in fact fall within this leaf cell:\n        assert valueInBounds(scratchBytesRef, minPackedValue, maxPackedValue);\n        writeLeafBlockPackedValue(out, commonPrefixLengths, scratchBytesRef.bytes, scratchBytesRef.offset);\n      }\n\n    } else {\n      // Inner node: partition/recurse\n\n      int splitDim;\n      if (numDims > 1) {\n        splitDim = split(minPackedValue, maxPackedValue);\n      } else {\n        splitDim = 0;\n      }\n\n      PathSlice source = slices[splitDim];\n\n      assert nodeID < splitPackedValues.length: \"nodeID=\" + nodeID + \" splitValues.length=\" + splitPackedValues.length;\n\n      // How many points will be in the left tree:\n      long rightCount = source.count / 2;\n      long leftCount = source.count - rightCount;\n\n      byte[] splitValue = markRightTree(rightCount, splitDim, source, ordBitSet);\n      int address = nodeID * (1+bytesPerDim);\n      splitPackedValues[address] = (byte) splitDim;\n      System.arraycopy(splitValue, 0, splitPackedValues, address + 1, bytesPerDim);\n\n      // Partition all PathSlice that are not the split dim into sorted left and right sets, so we can recurse:\n\n      PathSlice[] leftSlices = new PathSlice[numDims];\n      PathSlice[] rightSlices = new PathSlice[numDims];\n\n      byte[] minSplitPackedValue = new byte[packedBytesLength];\n      System.arraycopy(minPackedValue, 0, minSplitPackedValue, 0, packedBytesLength);\n\n      byte[] maxSplitPackedValue = new byte[packedBytesLength];\n      System.arraycopy(maxPackedValue, 0, maxSplitPackedValue, 0, packedBytesLength);\n\n      // When we are on this dim, below, we clear the ordBitSet:\n      int dimToClear;\n      if (numDims - 1 == splitDim) {\n        dimToClear = numDims - 2;\n      } else {\n        dimToClear = numDims - 1;\n      }\n\n      for(int dim=0;dim<numDims;dim++) {\n\n        if (dim == splitDim) {\n          // No need to partition on this dim since it's a simple slice of the incoming already sorted slice, and we\n          // will re-use its shared reader when visiting it as we recurse:\n          leftSlices[dim] = new PathSlice(source.writer, source.start, leftCount);\n          rightSlices[dim] = new PathSlice(source.writer, source.start + leftCount, rightCount);\n          System.arraycopy(splitValue, 0, minSplitPackedValue, dim*bytesPerDim, bytesPerDim);\n          System.arraycopy(splitValue, 0, maxSplitPackedValue, dim*bytesPerDim, bytesPerDim);\n          continue;\n        }\n\n        // Not inside the try because we don't want to close this one now, so that after recursion is done,\n        // we will have done a singel full sweep of the file:\n        PointReader reader = slices[dim].writer.getSharedReader(slices[dim].start, slices[dim].count, toCloseHeroically);\n\n        try (PointWriter leftPointWriter = getPointWriter(leftCount, \"left\" + dim);\n             PointWriter rightPointWriter = getPointWriter(source.count - leftCount, \"right\" + dim)) {\n\n          long nextRightCount = reader.split(source.count, ordBitSet, leftPointWriter, rightPointWriter, dim == dimToClear);\n          if (rightCount != nextRightCount) {\n            throw new IllegalStateException(\"wrong number of points in split: expected=\" + rightCount + \" but actual=\" + nextRightCount);\n          }\n\n          leftSlices[dim] = new PathSlice(leftPointWriter, 0, leftCount);\n          rightSlices[dim] = new PathSlice(rightPointWriter, 0, rightCount);\n        } catch (Throwable t) {\n          verifyChecksum(t, slices[dim].writer);\n        }\n      }\n\n      // Recurse on left tree:\n      build(2*nodeID, leafNodeOffset, leftSlices,\n            ordBitSet, out,\n            minPackedValue, maxSplitPackedValue,\n            splitPackedValues, leafBlockFPs, toCloseHeroically);\n      for(int dim=0;dim<numDims;dim++) {\n        // Don't destroy the dim we split on because we just re-used what our caller above gave us for that dim:\n        if (dim != splitDim) {\n          leftSlices[dim].writer.destroy();\n        }\n      }\n\n      // TODO: we could \"tail recurse\" here?  have our parent discard its refs as we recurse right?\n      // Recurse on right tree:\n      build(2*nodeID+1, leafNodeOffset, rightSlices,\n            ordBitSet, out,\n            minSplitPackedValue, maxPackedValue,\n            splitPackedValues, leafBlockFPs, toCloseHeroically);\n      for(int dim=0;dim<numDims;dim++) {\n        // Don't destroy the dim we split on because we just re-used what our caller above gave us for that dim:\n        if (dim != splitDim) {\n          rightSlices[dim].writer.destroy();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9fc0d60683b47b5d922124c31f57c8b34734f9e6","date":1480846684,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#build(int,int,PathSlice[],LongBitSet,IndexOutput,byte[],byte[],byte[],long[],List[Closeable]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#build(int,int,PathSlice[],LongBitSet,IndexOutput,byte[],byte[],byte[],long[],List[Closeable]).mjava","sourceNew":"  /** The array (sized numDims) of PathSlice describe the cell we have currently recursed to. */\n  private void build(int nodeID, int leafNodeOffset,\n                     PathSlice[] slices,\n                     LongBitSet ordBitSet,\n                     IndexOutput out,\n                     byte[] minPackedValue, byte[] maxPackedValue,\n                     byte[] splitPackedValues,\n                     long[] leafBlockFPs,\n                     List<Closeable> toCloseHeroically) throws IOException {\n\n    for(PathSlice slice : slices) {\n      assert slice.count == slices[0].count;\n    }\n    \n    if (numDims == 1 && slices[0].writer instanceof OfflinePointWriter && slices[0].count <= maxPointsSortInHeap) {\n      // Special case for 1D, to cutover to heap once we recurse deeply enough:\n      slices[0] = switchToHeap(slices[0], toCloseHeroically);\n    }\n\n    if (nodeID >= leafNodeOffset) {\n\n      // Leaf node: write block\n      // We can write the block in any order so by default we write it sorted by the dimension that has the\n      // least number of unique bytes at commonPrefixLengths[dim], which makes compression more efficient\n      int sortedDim = 0;\n      int sortedDimCardinality = Integer.MAX_VALUE;\n\n      for (int dim=0;dim<numDims;dim++) {\n        if (slices[dim].writer instanceof HeapPointWriter == false) {\n          // Adversarial cases can cause this, e.g. very lopsided data, all equal points, such that we started\n          // offline, but then kept splitting only in one dimension, and so never had to rewrite into heap writer\n          slices[dim] = switchToHeap(slices[dim], toCloseHeroically);\n        }\n\n        PathSlice source = slices[dim];\n\n        HeapPointWriter heapSource = (HeapPointWriter) source.writer;\n\n        // Find common prefix by comparing first and last values, already sorted in this dimension:\n        heapSource.readPackedValue(Math.toIntExact(source.start), scratch1);\n        heapSource.readPackedValue(Math.toIntExact(source.start + source.count - 1), scratch2);\n\n        int offset = dim * bytesPerDim;\n        commonPrefixLengths[dim] = bytesPerDim;\n        for(int j=0;j<bytesPerDim;j++) {\n          if (scratch1[offset+j] != scratch2[offset+j]) {\n            commonPrefixLengths[dim] = j;\n            break;\n          }\n        }\n\n        int prefix = commonPrefixLengths[dim];\n        if (prefix < bytesPerDim) {\n          int cardinality = 1;\n          byte previous = scratch1[offset + prefix];\n          for (long i = 1; i < source.count; ++i) {\n            heapSource.readPackedValue(Math.toIntExact(source.start + i), scratch2);\n            byte b = scratch2[offset + prefix];\n            assert Byte.toUnsignedInt(previous) <= Byte.toUnsignedInt(b);\n            if (b != previous) {\n              cardinality++;\n              previous = b;\n            }\n          }\n          assert cardinality <= 256;\n          if (cardinality < sortedDimCardinality) {\n            sortedDim = dim;\n            sortedDimCardinality = cardinality;\n          }\n        }\n      }\n\n      PathSlice source = slices[sortedDim];\n\n      // We ensured that maxPointsSortInHeap was >= maxPointsInLeafNode, so we better be in heap at this point:\n      HeapPointWriter heapSource = (HeapPointWriter) source.writer;\n\n      // Save the block file pointer:\n      leafBlockFPs[nodeID - leafNodeOffset] = out.getFilePointer();\n      //System.out.println(\"  write leaf block @ fp=\" + out.getFilePointer());\n\n      // Write docIDs first, as their own chunk, so that at intersect time we can add all docIDs w/o\n      // loading the values:\n      int count = Math.toIntExact(source.count);\n      assert count > 0: \"nodeID=\" + nodeID + \" leafNodeOffset=\" + leafNodeOffset;\n      writeLeafBlockDocs(out, heapSource.docIDs, Math.toIntExact(source.start), count);\n\n      // TODO: minor opto: we don't really have to write the actual common prefixes, because BKDReader on recursing can regenerate it for us\n      // from the index, much like how terms dict does so from the FST:\n\n      // Write the common prefixes:\n      writeCommonPrefixes(out, commonPrefixLengths, scratch1);\n\n      // Write the full values:\n      IntFunction<BytesRef> packedValues = new IntFunction<BytesRef>() {\n        final BytesRef scratch = new BytesRef();\n\n        {\n          scratch.length = packedBytesLength;\n        }\n\n        @Override\n        public BytesRef apply(int i) {\n          heapSource.getPackedValueSlice(Math.toIntExact(source.start + i), scratch);\n          return scratch;\n        }\n      };\n      assert valuesInOrderAndBounds(count, sortedDim, minPackedValue, maxPackedValue, packedValues,\n          heapSource.docIDs, Math.toIntExact(source.start));\n      writeLeafBlockPackedValues(out, commonPrefixLengths, count, sortedDim, packedValues);\n\n    } else {\n      // Inner node: partition/recurse\n\n      int splitDim;\n      if (numDims > 1) {\n        splitDim = split(minPackedValue, maxPackedValue);\n      } else {\n        splitDim = 0;\n      }\n\n      PathSlice source = slices[splitDim];\n\n      assert nodeID < splitPackedValues.length: \"nodeID=\" + nodeID + \" splitValues.length=\" + splitPackedValues.length;\n\n      // How many points will be in the left tree:\n      long rightCount = source.count / 2;\n      long leftCount = source.count - rightCount;\n\n      byte[] splitValue = markRightTree(rightCount, splitDim, source, ordBitSet);\n      int address = nodeID * (1+bytesPerDim);\n      splitPackedValues[address] = (byte) splitDim;\n      System.arraycopy(splitValue, 0, splitPackedValues, address + 1, bytesPerDim);\n\n      // Partition all PathSlice that are not the split dim into sorted left and right sets, so we can recurse:\n\n      PathSlice[] leftSlices = new PathSlice[numDims];\n      PathSlice[] rightSlices = new PathSlice[numDims];\n\n      byte[] minSplitPackedValue = new byte[packedBytesLength];\n      System.arraycopy(minPackedValue, 0, minSplitPackedValue, 0, packedBytesLength);\n\n      byte[] maxSplitPackedValue = new byte[packedBytesLength];\n      System.arraycopy(maxPackedValue, 0, maxSplitPackedValue, 0, packedBytesLength);\n\n      // When we are on this dim, below, we clear the ordBitSet:\n      int dimToClear;\n      if (numDims - 1 == splitDim) {\n        dimToClear = numDims - 2;\n      } else {\n        dimToClear = numDims - 1;\n      }\n\n      for(int dim=0;dim<numDims;dim++) {\n\n        if (dim == splitDim) {\n          // No need to partition on this dim since it's a simple slice of the incoming already sorted slice, and we\n          // will re-use its shared reader when visiting it as we recurse:\n          leftSlices[dim] = new PathSlice(source.writer, source.start, leftCount);\n          rightSlices[dim] = new PathSlice(source.writer, source.start + leftCount, rightCount);\n          System.arraycopy(splitValue, 0, minSplitPackedValue, dim*bytesPerDim, bytesPerDim);\n          System.arraycopy(splitValue, 0, maxSplitPackedValue, dim*bytesPerDim, bytesPerDim);\n          continue;\n        }\n\n        // Not inside the try because we don't want to close this one now, so that after recursion is done,\n        // we will have done a singel full sweep of the file:\n        PointReader reader = slices[dim].writer.getSharedReader(slices[dim].start, slices[dim].count, toCloseHeroically);\n\n        try (PointWriter leftPointWriter = getPointWriter(leftCount, \"left\" + dim);\n             PointWriter rightPointWriter = getPointWriter(source.count - leftCount, \"right\" + dim)) {\n\n          long nextRightCount = reader.split(source.count, ordBitSet, leftPointWriter, rightPointWriter, dim == dimToClear);\n          if (rightCount != nextRightCount) {\n            throw new IllegalStateException(\"wrong number of points in split: expected=\" + rightCount + \" but actual=\" + nextRightCount);\n          }\n\n          leftSlices[dim] = new PathSlice(leftPointWriter, 0, leftCount);\n          rightSlices[dim] = new PathSlice(rightPointWriter, 0, rightCount);\n        } catch (Throwable t) {\n          verifyChecksum(t, slices[dim].writer);\n        }\n      }\n\n      // Recurse on left tree:\n      build(2*nodeID, leafNodeOffset, leftSlices,\n            ordBitSet, out,\n            minPackedValue, maxSplitPackedValue,\n            splitPackedValues, leafBlockFPs, toCloseHeroically);\n      for(int dim=0;dim<numDims;dim++) {\n        // Don't destroy the dim we split on because we just re-used what our caller above gave us for that dim:\n        if (dim != splitDim) {\n          leftSlices[dim].writer.destroy();\n        }\n      }\n\n      // TODO: we could \"tail recurse\" here?  have our parent discard its refs as we recurse right?\n      // Recurse on right tree:\n      build(2*nodeID+1, leafNodeOffset, rightSlices,\n            ordBitSet, out,\n            minSplitPackedValue, maxPackedValue,\n            splitPackedValues, leafBlockFPs, toCloseHeroically);\n      for(int dim=0;dim<numDims;dim++) {\n        // Don't destroy the dim we split on because we just re-used what our caller above gave us for that dim:\n        if (dim != splitDim) {\n          rightSlices[dim].writer.destroy();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /** The array (sized numDims) of PathSlice describe the cell we have currently recursed to. */\n  private void build(int nodeID, int leafNodeOffset,\n                     PathSlice[] slices,\n                     LongBitSet ordBitSet,\n                     IndexOutput out,\n                     byte[] minPackedValue, byte[] maxPackedValue,\n                     byte[] splitPackedValues,\n                     long[] leafBlockFPs,\n                     List<Closeable> toCloseHeroically) throws IOException {\n\n    for(PathSlice slice : slices) {\n      assert slice.count == slices[0].count;\n    }\n\n    if (numDims == 1 && slices[0].writer instanceof OfflinePointWriter && slices[0].count <= maxPointsSortInHeap) {\n      // Special case for 1D, to cutover to heap once we recurse deeply enough:\n      slices[0] = switchToHeap(slices[0], toCloseHeroically);\n    }\n\n    if (nodeID >= leafNodeOffset) {\n\n      // Leaf node: write block\n      // We can write the block in any order so by default we write it sorted by the dimension that has the\n      // least number of unique bytes at commonPrefixLengths[dim], which makes compression more efficient\n      int sortedDim = 0;\n      int sortedDimCardinality = Integer.MAX_VALUE;\n\n      for (int dim=0;dim<numDims;dim++) {\n        if (slices[dim].writer instanceof HeapPointWriter == false) {\n          // Adversarial cases can cause this, e.g. very lopsided data, all equal points, such that we started\n          // offline, but then kept splitting only in one dimension, and so never had to rewrite into heap writer\n          slices[dim] = switchToHeap(slices[dim], toCloseHeroically);\n        }\n\n        PathSlice source = slices[dim];\n\n        HeapPointWriter heapSource = (HeapPointWriter) source.writer;\n\n        // Find common prefix by comparing first and last values, already sorted in this dimension:\n        heapSource.readPackedValue(Math.toIntExact(source.start), scratch1);\n        heapSource.readPackedValue(Math.toIntExact(source.start + source.count - 1), scratch2);\n\n        int offset = dim * bytesPerDim;\n        commonPrefixLengths[dim] = bytesPerDim;\n        for(int j=0;j<bytesPerDim;j++) {\n          if (scratch1[offset+j] != scratch2[offset+j]) {\n            commonPrefixLengths[dim] = j;\n            break;\n          }\n        }\n\n        int prefix = commonPrefixLengths[dim];\n        if (prefix < bytesPerDim) {\n          int cardinality = 1;\n          byte previous = scratch1[offset + prefix];\n          for (long i = 1; i < source.count; ++i) {\n            heapSource.readPackedValue(Math.toIntExact(source.start + i), scratch2);\n            byte b = scratch2[offset + prefix];\n            assert Byte.toUnsignedInt(previous) <= Byte.toUnsignedInt(b);\n            if (b != previous) {\n              cardinality++;\n              previous = b;\n            }\n          }\n          assert cardinality <= 256;\n          if (cardinality < sortedDimCardinality) {\n            sortedDim = dim;\n            sortedDimCardinality = cardinality;\n          }\n        }\n      }\n\n      PathSlice source = slices[sortedDim];\n\n      // We ensured that maxPointsSortInHeap was >= maxPointsInLeafNode, so we better be in heap at this point:\n      HeapPointWriter heapSource = (HeapPointWriter) source.writer;\n\n      // Save the block file pointer:\n      leafBlockFPs[nodeID - leafNodeOffset] = out.getFilePointer();\n      //System.out.println(\"  write leaf block @ fp=\" + out.getFilePointer());\n\n      // Write docIDs first, as their own chunk, so that at intersect time we can add all docIDs w/o\n      // loading the values:\n      int count = Math.toIntExact(source.count);\n      assert count > 0: \"nodeID=\" + nodeID + \" leafNodeOffset=\" + leafNodeOffset;\n      writeLeafBlockDocs(out, heapSource.docIDs, Math.toIntExact(source.start), count);\n\n      // TODO: minor opto: we don't really have to write the actual common prefixes, because BKDReader on recursing can regenerate it for us\n      // from the index, much like how terms dict does so from the FST:\n\n      // Write the common prefixes:\n      writeCommonPrefixes(out, commonPrefixLengths, scratch1);\n\n      // Write the full values:\n      IntFunction<BytesRef> packedValues = new IntFunction<BytesRef>() {\n        final BytesRef scratch = new BytesRef();\n\n        {\n          scratch.length = packedBytesLength;\n        }\n\n        @Override\n        public BytesRef apply(int i) {\n          heapSource.getPackedValueSlice(Math.toIntExact(source.start + i), scratch);\n          return scratch;\n        }\n      };\n      assert valuesInOrderAndBounds(count, sortedDim, minPackedValue, maxPackedValue, packedValues,\n          heapSource.docIDs, Math.toIntExact(source.start));\n      writeLeafBlockPackedValues(out, commonPrefixLengths, count, sortedDim, packedValues);\n\n    } else {\n      // Inner node: partition/recurse\n\n      int splitDim;\n      if (numDims > 1) {\n        splitDim = split(minPackedValue, maxPackedValue);\n      } else {\n        splitDim = 0;\n      }\n\n      PathSlice source = slices[splitDim];\n\n      assert nodeID < splitPackedValues.length: \"nodeID=\" + nodeID + \" splitValues.length=\" + splitPackedValues.length;\n\n      // How many points will be in the left tree:\n      long rightCount = source.count / 2;\n      long leftCount = source.count - rightCount;\n\n      byte[] splitValue = markRightTree(rightCount, splitDim, source, ordBitSet);\n      int address = nodeID * (1+bytesPerDim);\n      splitPackedValues[address] = (byte) splitDim;\n      System.arraycopy(splitValue, 0, splitPackedValues, address + 1, bytesPerDim);\n\n      // Partition all PathSlice that are not the split dim into sorted left and right sets, so we can recurse:\n\n      PathSlice[] leftSlices = new PathSlice[numDims];\n      PathSlice[] rightSlices = new PathSlice[numDims];\n\n      byte[] minSplitPackedValue = new byte[packedBytesLength];\n      System.arraycopy(minPackedValue, 0, minSplitPackedValue, 0, packedBytesLength);\n\n      byte[] maxSplitPackedValue = new byte[packedBytesLength];\n      System.arraycopy(maxPackedValue, 0, maxSplitPackedValue, 0, packedBytesLength);\n\n      // When we are on this dim, below, we clear the ordBitSet:\n      int dimToClear;\n      if (numDims - 1 == splitDim) {\n        dimToClear = numDims - 2;\n      } else {\n        dimToClear = numDims - 1;\n      }\n\n      for(int dim=0;dim<numDims;dim++) {\n\n        if (dim == splitDim) {\n          // No need to partition on this dim since it's a simple slice of the incoming already sorted slice, and we\n          // will re-use its shared reader when visiting it as we recurse:\n          leftSlices[dim] = new PathSlice(source.writer, source.start, leftCount);\n          rightSlices[dim] = new PathSlice(source.writer, source.start + leftCount, rightCount);\n          System.arraycopy(splitValue, 0, minSplitPackedValue, dim*bytesPerDim, bytesPerDim);\n          System.arraycopy(splitValue, 0, maxSplitPackedValue, dim*bytesPerDim, bytesPerDim);\n          continue;\n        }\n\n        // Not inside the try because we don't want to close this one now, so that after recursion is done,\n        // we will have done a singel full sweep of the file:\n        PointReader reader = slices[dim].writer.getSharedReader(slices[dim].start, slices[dim].count, toCloseHeroically);\n\n        try (PointWriter leftPointWriter = getPointWriter(leftCount, \"left\" + dim);\n             PointWriter rightPointWriter = getPointWriter(source.count - leftCount, \"right\" + dim)) {\n\n          long nextRightCount = reader.split(source.count, ordBitSet, leftPointWriter, rightPointWriter, dim == dimToClear);\n          if (rightCount != nextRightCount) {\n            throw new IllegalStateException(\"wrong number of points in split: expected=\" + rightCount + \" but actual=\" + nextRightCount);\n          }\n\n          leftSlices[dim] = new PathSlice(leftPointWriter, 0, leftCount);\n          rightSlices[dim] = new PathSlice(rightPointWriter, 0, rightCount);\n        } catch (Throwable t) {\n          verifyChecksum(t, slices[dim].writer);\n        }\n      }\n\n      // Recurse on left tree:\n      build(2*nodeID, leafNodeOffset, leftSlices,\n            ordBitSet, out,\n            minPackedValue, maxSplitPackedValue,\n            splitPackedValues, leafBlockFPs, toCloseHeroically);\n      for(int dim=0;dim<numDims;dim++) {\n        // Don't destroy the dim we split on because we just re-used what our caller above gave us for that dim:\n        if (dim != splitDim) {\n          leftSlices[dim].writer.destroy();\n        }\n      }\n\n      // TODO: we could \"tail recurse\" here?  have our parent discard its refs as we recurse right?\n      // Recurse on right tree:\n      build(2*nodeID+1, leafNodeOffset, rightSlices,\n            ordBitSet, out,\n            minSplitPackedValue, maxPackedValue,\n            splitPackedValues, leafBlockFPs, toCloseHeroically);\n      for(int dim=0;dim<numDims;dim++) {\n        // Don't destroy the dim we split on because we just re-used what our caller above gave us for that dim:\n        if (dim != splitDim) {\n          rightSlices[dim].writer.destroy();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c4ad863d796f4e72a3a1ef4bacd2e19c3e9258c9","date":1481155163,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#build(int,int,PathSlice[],LongBitSet,IndexOutput,byte[],byte[],byte[],long[],List[Closeable]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#build(int,int,PathSlice[],LongBitSet,IndexOutput,byte[],byte[],byte[],long[],List[Closeable]).mjava","sourceNew":"  /** The array (sized numDims) of PathSlice describe the cell we have currently recursed to.\n  /*  This method is used when we are merging previously written segments, in the numDims > 1 case. */\n  private void build(int nodeID, int leafNodeOffset,\n                     PathSlice[] slices,\n                     LongBitSet ordBitSet,\n                     IndexOutput out,\n                     byte[] minPackedValue, byte[] maxPackedValue,\n                     byte[] splitPackedValues,\n                     long[] leafBlockFPs,\n                     List<Closeable> toCloseHeroically) throws IOException {\n\n    for(PathSlice slice : slices) {\n      assert slice.count == slices[0].count;\n    }\n    \n    if (numDims == 1 && slices[0].writer instanceof OfflinePointWriter && slices[0].count <= maxPointsSortInHeap) {\n      // Special case for 1D, to cutover to heap once we recurse deeply enough:\n      slices[0] = switchToHeap(slices[0], toCloseHeroically);\n    }\n\n    if (nodeID >= leafNodeOffset) {\n\n      // Leaf node: write block\n      // We can write the block in any order so by default we write it sorted by the dimension that has the\n      // least number of unique bytes at commonPrefixLengths[dim], which makes compression more efficient\n      int sortedDim = 0;\n      int sortedDimCardinality = Integer.MAX_VALUE;\n\n      for (int dim=0;dim<numDims;dim++) {\n        if (slices[dim].writer instanceof HeapPointWriter == false) {\n          // Adversarial cases can cause this, e.g. very lopsided data, all equal points, such that we started\n          // offline, but then kept splitting only in one dimension, and so never had to rewrite into heap writer\n          slices[dim] = switchToHeap(slices[dim], toCloseHeroically);\n        }\n\n        PathSlice source = slices[dim];\n\n        HeapPointWriter heapSource = (HeapPointWriter) source.writer;\n\n        // Find common prefix by comparing first and last values, already sorted in this dimension:\n        heapSource.readPackedValue(Math.toIntExact(source.start), scratch1);\n        heapSource.readPackedValue(Math.toIntExact(source.start + source.count - 1), scratch2);\n\n        int offset = dim * bytesPerDim;\n        commonPrefixLengths[dim] = bytesPerDim;\n        for(int j=0;j<bytesPerDim;j++) {\n          if (scratch1[offset+j] != scratch2[offset+j]) {\n            commonPrefixLengths[dim] = j;\n            break;\n          }\n        }\n\n        int prefix = commonPrefixLengths[dim];\n        if (prefix < bytesPerDim) {\n          int cardinality = 1;\n          byte previous = scratch1[offset + prefix];\n          for (long i = 1; i < source.count; ++i) {\n            heapSource.readPackedValue(Math.toIntExact(source.start + i), scratch2);\n            byte b = scratch2[offset + prefix];\n            assert Byte.toUnsignedInt(previous) <= Byte.toUnsignedInt(b);\n            if (b != previous) {\n              cardinality++;\n              previous = b;\n            }\n          }\n          assert cardinality <= 256;\n          if (cardinality < sortedDimCardinality) {\n            sortedDim = dim;\n            sortedDimCardinality = cardinality;\n          }\n        }\n      }\n\n      PathSlice source = slices[sortedDim];\n\n      // We ensured that maxPointsSortInHeap was >= maxPointsInLeafNode, so we better be in heap at this point:\n      HeapPointWriter heapSource = (HeapPointWriter) source.writer;\n\n      // Save the block file pointer:\n      leafBlockFPs[nodeID - leafNodeOffset] = out.getFilePointer();\n      //System.out.println(\"  write leaf block @ fp=\" + out.getFilePointer());\n\n      // Write docIDs first, as their own chunk, so that at intersect time we can add all docIDs w/o\n      // loading the values:\n      int count = Math.toIntExact(source.count);\n      assert count > 0: \"nodeID=\" + nodeID + \" leafNodeOffset=\" + leafNodeOffset;\n      writeLeafBlockDocs(out, heapSource.docIDs, Math.toIntExact(source.start), count);\n\n      // TODO: minor opto: we don't really have to write the actual common prefixes, because BKDReader on recursing can regenerate it for us\n      // from the index, much like how terms dict does so from the FST:\n\n      // Write the common prefixes:\n      writeCommonPrefixes(out, commonPrefixLengths, scratch1);\n\n      // Write the full values:\n      IntFunction<BytesRef> packedValues = new IntFunction<BytesRef>() {\n        final BytesRef scratch = new BytesRef();\n\n        {\n          scratch.length = packedBytesLength;\n        }\n\n        @Override\n        public BytesRef apply(int i) {\n          heapSource.getPackedValueSlice(Math.toIntExact(source.start + i), scratch);\n          return scratch;\n        }\n      };\n      assert valuesInOrderAndBounds(count, sortedDim, minPackedValue, maxPackedValue, packedValues,\n          heapSource.docIDs, Math.toIntExact(source.start));\n      writeLeafBlockPackedValues(out, commonPrefixLengths, count, sortedDim, packedValues);\n\n    } else {\n      // Inner node: partition/recurse\n\n      int splitDim;\n      if (numDims > 1) {\n        splitDim = split(minPackedValue, maxPackedValue);\n      } else {\n        splitDim = 0;\n      }\n\n      PathSlice source = slices[splitDim];\n\n      assert nodeID < splitPackedValues.length: \"nodeID=\" + nodeID + \" splitValues.length=\" + splitPackedValues.length;\n\n      // How many points will be in the left tree:\n      long rightCount = source.count / 2;\n      long leftCount = source.count - rightCount;\n\n      byte[] splitValue = markRightTree(rightCount, splitDim, source, ordBitSet);\n      int address = nodeID * (1+bytesPerDim);\n      splitPackedValues[address] = (byte) splitDim;\n      System.arraycopy(splitValue, 0, splitPackedValues, address + 1, bytesPerDim);\n\n      // Partition all PathSlice that are not the split dim into sorted left and right sets, so we can recurse:\n\n      PathSlice[] leftSlices = new PathSlice[numDims];\n      PathSlice[] rightSlices = new PathSlice[numDims];\n\n      byte[] minSplitPackedValue = new byte[packedBytesLength];\n      System.arraycopy(minPackedValue, 0, minSplitPackedValue, 0, packedBytesLength);\n\n      byte[] maxSplitPackedValue = new byte[packedBytesLength];\n      System.arraycopy(maxPackedValue, 0, maxSplitPackedValue, 0, packedBytesLength);\n\n      // When we are on this dim, below, we clear the ordBitSet:\n      int dimToClear;\n      if (numDims - 1 == splitDim) {\n        dimToClear = numDims - 2;\n      } else {\n        dimToClear = numDims - 1;\n      }\n\n      for(int dim=0;dim<numDims;dim++) {\n\n        if (dim == splitDim) {\n          // No need to partition on this dim since it's a simple slice of the incoming already sorted slice, and we\n          // will re-use its shared reader when visiting it as we recurse:\n          leftSlices[dim] = new PathSlice(source.writer, source.start, leftCount);\n          rightSlices[dim] = new PathSlice(source.writer, source.start + leftCount, rightCount);\n          System.arraycopy(splitValue, 0, minSplitPackedValue, dim*bytesPerDim, bytesPerDim);\n          System.arraycopy(splitValue, 0, maxSplitPackedValue, dim*bytesPerDim, bytesPerDim);\n          continue;\n        }\n\n        // Not inside the try because we don't want to close this one now, so that after recursion is done,\n        // we will have done a singel full sweep of the file:\n        PointReader reader = slices[dim].writer.getSharedReader(slices[dim].start, slices[dim].count, toCloseHeroically);\n\n        try (PointWriter leftPointWriter = getPointWriter(leftCount, \"left\" + dim);\n             PointWriter rightPointWriter = getPointWriter(source.count - leftCount, \"right\" + dim)) {\n\n          long nextRightCount = reader.split(source.count, ordBitSet, leftPointWriter, rightPointWriter, dim == dimToClear);\n          if (rightCount != nextRightCount) {\n            throw new IllegalStateException(\"wrong number of points in split: expected=\" + rightCount + \" but actual=\" + nextRightCount);\n          }\n\n          leftSlices[dim] = new PathSlice(leftPointWriter, 0, leftCount);\n          rightSlices[dim] = new PathSlice(rightPointWriter, 0, rightCount);\n        } catch (Throwable t) {\n          verifyChecksum(t, slices[dim].writer);\n        }\n      }\n\n      // Recurse on left tree:\n      build(2*nodeID, leafNodeOffset, leftSlices,\n            ordBitSet, out,\n            minPackedValue, maxSplitPackedValue,\n            splitPackedValues, leafBlockFPs, toCloseHeroically);\n      for(int dim=0;dim<numDims;dim++) {\n        // Don't destroy the dim we split on because we just re-used what our caller above gave us for that dim:\n        if (dim != splitDim) {\n          leftSlices[dim].writer.destroy();\n        }\n      }\n\n      // TODO: we could \"tail recurse\" here?  have our parent discard its refs as we recurse right?\n      // Recurse on right tree:\n      build(2*nodeID+1, leafNodeOffset, rightSlices,\n            ordBitSet, out,\n            minSplitPackedValue, maxPackedValue,\n            splitPackedValues, leafBlockFPs, toCloseHeroically);\n      for(int dim=0;dim<numDims;dim++) {\n        // Don't destroy the dim we split on because we just re-used what our caller above gave us for that dim:\n        if (dim != splitDim) {\n          rightSlices[dim].writer.destroy();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /** The array (sized numDims) of PathSlice describe the cell we have currently recursed to. */\n  private void build(int nodeID, int leafNodeOffset,\n                     PathSlice[] slices,\n                     LongBitSet ordBitSet,\n                     IndexOutput out,\n                     byte[] minPackedValue, byte[] maxPackedValue,\n                     byte[] splitPackedValues,\n                     long[] leafBlockFPs,\n                     List<Closeable> toCloseHeroically) throws IOException {\n\n    for(PathSlice slice : slices) {\n      assert slice.count == slices[0].count;\n    }\n    \n    if (numDims == 1 && slices[0].writer instanceof OfflinePointWriter && slices[0].count <= maxPointsSortInHeap) {\n      // Special case for 1D, to cutover to heap once we recurse deeply enough:\n      slices[0] = switchToHeap(slices[0], toCloseHeroically);\n    }\n\n    if (nodeID >= leafNodeOffset) {\n\n      // Leaf node: write block\n      // We can write the block in any order so by default we write it sorted by the dimension that has the\n      // least number of unique bytes at commonPrefixLengths[dim], which makes compression more efficient\n      int sortedDim = 0;\n      int sortedDimCardinality = Integer.MAX_VALUE;\n\n      for (int dim=0;dim<numDims;dim++) {\n        if (slices[dim].writer instanceof HeapPointWriter == false) {\n          // Adversarial cases can cause this, e.g. very lopsided data, all equal points, such that we started\n          // offline, but then kept splitting only in one dimension, and so never had to rewrite into heap writer\n          slices[dim] = switchToHeap(slices[dim], toCloseHeroically);\n        }\n\n        PathSlice source = slices[dim];\n\n        HeapPointWriter heapSource = (HeapPointWriter) source.writer;\n\n        // Find common prefix by comparing first and last values, already sorted in this dimension:\n        heapSource.readPackedValue(Math.toIntExact(source.start), scratch1);\n        heapSource.readPackedValue(Math.toIntExact(source.start + source.count - 1), scratch2);\n\n        int offset = dim * bytesPerDim;\n        commonPrefixLengths[dim] = bytesPerDim;\n        for(int j=0;j<bytesPerDim;j++) {\n          if (scratch1[offset+j] != scratch2[offset+j]) {\n            commonPrefixLengths[dim] = j;\n            break;\n          }\n        }\n\n        int prefix = commonPrefixLengths[dim];\n        if (prefix < bytesPerDim) {\n          int cardinality = 1;\n          byte previous = scratch1[offset + prefix];\n          for (long i = 1; i < source.count; ++i) {\n            heapSource.readPackedValue(Math.toIntExact(source.start + i), scratch2);\n            byte b = scratch2[offset + prefix];\n            assert Byte.toUnsignedInt(previous) <= Byte.toUnsignedInt(b);\n            if (b != previous) {\n              cardinality++;\n              previous = b;\n            }\n          }\n          assert cardinality <= 256;\n          if (cardinality < sortedDimCardinality) {\n            sortedDim = dim;\n            sortedDimCardinality = cardinality;\n          }\n        }\n      }\n\n      PathSlice source = slices[sortedDim];\n\n      // We ensured that maxPointsSortInHeap was >= maxPointsInLeafNode, so we better be in heap at this point:\n      HeapPointWriter heapSource = (HeapPointWriter) source.writer;\n\n      // Save the block file pointer:\n      leafBlockFPs[nodeID - leafNodeOffset] = out.getFilePointer();\n      //System.out.println(\"  write leaf block @ fp=\" + out.getFilePointer());\n\n      // Write docIDs first, as their own chunk, so that at intersect time we can add all docIDs w/o\n      // loading the values:\n      int count = Math.toIntExact(source.count);\n      assert count > 0: \"nodeID=\" + nodeID + \" leafNodeOffset=\" + leafNodeOffset;\n      writeLeafBlockDocs(out, heapSource.docIDs, Math.toIntExact(source.start), count);\n\n      // TODO: minor opto: we don't really have to write the actual common prefixes, because BKDReader on recursing can regenerate it for us\n      // from the index, much like how terms dict does so from the FST:\n\n      // Write the common prefixes:\n      writeCommonPrefixes(out, commonPrefixLengths, scratch1);\n\n      // Write the full values:\n      IntFunction<BytesRef> packedValues = new IntFunction<BytesRef>() {\n        final BytesRef scratch = new BytesRef();\n\n        {\n          scratch.length = packedBytesLength;\n        }\n\n        @Override\n        public BytesRef apply(int i) {\n          heapSource.getPackedValueSlice(Math.toIntExact(source.start + i), scratch);\n          return scratch;\n        }\n      };\n      assert valuesInOrderAndBounds(count, sortedDim, minPackedValue, maxPackedValue, packedValues,\n          heapSource.docIDs, Math.toIntExact(source.start));\n      writeLeafBlockPackedValues(out, commonPrefixLengths, count, sortedDim, packedValues);\n\n    } else {\n      // Inner node: partition/recurse\n\n      int splitDim;\n      if (numDims > 1) {\n        splitDim = split(minPackedValue, maxPackedValue);\n      } else {\n        splitDim = 0;\n      }\n\n      PathSlice source = slices[splitDim];\n\n      assert nodeID < splitPackedValues.length: \"nodeID=\" + nodeID + \" splitValues.length=\" + splitPackedValues.length;\n\n      // How many points will be in the left tree:\n      long rightCount = source.count / 2;\n      long leftCount = source.count - rightCount;\n\n      byte[] splitValue = markRightTree(rightCount, splitDim, source, ordBitSet);\n      int address = nodeID * (1+bytesPerDim);\n      splitPackedValues[address] = (byte) splitDim;\n      System.arraycopy(splitValue, 0, splitPackedValues, address + 1, bytesPerDim);\n\n      // Partition all PathSlice that are not the split dim into sorted left and right sets, so we can recurse:\n\n      PathSlice[] leftSlices = new PathSlice[numDims];\n      PathSlice[] rightSlices = new PathSlice[numDims];\n\n      byte[] minSplitPackedValue = new byte[packedBytesLength];\n      System.arraycopy(minPackedValue, 0, minSplitPackedValue, 0, packedBytesLength);\n\n      byte[] maxSplitPackedValue = new byte[packedBytesLength];\n      System.arraycopy(maxPackedValue, 0, maxSplitPackedValue, 0, packedBytesLength);\n\n      // When we are on this dim, below, we clear the ordBitSet:\n      int dimToClear;\n      if (numDims - 1 == splitDim) {\n        dimToClear = numDims - 2;\n      } else {\n        dimToClear = numDims - 1;\n      }\n\n      for(int dim=0;dim<numDims;dim++) {\n\n        if (dim == splitDim) {\n          // No need to partition on this dim since it's a simple slice of the incoming already sorted slice, and we\n          // will re-use its shared reader when visiting it as we recurse:\n          leftSlices[dim] = new PathSlice(source.writer, source.start, leftCount);\n          rightSlices[dim] = new PathSlice(source.writer, source.start + leftCount, rightCount);\n          System.arraycopy(splitValue, 0, minSplitPackedValue, dim*bytesPerDim, bytesPerDim);\n          System.arraycopy(splitValue, 0, maxSplitPackedValue, dim*bytesPerDim, bytesPerDim);\n          continue;\n        }\n\n        // Not inside the try because we don't want to close this one now, so that after recursion is done,\n        // we will have done a singel full sweep of the file:\n        PointReader reader = slices[dim].writer.getSharedReader(slices[dim].start, slices[dim].count, toCloseHeroically);\n\n        try (PointWriter leftPointWriter = getPointWriter(leftCount, \"left\" + dim);\n             PointWriter rightPointWriter = getPointWriter(source.count - leftCount, \"right\" + dim)) {\n\n          long nextRightCount = reader.split(source.count, ordBitSet, leftPointWriter, rightPointWriter, dim == dimToClear);\n          if (rightCount != nextRightCount) {\n            throw new IllegalStateException(\"wrong number of points in split: expected=\" + rightCount + \" but actual=\" + nextRightCount);\n          }\n\n          leftSlices[dim] = new PathSlice(leftPointWriter, 0, leftCount);\n          rightSlices[dim] = new PathSlice(rightPointWriter, 0, rightCount);\n        } catch (Throwable t) {\n          verifyChecksum(t, slices[dim].writer);\n        }\n      }\n\n      // Recurse on left tree:\n      build(2*nodeID, leafNodeOffset, leftSlices,\n            ordBitSet, out,\n            minPackedValue, maxSplitPackedValue,\n            splitPackedValues, leafBlockFPs, toCloseHeroically);\n      for(int dim=0;dim<numDims;dim++) {\n        // Don't destroy the dim we split on because we just re-used what our caller above gave us for that dim:\n        if (dim != splitDim) {\n          leftSlices[dim].writer.destroy();\n        }\n      }\n\n      // TODO: we could \"tail recurse\" here?  have our parent discard its refs as we recurse right?\n      // Recurse on right tree:\n      build(2*nodeID+1, leafNodeOffset, rightSlices,\n            ordBitSet, out,\n            minSplitPackedValue, maxPackedValue,\n            splitPackedValues, leafBlockFPs, toCloseHeroically);\n      for(int dim=0;dim<numDims;dim++) {\n        // Don't destroy the dim we split on because we just re-used what our caller above gave us for that dim:\n        if (dim != splitDim) {\n          rightSlices[dim].writer.destroy();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9856095f7afb5a607bf5e65077615ed91273508c","date":1481837697,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#build(int,int,PathSlice[],LongBitSet,IndexOutput,byte[],byte[],byte[],long[],List[Closeable]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#build(int,int,PathSlice[],LongBitSet,IndexOutput,byte[],byte[],byte[],long[],List[Closeable]).mjava","sourceNew":"  /** The array (sized numDims) of PathSlice describe the cell we have currently recursed to.\n  /*  This method is used when we are merging previously written segments, in the numDims > 1 case. */\n  private void build(int nodeID, int leafNodeOffset,\n                     PathSlice[] slices,\n                     LongBitSet ordBitSet,\n                     IndexOutput out,\n                     byte[] minPackedValue, byte[] maxPackedValue,\n                     byte[] splitPackedValues,\n                     long[] leafBlockFPs,\n                     List<Closeable> toCloseHeroically) throws IOException {\n\n    for(PathSlice slice : slices) {\n      assert slice.count == slices[0].count;\n    }\n    \n    if (numDims == 1 && slices[0].writer instanceof OfflinePointWriter && slices[0].count <= maxPointsSortInHeap) {\n      // Special case for 1D, to cutover to heap once we recurse deeply enough:\n      slices[0] = switchToHeap(slices[0], toCloseHeroically);\n    }\n\n    if (nodeID >= leafNodeOffset) {\n\n      // Leaf node: write block\n      // We can write the block in any order so by default we write it sorted by the dimension that has the\n      // least number of unique bytes at commonPrefixLengths[dim], which makes compression more efficient\n      int sortedDim = 0;\n      int sortedDimCardinality = Integer.MAX_VALUE;\n\n      for (int dim=0;dim<numDims;dim++) {\n        if (slices[dim].writer instanceof HeapPointWriter == false) {\n          // Adversarial cases can cause this, e.g. very lopsided data, all equal points, such that we started\n          // offline, but then kept splitting only in one dimension, and so never had to rewrite into heap writer\n          slices[dim] = switchToHeap(slices[dim], toCloseHeroically);\n        }\n\n        PathSlice source = slices[dim];\n\n        HeapPointWriter heapSource = (HeapPointWriter) source.writer;\n\n        // Find common prefix by comparing first and last values, already sorted in this dimension:\n        heapSource.readPackedValue(Math.toIntExact(source.start), scratch1);\n        heapSource.readPackedValue(Math.toIntExact(source.start + source.count - 1), scratch2);\n\n        int offset = dim * bytesPerDim;\n        commonPrefixLengths[dim] = bytesPerDim;\n        for(int j=0;j<bytesPerDim;j++) {\n          if (scratch1[offset+j] != scratch2[offset+j]) {\n            commonPrefixLengths[dim] = j;\n            break;\n          }\n        }\n\n        int prefix = commonPrefixLengths[dim];\n        if (prefix < bytesPerDim) {\n          int cardinality = 1;\n          byte previous = scratch1[offset + prefix];\n          for (long i = 1; i < source.count; ++i) {\n            heapSource.readPackedValue(Math.toIntExact(source.start + i), scratch2);\n            byte b = scratch2[offset + prefix];\n            assert Byte.toUnsignedInt(previous) <= Byte.toUnsignedInt(b);\n            if (b != previous) {\n              cardinality++;\n              previous = b;\n            }\n          }\n          assert cardinality <= 256;\n          if (cardinality < sortedDimCardinality) {\n            sortedDim = dim;\n            sortedDimCardinality = cardinality;\n          }\n        }\n      }\n\n      PathSlice source = slices[sortedDim];\n\n      // We ensured that maxPointsSortInHeap was >= maxPointsInLeafNode, so we better be in heap at this point:\n      HeapPointWriter heapSource = (HeapPointWriter) source.writer;\n\n      // Save the block file pointer:\n      leafBlockFPs[nodeID - leafNodeOffset] = out.getFilePointer();\n      //System.out.println(\"  write leaf block @ fp=\" + out.getFilePointer());\n\n      // Write docIDs first, as their own chunk, so that at intersect time we can add all docIDs w/o\n      // loading the values:\n      int count = Math.toIntExact(source.count);\n      assert count > 0: \"nodeID=\" + nodeID + \" leafNodeOffset=\" + leafNodeOffset;\n      writeLeafBlockDocs(out, heapSource.docIDs, Math.toIntExact(source.start), count);\n\n      // TODO: minor opto: we don't really have to write the actual common prefixes, because BKDReader on recursing can regenerate it for us\n      // from the index, much like how terms dict does so from the FST:\n\n      // Write the common prefixes:\n      writeCommonPrefixes(out, commonPrefixLengths, scratch1);\n\n      // Write the full values:\n      IntFunction<BytesRef> packedValues = new IntFunction<BytesRef>() {\n        final BytesRef scratch = new BytesRef();\n\n        {\n          scratch.length = packedBytesLength;\n        }\n\n        @Override\n        public BytesRef apply(int i) {\n          heapSource.getPackedValueSlice(Math.toIntExact(source.start + i), scratch);\n          return scratch;\n        }\n      };\n      assert valuesInOrderAndBounds(count, sortedDim, minPackedValue, maxPackedValue, packedValues,\n          heapSource.docIDs, Math.toIntExact(source.start));\n      writeLeafBlockPackedValues(out, commonPrefixLengths, count, sortedDim, packedValues);\n\n    } else {\n      // Inner node: partition/recurse\n\n      int splitDim;\n      if (numDims > 1) {\n        splitDim = split(minPackedValue, maxPackedValue);\n      } else {\n        splitDim = 0;\n      }\n\n      PathSlice source = slices[splitDim];\n\n      assert nodeID < splitPackedValues.length: \"nodeID=\" + nodeID + \" splitValues.length=\" + splitPackedValues.length;\n\n      // How many points will be in the left tree:\n      long rightCount = source.count / 2;\n      long leftCount = source.count - rightCount;\n\n      byte[] splitValue = markRightTree(rightCount, splitDim, source, ordBitSet);\n      int address = nodeID * (1+bytesPerDim);\n      splitPackedValues[address] = (byte) splitDim;\n      System.arraycopy(splitValue, 0, splitPackedValues, address + 1, bytesPerDim);\n\n      // Partition all PathSlice that are not the split dim into sorted left and right sets, so we can recurse:\n\n      PathSlice[] leftSlices = new PathSlice[numDims];\n      PathSlice[] rightSlices = new PathSlice[numDims];\n\n      byte[] minSplitPackedValue = new byte[packedBytesLength];\n      System.arraycopy(minPackedValue, 0, minSplitPackedValue, 0, packedBytesLength);\n\n      byte[] maxSplitPackedValue = new byte[packedBytesLength];\n      System.arraycopy(maxPackedValue, 0, maxSplitPackedValue, 0, packedBytesLength);\n\n      // When we are on this dim, below, we clear the ordBitSet:\n      int dimToClear;\n      if (numDims - 1 == splitDim) {\n        dimToClear = numDims - 2;\n      } else {\n        dimToClear = numDims - 1;\n      }\n\n      for(int dim=0;dim<numDims;dim++) {\n\n        if (dim == splitDim) {\n          // No need to partition on this dim since it's a simple slice of the incoming already sorted slice, and we\n          // will re-use its shared reader when visiting it as we recurse:\n          leftSlices[dim] = new PathSlice(source.writer, source.start, leftCount);\n          rightSlices[dim] = new PathSlice(source.writer, source.start + leftCount, rightCount);\n          System.arraycopy(splitValue, 0, minSplitPackedValue, dim*bytesPerDim, bytesPerDim);\n          System.arraycopy(splitValue, 0, maxSplitPackedValue, dim*bytesPerDim, bytesPerDim);\n          continue;\n        }\n\n        // Not inside the try because we don't want to close this one now, so that after recursion is done,\n        // we will have done a singel full sweep of the file:\n        PointReader reader = slices[dim].writer.getSharedReader(slices[dim].start, slices[dim].count, toCloseHeroically);\n\n        try (PointWriter leftPointWriter = getPointWriter(leftCount, \"left\" + dim);\n             PointWriter rightPointWriter = getPointWriter(source.count - leftCount, \"right\" + dim)) {\n\n          long nextRightCount = reader.split(source.count, ordBitSet, leftPointWriter, rightPointWriter, dim == dimToClear);\n          if (rightCount != nextRightCount) {\n            throw new IllegalStateException(\"wrong number of points in split: expected=\" + rightCount + \" but actual=\" + nextRightCount);\n          }\n\n          leftSlices[dim] = new PathSlice(leftPointWriter, 0, leftCount);\n          rightSlices[dim] = new PathSlice(rightPointWriter, 0, rightCount);\n        } catch (Throwable t) {\n          verifyChecksum(t, slices[dim].writer);\n        }\n      }\n\n      // Recurse on left tree:\n      build(2*nodeID, leafNodeOffset, leftSlices,\n            ordBitSet, out,\n            minPackedValue, maxSplitPackedValue,\n            splitPackedValues, leafBlockFPs, toCloseHeroically);\n      for(int dim=0;dim<numDims;dim++) {\n        // Don't destroy the dim we split on because we just re-used what our caller above gave us for that dim:\n        if (dim != splitDim) {\n          leftSlices[dim].writer.destroy();\n        }\n      }\n\n      // TODO: we could \"tail recurse\" here?  have our parent discard its refs as we recurse right?\n      // Recurse on right tree:\n      build(2*nodeID+1, leafNodeOffset, rightSlices,\n            ordBitSet, out,\n            minSplitPackedValue, maxPackedValue,\n            splitPackedValues, leafBlockFPs, toCloseHeroically);\n      for(int dim=0;dim<numDims;dim++) {\n        // Don't destroy the dim we split on because we just re-used what our caller above gave us for that dim:\n        if (dim != splitDim) {\n          rightSlices[dim].writer.destroy();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /** The array (sized numDims) of PathSlice describe the cell we have currently recursed to. */\n  private void build(int nodeID, int leafNodeOffset,\n                     PathSlice[] slices,\n                     LongBitSet ordBitSet,\n                     IndexOutput out,\n                     byte[] minPackedValue, byte[] maxPackedValue,\n                     byte[] splitPackedValues,\n                     long[] leafBlockFPs,\n                     List<Closeable> toCloseHeroically) throws IOException {\n\n    for(PathSlice slice : slices) {\n      assert slice.count == slices[0].count;\n    }\n\n    if (numDims == 1 && slices[0].writer instanceof OfflinePointWriter && slices[0].count <= maxPointsSortInHeap) {\n      // Special case for 1D, to cutover to heap once we recurse deeply enough:\n      slices[0] = switchToHeap(slices[0], toCloseHeroically);\n    }\n\n    if (nodeID >= leafNodeOffset) {\n\n      // Leaf node: write block\n      // We can write the block in any order so by default we write it sorted by the dimension that has the\n      // least number of unique bytes at commonPrefixLengths[dim], which makes compression more efficient\n      int sortedDim = 0;\n      int sortedDimCardinality = Integer.MAX_VALUE;\n\n      for (int dim=0;dim<numDims;dim++) {\n        if (slices[dim].writer instanceof HeapPointWriter == false) {\n          // Adversarial cases can cause this, e.g. very lopsided data, all equal points, such that we started\n          // offline, but then kept splitting only in one dimension, and so never had to rewrite into heap writer\n          slices[dim] = switchToHeap(slices[dim], toCloseHeroically);\n        }\n\n        PathSlice source = slices[dim];\n\n        HeapPointWriter heapSource = (HeapPointWriter) source.writer;\n\n        // Find common prefix by comparing first and last values, already sorted in this dimension:\n        heapSource.readPackedValue(Math.toIntExact(source.start), scratch1);\n        heapSource.readPackedValue(Math.toIntExact(source.start + source.count - 1), scratch2);\n\n        int offset = dim * bytesPerDim;\n        commonPrefixLengths[dim] = bytesPerDim;\n        for(int j=0;j<bytesPerDim;j++) {\n          if (scratch1[offset+j] != scratch2[offset+j]) {\n            commonPrefixLengths[dim] = j;\n            break;\n          }\n        }\n\n        int prefix = commonPrefixLengths[dim];\n        if (prefix < bytesPerDim) {\n          int cardinality = 1;\n          byte previous = scratch1[offset + prefix];\n          for (long i = 1; i < source.count; ++i) {\n            heapSource.readPackedValue(Math.toIntExact(source.start + i), scratch2);\n            byte b = scratch2[offset + prefix];\n            assert Byte.toUnsignedInt(previous) <= Byte.toUnsignedInt(b);\n            if (b != previous) {\n              cardinality++;\n              previous = b;\n            }\n          }\n          assert cardinality <= 256;\n          if (cardinality < sortedDimCardinality) {\n            sortedDim = dim;\n            sortedDimCardinality = cardinality;\n          }\n        }\n      }\n\n      PathSlice source = slices[sortedDim];\n\n      // We ensured that maxPointsSortInHeap was >= maxPointsInLeafNode, so we better be in heap at this point:\n      HeapPointWriter heapSource = (HeapPointWriter) source.writer;\n\n      // Save the block file pointer:\n      leafBlockFPs[nodeID - leafNodeOffset] = out.getFilePointer();\n      //System.out.println(\"  write leaf block @ fp=\" + out.getFilePointer());\n\n      // Write docIDs first, as their own chunk, so that at intersect time we can add all docIDs w/o\n      // loading the values:\n      int count = Math.toIntExact(source.count);\n      assert count > 0: \"nodeID=\" + nodeID + \" leafNodeOffset=\" + leafNodeOffset;\n      writeLeafBlockDocs(out, heapSource.docIDs, Math.toIntExact(source.start), count);\n\n      // TODO: minor opto: we don't really have to write the actual common prefixes, because BKDReader on recursing can regenerate it for us\n      // from the index, much like how terms dict does so from the FST:\n\n      // Write the common prefixes:\n      writeCommonPrefixes(out, commonPrefixLengths, scratch1);\n\n      // Write the full values:\n      IntFunction<BytesRef> packedValues = new IntFunction<BytesRef>() {\n        final BytesRef scratch = new BytesRef();\n\n        {\n          scratch.length = packedBytesLength;\n        }\n\n        @Override\n        public BytesRef apply(int i) {\n          heapSource.getPackedValueSlice(Math.toIntExact(source.start + i), scratch);\n          return scratch;\n        }\n      };\n      assert valuesInOrderAndBounds(count, sortedDim, minPackedValue, maxPackedValue, packedValues,\n          heapSource.docIDs, Math.toIntExact(source.start));\n      writeLeafBlockPackedValues(out, commonPrefixLengths, count, sortedDim, packedValues);\n\n    } else {\n      // Inner node: partition/recurse\n\n      int splitDim;\n      if (numDims > 1) {\n        splitDim = split(minPackedValue, maxPackedValue);\n      } else {\n        splitDim = 0;\n      }\n\n      PathSlice source = slices[splitDim];\n\n      assert nodeID < splitPackedValues.length: \"nodeID=\" + nodeID + \" splitValues.length=\" + splitPackedValues.length;\n\n      // How many points will be in the left tree:\n      long rightCount = source.count / 2;\n      long leftCount = source.count - rightCount;\n\n      byte[] splitValue = markRightTree(rightCount, splitDim, source, ordBitSet);\n      int address = nodeID * (1+bytesPerDim);\n      splitPackedValues[address] = (byte) splitDim;\n      System.arraycopy(splitValue, 0, splitPackedValues, address + 1, bytesPerDim);\n\n      // Partition all PathSlice that are not the split dim into sorted left and right sets, so we can recurse:\n\n      PathSlice[] leftSlices = new PathSlice[numDims];\n      PathSlice[] rightSlices = new PathSlice[numDims];\n\n      byte[] minSplitPackedValue = new byte[packedBytesLength];\n      System.arraycopy(minPackedValue, 0, minSplitPackedValue, 0, packedBytesLength);\n\n      byte[] maxSplitPackedValue = new byte[packedBytesLength];\n      System.arraycopy(maxPackedValue, 0, maxSplitPackedValue, 0, packedBytesLength);\n\n      // When we are on this dim, below, we clear the ordBitSet:\n      int dimToClear;\n      if (numDims - 1 == splitDim) {\n        dimToClear = numDims - 2;\n      } else {\n        dimToClear = numDims - 1;\n      }\n\n      for(int dim=0;dim<numDims;dim++) {\n\n        if (dim == splitDim) {\n          // No need to partition on this dim since it's a simple slice of the incoming already sorted slice, and we\n          // will re-use its shared reader when visiting it as we recurse:\n          leftSlices[dim] = new PathSlice(source.writer, source.start, leftCount);\n          rightSlices[dim] = new PathSlice(source.writer, source.start + leftCount, rightCount);\n          System.arraycopy(splitValue, 0, minSplitPackedValue, dim*bytesPerDim, bytesPerDim);\n          System.arraycopy(splitValue, 0, maxSplitPackedValue, dim*bytesPerDim, bytesPerDim);\n          continue;\n        }\n\n        // Not inside the try because we don't want to close this one now, so that after recursion is done,\n        // we will have done a singel full sweep of the file:\n        PointReader reader = slices[dim].writer.getSharedReader(slices[dim].start, slices[dim].count, toCloseHeroically);\n\n        try (PointWriter leftPointWriter = getPointWriter(leftCount, \"left\" + dim);\n             PointWriter rightPointWriter = getPointWriter(source.count - leftCount, \"right\" + dim)) {\n\n          long nextRightCount = reader.split(source.count, ordBitSet, leftPointWriter, rightPointWriter, dim == dimToClear);\n          if (rightCount != nextRightCount) {\n            throw new IllegalStateException(\"wrong number of points in split: expected=\" + rightCount + \" but actual=\" + nextRightCount);\n          }\n\n          leftSlices[dim] = new PathSlice(leftPointWriter, 0, leftCount);\n          rightSlices[dim] = new PathSlice(rightPointWriter, 0, rightCount);\n        } catch (Throwable t) {\n          verifyChecksum(t, slices[dim].writer);\n        }\n      }\n\n      // Recurse on left tree:\n      build(2*nodeID, leafNodeOffset, leftSlices,\n            ordBitSet, out,\n            minPackedValue, maxSplitPackedValue,\n            splitPackedValues, leafBlockFPs, toCloseHeroically);\n      for(int dim=0;dim<numDims;dim++) {\n        // Don't destroy the dim we split on because we just re-used what our caller above gave us for that dim:\n        if (dim != splitDim) {\n          leftSlices[dim].writer.destroy();\n        }\n      }\n\n      // TODO: we could \"tail recurse\" here?  have our parent discard its refs as we recurse right?\n      // Recurse on right tree:\n      build(2*nodeID+1, leafNodeOffset, rightSlices,\n            ordBitSet, out,\n            minSplitPackedValue, maxPackedValue,\n            splitPackedValues, leafBlockFPs, toCloseHeroically);\n      for(int dim=0;dim<numDims;dim++) {\n        // Don't destroy the dim we split on because we just re-used what our caller above gave us for that dim:\n        if (dim != splitDim) {\n          rightSlices[dim].writer.destroy();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b35cfd3fa0a5c9e066b0256c4818af1d2a9f22d7","date":1482745036,"type":5,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#build(int,int,PathSlice[],LongBitSet,IndexOutput,byte[],byte[],int[],byte[],long[],List[Closeable]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#build(int,int,PathSlice[],LongBitSet,IndexOutput,byte[],byte[],byte[],long[],List[Closeable]).mjava","sourceNew":"  /** The array (sized numDims) of PathSlice describe the cell we have currently recursed to.\n  /*  This method is used when we are merging previously written segments, in the numDims > 1 case. */\n  private void build(int nodeID, int leafNodeOffset,\n                     PathSlice[] slices,\n                     LongBitSet ordBitSet,\n                     IndexOutput out,\n                     byte[] minPackedValue, byte[] maxPackedValue,\n                     int[] parentSplits,\n                     byte[] splitPackedValues,\n                     long[] leafBlockFPs,\n                     List<Closeable> toCloseHeroically) throws IOException {\n\n    for(PathSlice slice : slices) {\n      assert slice.count == slices[0].count;\n    }\n    \n    if (numDims == 1 && slices[0].writer instanceof OfflinePointWriter && slices[0].count <= maxPointsSortInHeap) {\n      // Special case for 1D, to cutover to heap once we recurse deeply enough:\n      slices[0] = switchToHeap(slices[0], toCloseHeroically);\n    }\n\n    if (nodeID >= leafNodeOffset) {\n\n      // Leaf node: write block\n      // We can write the block in any order so by default we write it sorted by the dimension that has the\n      // least number of unique bytes at commonPrefixLengths[dim], which makes compression more efficient\n      int sortedDim = 0;\n      int sortedDimCardinality = Integer.MAX_VALUE;\n\n      for (int dim=0;dim<numDims;dim++) {\n        if (slices[dim].writer instanceof HeapPointWriter == false) {\n          // Adversarial cases can cause this, e.g. very lopsided data, all equal points, such that we started\n          // offline, but then kept splitting only in one dimension, and so never had to rewrite into heap writer\n          slices[dim] = switchToHeap(slices[dim], toCloseHeroically);\n        }\n\n        PathSlice source = slices[dim];\n\n        HeapPointWriter heapSource = (HeapPointWriter) source.writer;\n\n        // Find common prefix by comparing first and last values, already sorted in this dimension:\n        heapSource.readPackedValue(Math.toIntExact(source.start), scratch1);\n        heapSource.readPackedValue(Math.toIntExact(source.start + source.count - 1), scratch2);\n\n        int offset = dim * bytesPerDim;\n        commonPrefixLengths[dim] = bytesPerDim;\n        for(int j=0;j<bytesPerDim;j++) {\n          if (scratch1[offset+j] != scratch2[offset+j]) {\n            commonPrefixLengths[dim] = j;\n            break;\n          }\n        }\n\n        int prefix = commonPrefixLengths[dim];\n        if (prefix < bytesPerDim) {\n          int cardinality = 1;\n          byte previous = scratch1[offset + prefix];\n          for (long i = 1; i < source.count; ++i) {\n            heapSource.readPackedValue(Math.toIntExact(source.start + i), scratch2);\n            byte b = scratch2[offset + prefix];\n            assert Byte.toUnsignedInt(previous) <= Byte.toUnsignedInt(b);\n            if (b != previous) {\n              cardinality++;\n              previous = b;\n            }\n          }\n          assert cardinality <= 256;\n          if (cardinality < sortedDimCardinality) {\n            sortedDim = dim;\n            sortedDimCardinality = cardinality;\n          }\n        }\n      }\n\n      PathSlice source = slices[sortedDim];\n\n      // We ensured that maxPointsSortInHeap was >= maxPointsInLeafNode, so we better be in heap at this point:\n      HeapPointWriter heapSource = (HeapPointWriter) source.writer;\n\n      // Save the block file pointer:\n      leafBlockFPs[nodeID - leafNodeOffset] = out.getFilePointer();\n      //System.out.println(\"  write leaf block @ fp=\" + out.getFilePointer());\n\n      // Write docIDs first, as their own chunk, so that at intersect time we can add all docIDs w/o\n      // loading the values:\n      int count = Math.toIntExact(source.count);\n      assert count > 0: \"nodeID=\" + nodeID + \" leafNodeOffset=\" + leafNodeOffset;\n      writeLeafBlockDocs(out, heapSource.docIDs, Math.toIntExact(source.start), count);\n\n      // TODO: minor opto: we don't really have to write the actual common prefixes, because BKDReader on recursing can regenerate it for us\n      // from the index, much like how terms dict does so from the FST:\n\n      // Write the common prefixes:\n      writeCommonPrefixes(out, commonPrefixLengths, scratch1);\n\n      // Write the full values:\n      IntFunction<BytesRef> packedValues = new IntFunction<BytesRef>() {\n        final BytesRef scratch = new BytesRef();\n\n        {\n          scratch.length = packedBytesLength;\n        }\n\n        @Override\n        public BytesRef apply(int i) {\n          heapSource.getPackedValueSlice(Math.toIntExact(source.start + i), scratch);\n          return scratch;\n        }\n      };\n      assert valuesInOrderAndBounds(count, sortedDim, minPackedValue, maxPackedValue, packedValues,\n          heapSource.docIDs, Math.toIntExact(source.start));\n      writeLeafBlockPackedValues(out, commonPrefixLengths, count, sortedDim, packedValues);\n\n    } else {\n      // Inner node: partition/recurse\n\n      int splitDim;\n      if (numDims > 1) {\n        splitDim = split(minPackedValue, maxPackedValue, parentSplits);\n      } else {\n        splitDim = 0;\n      }\n\n      PathSlice source = slices[splitDim];\n\n      assert nodeID < splitPackedValues.length: \"nodeID=\" + nodeID + \" splitValues.length=\" + splitPackedValues.length;\n\n      // How many points will be in the left tree:\n      long rightCount = source.count / 2;\n      long leftCount = source.count - rightCount;\n\n      byte[] splitValue = markRightTree(rightCount, splitDim, source, ordBitSet);\n      int address = nodeID * (1+bytesPerDim);\n      splitPackedValues[address] = (byte) splitDim;\n      System.arraycopy(splitValue, 0, splitPackedValues, address + 1, bytesPerDim);\n\n      // Partition all PathSlice that are not the split dim into sorted left and right sets, so we can recurse:\n\n      PathSlice[] leftSlices = new PathSlice[numDims];\n      PathSlice[] rightSlices = new PathSlice[numDims];\n\n      byte[] minSplitPackedValue = new byte[packedBytesLength];\n      System.arraycopy(minPackedValue, 0, minSplitPackedValue, 0, packedBytesLength);\n\n      byte[] maxSplitPackedValue = new byte[packedBytesLength];\n      System.arraycopy(maxPackedValue, 0, maxSplitPackedValue, 0, packedBytesLength);\n\n      // When we are on this dim, below, we clear the ordBitSet:\n      int dimToClear;\n      if (numDims - 1 == splitDim) {\n        dimToClear = numDims - 2;\n      } else {\n        dimToClear = numDims - 1;\n      }\n\n      for(int dim=0;dim<numDims;dim++) {\n\n        if (dim == splitDim) {\n          // No need to partition on this dim since it's a simple slice of the incoming already sorted slice, and we\n          // will re-use its shared reader when visiting it as we recurse:\n          leftSlices[dim] = new PathSlice(source.writer, source.start, leftCount);\n          rightSlices[dim] = new PathSlice(source.writer, source.start + leftCount, rightCount);\n          System.arraycopy(splitValue, 0, minSplitPackedValue, dim*bytesPerDim, bytesPerDim);\n          System.arraycopy(splitValue, 0, maxSplitPackedValue, dim*bytesPerDim, bytesPerDim);\n          continue;\n        }\n\n        // Not inside the try because we don't want to close this one now, so that after recursion is done,\n        // we will have done a singel full sweep of the file:\n        PointReader reader = slices[dim].writer.getSharedReader(slices[dim].start, slices[dim].count, toCloseHeroically);\n\n        try (PointWriter leftPointWriter = getPointWriter(leftCount, \"left\" + dim);\n             PointWriter rightPointWriter = getPointWriter(source.count - leftCount, \"right\" + dim)) {\n\n          long nextRightCount = reader.split(source.count, ordBitSet, leftPointWriter, rightPointWriter, dim == dimToClear);\n          if (rightCount != nextRightCount) {\n            throw new IllegalStateException(\"wrong number of points in split: expected=\" + rightCount + \" but actual=\" + nextRightCount);\n          }\n\n          leftSlices[dim] = new PathSlice(leftPointWriter, 0, leftCount);\n          rightSlices[dim] = new PathSlice(rightPointWriter, 0, rightCount);\n        } catch (Throwable t) {\n          verifyChecksum(t, slices[dim].writer);\n        }\n      }\n\n      parentSplits[splitDim]++;\n      // Recurse on left tree:\n      build(2*nodeID, leafNodeOffset, leftSlices,\n            ordBitSet, out,\n            minPackedValue, maxSplitPackedValue, parentSplits,\n            splitPackedValues, leafBlockFPs, toCloseHeroically);\n      for(int dim=0;dim<numDims;dim++) {\n        // Don't destroy the dim we split on because we just re-used what our caller above gave us for that dim:\n        if (dim != splitDim) {\n          leftSlices[dim].writer.destroy();\n        }\n      }\n\n      // TODO: we could \"tail recurse\" here?  have our parent discard its refs as we recurse right?\n      // Recurse on right tree:\n      build(2*nodeID+1, leafNodeOffset, rightSlices,\n            ordBitSet, out,\n            minSplitPackedValue, maxPackedValue, parentSplits,\n            splitPackedValues, leafBlockFPs, toCloseHeroically);\n      for(int dim=0;dim<numDims;dim++) {\n        // Don't destroy the dim we split on because we just re-used what our caller above gave us for that dim:\n        if (dim != splitDim) {\n          rightSlices[dim].writer.destroy();\n        }\n      }\n      parentSplits[splitDim]--;\n    }\n  }\n\n","sourceOld":"  /** The array (sized numDims) of PathSlice describe the cell we have currently recursed to.\n  /*  This method is used when we are merging previously written segments, in the numDims > 1 case. */\n  private void build(int nodeID, int leafNodeOffset,\n                     PathSlice[] slices,\n                     LongBitSet ordBitSet,\n                     IndexOutput out,\n                     byte[] minPackedValue, byte[] maxPackedValue,\n                     byte[] splitPackedValues,\n                     long[] leafBlockFPs,\n                     List<Closeable> toCloseHeroically) throws IOException {\n\n    for(PathSlice slice : slices) {\n      assert slice.count == slices[0].count;\n    }\n    \n    if (numDims == 1 && slices[0].writer instanceof OfflinePointWriter && slices[0].count <= maxPointsSortInHeap) {\n      // Special case for 1D, to cutover to heap once we recurse deeply enough:\n      slices[0] = switchToHeap(slices[0], toCloseHeroically);\n    }\n\n    if (nodeID >= leafNodeOffset) {\n\n      // Leaf node: write block\n      // We can write the block in any order so by default we write it sorted by the dimension that has the\n      // least number of unique bytes at commonPrefixLengths[dim], which makes compression more efficient\n      int sortedDim = 0;\n      int sortedDimCardinality = Integer.MAX_VALUE;\n\n      for (int dim=0;dim<numDims;dim++) {\n        if (slices[dim].writer instanceof HeapPointWriter == false) {\n          // Adversarial cases can cause this, e.g. very lopsided data, all equal points, such that we started\n          // offline, but then kept splitting only in one dimension, and so never had to rewrite into heap writer\n          slices[dim] = switchToHeap(slices[dim], toCloseHeroically);\n        }\n\n        PathSlice source = slices[dim];\n\n        HeapPointWriter heapSource = (HeapPointWriter) source.writer;\n\n        // Find common prefix by comparing first and last values, already sorted in this dimension:\n        heapSource.readPackedValue(Math.toIntExact(source.start), scratch1);\n        heapSource.readPackedValue(Math.toIntExact(source.start + source.count - 1), scratch2);\n\n        int offset = dim * bytesPerDim;\n        commonPrefixLengths[dim] = bytesPerDim;\n        for(int j=0;j<bytesPerDim;j++) {\n          if (scratch1[offset+j] != scratch2[offset+j]) {\n            commonPrefixLengths[dim] = j;\n            break;\n          }\n        }\n\n        int prefix = commonPrefixLengths[dim];\n        if (prefix < bytesPerDim) {\n          int cardinality = 1;\n          byte previous = scratch1[offset + prefix];\n          for (long i = 1; i < source.count; ++i) {\n            heapSource.readPackedValue(Math.toIntExact(source.start + i), scratch2);\n            byte b = scratch2[offset + prefix];\n            assert Byte.toUnsignedInt(previous) <= Byte.toUnsignedInt(b);\n            if (b != previous) {\n              cardinality++;\n              previous = b;\n            }\n          }\n          assert cardinality <= 256;\n          if (cardinality < sortedDimCardinality) {\n            sortedDim = dim;\n            sortedDimCardinality = cardinality;\n          }\n        }\n      }\n\n      PathSlice source = slices[sortedDim];\n\n      // We ensured that maxPointsSortInHeap was >= maxPointsInLeafNode, so we better be in heap at this point:\n      HeapPointWriter heapSource = (HeapPointWriter) source.writer;\n\n      // Save the block file pointer:\n      leafBlockFPs[nodeID - leafNodeOffset] = out.getFilePointer();\n      //System.out.println(\"  write leaf block @ fp=\" + out.getFilePointer());\n\n      // Write docIDs first, as their own chunk, so that at intersect time we can add all docIDs w/o\n      // loading the values:\n      int count = Math.toIntExact(source.count);\n      assert count > 0: \"nodeID=\" + nodeID + \" leafNodeOffset=\" + leafNodeOffset;\n      writeLeafBlockDocs(out, heapSource.docIDs, Math.toIntExact(source.start), count);\n\n      // TODO: minor opto: we don't really have to write the actual common prefixes, because BKDReader on recursing can regenerate it for us\n      // from the index, much like how terms dict does so from the FST:\n\n      // Write the common prefixes:\n      writeCommonPrefixes(out, commonPrefixLengths, scratch1);\n\n      // Write the full values:\n      IntFunction<BytesRef> packedValues = new IntFunction<BytesRef>() {\n        final BytesRef scratch = new BytesRef();\n\n        {\n          scratch.length = packedBytesLength;\n        }\n\n        @Override\n        public BytesRef apply(int i) {\n          heapSource.getPackedValueSlice(Math.toIntExact(source.start + i), scratch);\n          return scratch;\n        }\n      };\n      assert valuesInOrderAndBounds(count, sortedDim, minPackedValue, maxPackedValue, packedValues,\n          heapSource.docIDs, Math.toIntExact(source.start));\n      writeLeafBlockPackedValues(out, commonPrefixLengths, count, sortedDim, packedValues);\n\n    } else {\n      // Inner node: partition/recurse\n\n      int splitDim;\n      if (numDims > 1) {\n        splitDim = split(minPackedValue, maxPackedValue);\n      } else {\n        splitDim = 0;\n      }\n\n      PathSlice source = slices[splitDim];\n\n      assert nodeID < splitPackedValues.length: \"nodeID=\" + nodeID + \" splitValues.length=\" + splitPackedValues.length;\n\n      // How many points will be in the left tree:\n      long rightCount = source.count / 2;\n      long leftCount = source.count - rightCount;\n\n      byte[] splitValue = markRightTree(rightCount, splitDim, source, ordBitSet);\n      int address = nodeID * (1+bytesPerDim);\n      splitPackedValues[address] = (byte) splitDim;\n      System.arraycopy(splitValue, 0, splitPackedValues, address + 1, bytesPerDim);\n\n      // Partition all PathSlice that are not the split dim into sorted left and right sets, so we can recurse:\n\n      PathSlice[] leftSlices = new PathSlice[numDims];\n      PathSlice[] rightSlices = new PathSlice[numDims];\n\n      byte[] minSplitPackedValue = new byte[packedBytesLength];\n      System.arraycopy(minPackedValue, 0, minSplitPackedValue, 0, packedBytesLength);\n\n      byte[] maxSplitPackedValue = new byte[packedBytesLength];\n      System.arraycopy(maxPackedValue, 0, maxSplitPackedValue, 0, packedBytesLength);\n\n      // When we are on this dim, below, we clear the ordBitSet:\n      int dimToClear;\n      if (numDims - 1 == splitDim) {\n        dimToClear = numDims - 2;\n      } else {\n        dimToClear = numDims - 1;\n      }\n\n      for(int dim=0;dim<numDims;dim++) {\n\n        if (dim == splitDim) {\n          // No need to partition on this dim since it's a simple slice of the incoming already sorted slice, and we\n          // will re-use its shared reader when visiting it as we recurse:\n          leftSlices[dim] = new PathSlice(source.writer, source.start, leftCount);\n          rightSlices[dim] = new PathSlice(source.writer, source.start + leftCount, rightCount);\n          System.arraycopy(splitValue, 0, minSplitPackedValue, dim*bytesPerDim, bytesPerDim);\n          System.arraycopy(splitValue, 0, maxSplitPackedValue, dim*bytesPerDim, bytesPerDim);\n          continue;\n        }\n\n        // Not inside the try because we don't want to close this one now, so that after recursion is done,\n        // we will have done a singel full sweep of the file:\n        PointReader reader = slices[dim].writer.getSharedReader(slices[dim].start, slices[dim].count, toCloseHeroically);\n\n        try (PointWriter leftPointWriter = getPointWriter(leftCount, \"left\" + dim);\n             PointWriter rightPointWriter = getPointWriter(source.count - leftCount, \"right\" + dim)) {\n\n          long nextRightCount = reader.split(source.count, ordBitSet, leftPointWriter, rightPointWriter, dim == dimToClear);\n          if (rightCount != nextRightCount) {\n            throw new IllegalStateException(\"wrong number of points in split: expected=\" + rightCount + \" but actual=\" + nextRightCount);\n          }\n\n          leftSlices[dim] = new PathSlice(leftPointWriter, 0, leftCount);\n          rightSlices[dim] = new PathSlice(rightPointWriter, 0, rightCount);\n        } catch (Throwable t) {\n          verifyChecksum(t, slices[dim].writer);\n        }\n      }\n\n      // Recurse on left tree:\n      build(2*nodeID, leafNodeOffset, leftSlices,\n            ordBitSet, out,\n            minPackedValue, maxSplitPackedValue,\n            splitPackedValues, leafBlockFPs, toCloseHeroically);\n      for(int dim=0;dim<numDims;dim++) {\n        // Don't destroy the dim we split on because we just re-used what our caller above gave us for that dim:\n        if (dim != splitDim) {\n          leftSlices[dim].writer.destroy();\n        }\n      }\n\n      // TODO: we could \"tail recurse\" here?  have our parent discard its refs as we recurse right?\n      // Recurse on right tree:\n      build(2*nodeID+1, leafNodeOffset, rightSlices,\n            ordBitSet, out,\n            minSplitPackedValue, maxPackedValue,\n            splitPackedValues, leafBlockFPs, toCloseHeroically);\n      for(int dim=0;dim<numDims;dim++) {\n        // Don't destroy the dim we split on because we just re-used what our caller above gave us for that dim:\n        if (dim != splitDim) {\n          rightSlices[dim].writer.destroy();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f03e4bed5023ec3ef93a771b8888cae991cf448d","date":1483469262,"type":4,"author":"Kevin Risden","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#build(int,int,PathSlice[],LongBitSet,IndexOutput,byte[],byte[],byte[],long[],List[Closeable]).mjava","sourceNew":null,"sourceOld":"  /** The array (sized numDims) of PathSlice describe the cell we have currently recursed to.\n  /*  This method is used when we are merging previously written segments, in the numDims > 1 case. */\n  private void build(int nodeID, int leafNodeOffset,\n                     PathSlice[] slices,\n                     LongBitSet ordBitSet,\n                     IndexOutput out,\n                     byte[] minPackedValue, byte[] maxPackedValue,\n                     byte[] splitPackedValues,\n                     long[] leafBlockFPs,\n                     List<Closeable> toCloseHeroically) throws IOException {\n\n    for(PathSlice slice : slices) {\n      assert slice.count == slices[0].count;\n    }\n    \n    if (numDims == 1 && slices[0].writer instanceof OfflinePointWriter && slices[0].count <= maxPointsSortInHeap) {\n      // Special case for 1D, to cutover to heap once we recurse deeply enough:\n      slices[0] = switchToHeap(slices[0], toCloseHeroically);\n    }\n\n    if (nodeID >= leafNodeOffset) {\n\n      // Leaf node: write block\n      // We can write the block in any order so by default we write it sorted by the dimension that has the\n      // least number of unique bytes at commonPrefixLengths[dim], which makes compression more efficient\n      int sortedDim = 0;\n      int sortedDimCardinality = Integer.MAX_VALUE;\n\n      for (int dim=0;dim<numDims;dim++) {\n        if (slices[dim].writer instanceof HeapPointWriter == false) {\n          // Adversarial cases can cause this, e.g. very lopsided data, all equal points, such that we started\n          // offline, but then kept splitting only in one dimension, and so never had to rewrite into heap writer\n          slices[dim] = switchToHeap(slices[dim], toCloseHeroically);\n        }\n\n        PathSlice source = slices[dim];\n\n        HeapPointWriter heapSource = (HeapPointWriter) source.writer;\n\n        // Find common prefix by comparing first and last values, already sorted in this dimension:\n        heapSource.readPackedValue(Math.toIntExact(source.start), scratch1);\n        heapSource.readPackedValue(Math.toIntExact(source.start + source.count - 1), scratch2);\n\n        int offset = dim * bytesPerDim;\n        commonPrefixLengths[dim] = bytesPerDim;\n        for(int j=0;j<bytesPerDim;j++) {\n          if (scratch1[offset+j] != scratch2[offset+j]) {\n            commonPrefixLengths[dim] = j;\n            break;\n          }\n        }\n\n        int prefix = commonPrefixLengths[dim];\n        if (prefix < bytesPerDim) {\n          int cardinality = 1;\n          byte previous = scratch1[offset + prefix];\n          for (long i = 1; i < source.count; ++i) {\n            heapSource.readPackedValue(Math.toIntExact(source.start + i), scratch2);\n            byte b = scratch2[offset + prefix];\n            assert Byte.toUnsignedInt(previous) <= Byte.toUnsignedInt(b);\n            if (b != previous) {\n              cardinality++;\n              previous = b;\n            }\n          }\n          assert cardinality <= 256;\n          if (cardinality < sortedDimCardinality) {\n            sortedDim = dim;\n            sortedDimCardinality = cardinality;\n          }\n        }\n      }\n\n      PathSlice source = slices[sortedDim];\n\n      // We ensured that maxPointsSortInHeap was >= maxPointsInLeafNode, so we better be in heap at this point:\n      HeapPointWriter heapSource = (HeapPointWriter) source.writer;\n\n      // Save the block file pointer:\n      leafBlockFPs[nodeID - leafNodeOffset] = out.getFilePointer();\n      //System.out.println(\"  write leaf block @ fp=\" + out.getFilePointer());\n\n      // Write docIDs first, as their own chunk, so that at intersect time we can add all docIDs w/o\n      // loading the values:\n      int count = Math.toIntExact(source.count);\n      assert count > 0: \"nodeID=\" + nodeID + \" leafNodeOffset=\" + leafNodeOffset;\n      writeLeafBlockDocs(out, heapSource.docIDs, Math.toIntExact(source.start), count);\n\n      // TODO: minor opto: we don't really have to write the actual common prefixes, because BKDReader on recursing can regenerate it for us\n      // from the index, much like how terms dict does so from the FST:\n\n      // Write the common prefixes:\n      writeCommonPrefixes(out, commonPrefixLengths, scratch1);\n\n      // Write the full values:\n      IntFunction<BytesRef> packedValues = new IntFunction<BytesRef>() {\n        final BytesRef scratch = new BytesRef();\n\n        {\n          scratch.length = packedBytesLength;\n        }\n\n        @Override\n        public BytesRef apply(int i) {\n          heapSource.getPackedValueSlice(Math.toIntExact(source.start + i), scratch);\n          return scratch;\n        }\n      };\n      assert valuesInOrderAndBounds(count, sortedDim, minPackedValue, maxPackedValue, packedValues,\n          heapSource.docIDs, Math.toIntExact(source.start));\n      writeLeafBlockPackedValues(out, commonPrefixLengths, count, sortedDim, packedValues);\n\n    } else {\n      // Inner node: partition/recurse\n\n      int splitDim;\n      if (numDims > 1) {\n        splitDim = split(minPackedValue, maxPackedValue);\n      } else {\n        splitDim = 0;\n      }\n\n      PathSlice source = slices[splitDim];\n\n      assert nodeID < splitPackedValues.length: \"nodeID=\" + nodeID + \" splitValues.length=\" + splitPackedValues.length;\n\n      // How many points will be in the left tree:\n      long rightCount = source.count / 2;\n      long leftCount = source.count - rightCount;\n\n      byte[] splitValue = markRightTree(rightCount, splitDim, source, ordBitSet);\n      int address = nodeID * (1+bytesPerDim);\n      splitPackedValues[address] = (byte) splitDim;\n      System.arraycopy(splitValue, 0, splitPackedValues, address + 1, bytesPerDim);\n\n      // Partition all PathSlice that are not the split dim into sorted left and right sets, so we can recurse:\n\n      PathSlice[] leftSlices = new PathSlice[numDims];\n      PathSlice[] rightSlices = new PathSlice[numDims];\n\n      byte[] minSplitPackedValue = new byte[packedBytesLength];\n      System.arraycopy(minPackedValue, 0, minSplitPackedValue, 0, packedBytesLength);\n\n      byte[] maxSplitPackedValue = new byte[packedBytesLength];\n      System.arraycopy(maxPackedValue, 0, maxSplitPackedValue, 0, packedBytesLength);\n\n      // When we are on this dim, below, we clear the ordBitSet:\n      int dimToClear;\n      if (numDims - 1 == splitDim) {\n        dimToClear = numDims - 2;\n      } else {\n        dimToClear = numDims - 1;\n      }\n\n      for(int dim=0;dim<numDims;dim++) {\n\n        if (dim == splitDim) {\n          // No need to partition on this dim since it's a simple slice of the incoming already sorted slice, and we\n          // will re-use its shared reader when visiting it as we recurse:\n          leftSlices[dim] = new PathSlice(source.writer, source.start, leftCount);\n          rightSlices[dim] = new PathSlice(source.writer, source.start + leftCount, rightCount);\n          System.arraycopy(splitValue, 0, minSplitPackedValue, dim*bytesPerDim, bytesPerDim);\n          System.arraycopy(splitValue, 0, maxSplitPackedValue, dim*bytesPerDim, bytesPerDim);\n          continue;\n        }\n\n        // Not inside the try because we don't want to close this one now, so that after recursion is done,\n        // we will have done a singel full sweep of the file:\n        PointReader reader = slices[dim].writer.getSharedReader(slices[dim].start, slices[dim].count, toCloseHeroically);\n\n        try (PointWriter leftPointWriter = getPointWriter(leftCount, \"left\" + dim);\n             PointWriter rightPointWriter = getPointWriter(source.count - leftCount, \"right\" + dim)) {\n\n          long nextRightCount = reader.split(source.count, ordBitSet, leftPointWriter, rightPointWriter, dim == dimToClear);\n          if (rightCount != nextRightCount) {\n            throw new IllegalStateException(\"wrong number of points in split: expected=\" + rightCount + \" but actual=\" + nextRightCount);\n          }\n\n          leftSlices[dim] = new PathSlice(leftPointWriter, 0, leftCount);\n          rightSlices[dim] = new PathSlice(rightPointWriter, 0, rightCount);\n        } catch (Throwable t) {\n          verifyChecksum(t, slices[dim].writer);\n        }\n      }\n\n      // Recurse on left tree:\n      build(2*nodeID, leafNodeOffset, leftSlices,\n            ordBitSet, out,\n            minPackedValue, maxSplitPackedValue,\n            splitPackedValues, leafBlockFPs, toCloseHeroically);\n      for(int dim=0;dim<numDims;dim++) {\n        // Don't destroy the dim we split on because we just re-used what our caller above gave us for that dim:\n        if (dim != splitDim) {\n          leftSlices[dim].writer.destroy();\n        }\n      }\n\n      // TODO: we could \"tail recurse\" here?  have our parent discard its refs as we recurse right?\n      // Recurse on right tree:\n      build(2*nodeID+1, leafNodeOffset, rightSlices,\n            ordBitSet, out,\n            minSplitPackedValue, maxPackedValue,\n            splitPackedValues, leafBlockFPs, toCloseHeroically);\n      for(int dim=0;dim<numDims;dim++) {\n        // Don't destroy the dim we split on because we just re-used what our caller above gave us for that dim:\n        if (dim != splitDim) {\n          rightSlices[dim].writer.destroy();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"9fc0d60683b47b5d922124c31f57c8b34734f9e6":["d561885e9bb6238af1ff8afe8630dcfe49b66ac7"],"51b2a4c0f6c28a8ba7c41911b421cea2ede8ef40":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"3ca40baa99f9578eb8408ee5b9177f7ffe6f65d6":["51b2a4c0f6c28a8ba7c41911b421cea2ede8ef40"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"9856095f7afb5a607bf5e65077615ed91273508c":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","c4ad863d796f4e72a3a1ef4bacd2e19c3e9258c9"],"d561885e9bb6238af1ff8afe8630dcfe49b66ac7":["3ca40baa99f9578eb8408ee5b9177f7ffe6f65d6"],"b35cfd3fa0a5c9e066b0256c4818af1d2a9f22d7":["c4ad863d796f4e72a3a1ef4bacd2e19c3e9258c9"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["51b2a4c0f6c28a8ba7c41911b421cea2ede8ef40","d561885e9bb6238af1ff8afe8630dcfe49b66ac7"],"3b013574eedcdbac35dc7e35b0ee616ffc38895d":["3ca40baa99f9578eb8408ee5b9177f7ffe6f65d6","d561885e9bb6238af1ff8afe8630dcfe49b66ac7"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b35cfd3fa0a5c9e066b0256c4818af1d2a9f22d7"],"c4ad863d796f4e72a3a1ef4bacd2e19c3e9258c9":["9fc0d60683b47b5d922124c31f57c8b34734f9e6"],"f03e4bed5023ec3ef93a771b8888cae991cf448d":["9856095f7afb5a607bf5e65077615ed91273508c","b35cfd3fa0a5c9e066b0256c4818af1d2a9f22d7"]},"commit2Childs":{"9fc0d60683b47b5d922124c31f57c8b34734f9e6":["c4ad863d796f4e72a3a1ef4bacd2e19c3e9258c9"],"51b2a4c0f6c28a8ba7c41911b421cea2ede8ef40":["3ca40baa99f9578eb8408ee5b9177f7ffe6f65d6","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["51b2a4c0f6c28a8ba7c41911b421cea2ede8ef40"],"3ca40baa99f9578eb8408ee5b9177f7ffe6f65d6":["d561885e9bb6238af1ff8afe8630dcfe49b66ac7","3b013574eedcdbac35dc7e35b0ee616ffc38895d"],"9856095f7afb5a607bf5e65077615ed91273508c":["f03e4bed5023ec3ef93a771b8888cae991cf448d"],"d561885e9bb6238af1ff8afe8630dcfe49b66ac7":["9fc0d60683b47b5d922124c31f57c8b34734f9e6","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","3b013574eedcdbac35dc7e35b0ee616ffc38895d"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["9856095f7afb5a607bf5e65077615ed91273508c"],"b35cfd3fa0a5c9e066b0256c4818af1d2a9f22d7":["cd5edd1f2b162a5cfa08efd17851a07373a96817","f03e4bed5023ec3ef93a771b8888cae991cf448d"],"3b013574eedcdbac35dc7e35b0ee616ffc38895d":[],"c4ad863d796f4e72a3a1ef4bacd2e19c3e9258c9":["9856095f7afb5a607bf5e65077615ed91273508c","b35cfd3fa0a5c9e066b0256c4818af1d2a9f22d7"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"f03e4bed5023ec3ef93a771b8888cae991cf448d":[]},"heads":["3b013574eedcdbac35dc7e35b0ee616ffc38895d","cd5edd1f2b162a5cfa08efd17851a07373a96817","f03e4bed5023ec3ef93a771b8888cae991cf448d"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}