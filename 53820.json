{"path":"lucene/core/src/java/org/apache/lucene/index/OrdinalMap#OrdinalMap(IndexReader.CacheKey,TermsEnum[],SegmentMap,float).mjava","commits":[{"id":"957c610636f393a85a38f1af670540028db13e6b","date":1500044517,"type":0,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/OrdinalMap#OrdinalMap(IndexReader.CacheKey,TermsEnum[],SegmentMap,float).mjava","pathOld":"/dev/null","sourceNew":"  OrdinalMap(IndexReader.CacheKey owner, TermsEnum subs[], SegmentMap segmentMap, float acceptableOverheadRatio) throws IOException {\n    // create the ordinal mappings by pulling a termsenum over each sub's \n    // unique terms, and walking a multitermsenum over those\n    this.owner = owner;\n    this.segmentMap = segmentMap;\n    // even though we accept an overhead ratio, we keep these ones with COMPACT\n    // since they are only used to resolve values given a global ord, which is\n    // slow anyway\n    PackedLongValues.Builder globalOrdDeltas = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n    PackedLongValues.Builder firstSegments = PackedLongValues.packedBuilder(PackedInts.COMPACT);\n    final PackedLongValues.Builder[] ordDeltas = new PackedLongValues.Builder[subs.length];\n    for (int i = 0; i < ordDeltas.length; i++) {\n      ordDeltas[i] = PackedLongValues.monotonicBuilder(acceptableOverheadRatio);\n    }\n    long[] ordDeltaBits = new long[subs.length];\n    long[] segmentOrds = new long[subs.length];\n\n    // Just merge-sorts by term:\n    PriorityQueue<TermsEnumIndex> queue = new PriorityQueue<TermsEnumIndex>(subs.length) {\n        @Override\n        protected boolean lessThan(TermsEnumIndex a, TermsEnumIndex b) {\n          return a.currentTerm.compareTo(b.currentTerm) < 0;\n        }\n      };\n    \n    for (int i = 0; i < subs.length; i++) {\n      TermsEnumIndex sub = new TermsEnumIndex(subs[segmentMap.newToOld(i)], i);\n      if (sub.next() != null) {\n        queue.add(sub);\n      }\n    }\n\n    BytesRefBuilder scratch = new BytesRefBuilder();\n      \n    long globalOrd = 0;\n    while (queue.size() != 0) {\n      TermsEnumIndex top = queue.top();\n      scratch.copyBytes(top.currentTerm);\n\n      int firstSegmentIndex = Integer.MAX_VALUE;\n      long globalOrdDelta = Long.MAX_VALUE;\n\n      // Advance past this term, recording the per-segment ord deltas:\n      while (true) {\n        top = queue.top();\n        long segmentOrd = top.termsEnum.ord();\n        long delta = globalOrd - segmentOrd;\n        int segmentIndex = top.subIndex;\n        // We compute the least segment where the term occurs. In case the\n        // first segment contains most (or better all) values, this will\n        // help save significant memory\n        if (segmentIndex < firstSegmentIndex) {\n          firstSegmentIndex = segmentIndex;\n          globalOrdDelta = delta;\n        }\n        ordDeltaBits[segmentIndex] |= delta;\n\n        // for each per-segment ord, map it back to the global term; the while loop is needed\n        // in case the incoming TermsEnums don't have compact ordinals (some ordinal values\n        // are skipped), which can happen e.g. with a FilteredTermsEnum:\n        assert segmentOrds[segmentIndex] <= segmentOrd;\n\n        // TODO: we could specialize this case (the while loop is not needed when the ords\n        // are compact)\n        do {\n          ordDeltas[segmentIndex].add(delta);\n          segmentOrds[segmentIndex]++;\n        } while (segmentOrds[segmentIndex] <= segmentOrd);\n        \n        if (top.next() == null) {\n          queue.pop();\n          if (queue.size() == 0) {\n            break;\n          }\n        } else {\n          queue.updateTop();\n        }\n        if (queue.top().currentTerm.equals(scratch.get()) == false) {\n          break;\n        }\n      }\n\n      // for each unique term, just mark the first segment index/delta where it occurs\n      firstSegments.add(firstSegmentIndex);\n      globalOrdDeltas.add(globalOrdDelta);\n      globalOrd++;\n    }\n\n    this.firstSegments = firstSegments.build();\n    this.globalOrdDeltas = globalOrdDeltas.build();\n    // ordDeltas is typically the bottleneck, so let's see what we can do to make it faster\n    segmentToGlobalOrds = new LongValues[subs.length];\n    long ramBytesUsed = BASE_RAM_BYTES_USED + this.globalOrdDeltas.ramBytesUsed()\n      + this.firstSegments.ramBytesUsed() + RamUsageEstimator.shallowSizeOf(segmentToGlobalOrds)\n      + segmentMap.ramBytesUsed();\n    for (int i = 0; i < ordDeltas.length; ++i) {\n      final PackedLongValues deltas = ordDeltas[i].build();\n      if (ordDeltaBits[i] == 0L) {\n        // segment ords perfectly match global ordinals\n        // likely in case of low cardinalities and large segments\n        segmentToGlobalOrds[i] = LongValues.IDENTITY;\n      } else {\n        final int bitsRequired = ordDeltaBits[i] < 0 ? 64 : PackedInts.bitsRequired(ordDeltaBits[i]);\n        final long monotonicBits = deltas.ramBytesUsed() * 8;\n        final long packedBits = bitsRequired * deltas.size();\n        if (deltas.size() <= Integer.MAX_VALUE\n            && packedBits <= monotonicBits * (1 + acceptableOverheadRatio)) {\n          // monotonic compression mostly adds overhead, let's keep the mapping in plain packed ints\n          final int size = (int) deltas.size();\n          final PackedInts.Mutable newDeltas = PackedInts.getMutable(size, bitsRequired, acceptableOverheadRatio);\n          final PackedLongValues.Iterator it = deltas.iterator();\n          for (int ord = 0; ord < size; ++ord) {\n            newDeltas.set(ord, it.next());\n          }\n          assert it.hasNext() == false;\n          segmentToGlobalOrds[i] = new LongValues() {\n              @Override\n              public long get(long ord) {\n                return ord + newDeltas.get((int) ord);\n              }\n            };\n          ramBytesUsed += newDeltas.ramBytesUsed();\n        } else {\n          segmentToGlobalOrds[i] = new LongValues() {\n              @Override\n              public long get(long ord) {\n                return ord + deltas.get(ord);\n              }\n            };\n          ramBytesUsed += deltas.ramBytesUsed();\n        }\n        ramBytesUsed += RamUsageEstimator.shallowSizeOf(segmentToGlobalOrds[i]);\n      }\n    }\n    this.ramBytesUsed = ramBytesUsed;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"aaf90fc29510e72665ac7934f34c3d1c25efad64","date":1500354819,"type":0,"author":"Cao Manh Dat","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/OrdinalMap#OrdinalMap(IndexReader.CacheKey,TermsEnum[],SegmentMap,float).mjava","pathOld":"/dev/null","sourceNew":"  OrdinalMap(IndexReader.CacheKey owner, TermsEnum subs[], SegmentMap segmentMap, float acceptableOverheadRatio) throws IOException {\n    // create the ordinal mappings by pulling a termsenum over each sub's \n    // unique terms, and walking a multitermsenum over those\n    this.owner = owner;\n    this.segmentMap = segmentMap;\n    // even though we accept an overhead ratio, we keep these ones with COMPACT\n    // since they are only used to resolve values given a global ord, which is\n    // slow anyway\n    PackedLongValues.Builder globalOrdDeltas = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n    PackedLongValues.Builder firstSegments = PackedLongValues.packedBuilder(PackedInts.COMPACT);\n    final PackedLongValues.Builder[] ordDeltas = new PackedLongValues.Builder[subs.length];\n    for (int i = 0; i < ordDeltas.length; i++) {\n      ordDeltas[i] = PackedLongValues.monotonicBuilder(acceptableOverheadRatio);\n    }\n    long[] ordDeltaBits = new long[subs.length];\n    long[] segmentOrds = new long[subs.length];\n\n    // Just merge-sorts by term:\n    PriorityQueue<TermsEnumIndex> queue = new PriorityQueue<TermsEnumIndex>(subs.length) {\n        @Override\n        protected boolean lessThan(TermsEnumIndex a, TermsEnumIndex b) {\n          return a.currentTerm.compareTo(b.currentTerm) < 0;\n        }\n      };\n    \n    for (int i = 0; i < subs.length; i++) {\n      TermsEnumIndex sub = new TermsEnumIndex(subs[segmentMap.newToOld(i)], i);\n      if (sub.next() != null) {\n        queue.add(sub);\n      }\n    }\n\n    BytesRefBuilder scratch = new BytesRefBuilder();\n      \n    long globalOrd = 0;\n    while (queue.size() != 0) {\n      TermsEnumIndex top = queue.top();\n      scratch.copyBytes(top.currentTerm);\n\n      int firstSegmentIndex = Integer.MAX_VALUE;\n      long globalOrdDelta = Long.MAX_VALUE;\n\n      // Advance past this term, recording the per-segment ord deltas:\n      while (true) {\n        top = queue.top();\n        long segmentOrd = top.termsEnum.ord();\n        long delta = globalOrd - segmentOrd;\n        int segmentIndex = top.subIndex;\n        // We compute the least segment where the term occurs. In case the\n        // first segment contains most (or better all) values, this will\n        // help save significant memory\n        if (segmentIndex < firstSegmentIndex) {\n          firstSegmentIndex = segmentIndex;\n          globalOrdDelta = delta;\n        }\n        ordDeltaBits[segmentIndex] |= delta;\n\n        // for each per-segment ord, map it back to the global term; the while loop is needed\n        // in case the incoming TermsEnums don't have compact ordinals (some ordinal values\n        // are skipped), which can happen e.g. with a FilteredTermsEnum:\n        assert segmentOrds[segmentIndex] <= segmentOrd;\n\n        // TODO: we could specialize this case (the while loop is not needed when the ords\n        // are compact)\n        do {\n          ordDeltas[segmentIndex].add(delta);\n          segmentOrds[segmentIndex]++;\n        } while (segmentOrds[segmentIndex] <= segmentOrd);\n        \n        if (top.next() == null) {\n          queue.pop();\n          if (queue.size() == 0) {\n            break;\n          }\n        } else {\n          queue.updateTop();\n        }\n        if (queue.top().currentTerm.equals(scratch.get()) == false) {\n          break;\n        }\n      }\n\n      // for each unique term, just mark the first segment index/delta where it occurs\n      firstSegments.add(firstSegmentIndex);\n      globalOrdDeltas.add(globalOrdDelta);\n      globalOrd++;\n    }\n\n    this.firstSegments = firstSegments.build();\n    this.globalOrdDeltas = globalOrdDeltas.build();\n    // ordDeltas is typically the bottleneck, so let's see what we can do to make it faster\n    segmentToGlobalOrds = new LongValues[subs.length];\n    long ramBytesUsed = BASE_RAM_BYTES_USED + this.globalOrdDeltas.ramBytesUsed()\n      + this.firstSegments.ramBytesUsed() + RamUsageEstimator.shallowSizeOf(segmentToGlobalOrds)\n      + segmentMap.ramBytesUsed();\n    for (int i = 0; i < ordDeltas.length; ++i) {\n      final PackedLongValues deltas = ordDeltas[i].build();\n      if (ordDeltaBits[i] == 0L) {\n        // segment ords perfectly match global ordinals\n        // likely in case of low cardinalities and large segments\n        segmentToGlobalOrds[i] = LongValues.IDENTITY;\n      } else {\n        final int bitsRequired = ordDeltaBits[i] < 0 ? 64 : PackedInts.bitsRequired(ordDeltaBits[i]);\n        final long monotonicBits = deltas.ramBytesUsed() * 8;\n        final long packedBits = bitsRequired * deltas.size();\n        if (deltas.size() <= Integer.MAX_VALUE\n            && packedBits <= monotonicBits * (1 + acceptableOverheadRatio)) {\n          // monotonic compression mostly adds overhead, let's keep the mapping in plain packed ints\n          final int size = (int) deltas.size();\n          final PackedInts.Mutable newDeltas = PackedInts.getMutable(size, bitsRequired, acceptableOverheadRatio);\n          final PackedLongValues.Iterator it = deltas.iterator();\n          for (int ord = 0; ord < size; ++ord) {\n            newDeltas.set(ord, it.next());\n          }\n          assert it.hasNext() == false;\n          segmentToGlobalOrds[i] = new LongValues() {\n              @Override\n              public long get(long ord) {\n                return ord + newDeltas.get((int) ord);\n              }\n            };\n          ramBytesUsed += newDeltas.ramBytesUsed();\n        } else {\n          segmentToGlobalOrds[i] = new LongValues() {\n              @Override\n              public long get(long ord) {\n                return ord + deltas.get(ord);\n              }\n            };\n          ramBytesUsed += deltas.ramBytesUsed();\n        }\n        ramBytesUsed += RamUsageEstimator.shallowSizeOf(segmentToGlobalOrds[i]);\n      }\n    }\n    this.ramBytesUsed = ramBytesUsed;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"957c610636f393a85a38f1af670540028db13e6b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"aaf90fc29510e72665ac7934f34c3d1c25efad64":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","957c610636f393a85a38f1af670540028db13e6b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["957c610636f393a85a38f1af670540028db13e6b"]},"commit2Childs":{"957c610636f393a85a38f1af670540028db13e6b":["aaf90fc29510e72665ac7934f34c3d1c25efad64","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"aaf90fc29510e72665ac7934f34c3d1c25efad64":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["957c610636f393a85a38f1af670540028db13e6b","aaf90fc29510e72665ac7934f34c3d1c25efad64"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["aaf90fc29510e72665ac7934f34c3d1c25efad64","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}