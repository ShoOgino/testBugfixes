{"path":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flushDocStores(DocumentsWriterPerThread).mjava","commits":[{"id":"9b832cbed6eb3d54a8bb9339296bdda8eeb53014","date":1279708040,"type":0,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flushDocStores(DocumentsWriterPerThread).mjava","pathOld":"/dev/null","sourceNew":"  private boolean flushDocStores(DocumentsWriterPerThread perThread) throws IOException {\n      boolean useCompoundDocStore = false;\n  \n      String docStoreSegment;\n      \n      boolean success = false;\n      try {\n        docStoreSegment = perThread.closeDocStore();\n        success = true;\n      } finally {\n        if (!success && infoStream != null) {\n          message(\"hit exception closing doc store segment\");\n        }\n      }\n  \n      useCompoundDocStore = indexWriter.mergePolicy.useCompoundDocStore(indexWriter.segmentInfos);\n  \n      if (useCompoundDocStore && docStoreSegment != null && perThread.closedFiles().size() != 0) {\n        // Now build compound doc store file\n  \n        if (infoStream != null) {\n          message(\"create compound file \"\n              + IndexFileNames.segmentFileName(docStoreSegment, \"\",\n                  IndexFileNames.COMPOUND_FILE_STORE_EXTENSION));\n        }\n  \n        success = false;\n  \n        final int numSegments = indexWriter.segmentInfos.size();\n        final String compoundFileName = IndexFileNames.segmentFileName(docStoreSegment, \"\",\n            IndexFileNames.COMPOUND_FILE_STORE_EXTENSION);\n  \n        try {\n          CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, compoundFileName);\n          for (final String file : perThread.closedFiles()) {\n            cfsWriter.addFile(file);\n          }\n  \n          // Perform the merge\n          cfsWriter.close();\n          success = true;\n  \n        } finally {\n          if (!success) {\n            if (infoStream != null)\n              message(\"hit exception building compound file doc store for segment \" + docStoreSegment);\n            synchronized(indexWriter) {\n              indexWriter.deleter.deleteFile(compoundFileName);\n            }\n            abort();\n          }\n        }\n  \n        synchronized(indexWriter) {\n          for (int i = 0; i < numSegments; i++) {\n            SegmentInfo si = indexWriter.segmentInfos.info(i);\n            if (si.getDocStoreOffset() != -1 &&\n                  si.getDocStoreSegment().equals(docStoreSegment))\n              si.setDocStoreIsCompoundFile(true);\n          }\n    \n          indexWriter.checkpoint();\n    \n          // In case the files we just merged into a CFS were\n          // not previously checkpointed:\n          indexWriter.deleter.deleteNewFiles(perThread.closedFiles());\n        }\n      }\n  \n      return useCompoundDocStore;\n    \n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"334c1175813aea771a71728cd2c4ee4754fd0603","date":1279710173,"type":4,"author":"Uwe Schindler","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flushDocStores(DocumentsWriterPerThread).mjava","sourceNew":null,"sourceOld":"  private boolean flushDocStores(DocumentsWriterPerThread perThread) throws IOException {\n      boolean useCompoundDocStore = false;\n  \n      String docStoreSegment;\n      \n      boolean success = false;\n      try {\n        docStoreSegment = perThread.closeDocStore();\n        success = true;\n      } finally {\n        if (!success && infoStream != null) {\n          message(\"hit exception closing doc store segment\");\n        }\n      }\n  \n      useCompoundDocStore = indexWriter.mergePolicy.useCompoundDocStore(indexWriter.segmentInfos);\n  \n      if (useCompoundDocStore && docStoreSegment != null && perThread.closedFiles().size() != 0) {\n        // Now build compound doc store file\n  \n        if (infoStream != null) {\n          message(\"create compound file \"\n              + IndexFileNames.segmentFileName(docStoreSegment, \"\",\n                  IndexFileNames.COMPOUND_FILE_STORE_EXTENSION));\n        }\n  \n        success = false;\n  \n        final int numSegments = indexWriter.segmentInfos.size();\n        final String compoundFileName = IndexFileNames.segmentFileName(docStoreSegment, \"\",\n            IndexFileNames.COMPOUND_FILE_STORE_EXTENSION);\n  \n        try {\n          CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, compoundFileName);\n          for (final String file : perThread.closedFiles()) {\n            cfsWriter.addFile(file);\n          }\n  \n          // Perform the merge\n          cfsWriter.close();\n          success = true;\n  \n        } finally {\n          if (!success) {\n            if (infoStream != null)\n              message(\"hit exception building compound file doc store for segment \" + docStoreSegment);\n            synchronized(indexWriter) {\n              indexWriter.deleter.deleteFile(compoundFileName);\n            }\n            abort();\n          }\n        }\n  \n        synchronized(indexWriter) {\n          for (int i = 0; i < numSegments; i++) {\n            SegmentInfo si = indexWriter.segmentInfos.info(i);\n            if (si.getDocStoreOffset() != -1 &&\n                  si.getDocStoreSegment().equals(docStoreSegment))\n              si.setDocStoreIsCompoundFile(true);\n          }\n    \n          indexWriter.checkpoint();\n    \n          // In case the files we just merged into a CFS were\n          // not previously checkpointed:\n          indexWriter.deleter.deleteNewFiles(perThread.closedFiles());\n        }\n      }\n  \n      return useCompoundDocStore;\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8fe956d65251358d755c56f14fe8380644790e47","date":1279711318,"type":0,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flushDocStores(DocumentsWriterPerThread).mjava","pathOld":"/dev/null","sourceNew":"  private boolean flushDocStores(DocumentsWriterPerThread perThread) throws IOException {\n      boolean useCompoundDocStore = false;\n  \n      String docStoreSegment;\n      \n      boolean success = false;\n      try {\n        docStoreSegment = perThread.closeDocStore();\n        success = true;\n      } finally {\n        if (!success && infoStream != null) {\n          message(\"hit exception closing doc store segment\");\n        }\n      }\n  \n      useCompoundDocStore = indexWriter.mergePolicy.useCompoundDocStore(indexWriter.segmentInfos);\n  \n      if (useCompoundDocStore && docStoreSegment != null && perThread.closedFiles().size() != 0) {\n        // Now build compound doc store file\n  \n        if (infoStream != null) {\n          message(\"create compound file \"\n              + IndexFileNames.segmentFileName(docStoreSegment, \"\",\n                  IndexFileNames.COMPOUND_FILE_STORE_EXTENSION));\n        }\n  \n        success = false;\n  \n        final int numSegments = indexWriter.segmentInfos.size();\n        final String compoundFileName = IndexFileNames.segmentFileName(docStoreSegment, \"\",\n            IndexFileNames.COMPOUND_FILE_STORE_EXTENSION);\n  \n        try {\n          CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, compoundFileName);\n          for (final String file : perThread.closedFiles()) {\n            cfsWriter.addFile(file);\n          }\n  \n          // Perform the merge\n          cfsWriter.close();\n          success = true;\n  \n        } finally {\n          if (!success) {\n            if (infoStream != null)\n              message(\"hit exception building compound file doc store for segment \" + docStoreSegment);\n            synchronized(indexWriter) {\n              indexWriter.deleter.deleteFile(compoundFileName);\n            }\n            abort();\n          }\n        }\n  \n        synchronized(indexWriter) {\n          for (int i = 0; i < numSegments; i++) {\n            SegmentInfo si = indexWriter.segmentInfos.info(i);\n            if (si.getDocStoreOffset() != -1 &&\n                  si.getDocStoreSegment().equals(docStoreSegment))\n              si.setDocStoreIsCompoundFile(true);\n          }\n    \n          indexWriter.checkpoint();\n    \n          // In case the files we just merged into a CFS were\n          // not previously checkpointed:\n          indexWriter.deleter.deleteNewFiles(perThread.closedFiles());\n        }\n      }\n  \n      return useCompoundDocStore;\n    \n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"833a7987bc1c94455fde83e3311f72bddedcfb93","date":1279951470,"type":4,"author":"Michael Busch","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flushDocStores(DocumentsWriterPerThread).mjava","sourceNew":null,"sourceOld":"  private boolean flushDocStores(DocumentsWriterPerThread perThread) throws IOException {\n      boolean useCompoundDocStore = false;\n  \n      String docStoreSegment;\n      \n      boolean success = false;\n      try {\n        docStoreSegment = perThread.closeDocStore();\n        success = true;\n      } finally {\n        if (!success && infoStream != null) {\n          message(\"hit exception closing doc store segment\");\n        }\n      }\n  \n      useCompoundDocStore = indexWriter.mergePolicy.useCompoundDocStore(indexWriter.segmentInfos);\n  \n      if (useCompoundDocStore && docStoreSegment != null && perThread.closedFiles().size() != 0) {\n        // Now build compound doc store file\n  \n        if (infoStream != null) {\n          message(\"create compound file \"\n              + IndexFileNames.segmentFileName(docStoreSegment, \"\",\n                  IndexFileNames.COMPOUND_FILE_STORE_EXTENSION));\n        }\n  \n        success = false;\n  \n        final int numSegments = indexWriter.segmentInfos.size();\n        final String compoundFileName = IndexFileNames.segmentFileName(docStoreSegment, \"\",\n            IndexFileNames.COMPOUND_FILE_STORE_EXTENSION);\n  \n        try {\n          CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, compoundFileName);\n          for (final String file : perThread.closedFiles()) {\n            cfsWriter.addFile(file);\n          }\n  \n          // Perform the merge\n          cfsWriter.close();\n          success = true;\n  \n        } finally {\n          if (!success) {\n            if (infoStream != null)\n              message(\"hit exception building compound file doc store for segment \" + docStoreSegment);\n            synchronized(indexWriter) {\n              indexWriter.deleter.deleteFile(compoundFileName);\n            }\n            abort();\n          }\n        }\n  \n        synchronized(indexWriter) {\n          for (int i = 0; i < numSegments; i++) {\n            SegmentInfo si = indexWriter.segmentInfos.info(i);\n            if (si.getDocStoreOffset() != -1 &&\n                  si.getDocStoreSegment().equals(docStoreSegment))\n              si.setDocStoreIsCompoundFile(true);\n          }\n    \n          indexWriter.checkpoint();\n    \n          // In case the files we just merged into a CFS were\n          // not previously checkpointed:\n          indexWriter.deleter.deleteNewFiles(perThread.closedFiles());\n        }\n      }\n  \n      return useCompoundDocStore;\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"8fe956d65251358d755c56f14fe8380644790e47":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"334c1175813aea771a71728cd2c4ee4754fd0603":["9b832cbed6eb3d54a8bb9339296bdda8eeb53014"],"9b832cbed6eb3d54a8bb9339296bdda8eeb53014":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["334c1175813aea771a71728cd2c4ee4754fd0603"],"833a7987bc1c94455fde83e3311f72bddedcfb93":["8fe956d65251358d755c56f14fe8380644790e47"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["8fe956d65251358d755c56f14fe8380644790e47","9b832cbed6eb3d54a8bb9339296bdda8eeb53014"],"8fe956d65251358d755c56f14fe8380644790e47":["833a7987bc1c94455fde83e3311f72bddedcfb93"],"334c1175813aea771a71728cd2c4ee4754fd0603":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"9b832cbed6eb3d54a8bb9339296bdda8eeb53014":["334c1175813aea771a71728cd2c4ee4754fd0603"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"833a7987bc1c94455fde83e3311f72bddedcfb93":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817","833a7987bc1c94455fde83e3311f72bddedcfb93"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}