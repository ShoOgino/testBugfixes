{"path":"src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","commits":[{"id":"91109046a59c58ee0ee5d0d2767b08d1f30d6702","date":1000830588,"type":0,"author":"Jason van Zyl","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"/dev/null","sourceNew":"  private final void mergeNorms() throws IOException {\n    for (int i = 0; i < fieldInfos.size(); i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.isIndexed) {\n\tOutputStream output = directory.createFile(segment + \".f\" + i);\n\ttry {\n\t  for (int j = 0; j < readers.size(); j++) {\n\t    SegmentReader reader = (SegmentReader)readers.elementAt(j);\n\t    BitVector deletedDocs = reader.deletedDocs;\n\t    InputStream input = reader.normStream(fi.name);\n            int maxDoc = reader.maxDoc();\n\t    try {\n\t      for (int k = 0; k < maxDoc; k++) {\n\t\tbyte norm = input != null ? input.readByte() : (byte)0;\n\t\tif (deletedDocs == null || !deletedDocs.get(k))\n\t\t  output.writeByte(norm);\n\t      }\n\t    } finally {\n\t      if (input != null)\n\t\tinput.close();\n\t    }\n\t  }\n\t} finally {\n\t  output.close();\n\t}\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8fb95844e4ba5160067c64c5eb1cd8a09f7a94f1","date":1064527311,"type":3,"author":"Dmitry Serebrennikov","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private final void mergeNorms() throws IOException {\n    for (int i = 0; i < fieldInfos.size(); i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.isIndexed) {\n        OutputStream output = directory.createFile(segment + \".f\" + i);\n        try {\n          for (int j = 0; j < readers.size(); j++) {\n            SegmentReader reader = (SegmentReader)readers.elementAt(j);\n            BitVector deletedDocs = reader.deletedDocs;\n            InputStream input = reader.normStream(fi.name);\n            int maxDoc = reader.maxDoc();\n            try {\n              for (int k = 0; k < maxDoc; k++) {\n                byte norm = input != null ? input.readByte() : (byte)0;\n                if (deletedDocs == null || !deletedDocs.get(k))\n                  output.writeByte(norm);\n              }\n            } finally {\n              if (input != null)\n                input.close();\n            }\n          }\n        } finally {\n          output.close();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private final void mergeNorms() throws IOException {\n    for (int i = 0; i < fieldInfos.size(); i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.isIndexed) {\n\tOutputStream output = directory.createFile(segment + \".f\" + i);\n\ttry {\n\t  for (int j = 0; j < readers.size(); j++) {\n\t    SegmentReader reader = (SegmentReader)readers.elementAt(j);\n\t    BitVector deletedDocs = reader.deletedDocs;\n\t    InputStream input = reader.normStream(fi.name);\n            int maxDoc = reader.maxDoc();\n\t    try {\n\t      for (int k = 0; k < maxDoc; k++) {\n\t\tbyte norm = input != null ? input.readByte() : (byte)0;\n\t\tif (deletedDocs == null || !deletedDocs.get(k))\n\t\t  output.writeByte(norm);\n\t      }\n\t    } finally {\n\t      if (input != null)\n\t\tinput.close();\n\t    }\n\t  }\n\t} finally {\n\t  output.close();\n\t}\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7fb6d70db034a5456ae560175dd1b821eea9ff4","date":1066759157,"type":3,"author":"Doug Cutting","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private final int mergeNorms() throws IOException {\n    int docCount = 0;\n    for (int i = 0; i < fieldInfos.size(); i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.isIndexed) {\n\tOutputStream output = directory.createFile(segment + \".f\" + i);\n\ttry {\n\t  for (int j = 0; j < readers.size(); j++) {\n\t    IndexReader reader = (IndexReader)readers.elementAt(j);\n\t    byte[] input = reader.norms(fi.name);\n            int maxDoc = reader.maxDoc();\n            for (int k = 0; k < maxDoc; k++) {\n              byte norm = input != null ? input[k] : (byte)0;\n              if (!reader.isDeleted(k)) {\n                output.writeByte(norm);\n                docCount++;\n              }\n\t    }\n\t  }\n\t} finally {\n\t  output.close();\n\t}\n      }\n    }\n    return docCount;\n  }\n\n","sourceOld":"  private final void mergeNorms() throws IOException {\n    for (int i = 0; i < fieldInfos.size(); i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.isIndexed) {\n        OutputStream output = directory.createFile(segment + \".f\" + i);\n        try {\n          for (int j = 0; j < readers.size(); j++) {\n            SegmentReader reader = (SegmentReader)readers.elementAt(j);\n            BitVector deletedDocs = reader.deletedDocs;\n            InputStream input = reader.normStream(fi.name);\n            int maxDoc = reader.maxDoc();\n            try {\n              for (int k = 0; k < maxDoc; k++) {\n                byte norm = input != null ? input.readByte() : (byte)0;\n                if (deletedDocs == null || !deletedDocs.get(k))\n                  output.writeByte(norm);\n              }\n            } finally {\n              if (input != null)\n                input.close();\n            }\n          }\n        } finally {\n          output.close();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f22ffa72cdb7463ca3e818b15f8fb8b0330e7cf5","date":1067592524,"type":3,"author":"Christoph Goller","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private final void mergeNorms() throws IOException {\n    for (int i = 0; i < fieldInfos.size(); i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.isIndexed) {\n\tOutputStream output = directory.createFile(segment + \".f\" + i);\n\ttry {\n\t  for (int j = 0; j < readers.size(); j++) {\n\t    IndexReader reader = (IndexReader)readers.elementAt(j);\n\t    byte[] input = reader.norms(fi.name);\n            int maxDoc = reader.maxDoc();\n            for (int k = 0; k < maxDoc; k++) {\n              byte norm = input != null ? input[k] : (byte)0;\n              if (!reader.isDeleted(k)) {\n                output.writeByte(norm);\n              }\n\t    }\n\t  }\n\t} finally {\n\t  output.close();\n\t}\n      }\n    }\n  }\n\n","sourceOld":"  private final int mergeNorms() throws IOException {\n    int docCount = 0;\n    for (int i = 0; i < fieldInfos.size(); i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.isIndexed) {\n\tOutputStream output = directory.createFile(segment + \".f\" + i);\n\ttry {\n\t  for (int j = 0; j < readers.size(); j++) {\n\t    IndexReader reader = (IndexReader)readers.elementAt(j);\n\t    byte[] input = reader.norms(fi.name);\n            int maxDoc = reader.maxDoc();\n            for (int k = 0; k < maxDoc; k++) {\n              byte norm = input != null ? input[k] : (byte)0;\n              if (!reader.isDeleted(k)) {\n                output.writeByte(norm);\n                docCount++;\n              }\n\t    }\n\t  }\n\t} finally {\n\t  output.close();\n\t}\n      }\n    }\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"15c469602973ef1a33c9a07367a380d278ffab20","date":1074206555,"type":3,"author":"Doug Cutting","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    for (int i = 0; i < fieldInfos.size(); i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.isIndexed) {\n\tOutputStream output = directory.createFile(segment + \".f\" + i);\n\ttry {\n\t  for (int j = 0; j < readers.size(); j++) {\n\t    IndexReader reader = (IndexReader)readers.elementAt(j);\n\t    byte[] input = reader.norms(fi.name);\n            int maxDoc = reader.maxDoc();\n            for (int k = 0; k < maxDoc; k++) {\n              byte norm = input != null ? input[k] : (byte)0;\n              if (!reader.isDeleted(k)) {\n                output.writeByte(norm);\n              }\n\t    }\n\t  }\n\t} finally {\n\t  output.close();\n\t}\n      }\n    }\n  }\n\n","sourceOld":"  private final void mergeNorms() throws IOException {\n    for (int i = 0; i < fieldInfos.size(); i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.isIndexed) {\n\tOutputStream output = directory.createFile(segment + \".f\" + i);\n\ttry {\n\t  for (int j = 0; j < readers.size(); j++) {\n\t    IndexReader reader = (IndexReader)readers.elementAt(j);\n\t    byte[] input = reader.norms(fi.name);\n            int maxDoc = reader.maxDoc();\n            for (int k = 0; k < maxDoc; k++) {\n              byte norm = input != null ? input[k] : (byte)0;\n              if (!reader.isDeleted(k)) {\n                output.writeByte(norm);\n              }\n\t    }\n\t  }\n\t} finally {\n\t  output.close();\n\t}\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"770281b8a8459cafcdd2354b6a06078fea2d83c9","date":1077308096,"type":3,"author":"Doug Cutting","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    for (int i = 0; i < fieldInfos.size(); i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.isIndexed) {\n        OutputStream output = directory.createFile(segment + \".f\" + i);\n        try {\n          for (int j = 0; j < readers.size(); j++) {\n            IndexReader reader = (IndexReader) readers.elementAt(j);\n            byte[] input = reader.norms(fi.name);\n            int maxDoc = reader.maxDoc();\n            for (int k = 0; k < maxDoc; k++) {\n              byte norm = input != null ? input[k] : (byte) 0;\n              if (!reader.isDeleted(k)) {\n                output.writeByte(norm);\n              }\n            }\n          }\n        } finally {\n          output.close();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    for (int i = 0; i < fieldInfos.size(); i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.isIndexed) {\n\tOutputStream output = directory.createFile(segment + \".f\" + i);\n\ttry {\n\t  for (int j = 0; j < readers.size(); j++) {\n\t    IndexReader reader = (IndexReader)readers.elementAt(j);\n\t    byte[] input = reader.norms(fi.name);\n            int maxDoc = reader.maxDoc();\n            for (int k = 0; k < maxDoc; k++) {\n              byte norm = input != null ? input[k] : (byte)0;\n              if (!reader.isDeleted(k)) {\n                output.writeByte(norm);\n              }\n\t    }\n\t  }\n\t} finally {\n\t  output.close();\n\t}\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"dda77265180d41bf85c84c995e25eda7b8e1b74d","date":1096395352,"type":3,"author":"Doug Cutting","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    for (int i = 0; i < fieldInfos.size(); i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.isIndexed) {\n        IndexOutput output = directory.createOutput(segment + \".f\" + i);\n        try {\n          for (int j = 0; j < readers.size(); j++) {\n            IndexReader reader = (IndexReader) readers.elementAt(j);\n            byte[] input = reader.norms(fi.name);\n            int maxDoc = reader.maxDoc();\n            for (int k = 0; k < maxDoc; k++) {\n              byte norm = input != null ? input[k] : (byte) 0;\n              if (!reader.isDeleted(k)) {\n                output.writeByte(norm);\n              }\n            }\n          }\n        } finally {\n          output.close();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    for (int i = 0; i < fieldInfos.size(); i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.isIndexed) {\n        OutputStream output = directory.createFile(segment + \".f\" + i);\n        try {\n          for (int j = 0; j < readers.size(); j++) {\n            IndexReader reader = (IndexReader) readers.elementAt(j);\n            byte[] input = reader.norms(fi.name);\n            int maxDoc = reader.maxDoc();\n            for (int k = 0; k < maxDoc; k++) {\n              byte norm = input != null ? input[k] : (byte) 0;\n              if (!reader.isDeleted(k)) {\n                output.writeByte(norm);\n              }\n            }\n          }\n        } finally {\n          output.close();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"43b80d9d07269dbaebb7a7696b780724bd46c177","date":1107544193,"type":3,"author":"Doug Cutting","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    for (int i = 0; i < fieldInfos.size(); i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.isIndexed) {\n        IndexOutput output = directory.createOutput(segment + \".f\" + i);\n        try {\n          for (int j = 0; j < readers.size(); j++) {\n            IndexReader reader = (IndexReader) readers.elementAt(j);\n            int maxDoc = reader.maxDoc();\n            byte[] input = new byte[maxDoc];\n            reader.norms(fi.name, input, 0);\n            for (int k = 0; k < maxDoc; k++) {\n              if (!reader.isDeleted(k)) {\n                output.writeByte(input[k]);\n              }\n            }\n          }\n        } finally {\n          output.close();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    for (int i = 0; i < fieldInfos.size(); i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.isIndexed) {\n        IndexOutput output = directory.createOutput(segment + \".f\" + i);\n        try {\n          for (int j = 0; j < readers.size(); j++) {\n            IndexReader reader = (IndexReader) readers.elementAt(j);\n            byte[] input = reader.norms(fi.name);\n            int maxDoc = reader.maxDoc();\n            for (int k = 0; k < maxDoc; k++) {\n              byte norm = input != null ? input[k] : (byte) 0;\n              if (!reader.isDeleted(k)) {\n                output.writeByte(norm);\n              }\n            }\n          }\n        } finally {\n          output.close();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"950f3c7592cb559e2534e5089c78833250e156a3","date":1130557968,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    for (int i = 0; i < fieldInfos.size(); i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.isIndexed && !fi.omitNorms) {\n        IndexOutput output = directory.createOutput(segment + \".f\" + i);\n        try {\n          for (int j = 0; j < readers.size(); j++) {\n            IndexReader reader = (IndexReader) readers.elementAt(j);\n            int maxDoc = reader.maxDoc();\n            byte[] input = new byte[maxDoc];\n            reader.norms(fi.name, input, 0);\n            for (int k = 0; k < maxDoc; k++) {\n              if (!reader.isDeleted(k)) {\n                output.writeByte(input[k]);\n              }\n            }\n          }\n        } finally {\n          output.close();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    for (int i = 0; i < fieldInfos.size(); i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.isIndexed) {\n        IndexOutput output = directory.createOutput(segment + \".f\" + i);\n        try {\n          for (int j = 0; j < readers.size(); j++) {\n            IndexReader reader = (IndexReader) readers.elementAt(j);\n            int maxDoc = reader.maxDoc();\n            byte[] input = new byte[maxDoc];\n            reader.norms(fi.name, input, 0);\n            for (int k = 0; k < maxDoc; k++) {\n              if (!reader.isDeleted(k)) {\n                output.writeByte(input[k]);\n              }\n            }\n          }\n        } finally {\n          output.close();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bcc1e326920283403c98c028a51787e36e348983","date":1165804709,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    byte[] normBuffer = null;\n    for (int i = 0; i < fieldInfos.size(); i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.isIndexed && !fi.omitNorms) {\n        IndexOutput output = directory.createOutput(segment + \".f\" + i);\n        try {\n          for (int j = 0; j < readers.size(); j++) {\n            IndexReader reader = (IndexReader) readers.elementAt(j);\n            int maxDoc = reader.maxDoc();\n            if (normBuffer == null || normBuffer.length < maxDoc) {\n              // the buffer is too small for the current segment\n              normBuffer = new byte[maxDoc];\n            }\n            reader.norms(fi.name, normBuffer, 0);\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              for (int k = 0; k < maxDoc; k++) {\n                if (!reader.isDeleted(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n          }\n        } finally {\n          output.close();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    for (int i = 0; i < fieldInfos.size(); i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.isIndexed && !fi.omitNorms) {\n        IndexOutput output = directory.createOutput(segment + \".f\" + i);\n        try {\n          for (int j = 0; j < readers.size(); j++) {\n            IndexReader reader = (IndexReader) readers.elementAt(j);\n            int maxDoc = reader.maxDoc();\n            byte[] input = new byte[maxDoc];\n            reader.norms(fi.name, input, 0);\n            for (int k = 0; k < maxDoc; k++) {\n              if (!reader.isDeleted(k)) {\n                output.writeByte(input[k]);\n              }\n            }\n          }\n        } finally {\n          output.close();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["406e7055a3e99d3fa6ce49a555a51dd18b321806"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8969a184df55d25d61e85be785987fbf830d4028","date":1168143561,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    byte[] normBuffer = null;\n    IndexOutput output = null;\n    try {\n      for (int i = 0; i < fieldInfos.size(); i++) {\n        FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(segment + \".\" + IndexFileNames.NORMS_EXTENSION);\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for (int j = 0; j < readers.size(); j++) {\n            IndexReader reader = (IndexReader) readers.elementAt(j);\n            int maxDoc = reader.maxDoc();\n            if (normBuffer == null || normBuffer.length < maxDoc) {\n              // the buffer is too small for the current segment\n              normBuffer = new byte[maxDoc];\n            }\n            reader.norms(fi.name, normBuffer, 0);\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              for (int k = 0; k < maxDoc; k++) {\n                if (!reader.isDeleted(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    byte[] normBuffer = null;\n    for (int i = 0; i < fieldInfos.size(); i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.isIndexed && !fi.omitNorms) {\n        IndexOutput output = directory.createOutput(segment + \".f\" + i);\n        try {\n          for (int j = 0; j < readers.size(); j++) {\n            IndexReader reader = (IndexReader) readers.elementAt(j);\n            int maxDoc = reader.maxDoc();\n            if (normBuffer == null || normBuffer.length < maxDoc) {\n              // the buffer is too small for the current segment\n              normBuffer = new byte[maxDoc];\n            }\n            reader.norms(fi.name, normBuffer, 0);\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              for (int k = 0; k < maxDoc; k++) {\n                if (!reader.isDeleted(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n          }\n        } finally {\n          output.close();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"346d5897e4c4e77ed5dbd31f7730ff30973d5971","date":1198317988,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    byte[] normBuffer = null;\n    IndexOutput output = null;\n    try {\n      for (int i = 0; i < fieldInfos.size(); i++) {\n        FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(segment + \".\" + IndexFileNames.NORMS_EXTENSION);\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for (int j = 0; j < readers.size(); j++) {\n            IndexReader reader = (IndexReader) readers.elementAt(j);\n            int maxDoc = reader.maxDoc();\n            if (normBuffer == null || normBuffer.length < maxDoc) {\n              // the buffer is too small for the current segment\n              normBuffer = new byte[maxDoc];\n            }\n            reader.norms(fi.name, normBuffer, 0);\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              for (int k = 0; k < maxDoc; k++) {\n                if (!reader.isDeleted(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            if (checkAbort != null)\n              checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    byte[] normBuffer = null;\n    IndexOutput output = null;\n    try {\n      for (int i = 0; i < fieldInfos.size(); i++) {\n        FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(segment + \".\" + IndexFileNames.NORMS_EXTENSION);\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for (int j = 0; j < readers.size(); j++) {\n            IndexReader reader = (IndexReader) readers.elementAt(j);\n            int maxDoc = reader.maxDoc();\n            if (normBuffer == null || normBuffer.length < maxDoc) {\n              // the buffer is too small for the current segment\n              normBuffer = new byte[maxDoc];\n            }\n            reader.norms(fi.name, normBuffer, 0);\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              for (int k = 0; k < maxDoc; k++) {\n                if (!reader.isDeleted(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c7b6cdc70e097da94da79a655ed8f94477ff69f5","date":1220815360,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    byte[] normBuffer = null;\n    IndexOutput output = null;\n    try {\n      for (int i = 0; i < fieldInfos.size(); i++) {\n        FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(segment + \".\" + IndexFileNames.NORMS_EXTENSION);\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for (int j = 0; j < readers.size(); j++) {\n            IndexReader reader = (IndexReader) readers.get(j);\n            int maxDoc = reader.maxDoc();\n            if (normBuffer == null || normBuffer.length < maxDoc) {\n              // the buffer is too small for the current segment\n              normBuffer = new byte[maxDoc];\n            }\n            reader.norms(fi.name, normBuffer, 0);\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              for (int k = 0; k < maxDoc; k++) {\n                if (!reader.isDeleted(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            if (checkAbort != null)\n              checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    byte[] normBuffer = null;\n    IndexOutput output = null;\n    try {\n      for (int i = 0; i < fieldInfos.size(); i++) {\n        FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(segment + \".\" + IndexFileNames.NORMS_EXTENSION);\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for (int j = 0; j < readers.size(); j++) {\n            IndexReader reader = (IndexReader) readers.elementAt(j);\n            int maxDoc = reader.maxDoc();\n            if (normBuffer == null || normBuffer.length < maxDoc) {\n              // the buffer is too small for the current segment\n              normBuffer = new byte[maxDoc];\n            }\n            reader.norms(fi.name, normBuffer, 0);\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              for (int k = 0; k < maxDoc; k++) {\n                if (!reader.isDeleted(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            if (checkAbort != null)\n              checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d736930237c54e1516a9e3bae803c92ff19ec4e5","date":1245789156,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    byte[] normBuffer = null;\n    IndexOutput output = null;\n    try {\n      int numFieldInfos = fieldInfos.size();\n      for (int i = 0; i < numFieldInfos; i++) {\n        FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(segment + \".\" + IndexFileNames.NORMS_EXTENSION);\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for (Iterator iter = readers.iterator(); iter.hasNext();) {\n            IndexReader reader = (IndexReader) iter.next();\n            int maxDoc = reader.maxDoc();\n            if (normBuffer == null || normBuffer.length < maxDoc) {\n              // the buffer is too small for the current segment\n              normBuffer = new byte[maxDoc];\n            }\n            reader.norms(fi.name, normBuffer, 0);\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              for (int k = 0; k < maxDoc; k++) {\n                if (!reader.isDeleted(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    byte[] normBuffer = null;\n    IndexOutput output = null;\n    try {\n      for (int i = 0; i < fieldInfos.size(); i++) {\n        FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(segment + \".\" + IndexFileNames.NORMS_EXTENSION);\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for (int j = 0; j < readers.size(); j++) {\n            IndexReader reader = (IndexReader) readers.get(j);\n            int maxDoc = reader.maxDoc();\n            if (normBuffer == null || normBuffer.length < maxDoc) {\n              // the buffer is too small for the current segment\n              normBuffer = new byte[maxDoc];\n            }\n            reader.norms(fi.name, normBuffer, 0);\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              for (int k = 0; k < maxDoc; k++) {\n                if (!reader.isDeleted(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            if (checkAbort != null)\n              checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fa27b750ee9a51ec4bed93ef328aef9ca1e2153d","date":1255859449,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    byte[] normBuffer = null;\n    IndexOutput output = null;\n    try {\n      int numFieldInfos = fieldInfos.size();\n      for (int i = 0; i < numFieldInfos; i++) {\n        FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(segment + \".\" + IndexFileNames.NORMS_EXTENSION);\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for ( IndexReader reader : readers) {\n            int maxDoc = reader.maxDoc();\n            if (normBuffer == null || normBuffer.length < maxDoc) {\n              // the buffer is too small for the current segment\n              normBuffer = new byte[maxDoc];\n            }\n            reader.norms(fi.name, normBuffer, 0);\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              for (int k = 0; k < maxDoc; k++) {\n                if (!reader.isDeleted(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    byte[] normBuffer = null;\n    IndexOutput output = null;\n    try {\n      int numFieldInfos = fieldInfos.size();\n      for (int i = 0; i < numFieldInfos; i++) {\n        FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(segment + \".\" + IndexFileNames.NORMS_EXTENSION);\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for (Iterator iter = readers.iterator(); iter.hasNext();) {\n            IndexReader reader = (IndexReader) iter.next();\n            int maxDoc = reader.maxDoc();\n            if (normBuffer == null || normBuffer.length < maxDoc) {\n              // the buffer is too small for the current segment\n              normBuffer = new byte[maxDoc];\n            }\n            reader.norms(fi.name, normBuffer, 0);\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              for (int k = 0; k < maxDoc; k++) {\n                if (!reader.isDeleted(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"775efee7f959e0dd3df7960b93767d9e00b78751","date":1267203159,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    byte[] normBuffer = null;\n    IndexOutput output = null;\n    try {\n      int numFieldInfos = fieldInfos.size();\n      for (int i = 0; i < numFieldInfos; i++) {\n        FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for ( IndexReader reader : readers) {\n            int maxDoc = reader.maxDoc();\n            if (normBuffer == null || normBuffer.length < maxDoc) {\n              // the buffer is too small for the current segment\n              normBuffer = new byte[maxDoc];\n            }\n            reader.norms(fi.name, normBuffer, 0);\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              for (int k = 0; k < maxDoc; k++) {\n                if (!reader.isDeleted(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    byte[] normBuffer = null;\n    IndexOutput output = null;\n    try {\n      int numFieldInfos = fieldInfos.size();\n      for (int i = 0; i < numFieldInfos; i++) {\n        FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(segment + \".\" + IndexFileNames.NORMS_EXTENSION);\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for ( IndexReader reader : readers) {\n            int maxDoc = reader.maxDoc();\n            if (normBuffer == null || normBuffer.length < maxDoc) {\n              // the buffer is too small for the current segment\n              normBuffer = new byte[maxDoc];\n            }\n            reader.norms(fi.name, normBuffer, 0);\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              for (int k = 0; k < maxDoc; k++) {\n                if (!reader.isDeleted(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    byte[] normBuffer = null;\n    IndexOutput output = null;\n    try {\n      int numFieldInfos = fieldInfos.size();\n      for (int i = 0; i < numFieldInfos; i++) {\n        FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for ( IndexReader reader : readers) {\n            int maxDoc = reader.maxDoc();\n            if (normBuffer == null || normBuffer.length < maxDoc) {\n              // the buffer is too small for the current segment\n              normBuffer = new byte[maxDoc];\n            }\n            reader.norms(fi.name, normBuffer, 0);\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              for (int k = 0; k < maxDoc; k++) {\n                if (!reader.isDeleted(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    byte[] normBuffer = null;\n    IndexOutput output = null;\n    try {\n      int numFieldInfos = fieldInfos.size();\n      for (int i = 0; i < numFieldInfos; i++) {\n        FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for ( IndexReader reader : readers) {\n            int maxDoc = reader.maxDoc();\n            if (normBuffer == null || normBuffer.length < maxDoc) {\n              // the buffer is too small for the current segment\n              normBuffer = new byte[maxDoc];\n            }\n            reader.norms(fi.name, normBuffer, 0);\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              for (int k = 0; k < maxDoc; k++) {\n                if (!reader.isDeleted(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"346d5897e4c4e77ed5dbd31f7730ff30973d5971":["8969a184df55d25d61e85be785987fbf830d4028"],"91109046a59c58ee0ee5d0d2767b08d1f30d6702":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"950f3c7592cb559e2534e5089c78833250e156a3":["43b80d9d07269dbaebb7a7696b780724bd46c177"],"e7fb6d70db034a5456ae560175dd1b821eea9ff4":["8fb95844e4ba5160067c64c5eb1cd8a09f7a94f1"],"775efee7f959e0dd3df7960b93767d9e00b78751":["fa27b750ee9a51ec4bed93ef328aef9ca1e2153d"],"d736930237c54e1516a9e3bae803c92ff19ec4e5":["c7b6cdc70e097da94da79a655ed8f94477ff69f5"],"c7b6cdc70e097da94da79a655ed8f94477ff69f5":["346d5897e4c4e77ed5dbd31f7730ff30973d5971"],"bcc1e326920283403c98c028a51787e36e348983":["950f3c7592cb559e2534e5089c78833250e156a3"],"8fb95844e4ba5160067c64c5eb1cd8a09f7a94f1":["91109046a59c58ee0ee5d0d2767b08d1f30d6702"],"770281b8a8459cafcdd2354b6a06078fea2d83c9":["15c469602973ef1a33c9a07367a380d278ffab20"],"fa27b750ee9a51ec4bed93ef328aef9ca1e2153d":["d736930237c54e1516a9e3bae803c92ff19ec4e5"],"dda77265180d41bf85c84c995e25eda7b8e1b74d":["770281b8a8459cafcdd2354b6a06078fea2d83c9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"f22ffa72cdb7463ca3e818b15f8fb8b0330e7cf5":["e7fb6d70db034a5456ae560175dd1b821eea9ff4"],"43b80d9d07269dbaebb7a7696b780724bd46c177":["dda77265180d41bf85c84c995e25eda7b8e1b74d"],"8969a184df55d25d61e85be785987fbf830d4028":["bcc1e326920283403c98c028a51787e36e348983"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["775efee7f959e0dd3df7960b93767d9e00b78751"],"15c469602973ef1a33c9a07367a380d278ffab20":["f22ffa72cdb7463ca3e818b15f8fb8b0330e7cf5"]},"commit2Childs":{"346d5897e4c4e77ed5dbd31f7730ff30973d5971":["c7b6cdc70e097da94da79a655ed8f94477ff69f5"],"91109046a59c58ee0ee5d0d2767b08d1f30d6702":["8fb95844e4ba5160067c64c5eb1cd8a09f7a94f1"],"950f3c7592cb559e2534e5089c78833250e156a3":["bcc1e326920283403c98c028a51787e36e348983"],"e7fb6d70db034a5456ae560175dd1b821eea9ff4":["f22ffa72cdb7463ca3e818b15f8fb8b0330e7cf5"],"775efee7f959e0dd3df7960b93767d9e00b78751":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"d736930237c54e1516a9e3bae803c92ff19ec4e5":["fa27b750ee9a51ec4bed93ef328aef9ca1e2153d"],"8fb95844e4ba5160067c64c5eb1cd8a09f7a94f1":["e7fb6d70db034a5456ae560175dd1b821eea9ff4"],"c7b6cdc70e097da94da79a655ed8f94477ff69f5":["d736930237c54e1516a9e3bae803c92ff19ec4e5"],"bcc1e326920283403c98c028a51787e36e348983":["8969a184df55d25d61e85be785987fbf830d4028"],"770281b8a8459cafcdd2354b6a06078fea2d83c9":["dda77265180d41bf85c84c995e25eda7b8e1b74d"],"fa27b750ee9a51ec4bed93ef328aef9ca1e2153d":["775efee7f959e0dd3df7960b93767d9e00b78751"],"dda77265180d41bf85c84c995e25eda7b8e1b74d":["43b80d9d07269dbaebb7a7696b780724bd46c177"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["91109046a59c58ee0ee5d0d2767b08d1f30d6702"],"f22ffa72cdb7463ca3e818b15f8fb8b0330e7cf5":["15c469602973ef1a33c9a07367a380d278ffab20"],"43b80d9d07269dbaebb7a7696b780724bd46c177":["950f3c7592cb559e2534e5089c78833250e156a3"],"8969a184df55d25d61e85be785987fbf830d4028":["346d5897e4c4e77ed5dbd31f7730ff30973d5971"],"15c469602973ef1a33c9a07367a380d278ffab20":["770281b8a8459cafcdd2354b6a06078fea2d83c9"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}