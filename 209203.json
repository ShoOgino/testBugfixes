{"path":"lucene/core/src/test/org/apache/lucene/codecs/compressing/TestCompressingTermVectorsFormat#testChunkCleanup().mjava","commits":[{"id":"b88448324d3a96c5842455dabea63450b697b58f","date":1421779050,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/codecs/compressing/TestCompressingTermVectorsFormat#testChunkCleanup().mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * writes some tiny segments with incomplete compressed blocks,\n   * and ensures merge recompresses them.\n   */\n  public void testChunkCleanup() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMergePolicy(NoMergePolicy.INSTANCE);\n    \n    // we have to enforce certain things like maxDocsPerChunk to cause dirty chunks to be created\n    // by this test.\n    iwConf.setCodec(CompressingCodec.randomInstance(random(), 4*1024, 100, false, 8));\n    IndexWriter iw = new IndexWriter(dir, iwConf);\n    DirectoryReader ir = DirectoryReader.open(iw, true);\n    for (int i = 0; i < 5; i++) {\n      Document doc = new Document();\n      FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n      ft.setStoreTermVectors(true);\n      doc.add(new Field(\"text\", \"not very long at all\", ft));\n      iw.addDocument(doc);\n      // force flush\n      DirectoryReader ir2 = DirectoryReader.openIfChanged(ir);\n      assertNotNull(ir2);\n      ir.close();\n      ir = ir2;\n      // examine dirty counts:\n      for (LeafReaderContext leaf : ir2.leaves()) {\n        CodecReader sr = (CodecReader) leaf.reader();\n        CompressingTermVectorsReader reader = (CompressingTermVectorsReader)sr.getTermVectorsReader();\n        assertEquals(1, reader.getNumChunks());\n        assertEquals(1, reader.getNumDirtyChunks());\n      }\n    }\n    iw.getConfig().setMergePolicy(newLogMergePolicy());\n    iw.forceMerge(1);\n    DirectoryReader ir2 = DirectoryReader.openIfChanged(ir);\n    assertNotNull(ir2);\n    ir.close();\n    ir = ir2;\n    CodecReader sr = getOnlySegmentReader(ir);\n    CompressingTermVectorsReader reader = (CompressingTermVectorsReader)sr.getTermVectorsReader();\n    // we could get lucky, and have zero, but typically one.\n    assertTrue(reader.getNumDirtyChunks() <= 1);\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b52491e71f0d5d0f0160d6ed0d39e0dd661be68a","date":1429550638,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/test/org/apache/lucene/codecs/compressing/TestCompressingTermVectorsFormat#testChunkCleanup().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/codecs/compressing/TestCompressingTermVectorsFormat#testChunkCleanup().mjava","sourceNew":"  /**\n   * writes some tiny segments with incomplete compressed blocks,\n   * and ensures merge recompresses them.\n   */\n  public void testChunkCleanup() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMergePolicy(NoMergePolicy.INSTANCE);\n    \n    // we have to enforce certain things like maxDocsPerChunk to cause dirty chunks to be created\n    // by this test.\n    iwConf.setCodec(CompressingCodec.randomInstance(random(), 4*1024, 100, false, 8));\n    IndexWriter iw = new IndexWriter(dir, iwConf);\n    DirectoryReader ir = DirectoryReader.open(iw, true);\n    for (int i = 0; i < 5; i++) {\n      Document doc = new Document();\n      FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n      ft.setStoreTermVectors(true);\n      doc.add(new Field(\"text\", \"not very long at all\", ft));\n      iw.addDocument(doc);\n      // force flush\n      DirectoryReader ir2 = DirectoryReader.openIfChanged(ir);\n      assertNotNull(ir2);\n      ir.close();\n      ir = ir2;\n      // examine dirty counts:\n      for (LeafReaderContext leaf : ir2.leaves()) {\n        CodecReader sr = (CodecReader) leaf.reader();\n        CompressingTermVectorsReader reader = (CompressingTermVectorsReader)sr.getTermVectorsReader();\n        assertEquals(1, reader.getNumChunks());\n        assertEquals(1, reader.getNumDirtyChunks());\n      }\n    }\n    iw.getConfig().setMergePolicy(newLogMergePolicy());\n    iw.forceMerge(1);\n    DirectoryReader ir2 = DirectoryReader.openIfChanged(ir);\n    assertNotNull(ir2);\n    ir.close();\n    ir = ir2;\n    CodecReader sr = getOnlySegmentReader(ir);\n    CompressingTermVectorsReader reader = (CompressingTermVectorsReader)sr.getTermVectorsReader();\n    // we could get lucky, and have zero, but typically one.\n    assertTrue(reader.getNumDirtyChunks() <= 1);\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * writes some tiny segments with incomplete compressed blocks,\n   * and ensures merge recompresses them.\n   */\n  public void testChunkCleanup() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMergePolicy(NoMergePolicy.INSTANCE);\n    \n    // we have to enforce certain things like maxDocsPerChunk to cause dirty chunks to be created\n    // by this test.\n    iwConf.setCodec(CompressingCodec.randomInstance(random(), 4*1024, 100, false, 8));\n    IndexWriter iw = new IndexWriter(dir, iwConf);\n    DirectoryReader ir = DirectoryReader.open(iw, true);\n    for (int i = 0; i < 5; i++) {\n      Document doc = new Document();\n      FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n      ft.setStoreTermVectors(true);\n      doc.add(new Field(\"text\", \"not very long at all\", ft));\n      iw.addDocument(doc);\n      // force flush\n      DirectoryReader ir2 = DirectoryReader.openIfChanged(ir);\n      assertNotNull(ir2);\n      ir.close();\n      ir = ir2;\n      // examine dirty counts:\n      for (LeafReaderContext leaf : ir2.leaves()) {\n        CodecReader sr = (CodecReader) leaf.reader();\n        CompressingTermVectorsReader reader = (CompressingTermVectorsReader)sr.getTermVectorsReader();\n        assertEquals(1, reader.getNumChunks());\n        assertEquals(1, reader.getNumDirtyChunks());\n      }\n    }\n    iw.getConfig().setMergePolicy(newLogMergePolicy());\n    iw.forceMerge(1);\n    DirectoryReader ir2 = DirectoryReader.openIfChanged(ir);\n    assertNotNull(ir2);\n    ir.close();\n    ir = ir2;\n    CodecReader sr = getOnlySegmentReader(ir);\n    CompressingTermVectorsReader reader = (CompressingTermVectorsReader)sr.getTermVectorsReader();\n    // we could get lucky, and have zero, but typically one.\n    assertTrue(reader.getNumDirtyChunks() <= 1);\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"b88448324d3a96c5842455dabea63450b697b58f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b52491e71f0d5d0f0160d6ed0d39e0dd661be68a":["b88448324d3a96c5842455dabea63450b697b58f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b52491e71f0d5d0f0160d6ed0d39e0dd661be68a"]},"commit2Childs":{"b88448324d3a96c5842455dabea63450b697b58f":["b52491e71f0d5d0f0160d6ed0d39e0dd661be68a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b88448324d3a96c5842455dabea63450b697b58f"],"b52491e71f0d5d0f0160d6ed0d39e0dd661be68a":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}