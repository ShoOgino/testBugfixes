{"path":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","commits":[{"id":"95ae76773bf2b95987d5f9c8f566ab3738953fb4","date":1301758351,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#uninvert(SolrIndexSearcher).mjava","sourceNew":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : new BytesRef(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator();\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seek(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    DocsEnum docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !t.startsWith(termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(delDocs, docsEnum);\n\n        final DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int chunk = docsEnum.read();\n          if (chunk <= 0) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF += chunk;\n\n          for (int i=0; i<chunk; i++) {\n            termInstances++;\n            int doc = bulkResult.docs.ints[i];\n            //System.out.println(\"    docID=\" + doc);\n            // add TNUM_OFFSET to the term number to make room for special reserved values:\n            // 0 (end term) and 1 (index into byte array follows)\n            int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n            lastTerm[doc] = termNum;\n            int val = index[doc];\n\n            if ((val & 0xff)==1) {\n              // index into byte array (actually the end of\n              // the doc-specific byte[] when building)\n              int pos = val >>> 8;\n              int ilen = vIntSize(delta);\n              byte[] arr = bytes[doc];\n              int newend = pos+ilen;\n              if (newend > arr.length) {\n                // We avoid a doubling strategy to lower memory usage.\n                // this faceting method isn't for docs with many terms.\n                // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n                // TODO: figure out what array lengths we can round up to w/o actually using more memory\n                // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n                // It should be safe to round up to the nearest 32 bits in any case.\n                int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n                byte[] newarr = new byte[newLen];\n                System.arraycopy(arr, 0, newarr, 0, pos);\n                arr = newarr;\n                bytes[doc] = newarr;\n              }\n              pos = writeInt(delta, arr, pos);\n              index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n            } else {\n              // OK, this int has data in it... find the end (a zero starting byte - not\n              // part of another number, hence not following a byte with the high bit set).\n              int ipos;\n              if (val==0) {\n                ipos=0;\n              } else if ((val & 0x0000ff80)==0) {\n                ipos=1;\n              } else if ((val & 0x00ff8000)==0) {\n                ipos=2;\n              } else if ((val & 0xff800000)==0) {\n                ipos=3;\n              } else {\n                ipos=4;\n              }\n\n              //System.out.println(\"      ipos=\" + ipos);\n\n              int endPos = writeInt(delta, tempArr, ipos);\n              //System.out.println(\"      endpos=\" + endPos);\n              if (endPos <= 4) {\n                //System.out.println(\"      fits!\");\n                // value will fit in the integer... move bytes back\n                for (int j=ipos; j<endPos; j++) {\n                  val |= (tempArr[j] & 0xff) << (j<<3);\n                }\n                index[doc] = val;\n              } else {\n                // value won't fit... move integer into byte[]\n                for (int j=0; j<ipos; j++) {\n                  tempArr[j] = (byte)val;\n                  val >>>=8;\n                }\n                // point at the end index in the byte[]\n                index[doc] = (endPos<<8) | 1;\n                bytes[doc] = tempArr;\n                tempArr = new byte[12];\n              }\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","sourceOld":"  private void uninvert(SolrIndexSearcher searcher) throws IOException {\n    long startTime = System.currentTimeMillis();\n\n    IndexReader reader = searcher.getIndexReader();\n    int maxDoc = reader.maxDoc();\n\n    int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    this.index = index;\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n    maxTermCounts = new int[1024];\n\n    NumberedTermsEnum te = ti.getEnumerator(reader);\n\n    // threshold, over which we use set intersections instead of counting\n    // to (1) save memory, and (2) speed up faceting.\n    // Add 2 for testing purposes so that there will always be some terms under\n    // the threshold even when the index is very small.\n    int threshold = maxDoc / 20 + 2;\n    // threshold = 2000000000; //////////////////////////////// USE FOR TESTING\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    SolrIndexSearcher.DocsEnumState deState = null;\n\n    for (;;) {\n      BytesRef t = te.term();\n      if (t==null) break;\n\n      int termNum = te.getTermNumber();\n\n      if (termNum >= maxTermCounts.length) {\n        // resize by doubling - for very large number of unique terms, expanding\n        // by 4K and resultant GC will dominate uninvert times.  Resize at end if material\n        int[] newMaxTermCounts = new int[maxTermCounts.length*2];\n        System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, termNum);\n        maxTermCounts = newMaxTermCounts;\n      }\n\n      int df = te.docFreq();\n      if (df >= threshold) {\n        TopTerm topTerm = new TopTerm();\n        topTerm.term = new BytesRef(t);\n        topTerm.termNum = termNum;\n        bigTerms.put(topTerm.termNum, topTerm);\n\n        if (deState == null) {\n          deState = new SolrIndexSearcher.DocsEnumState();\n          deState.termsEnum = te.tenum;\n          deState.reuse = te.docsEnum;\n        }\n        DocSet set = searcher.getDocSet(new TermQuery(new Term(ti.field, topTerm.term)), deState);\n        te.docsEnum = deState.reuse;\n\n        maxTermCounts[termNum] = set.size();\n\n        te.next();\n        continue;\n      }\n\n      termsInverted++;\n\n      DocsEnum docsEnum = te.getDocsEnum();\n\n      DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n      for(;;) {\n        int n = docsEnum.read();\n        if (n <= 0) break;\n\n        maxTermCounts[termNum] += n;\n\n        for (int i=0; i<n; i++) {\n          termInstances++;\n          int doc = bulkResult.docs.ints[i];\n          // add 2 to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            if (endPos <= 4) {\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n\n          }\n\n        }\n\n      }\n\n      te.next();\n    }\n\n    numTermsInField = te.getTermNumber();\n    te.close();\n\n    // free space if outrageously wasteful (tradeoff memory/cpu) \n\n    if ((maxTermCounts.length - numTermsInField) > 1024) { // too much waste!\n      int[] newMaxTermCounts = new int[numTermsInField];\n      System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, numTermsInField);\n      maxTermCounts = newMaxTermCounts;\n   }\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      index = this.index = null;\n      tnums = null;\n    } else {\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n          if (target.length > (1<<24)*.9) {\n            SolrCore.log.warn(\"Approaching too many values for UnInvertedField faceting on field '\"+field+\"' : bucket size=\" + target.length);\n          }\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n\n    SolrCore.log.info(\"UnInverted multi-valued field \" + toString());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"45669a651c970812a680841b97a77cce06af559f","date":1301922222,"type":1,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#uninvert(SolrIndexSearcher).mjava","sourceNew":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : new BytesRef(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator();\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seek(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    DocsEnum docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !t.startsWith(termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(delDocs, docsEnum);\n\n        final DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int chunk = docsEnum.read();\n          if (chunk <= 0) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF += chunk;\n\n          for (int i=0; i<chunk; i++) {\n            termInstances++;\n            int doc = bulkResult.docs.ints[i];\n            //System.out.println(\"    docID=\" + doc);\n            // add TNUM_OFFSET to the term number to make room for special reserved values:\n            // 0 (end term) and 1 (index into byte array follows)\n            int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n            lastTerm[doc] = termNum;\n            int val = index[doc];\n\n            if ((val & 0xff)==1) {\n              // index into byte array (actually the end of\n              // the doc-specific byte[] when building)\n              int pos = val >>> 8;\n              int ilen = vIntSize(delta);\n              byte[] arr = bytes[doc];\n              int newend = pos+ilen;\n              if (newend > arr.length) {\n                // We avoid a doubling strategy to lower memory usage.\n                // this faceting method isn't for docs with many terms.\n                // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n                // TODO: figure out what array lengths we can round up to w/o actually using more memory\n                // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n                // It should be safe to round up to the nearest 32 bits in any case.\n                int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n                byte[] newarr = new byte[newLen];\n                System.arraycopy(arr, 0, newarr, 0, pos);\n                arr = newarr;\n                bytes[doc] = newarr;\n              }\n              pos = writeInt(delta, arr, pos);\n              index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n            } else {\n              // OK, this int has data in it... find the end (a zero starting byte - not\n              // part of another number, hence not following a byte with the high bit set).\n              int ipos;\n              if (val==0) {\n                ipos=0;\n              } else if ((val & 0x0000ff80)==0) {\n                ipos=1;\n              } else if ((val & 0x00ff8000)==0) {\n                ipos=2;\n              } else if ((val & 0xff800000)==0) {\n                ipos=3;\n              } else {\n                ipos=4;\n              }\n\n              //System.out.println(\"      ipos=\" + ipos);\n\n              int endPos = writeInt(delta, tempArr, ipos);\n              //System.out.println(\"      endpos=\" + endPos);\n              if (endPos <= 4) {\n                //System.out.println(\"      fits!\");\n                // value will fit in the integer... move bytes back\n                for (int j=ipos; j<endPos; j++) {\n                  val |= (tempArr[j] & 0xff) << (j<<3);\n                }\n                index[doc] = val;\n              } else {\n                // value won't fit... move integer into byte[]\n                for (int j=0; j<ipos; j++) {\n                  tempArr[j] = (byte)val;\n                  val >>>=8;\n                }\n                // point at the end index in the byte[]\n                index[doc] = (endPos<<8) | 1;\n                bytes[doc] = tempArr;\n                tempArr = new byte[12];\n              }\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","sourceOld":"  private void uninvert(SolrIndexSearcher searcher) throws IOException {\n    long startTime = System.currentTimeMillis();\n\n    IndexReader reader = searcher.getIndexReader();\n    int maxDoc = reader.maxDoc();\n\n    int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    this.index = index;\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n    maxTermCounts = new int[1024];\n\n    NumberedTermsEnum te = ti.getEnumerator(reader);\n\n    // threshold, over which we use set intersections instead of counting\n    // to (1) save memory, and (2) speed up faceting.\n    // Add 2 for testing purposes so that there will always be some terms under\n    // the threshold even when the index is very small.\n    int threshold = maxDoc / 20 + 2;\n    // threshold = 2000000000; //////////////////////////////// USE FOR TESTING\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    SolrIndexSearcher.DocsEnumState deState = null;\n\n    for (;;) {\n      BytesRef t = te.term();\n      if (t==null) break;\n\n      int termNum = te.getTermNumber();\n\n      if (termNum >= maxTermCounts.length) {\n        // resize by doubling - for very large number of unique terms, expanding\n        // by 4K and resultant GC will dominate uninvert times.  Resize at end if material\n        int[] newMaxTermCounts = new int[maxTermCounts.length*2];\n        System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, termNum);\n        maxTermCounts = newMaxTermCounts;\n      }\n\n      int df = te.docFreq();\n      if (df >= threshold) {\n        TopTerm topTerm = new TopTerm();\n        topTerm.term = new BytesRef(t);\n        topTerm.termNum = termNum;\n        bigTerms.put(topTerm.termNum, topTerm);\n\n        if (deState == null) {\n          deState = new SolrIndexSearcher.DocsEnumState();\n          deState.termsEnum = te.tenum;\n          deState.reuse = te.docsEnum;\n        }\n        DocSet set = searcher.getDocSet(new TermQuery(new Term(ti.field, topTerm.term)), deState);\n        te.docsEnum = deState.reuse;\n\n        maxTermCounts[termNum] = set.size();\n\n        te.next();\n        continue;\n      }\n\n      termsInverted++;\n\n      DocsEnum docsEnum = te.getDocsEnum();\n\n      DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n      for(;;) {\n        int n = docsEnum.read();\n        if (n <= 0) break;\n\n        maxTermCounts[termNum] += n;\n\n        for (int i=0; i<n; i++) {\n          termInstances++;\n          int doc = bulkResult.docs.ints[i];\n          // add 2 to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            if (endPos <= 4) {\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n\n          }\n\n        }\n\n      }\n\n      te.next();\n    }\n\n    numTermsInField = te.getTermNumber();\n    te.close();\n\n    // free space if outrageously wasteful (tradeoff memory/cpu) \n\n    if ((maxTermCounts.length - numTermsInField) > 1024) { // too much waste!\n      int[] newMaxTermCounts = new int[numTermsInField];\n      System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, numTermsInField);\n      maxTermCounts = newMaxTermCounts;\n   }\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      index = this.index = null;\n      tnums = null;\n    } else {\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n          if (target.length > (1<<24)*.9) {\n            SolrCore.log.warn(\"Approaching too many values for UnInvertedField faceting on field '\"+field+\"' : bucket size=\" + target.length);\n          }\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n\n    SolrCore.log.info(\"UnInverted multi-valued field \" + toString());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f20bb72b0dfa147c6f1fcd7693102c63a2714eae","date":1303767270,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","sourceNew":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : new BytesRef(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator();\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seek(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !t.startsWith(termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(delDocs, docsEnum);\n\n        final DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int chunk = docsEnum.read();\n          if (chunk <= 0) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF += chunk;\n\n          for (int i=0; i<chunk; i++) {\n            termInstances++;\n            int doc = bulkResult.docs.ints[i];\n            //System.out.println(\"    docID=\" + doc);\n            // add TNUM_OFFSET to the term number to make room for special reserved values:\n            // 0 (end term) and 1 (index into byte array follows)\n            int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n            lastTerm[doc] = termNum;\n            int val = index[doc];\n\n            if ((val & 0xff)==1) {\n              // index into byte array (actually the end of\n              // the doc-specific byte[] when building)\n              int pos = val >>> 8;\n              int ilen = vIntSize(delta);\n              byte[] arr = bytes[doc];\n              int newend = pos+ilen;\n              if (newend > arr.length) {\n                // We avoid a doubling strategy to lower memory usage.\n                // this faceting method isn't for docs with many terms.\n                // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n                // TODO: figure out what array lengths we can round up to w/o actually using more memory\n                // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n                // It should be safe to round up to the nearest 32 bits in any case.\n                int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n                byte[] newarr = new byte[newLen];\n                System.arraycopy(arr, 0, newarr, 0, pos);\n                arr = newarr;\n                bytes[doc] = newarr;\n              }\n              pos = writeInt(delta, arr, pos);\n              index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n            } else {\n              // OK, this int has data in it... find the end (a zero starting byte - not\n              // part of another number, hence not following a byte with the high bit set).\n              int ipos;\n              if (val==0) {\n                ipos=0;\n              } else if ((val & 0x0000ff80)==0) {\n                ipos=1;\n              } else if ((val & 0x00ff8000)==0) {\n                ipos=2;\n              } else if ((val & 0xff800000)==0) {\n                ipos=3;\n              } else {\n                ipos=4;\n              }\n\n              //System.out.println(\"      ipos=\" + ipos);\n\n              int endPos = writeInt(delta, tempArr, ipos);\n              //System.out.println(\"      endpos=\" + endPos);\n              if (endPos <= 4) {\n                //System.out.println(\"      fits!\");\n                // value will fit in the integer... move bytes back\n                for (int j=ipos; j<endPos; j++) {\n                  val |= (tempArr[j] & 0xff) << (j<<3);\n                }\n                index[doc] = val;\n              } else {\n                // value won't fit... move integer into byte[]\n                for (int j=0; j<ipos; j++) {\n                  tempArr[j] = (byte)val;\n                  val >>>=8;\n                }\n                // point at the end index in the byte[]\n                index[doc] = (endPos<<8) | 1;\n                bytes[doc] = tempArr;\n                tempArr = new byte[12];\n              }\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","sourceOld":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : new BytesRef(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator();\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seek(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    DocsEnum docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !t.startsWith(termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(delDocs, docsEnum);\n\n        final DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int chunk = docsEnum.read();\n          if (chunk <= 0) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF += chunk;\n\n          for (int i=0; i<chunk; i++) {\n            termInstances++;\n            int doc = bulkResult.docs.ints[i];\n            //System.out.println(\"    docID=\" + doc);\n            // add TNUM_OFFSET to the term number to make room for special reserved values:\n            // 0 (end term) and 1 (index into byte array follows)\n            int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n            lastTerm[doc] = termNum;\n            int val = index[doc];\n\n            if ((val & 0xff)==1) {\n              // index into byte array (actually the end of\n              // the doc-specific byte[] when building)\n              int pos = val >>> 8;\n              int ilen = vIntSize(delta);\n              byte[] arr = bytes[doc];\n              int newend = pos+ilen;\n              if (newend > arr.length) {\n                // We avoid a doubling strategy to lower memory usage.\n                // this faceting method isn't for docs with many terms.\n                // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n                // TODO: figure out what array lengths we can round up to w/o actually using more memory\n                // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n                // It should be safe to round up to the nearest 32 bits in any case.\n                int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n                byte[] newarr = new byte[newLen];\n                System.arraycopy(arr, 0, newarr, 0, pos);\n                arr = newarr;\n                bytes[doc] = newarr;\n              }\n              pos = writeInt(delta, arr, pos);\n              index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n            } else {\n              // OK, this int has data in it... find the end (a zero starting byte - not\n              // part of another number, hence not following a byte with the high bit set).\n              int ipos;\n              if (val==0) {\n                ipos=0;\n              } else if ((val & 0x0000ff80)==0) {\n                ipos=1;\n              } else if ((val & 0x00ff8000)==0) {\n                ipos=2;\n              } else if ((val & 0xff800000)==0) {\n                ipos=3;\n              } else {\n                ipos=4;\n              }\n\n              //System.out.println(\"      ipos=\" + ipos);\n\n              int endPos = writeInt(delta, tempArr, ipos);\n              //System.out.println(\"      endpos=\" + endPos);\n              if (endPos <= 4) {\n                //System.out.println(\"      fits!\");\n                // value will fit in the integer... move bytes back\n                for (int j=ipos; j<endPos; j++) {\n                  val |= (tempArr[j] & 0xff) << (j<<3);\n                }\n                index[doc] = val;\n              } else {\n                // value won't fit... move integer into byte[]\n                for (int j=0; j<ipos; j++) {\n                  tempArr[j] = (byte)val;\n                  val >>>=8;\n                }\n                // point at the end index in the byte[]\n                index[doc] = (endPos<<8) | 1;\n                bytes[doc] = tempArr;\n                tempArr = new byte[12];\n              }\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0663cc678850ea2c51151f9fd217342ea35b8568","date":1303828523,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","sourceNew":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : new BytesRef(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator();\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seek(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    DocsEnum docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !t.startsWith(termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(delDocs, docsEnum);\n\n        final DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int chunk = docsEnum.read();\n          if (chunk <= 0) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF += chunk;\n\n          for (int i=0; i<chunk; i++) {\n            termInstances++;\n            int doc = bulkResult.docs.ints[i];\n            //System.out.println(\"    docID=\" + doc);\n            // add TNUM_OFFSET to the term number to make room for special reserved values:\n            // 0 (end term) and 1 (index into byte array follows)\n            int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n            lastTerm[doc] = termNum;\n            int val = index[doc];\n\n            if ((val & 0xff)==1) {\n              // index into byte array (actually the end of\n              // the doc-specific byte[] when building)\n              int pos = val >>> 8;\n              int ilen = vIntSize(delta);\n              byte[] arr = bytes[doc];\n              int newend = pos+ilen;\n              if (newend > arr.length) {\n                // We avoid a doubling strategy to lower memory usage.\n                // this faceting method isn't for docs with many terms.\n                // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n                // TODO: figure out what array lengths we can round up to w/o actually using more memory\n                // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n                // It should be safe to round up to the nearest 32 bits in any case.\n                int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n                byte[] newarr = new byte[newLen];\n                System.arraycopy(arr, 0, newarr, 0, pos);\n                arr = newarr;\n                bytes[doc] = newarr;\n              }\n              pos = writeInt(delta, arr, pos);\n              index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n            } else {\n              // OK, this int has data in it... find the end (a zero starting byte - not\n              // part of another number, hence not following a byte with the high bit set).\n              int ipos;\n              if (val==0) {\n                ipos=0;\n              } else if ((val & 0x0000ff80)==0) {\n                ipos=1;\n              } else if ((val & 0x00ff8000)==0) {\n                ipos=2;\n              } else if ((val & 0xff800000)==0) {\n                ipos=3;\n              } else {\n                ipos=4;\n              }\n\n              //System.out.println(\"      ipos=\" + ipos);\n\n              int endPos = writeInt(delta, tempArr, ipos);\n              //System.out.println(\"      endpos=\" + endPos);\n              if (endPos <= 4) {\n                //System.out.println(\"      fits!\");\n                // value will fit in the integer... move bytes back\n                for (int j=ipos; j<endPos; j++) {\n                  val |= (tempArr[j] & 0xff) << (j<<3);\n                }\n                index[doc] = val;\n              } else {\n                // value won't fit... move integer into byte[]\n                for (int j=0; j<ipos; j++) {\n                  tempArr[j] = (byte)val;\n                  val >>>=8;\n                }\n                // point at the end index in the byte[]\n                index[doc] = (endPos<<8) | 1;\n                bytes[doc] = tempArr;\n                tempArr = new byte[12];\n              }\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","sourceOld":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : new BytesRef(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator();\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seek(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !t.startsWith(termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(delDocs, docsEnum);\n\n        final DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int chunk = docsEnum.read();\n          if (chunk <= 0) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF += chunk;\n\n          for (int i=0; i<chunk; i++) {\n            termInstances++;\n            int doc = bulkResult.docs.ints[i];\n            //System.out.println(\"    docID=\" + doc);\n            // add TNUM_OFFSET to the term number to make room for special reserved values:\n            // 0 (end term) and 1 (index into byte array follows)\n            int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n            lastTerm[doc] = termNum;\n            int val = index[doc];\n\n            if ((val & 0xff)==1) {\n              // index into byte array (actually the end of\n              // the doc-specific byte[] when building)\n              int pos = val >>> 8;\n              int ilen = vIntSize(delta);\n              byte[] arr = bytes[doc];\n              int newend = pos+ilen;\n              if (newend > arr.length) {\n                // We avoid a doubling strategy to lower memory usage.\n                // this faceting method isn't for docs with many terms.\n                // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n                // TODO: figure out what array lengths we can round up to w/o actually using more memory\n                // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n                // It should be safe to round up to the nearest 32 bits in any case.\n                int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n                byte[] newarr = new byte[newLen];\n                System.arraycopy(arr, 0, newarr, 0, pos);\n                arr = newarr;\n                bytes[doc] = newarr;\n              }\n              pos = writeInt(delta, arr, pos);\n              index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n            } else {\n              // OK, this int has data in it... find the end (a zero starting byte - not\n              // part of another number, hence not following a byte with the high bit set).\n              int ipos;\n              if (val==0) {\n                ipos=0;\n              } else if ((val & 0x0000ff80)==0) {\n                ipos=1;\n              } else if ((val & 0x00ff8000)==0) {\n                ipos=2;\n              } else if ((val & 0xff800000)==0) {\n                ipos=3;\n              } else {\n                ipos=4;\n              }\n\n              //System.out.println(\"      ipos=\" + ipos);\n\n              int endPos = writeInt(delta, tempArr, ipos);\n              //System.out.println(\"      endpos=\" + endPos);\n              if (endPos <= 4) {\n                //System.out.println(\"      fits!\");\n                // value will fit in the integer... move bytes back\n                for (int j=ipos; j<endPos; j++) {\n                  val |= (tempArr[j] & 0xff) << (j<<3);\n                }\n                index[doc] = val;\n              } else {\n                // value won't fit... move integer into byte[]\n                for (int j=0; j<ipos; j++) {\n                  tempArr[j] = (byte)val;\n                  val >>>=8;\n                }\n                // point at the end index in the byte[]\n                index[doc] = (endPos<<8) | 1;\n                bytes[doc] = tempArr;\n                tempArr = new byte[12];\n              }\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"21486a8058ee8d7503c7d7a5e55b6c3a218d0942","date":1303841712,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","sourceNew":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : new BytesRef(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator();\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seek(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !t.startsWith(termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(delDocs, docsEnum);\n\n        final DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int chunk = docsEnum.read();\n          if (chunk <= 0) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF += chunk;\n\n          for (int i=0; i<chunk; i++) {\n            termInstances++;\n            int doc = bulkResult.docs.ints[i];\n            //System.out.println(\"    docID=\" + doc);\n            // add TNUM_OFFSET to the term number to make room for special reserved values:\n            // 0 (end term) and 1 (index into byte array follows)\n            int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n            lastTerm[doc] = termNum;\n            int val = index[doc];\n\n            if ((val & 0xff)==1) {\n              // index into byte array (actually the end of\n              // the doc-specific byte[] when building)\n              int pos = val >>> 8;\n              int ilen = vIntSize(delta);\n              byte[] arr = bytes[doc];\n              int newend = pos+ilen;\n              if (newend > arr.length) {\n                // We avoid a doubling strategy to lower memory usage.\n                // this faceting method isn't for docs with many terms.\n                // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n                // TODO: figure out what array lengths we can round up to w/o actually using more memory\n                // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n                // It should be safe to round up to the nearest 32 bits in any case.\n                int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n                byte[] newarr = new byte[newLen];\n                System.arraycopy(arr, 0, newarr, 0, pos);\n                arr = newarr;\n                bytes[doc] = newarr;\n              }\n              pos = writeInt(delta, arr, pos);\n              index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n            } else {\n              // OK, this int has data in it... find the end (a zero starting byte - not\n              // part of another number, hence not following a byte with the high bit set).\n              int ipos;\n              if (val==0) {\n                ipos=0;\n              } else if ((val & 0x0000ff80)==0) {\n                ipos=1;\n              } else if ((val & 0x00ff8000)==0) {\n                ipos=2;\n              } else if ((val & 0xff800000)==0) {\n                ipos=3;\n              } else {\n                ipos=4;\n              }\n\n              //System.out.println(\"      ipos=\" + ipos);\n\n              int endPos = writeInt(delta, tempArr, ipos);\n              //System.out.println(\"      endpos=\" + endPos);\n              if (endPos <= 4) {\n                //System.out.println(\"      fits!\");\n                // value will fit in the integer... move bytes back\n                for (int j=ipos; j<endPos; j++) {\n                  val |= (tempArr[j] & 0xff) << (j<<3);\n                }\n                index[doc] = val;\n              } else {\n                // value won't fit... move integer into byte[]\n                for (int j=0; j<ipos; j++) {\n                  tempArr[j] = (byte)val;\n                  val >>>=8;\n                }\n                // point at the end index in the byte[]\n                index[doc] = (endPos<<8) | 1;\n                bytes[doc] = tempArr;\n                tempArr = new byte[12];\n              }\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","sourceOld":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : new BytesRef(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator();\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seek(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    DocsEnum docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !t.startsWith(termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(delDocs, docsEnum);\n\n        final DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int chunk = docsEnum.read();\n          if (chunk <= 0) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF += chunk;\n\n          for (int i=0; i<chunk; i++) {\n            termInstances++;\n            int doc = bulkResult.docs.ints[i];\n            //System.out.println(\"    docID=\" + doc);\n            // add TNUM_OFFSET to the term number to make room for special reserved values:\n            // 0 (end term) and 1 (index into byte array follows)\n            int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n            lastTerm[doc] = termNum;\n            int val = index[doc];\n\n            if ((val & 0xff)==1) {\n              // index into byte array (actually the end of\n              // the doc-specific byte[] when building)\n              int pos = val >>> 8;\n              int ilen = vIntSize(delta);\n              byte[] arr = bytes[doc];\n              int newend = pos+ilen;\n              if (newend > arr.length) {\n                // We avoid a doubling strategy to lower memory usage.\n                // this faceting method isn't for docs with many terms.\n                // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n                // TODO: figure out what array lengths we can round up to w/o actually using more memory\n                // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n                // It should be safe to round up to the nearest 32 bits in any case.\n                int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n                byte[] newarr = new byte[newLen];\n                System.arraycopy(arr, 0, newarr, 0, pos);\n                arr = newarr;\n                bytes[doc] = newarr;\n              }\n              pos = writeInt(delta, arr, pos);\n              index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n            } else {\n              // OK, this int has data in it... find the end (a zero starting byte - not\n              // part of another number, hence not following a byte with the high bit set).\n              int ipos;\n              if (val==0) {\n                ipos=0;\n              } else if ((val & 0x0000ff80)==0) {\n                ipos=1;\n              } else if ((val & 0x00ff8000)==0) {\n                ipos=2;\n              } else if ((val & 0xff800000)==0) {\n                ipos=3;\n              } else {\n                ipos=4;\n              }\n\n              //System.out.println(\"      ipos=\" + ipos);\n\n              int endPos = writeInt(delta, tempArr, ipos);\n              //System.out.println(\"      endpos=\" + endPos);\n              if (endPos <= 4) {\n                //System.out.println(\"      fits!\");\n                // value will fit in the integer... move bytes back\n                for (int j=ipos; j<endPos; j++) {\n                  val |= (tempArr[j] & 0xff) << (j<<3);\n                }\n                index[doc] = val;\n              } else {\n                // value won't fit... move integer into byte[]\n                for (int j=0; j<ipos; j++) {\n                  tempArr[j] = (byte)val;\n                  val >>>=8;\n                }\n                // point at the end index in the byte[]\n                index[doc] = (endPos<<8) | 1;\n                bytes[doc] = tempArr;\n                tempArr = new byte[12];\n              }\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7a405e749df166cf8c456ac9381f77f6c99a6270","date":1303842176,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","sourceNew":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : new BytesRef(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator();\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seek(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    DocsEnum docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !t.startsWith(termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(delDocs, docsEnum);\n\n        final DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int chunk = docsEnum.read();\n          if (chunk <= 0) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF += chunk;\n\n          for (int i=0; i<chunk; i++) {\n            termInstances++;\n            int doc = bulkResult.docs.ints[i];\n            //System.out.println(\"    docID=\" + doc);\n            // add TNUM_OFFSET to the term number to make room for special reserved values:\n            // 0 (end term) and 1 (index into byte array follows)\n            int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n            lastTerm[doc] = termNum;\n            int val = index[doc];\n\n            if ((val & 0xff)==1) {\n              // index into byte array (actually the end of\n              // the doc-specific byte[] when building)\n              int pos = val >>> 8;\n              int ilen = vIntSize(delta);\n              byte[] arr = bytes[doc];\n              int newend = pos+ilen;\n              if (newend > arr.length) {\n                // We avoid a doubling strategy to lower memory usage.\n                // this faceting method isn't for docs with many terms.\n                // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n                // TODO: figure out what array lengths we can round up to w/o actually using more memory\n                // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n                // It should be safe to round up to the nearest 32 bits in any case.\n                int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n                byte[] newarr = new byte[newLen];\n                System.arraycopy(arr, 0, newarr, 0, pos);\n                arr = newarr;\n                bytes[doc] = newarr;\n              }\n              pos = writeInt(delta, arr, pos);\n              index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n            } else {\n              // OK, this int has data in it... find the end (a zero starting byte - not\n              // part of another number, hence not following a byte with the high bit set).\n              int ipos;\n              if (val==0) {\n                ipos=0;\n              } else if ((val & 0x0000ff80)==0) {\n                ipos=1;\n              } else if ((val & 0x00ff8000)==0) {\n                ipos=2;\n              } else if ((val & 0xff800000)==0) {\n                ipos=3;\n              } else {\n                ipos=4;\n              }\n\n              //System.out.println(\"      ipos=\" + ipos);\n\n              int endPos = writeInt(delta, tempArr, ipos);\n              //System.out.println(\"      endpos=\" + endPos);\n              if (endPos <= 4) {\n                //System.out.println(\"      fits!\");\n                // value will fit in the integer... move bytes back\n                for (int j=ipos; j<endPos; j++) {\n                  val |= (tempArr[j] & 0xff) << (j<<3);\n                }\n                index[doc] = val;\n              } else {\n                // value won't fit... move integer into byte[]\n                for (int j=0; j<ipos; j++) {\n                  tempArr[j] = (byte)val;\n                  val >>>=8;\n                }\n                // point at the end index in the byte[]\n                index[doc] = (endPos<<8) | 1;\n                bytes[doc] = tempArr;\n                tempArr = new byte[12];\n              }\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","sourceOld":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : new BytesRef(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator();\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seek(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !t.startsWith(termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(delDocs, docsEnum);\n\n        final DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int chunk = docsEnum.read();\n          if (chunk <= 0) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF += chunk;\n\n          for (int i=0; i<chunk; i++) {\n            termInstances++;\n            int doc = bulkResult.docs.ints[i];\n            //System.out.println(\"    docID=\" + doc);\n            // add TNUM_OFFSET to the term number to make room for special reserved values:\n            // 0 (end term) and 1 (index into byte array follows)\n            int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n            lastTerm[doc] = termNum;\n            int val = index[doc];\n\n            if ((val & 0xff)==1) {\n              // index into byte array (actually the end of\n              // the doc-specific byte[] when building)\n              int pos = val >>> 8;\n              int ilen = vIntSize(delta);\n              byte[] arr = bytes[doc];\n              int newend = pos+ilen;\n              if (newend > arr.length) {\n                // We avoid a doubling strategy to lower memory usage.\n                // this faceting method isn't for docs with many terms.\n                // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n                // TODO: figure out what array lengths we can round up to w/o actually using more memory\n                // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n                // It should be safe to round up to the nearest 32 bits in any case.\n                int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n                byte[] newarr = new byte[newLen];\n                System.arraycopy(arr, 0, newarr, 0, pos);\n                arr = newarr;\n                bytes[doc] = newarr;\n              }\n              pos = writeInt(delta, arr, pos);\n              index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n            } else {\n              // OK, this int has data in it... find the end (a zero starting byte - not\n              // part of another number, hence not following a byte with the high bit set).\n              int ipos;\n              if (val==0) {\n                ipos=0;\n              } else if ((val & 0x0000ff80)==0) {\n                ipos=1;\n              } else if ((val & 0x00ff8000)==0) {\n                ipos=2;\n              } else if ((val & 0xff800000)==0) {\n                ipos=3;\n              } else {\n                ipos=4;\n              }\n\n              //System.out.println(\"      ipos=\" + ipos);\n\n              int endPos = writeInt(delta, tempArr, ipos);\n              //System.out.println(\"      endpos=\" + endPos);\n              if (endPos <= 4) {\n                //System.out.println(\"      fits!\");\n                // value will fit in the integer... move bytes back\n                for (int j=ipos; j<endPos; j++) {\n                  val |= (tempArr[j] & 0xff) << (j<<3);\n                }\n                index[doc] = val;\n              } else {\n                // value won't fit... move integer into byte[]\n                for (int j=0; j<ipos; j++) {\n                  tempArr[j] = (byte)val;\n                  val >>>=8;\n                }\n                // point at the end index in the byte[]\n                index[doc] = (endPos<<8) | 1;\n                bytes[doc] = tempArr;\n                tempArr = new byte[12];\n              }\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f8f944ac3fe3f9d40d825177507fb381d2b106b3","date":1303868525,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","sourceNew":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : new BytesRef(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator();\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seek(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !t.startsWith(termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(delDocs, docsEnum);\n\n        final DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int chunk = docsEnum.read();\n          if (chunk <= 0) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF += chunk;\n\n          for (int i=0; i<chunk; i++) {\n            termInstances++;\n            int doc = bulkResult.docs.ints[i];\n            //System.out.println(\"    docID=\" + doc);\n            // add TNUM_OFFSET to the term number to make room for special reserved values:\n            // 0 (end term) and 1 (index into byte array follows)\n            int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n            lastTerm[doc] = termNum;\n            int val = index[doc];\n\n            if ((val & 0xff)==1) {\n              // index into byte array (actually the end of\n              // the doc-specific byte[] when building)\n              int pos = val >>> 8;\n              int ilen = vIntSize(delta);\n              byte[] arr = bytes[doc];\n              int newend = pos+ilen;\n              if (newend > arr.length) {\n                // We avoid a doubling strategy to lower memory usage.\n                // this faceting method isn't for docs with many terms.\n                // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n                // TODO: figure out what array lengths we can round up to w/o actually using more memory\n                // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n                // It should be safe to round up to the nearest 32 bits in any case.\n                int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n                byte[] newarr = new byte[newLen];\n                System.arraycopy(arr, 0, newarr, 0, pos);\n                arr = newarr;\n                bytes[doc] = newarr;\n              }\n              pos = writeInt(delta, arr, pos);\n              index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n            } else {\n              // OK, this int has data in it... find the end (a zero starting byte - not\n              // part of another number, hence not following a byte with the high bit set).\n              int ipos;\n              if (val==0) {\n                ipos=0;\n              } else if ((val & 0x0000ff80)==0) {\n                ipos=1;\n              } else if ((val & 0x00ff8000)==0) {\n                ipos=2;\n              } else if ((val & 0xff800000)==0) {\n                ipos=3;\n              } else {\n                ipos=4;\n              }\n\n              //System.out.println(\"      ipos=\" + ipos);\n\n              int endPos = writeInt(delta, tempArr, ipos);\n              //System.out.println(\"      endpos=\" + endPos);\n              if (endPos <= 4) {\n                //System.out.println(\"      fits!\");\n                // value will fit in the integer... move bytes back\n                for (int j=ipos; j<endPos; j++) {\n                  val |= (tempArr[j] & 0xff) << (j<<3);\n                }\n                index[doc] = val;\n              } else {\n                // value won't fit... move integer into byte[]\n                for (int j=0; j<ipos; j++) {\n                  tempArr[j] = (byte)val;\n                  val >>>=8;\n                }\n                // point at the end index in the byte[]\n                index[doc] = (endPos<<8) | 1;\n                bytes[doc] = tempArr;\n                tempArr = new byte[12];\n              }\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","sourceOld":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : new BytesRef(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator();\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seek(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    DocsEnum docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !t.startsWith(termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(delDocs, docsEnum);\n\n        final DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int chunk = docsEnum.read();\n          if (chunk <= 0) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF += chunk;\n\n          for (int i=0; i<chunk; i++) {\n            termInstances++;\n            int doc = bulkResult.docs.ints[i];\n            //System.out.println(\"    docID=\" + doc);\n            // add TNUM_OFFSET to the term number to make room for special reserved values:\n            // 0 (end term) and 1 (index into byte array follows)\n            int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n            lastTerm[doc] = termNum;\n            int val = index[doc];\n\n            if ((val & 0xff)==1) {\n              // index into byte array (actually the end of\n              // the doc-specific byte[] when building)\n              int pos = val >>> 8;\n              int ilen = vIntSize(delta);\n              byte[] arr = bytes[doc];\n              int newend = pos+ilen;\n              if (newend > arr.length) {\n                // We avoid a doubling strategy to lower memory usage.\n                // this faceting method isn't for docs with many terms.\n                // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n                // TODO: figure out what array lengths we can round up to w/o actually using more memory\n                // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n                // It should be safe to round up to the nearest 32 bits in any case.\n                int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n                byte[] newarr = new byte[newLen];\n                System.arraycopy(arr, 0, newarr, 0, pos);\n                arr = newarr;\n                bytes[doc] = newarr;\n              }\n              pos = writeInt(delta, arr, pos);\n              index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n            } else {\n              // OK, this int has data in it... find the end (a zero starting byte - not\n              // part of another number, hence not following a byte with the high bit set).\n              int ipos;\n              if (val==0) {\n                ipos=0;\n              } else if ((val & 0x0000ff80)==0) {\n                ipos=1;\n              } else if ((val & 0x00ff8000)==0) {\n                ipos=2;\n              } else if ((val & 0xff800000)==0) {\n                ipos=3;\n              } else {\n                ipos=4;\n              }\n\n              //System.out.println(\"      ipos=\" + ipos);\n\n              int endPos = writeInt(delta, tempArr, ipos);\n              //System.out.println(\"      endpos=\" + endPos);\n              if (endPos <= 4) {\n                //System.out.println(\"      fits!\");\n                // value will fit in the integer... move bytes back\n                for (int j=ipos; j<endPos; j++) {\n                  val |= (tempArr[j] & 0xff) << (j<<3);\n                }\n                index[doc] = val;\n              } else {\n                // value won't fit... move integer into byte[]\n                for (int j=0; j<ipos; j++) {\n                  tempArr[j] = (byte)val;\n                  val >>>=8;\n                }\n                // point at the end index in the byte[]\n                index[doc] = (endPos<<8) | 1;\n                bytes[doc] = tempArr;\n                tempArr = new byte[12];\n              }\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d4d5df8e07c035d62d982894b439322da40e0938","date":1303923139,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","sourceNew":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : new BytesRef(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator();\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seek(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !t.startsWith(termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(delDocs, docsEnum);\n\n        final DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int chunk = docsEnum.read();\n          if (chunk <= 0) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF += chunk;\n\n          for (int i=0; i<chunk; i++) {\n            termInstances++;\n            int doc = bulkResult.docs.ints[i];\n            //System.out.println(\"    docID=\" + doc);\n            // add TNUM_OFFSET to the term number to make room for special reserved values:\n            // 0 (end term) and 1 (index into byte array follows)\n            int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n            lastTerm[doc] = termNum;\n            int val = index[doc];\n\n            if ((val & 0xff)==1) {\n              // index into byte array (actually the end of\n              // the doc-specific byte[] when building)\n              int pos = val >>> 8;\n              int ilen = vIntSize(delta);\n              byte[] arr = bytes[doc];\n              int newend = pos+ilen;\n              if (newend > arr.length) {\n                // We avoid a doubling strategy to lower memory usage.\n                // this faceting method isn't for docs with many terms.\n                // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n                // TODO: figure out what array lengths we can round up to w/o actually using more memory\n                // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n                // It should be safe to round up to the nearest 32 bits in any case.\n                int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n                byte[] newarr = new byte[newLen];\n                System.arraycopy(arr, 0, newarr, 0, pos);\n                arr = newarr;\n                bytes[doc] = newarr;\n              }\n              pos = writeInt(delta, arr, pos);\n              index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n            } else {\n              // OK, this int has data in it... find the end (a zero starting byte - not\n              // part of another number, hence not following a byte with the high bit set).\n              int ipos;\n              if (val==0) {\n                ipos=0;\n              } else if ((val & 0x0000ff80)==0) {\n                ipos=1;\n              } else if ((val & 0x00ff8000)==0) {\n                ipos=2;\n              } else if ((val & 0xff800000)==0) {\n                ipos=3;\n              } else {\n                ipos=4;\n              }\n\n              //System.out.println(\"      ipos=\" + ipos);\n\n              int endPos = writeInt(delta, tempArr, ipos);\n              //System.out.println(\"      endpos=\" + endPos);\n              if (endPos <= 4) {\n                //System.out.println(\"      fits!\");\n                // value will fit in the integer... move bytes back\n                for (int j=ipos; j<endPos; j++) {\n                  val |= (tempArr[j] & 0xff) << (j<<3);\n                }\n                index[doc] = val;\n              } else {\n                // value won't fit... move integer into byte[]\n                for (int j=0; j<ipos; j++) {\n                  tempArr[j] = (byte)val;\n                  val >>>=8;\n                }\n                // point at the end index in the byte[]\n                index[doc] = (endPos<<8) | 1;\n                bytes[doc] = tempArr;\n                tempArr = new byte[12];\n              }\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","sourceOld":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : new BytesRef(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator();\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seek(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    DocsEnum docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !t.startsWith(termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(delDocs, docsEnum);\n\n        final DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int chunk = docsEnum.read();\n          if (chunk <= 0) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF += chunk;\n\n          for (int i=0; i<chunk; i++) {\n            termInstances++;\n            int doc = bulkResult.docs.ints[i];\n            //System.out.println(\"    docID=\" + doc);\n            // add TNUM_OFFSET to the term number to make room for special reserved values:\n            // 0 (end term) and 1 (index into byte array follows)\n            int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n            lastTerm[doc] = termNum;\n            int val = index[doc];\n\n            if ((val & 0xff)==1) {\n              // index into byte array (actually the end of\n              // the doc-specific byte[] when building)\n              int pos = val >>> 8;\n              int ilen = vIntSize(delta);\n              byte[] arr = bytes[doc];\n              int newend = pos+ilen;\n              if (newend > arr.length) {\n                // We avoid a doubling strategy to lower memory usage.\n                // this faceting method isn't for docs with many terms.\n                // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n                // TODO: figure out what array lengths we can round up to w/o actually using more memory\n                // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n                // It should be safe to round up to the nearest 32 bits in any case.\n                int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n                byte[] newarr = new byte[newLen];\n                System.arraycopy(arr, 0, newarr, 0, pos);\n                arr = newarr;\n                bytes[doc] = newarr;\n              }\n              pos = writeInt(delta, arr, pos);\n              index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n            } else {\n              // OK, this int has data in it... find the end (a zero starting byte - not\n              // part of another number, hence not following a byte with the high bit set).\n              int ipos;\n              if (val==0) {\n                ipos=0;\n              } else if ((val & 0x0000ff80)==0) {\n                ipos=1;\n              } else if ((val & 0x00ff8000)==0) {\n                ipos=2;\n              } else if ((val & 0xff800000)==0) {\n                ipos=3;\n              } else {\n                ipos=4;\n              }\n\n              //System.out.println(\"      ipos=\" + ipos);\n\n              int endPos = writeInt(delta, tempArr, ipos);\n              //System.out.println(\"      endpos=\" + endPos);\n              if (endPos <= 4) {\n                //System.out.println(\"      fits!\");\n                // value will fit in the integer... move bytes back\n                for (int j=ipos; j<endPos; j++) {\n                  val |= (tempArr[j] & 0xff) << (j<<3);\n                }\n                index[doc] = val;\n              } else {\n                // value won't fit... move integer into byte[]\n                for (int j=0; j<ipos; j++) {\n                  tempArr[j] = (byte)val;\n                  val >>>=8;\n                }\n                // point at the end index in the byte[]\n                index[doc] = (endPos<<8) | 1;\n                bytes[doc] = tempArr;\n                tempArr = new byte[12];\n              }\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","pathOld":"/dev/null","sourceNew":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : new BytesRef(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator();\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seek(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !t.startsWith(termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(delDocs, docsEnum);\n\n        final DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int chunk = docsEnum.read();\n          if (chunk <= 0) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF += chunk;\n\n          for (int i=0; i<chunk; i++) {\n            termInstances++;\n            int doc = bulkResult.docs.ints[i];\n            //System.out.println(\"    docID=\" + doc);\n            // add TNUM_OFFSET to the term number to make room for special reserved values:\n            // 0 (end term) and 1 (index into byte array follows)\n            int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n            lastTerm[doc] = termNum;\n            int val = index[doc];\n\n            if ((val & 0xff)==1) {\n              // index into byte array (actually the end of\n              // the doc-specific byte[] when building)\n              int pos = val >>> 8;\n              int ilen = vIntSize(delta);\n              byte[] arr = bytes[doc];\n              int newend = pos+ilen;\n              if (newend > arr.length) {\n                // We avoid a doubling strategy to lower memory usage.\n                // this faceting method isn't for docs with many terms.\n                // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n                // TODO: figure out what array lengths we can round up to w/o actually using more memory\n                // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n                // It should be safe to round up to the nearest 32 bits in any case.\n                int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n                byte[] newarr = new byte[newLen];\n                System.arraycopy(arr, 0, newarr, 0, pos);\n                arr = newarr;\n                bytes[doc] = newarr;\n              }\n              pos = writeInt(delta, arr, pos);\n              index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n            } else {\n              // OK, this int has data in it... find the end (a zero starting byte - not\n              // part of another number, hence not following a byte with the high bit set).\n              int ipos;\n              if (val==0) {\n                ipos=0;\n              } else if ((val & 0x0000ff80)==0) {\n                ipos=1;\n              } else if ((val & 0x00ff8000)==0) {\n                ipos=2;\n              } else if ((val & 0xff800000)==0) {\n                ipos=3;\n              } else {\n                ipos=4;\n              }\n\n              //System.out.println(\"      ipos=\" + ipos);\n\n              int endPos = writeInt(delta, tempArr, ipos);\n              //System.out.println(\"      endpos=\" + endPos);\n              if (endPos <= 4) {\n                //System.out.println(\"      fits!\");\n                // value will fit in the integer... move bytes back\n                for (int j=ipos; j<endPos; j++) {\n                  val |= (tempArr[j] & 0xff) << (j<<3);\n                }\n                index[doc] = val;\n              } else {\n                // value won't fit... move integer into byte[]\n                for (int j=0; j<ipos; j++) {\n                  tempArr[j] = (byte)val;\n                  val >>>=8;\n                }\n                // point at the end index in the byte[]\n                index[doc] = (endPos<<8) | 1;\n                bytes[doc] = tempArr;\n                tempArr = new byte[12];\n              }\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":0,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","pathOld":"/dev/null","sourceNew":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : new BytesRef(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator();\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seek(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !t.startsWith(termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(delDocs, docsEnum);\n\n        final DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int chunk = docsEnum.read();\n          if (chunk <= 0) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF += chunk;\n\n          for (int i=0; i<chunk; i++) {\n            termInstances++;\n            int doc = bulkResult.docs.ints[i];\n            //System.out.println(\"    docID=\" + doc);\n            // add TNUM_OFFSET to the term number to make room for special reserved values:\n            // 0 (end term) and 1 (index into byte array follows)\n            int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n            lastTerm[doc] = termNum;\n            int val = index[doc];\n\n            if ((val & 0xff)==1) {\n              // index into byte array (actually the end of\n              // the doc-specific byte[] when building)\n              int pos = val >>> 8;\n              int ilen = vIntSize(delta);\n              byte[] arr = bytes[doc];\n              int newend = pos+ilen;\n              if (newend > arr.length) {\n                // We avoid a doubling strategy to lower memory usage.\n                // this faceting method isn't for docs with many terms.\n                // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n                // TODO: figure out what array lengths we can round up to w/o actually using more memory\n                // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n                // It should be safe to round up to the nearest 32 bits in any case.\n                int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n                byte[] newarr = new byte[newLen];\n                System.arraycopy(arr, 0, newarr, 0, pos);\n                arr = newarr;\n                bytes[doc] = newarr;\n              }\n              pos = writeInt(delta, arr, pos);\n              index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n            } else {\n              // OK, this int has data in it... find the end (a zero starting byte - not\n              // part of another number, hence not following a byte with the high bit set).\n              int ipos;\n              if (val==0) {\n                ipos=0;\n              } else if ((val & 0x0000ff80)==0) {\n                ipos=1;\n              } else if ((val & 0x00ff8000)==0) {\n                ipos=2;\n              } else if ((val & 0xff800000)==0) {\n                ipos=3;\n              } else {\n                ipos=4;\n              }\n\n              //System.out.println(\"      ipos=\" + ipos);\n\n              int endPos = writeInt(delta, tempArr, ipos);\n              //System.out.println(\"      endpos=\" + endPos);\n              if (endPos <= 4) {\n                //System.out.println(\"      fits!\");\n                // value will fit in the integer... move bytes back\n                for (int j=ipos; j<endPos; j++) {\n                  val |= (tempArr[j] & 0xff) << (j<<3);\n                }\n                index[doc] = val;\n              } else {\n                // value won't fit... move integer into byte[]\n                for (int j=0; j<ipos; j++) {\n                  tempArr[j] = (byte)val;\n                  val >>>=8;\n                }\n                // point at the end index in the byte[]\n                index[doc] = (endPos<<8) | 1;\n                bytes[doc] = tempArr;\n                tempArr = new byte[12];\n              }\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fd9cc9d77712aba3662f24632df7539ab75e3667","date":1309095238,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","sourceNew":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : new BytesRef(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator();\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !t.startsWith(termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(delDocs, docsEnum);\n\n        final DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int chunk = docsEnum.read();\n          if (chunk <= 0) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF += chunk;\n\n          for (int i=0; i<chunk; i++) {\n            termInstances++;\n            int doc = bulkResult.docs.ints[i];\n            //System.out.println(\"    docID=\" + doc);\n            // add TNUM_OFFSET to the term number to make room for special reserved values:\n            // 0 (end term) and 1 (index into byte array follows)\n            int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n            lastTerm[doc] = termNum;\n            int val = index[doc];\n\n            if ((val & 0xff)==1) {\n              // index into byte array (actually the end of\n              // the doc-specific byte[] when building)\n              int pos = val >>> 8;\n              int ilen = vIntSize(delta);\n              byte[] arr = bytes[doc];\n              int newend = pos+ilen;\n              if (newend > arr.length) {\n                // We avoid a doubling strategy to lower memory usage.\n                // this faceting method isn't for docs with many terms.\n                // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n                // TODO: figure out what array lengths we can round up to w/o actually using more memory\n                // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n                // It should be safe to round up to the nearest 32 bits in any case.\n                int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n                byte[] newarr = new byte[newLen];\n                System.arraycopy(arr, 0, newarr, 0, pos);\n                arr = newarr;\n                bytes[doc] = newarr;\n              }\n              pos = writeInt(delta, arr, pos);\n              index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n            } else {\n              // OK, this int has data in it... find the end (a zero starting byte - not\n              // part of another number, hence not following a byte with the high bit set).\n              int ipos;\n              if (val==0) {\n                ipos=0;\n              } else if ((val & 0x0000ff80)==0) {\n                ipos=1;\n              } else if ((val & 0x00ff8000)==0) {\n                ipos=2;\n              } else if ((val & 0xff800000)==0) {\n                ipos=3;\n              } else {\n                ipos=4;\n              }\n\n              //System.out.println(\"      ipos=\" + ipos);\n\n              int endPos = writeInt(delta, tempArr, ipos);\n              //System.out.println(\"      endpos=\" + endPos);\n              if (endPos <= 4) {\n                //System.out.println(\"      fits!\");\n                // value will fit in the integer... move bytes back\n                for (int j=ipos; j<endPos; j++) {\n                  val |= (tempArr[j] & 0xff) << (j<<3);\n                }\n                index[doc] = val;\n              } else {\n                // value won't fit... move integer into byte[]\n                for (int j=0; j<ipos; j++) {\n                  tempArr[j] = (byte)val;\n                  val >>>=8;\n                }\n                // point at the end index in the byte[]\n                index[doc] = (endPos<<8) | 1;\n                bytes[doc] = tempArr;\n                tempArr = new byte[12];\n              }\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","sourceOld":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : new BytesRef(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator();\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seek(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !t.startsWith(termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(delDocs, docsEnum);\n\n        final DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int chunk = docsEnum.read();\n          if (chunk <= 0) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF += chunk;\n\n          for (int i=0; i<chunk; i++) {\n            termInstances++;\n            int doc = bulkResult.docs.ints[i];\n            //System.out.println(\"    docID=\" + doc);\n            // add TNUM_OFFSET to the term number to make room for special reserved values:\n            // 0 (end term) and 1 (index into byte array follows)\n            int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n            lastTerm[doc] = termNum;\n            int val = index[doc];\n\n            if ((val & 0xff)==1) {\n              // index into byte array (actually the end of\n              // the doc-specific byte[] when building)\n              int pos = val >>> 8;\n              int ilen = vIntSize(delta);\n              byte[] arr = bytes[doc];\n              int newend = pos+ilen;\n              if (newend > arr.length) {\n                // We avoid a doubling strategy to lower memory usage.\n                // this faceting method isn't for docs with many terms.\n                // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n                // TODO: figure out what array lengths we can round up to w/o actually using more memory\n                // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n                // It should be safe to round up to the nearest 32 bits in any case.\n                int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n                byte[] newarr = new byte[newLen];\n                System.arraycopy(arr, 0, newarr, 0, pos);\n                arr = newarr;\n                bytes[doc] = newarr;\n              }\n              pos = writeInt(delta, arr, pos);\n              index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n            } else {\n              // OK, this int has data in it... find the end (a zero starting byte - not\n              // part of another number, hence not following a byte with the high bit set).\n              int ipos;\n              if (val==0) {\n                ipos=0;\n              } else if ((val & 0x0000ff80)==0) {\n                ipos=1;\n              } else if ((val & 0x00ff8000)==0) {\n                ipos=2;\n              } else if ((val & 0xff800000)==0) {\n                ipos=3;\n              } else {\n                ipos=4;\n              }\n\n              //System.out.println(\"      ipos=\" + ipos);\n\n              int endPos = writeInt(delta, tempArr, ipos);\n              //System.out.println(\"      endpos=\" + endPos);\n              if (endPos <= 4) {\n                //System.out.println(\"      fits!\");\n                // value will fit in the integer... move bytes back\n                for (int j=ipos; j<endPos; j++) {\n                  val |= (tempArr[j] & 0xff) << (j<<3);\n                }\n                index[doc] = val;\n              } else {\n                // value won't fit... move integer into byte[]\n                for (int j=0; j<ipos; j++) {\n                  tempArr[j] = (byte)val;\n                  val >>>=8;\n                }\n                // point at the end index in the byte[]\n                index[doc] = (endPos<<8) | 1;\n                bytes[doc] = tempArr;\n                tempArr = new byte[12];\n              }\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2553b00f699380c64959ccb27991289aae87be2e","date":1309290151,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","sourceNew":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : new BytesRef(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator();\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !t.startsWith(termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(delDocs, docsEnum);\n\n        final DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int chunk = docsEnum.read();\n          if (chunk <= 0) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF += chunk;\n\n          for (int i=0; i<chunk; i++) {\n            termInstances++;\n            int doc = bulkResult.docs.ints[i];\n            //System.out.println(\"    docID=\" + doc);\n            // add TNUM_OFFSET to the term number to make room for special reserved values:\n            // 0 (end term) and 1 (index into byte array follows)\n            int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n            lastTerm[doc] = termNum;\n            int val = index[doc];\n\n            if ((val & 0xff)==1) {\n              // index into byte array (actually the end of\n              // the doc-specific byte[] when building)\n              int pos = val >>> 8;\n              int ilen = vIntSize(delta);\n              byte[] arr = bytes[doc];\n              int newend = pos+ilen;\n              if (newend > arr.length) {\n                // We avoid a doubling strategy to lower memory usage.\n                // this faceting method isn't for docs with many terms.\n                // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n                // TODO: figure out what array lengths we can round up to w/o actually using more memory\n                // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n                // It should be safe to round up to the nearest 32 bits in any case.\n                int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n                byte[] newarr = new byte[newLen];\n                System.arraycopy(arr, 0, newarr, 0, pos);\n                arr = newarr;\n                bytes[doc] = newarr;\n              }\n              pos = writeInt(delta, arr, pos);\n              index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n            } else {\n              // OK, this int has data in it... find the end (a zero starting byte - not\n              // part of another number, hence not following a byte with the high bit set).\n              int ipos;\n              if (val==0) {\n                ipos=0;\n              } else if ((val & 0x0000ff80)==0) {\n                ipos=1;\n              } else if ((val & 0x00ff8000)==0) {\n                ipos=2;\n              } else if ((val & 0xff800000)==0) {\n                ipos=3;\n              } else {\n                ipos=4;\n              }\n\n              //System.out.println(\"      ipos=\" + ipos);\n\n              int endPos = writeInt(delta, tempArr, ipos);\n              //System.out.println(\"      endpos=\" + endPos);\n              if (endPos <= 4) {\n                //System.out.println(\"      fits!\");\n                // value will fit in the integer... move bytes back\n                for (int j=ipos; j<endPos; j++) {\n                  val |= (tempArr[j] & 0xff) << (j<<3);\n                }\n                index[doc] = val;\n              } else {\n                // value won't fit... move integer into byte[]\n                for (int j=0; j<ipos; j++) {\n                  tempArr[j] = (byte)val;\n                  val >>>=8;\n                }\n                // point at the end index in the byte[]\n                index[doc] = (endPos<<8) | 1;\n                bytes[doc] = tempArr;\n                tempArr = new byte[12];\n              }\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","sourceOld":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : new BytesRef(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator();\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seek(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !t.startsWith(termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(delDocs, docsEnum);\n\n        final DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int chunk = docsEnum.read();\n          if (chunk <= 0) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF += chunk;\n\n          for (int i=0; i<chunk; i++) {\n            termInstances++;\n            int doc = bulkResult.docs.ints[i];\n            //System.out.println(\"    docID=\" + doc);\n            // add TNUM_OFFSET to the term number to make room for special reserved values:\n            // 0 (end term) and 1 (index into byte array follows)\n            int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n            lastTerm[doc] = termNum;\n            int val = index[doc];\n\n            if ((val & 0xff)==1) {\n              // index into byte array (actually the end of\n              // the doc-specific byte[] when building)\n              int pos = val >>> 8;\n              int ilen = vIntSize(delta);\n              byte[] arr = bytes[doc];\n              int newend = pos+ilen;\n              if (newend > arr.length) {\n                // We avoid a doubling strategy to lower memory usage.\n                // this faceting method isn't for docs with many terms.\n                // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n                // TODO: figure out what array lengths we can round up to w/o actually using more memory\n                // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n                // It should be safe to round up to the nearest 32 bits in any case.\n                int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n                byte[] newarr = new byte[newLen];\n                System.arraycopy(arr, 0, newarr, 0, pos);\n                arr = newarr;\n                bytes[doc] = newarr;\n              }\n              pos = writeInt(delta, arr, pos);\n              index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n            } else {\n              // OK, this int has data in it... find the end (a zero starting byte - not\n              // part of another number, hence not following a byte with the high bit set).\n              int ipos;\n              if (val==0) {\n                ipos=0;\n              } else if ((val & 0x0000ff80)==0) {\n                ipos=1;\n              } else if ((val & 0x00ff8000)==0) {\n                ipos=2;\n              } else if ((val & 0xff800000)==0) {\n                ipos=3;\n              } else {\n                ipos=4;\n              }\n\n              //System.out.println(\"      ipos=\" + ipos);\n\n              int endPos = writeInt(delta, tempArr, ipos);\n              //System.out.println(\"      endpos=\" + endPos);\n              if (endPos <= 4) {\n                //System.out.println(\"      fits!\");\n                // value will fit in the integer... move bytes back\n                for (int j=ipos; j<endPos; j++) {\n                  val |= (tempArr[j] & 0xff) << (j<<3);\n                }\n                index[doc] = val;\n              } else {\n                // value won't fit... move integer into byte[]\n                for (int j=0; j<ipos; j++) {\n                  tempArr[j] = (byte)val;\n                  val >>>=8;\n                }\n                // point at the end index in the byte[]\n                index[doc] = (endPos<<8) | 1;\n                bytes[doc] = tempArr;\n                tempArr = new byte[12];\n              }\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","date":1309960478,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","sourceNew":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : new BytesRef(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator();\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !t.startsWith(termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(liveDocs, docsEnum);\n\n        final DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int chunk = docsEnum.read();\n          if (chunk <= 0) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF += chunk;\n\n          for (int i=0; i<chunk; i++) {\n            termInstances++;\n            int doc = bulkResult.docs.ints[i];\n            //System.out.println(\"    docID=\" + doc);\n            // add TNUM_OFFSET to the term number to make room for special reserved values:\n            // 0 (end term) and 1 (index into byte array follows)\n            int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n            lastTerm[doc] = termNum;\n            int val = index[doc];\n\n            if ((val & 0xff)==1) {\n              // index into byte array (actually the end of\n              // the doc-specific byte[] when building)\n              int pos = val >>> 8;\n              int ilen = vIntSize(delta);\n              byte[] arr = bytes[doc];\n              int newend = pos+ilen;\n              if (newend > arr.length) {\n                // We avoid a doubling strategy to lower memory usage.\n                // this faceting method isn't for docs with many terms.\n                // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n                // TODO: figure out what array lengths we can round up to w/o actually using more memory\n                // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n                // It should be safe to round up to the nearest 32 bits in any case.\n                int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n                byte[] newarr = new byte[newLen];\n                System.arraycopy(arr, 0, newarr, 0, pos);\n                arr = newarr;\n                bytes[doc] = newarr;\n              }\n              pos = writeInt(delta, arr, pos);\n              index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n            } else {\n              // OK, this int has data in it... find the end (a zero starting byte - not\n              // part of another number, hence not following a byte with the high bit set).\n              int ipos;\n              if (val==0) {\n                ipos=0;\n              } else if ((val & 0x0000ff80)==0) {\n                ipos=1;\n              } else if ((val & 0x00ff8000)==0) {\n                ipos=2;\n              } else if ((val & 0xff800000)==0) {\n                ipos=3;\n              } else {\n                ipos=4;\n              }\n\n              //System.out.println(\"      ipos=\" + ipos);\n\n              int endPos = writeInt(delta, tempArr, ipos);\n              //System.out.println(\"      endpos=\" + endPos);\n              if (endPos <= 4) {\n                //System.out.println(\"      fits!\");\n                // value will fit in the integer... move bytes back\n                for (int j=ipos; j<endPos; j++) {\n                  val |= (tempArr[j] & 0xff) << (j<<3);\n                }\n                index[doc] = val;\n              } else {\n                // value won't fit... move integer into byte[]\n                for (int j=0; j<ipos; j++) {\n                  tempArr[j] = (byte)val;\n                  val >>>=8;\n                }\n                // point at the end index in the byte[]\n                index[doc] = (endPos<<8) | 1;\n                bytes[doc] = tempArr;\n                tempArr = new byte[12];\n              }\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","sourceOld":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : new BytesRef(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator();\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !t.startsWith(termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(delDocs, docsEnum);\n\n        final DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int chunk = docsEnum.read();\n          if (chunk <= 0) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF += chunk;\n\n          for (int i=0; i<chunk; i++) {\n            termInstances++;\n            int doc = bulkResult.docs.ints[i];\n            //System.out.println(\"    docID=\" + doc);\n            // add TNUM_OFFSET to the term number to make room for special reserved values:\n            // 0 (end term) and 1 (index into byte array follows)\n            int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n            lastTerm[doc] = termNum;\n            int val = index[doc];\n\n            if ((val & 0xff)==1) {\n              // index into byte array (actually the end of\n              // the doc-specific byte[] when building)\n              int pos = val >>> 8;\n              int ilen = vIntSize(delta);\n              byte[] arr = bytes[doc];\n              int newend = pos+ilen;\n              if (newend > arr.length) {\n                // We avoid a doubling strategy to lower memory usage.\n                // this faceting method isn't for docs with many terms.\n                // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n                // TODO: figure out what array lengths we can round up to w/o actually using more memory\n                // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n                // It should be safe to round up to the nearest 32 bits in any case.\n                int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n                byte[] newarr = new byte[newLen];\n                System.arraycopy(arr, 0, newarr, 0, pos);\n                arr = newarr;\n                bytes[doc] = newarr;\n              }\n              pos = writeInt(delta, arr, pos);\n              index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n            } else {\n              // OK, this int has data in it... find the end (a zero starting byte - not\n              // part of another number, hence not following a byte with the high bit set).\n              int ipos;\n              if (val==0) {\n                ipos=0;\n              } else if ((val & 0x0000ff80)==0) {\n                ipos=1;\n              } else if ((val & 0x00ff8000)==0) {\n                ipos=2;\n              } else if ((val & 0xff800000)==0) {\n                ipos=3;\n              } else {\n                ipos=4;\n              }\n\n              //System.out.println(\"      ipos=\" + ipos);\n\n              int endPos = writeInt(delta, tempArr, ipos);\n              //System.out.println(\"      endpos=\" + endPos);\n              if (endPos <= 4) {\n                //System.out.println(\"      fits!\");\n                // value will fit in the integer... move bytes back\n                for (int j=ipos; j<endPos; j++) {\n                  val |= (tempArr[j] & 0xff) << (j<<3);\n                }\n                index[doc] = val;\n              } else {\n                // value won't fit... move integer into byte[]\n                for (int j=0; j<ipos; j++) {\n                  tempArr[j] = (byte)val;\n                  val >>>=8;\n                }\n                // point at the end index in the byte[]\n                index[doc] = (endPos<<8) | 1;\n                bytes[doc] = tempArr;\n                tempArr = new byte[12];\n              }\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"817d8435e9135b756f08ce6710ab0baac51bdf88","date":1309986993,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","sourceNew":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : new BytesRef(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator();\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !t.startsWith(termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(liveDocs, docsEnum);\n\n        final DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int chunk = docsEnum.read();\n          if (chunk <= 0) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF += chunk;\n\n          for (int i=0; i<chunk; i++) {\n            termInstances++;\n            int doc = bulkResult.docs.ints[i];\n            //System.out.println(\"    docID=\" + doc);\n            // add TNUM_OFFSET to the term number to make room for special reserved values:\n            // 0 (end term) and 1 (index into byte array follows)\n            int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n            lastTerm[doc] = termNum;\n            int val = index[doc];\n\n            if ((val & 0xff)==1) {\n              // index into byte array (actually the end of\n              // the doc-specific byte[] when building)\n              int pos = val >>> 8;\n              int ilen = vIntSize(delta);\n              byte[] arr = bytes[doc];\n              int newend = pos+ilen;\n              if (newend > arr.length) {\n                // We avoid a doubling strategy to lower memory usage.\n                // this faceting method isn't for docs with many terms.\n                // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n                // TODO: figure out what array lengths we can round up to w/o actually using more memory\n                // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n                // It should be safe to round up to the nearest 32 bits in any case.\n                int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n                byte[] newarr = new byte[newLen];\n                System.arraycopy(arr, 0, newarr, 0, pos);\n                arr = newarr;\n                bytes[doc] = newarr;\n              }\n              pos = writeInt(delta, arr, pos);\n              index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n            } else {\n              // OK, this int has data in it... find the end (a zero starting byte - not\n              // part of another number, hence not following a byte with the high bit set).\n              int ipos;\n              if (val==0) {\n                ipos=0;\n              } else if ((val & 0x0000ff80)==0) {\n                ipos=1;\n              } else if ((val & 0x00ff8000)==0) {\n                ipos=2;\n              } else if ((val & 0xff800000)==0) {\n                ipos=3;\n              } else {\n                ipos=4;\n              }\n\n              //System.out.println(\"      ipos=\" + ipos);\n\n              int endPos = writeInt(delta, tempArr, ipos);\n              //System.out.println(\"      endpos=\" + endPos);\n              if (endPos <= 4) {\n                //System.out.println(\"      fits!\");\n                // value will fit in the integer... move bytes back\n                for (int j=ipos; j<endPos; j++) {\n                  val |= (tempArr[j] & 0xff) << (j<<3);\n                }\n                index[doc] = val;\n              } else {\n                // value won't fit... move integer into byte[]\n                for (int j=0; j<ipos; j++) {\n                  tempArr[j] = (byte)val;\n                  val >>>=8;\n                }\n                // point at the end index in the byte[]\n                index[doc] = (endPos<<8) | 1;\n                bytes[doc] = tempArr;\n                tempArr = new byte[12];\n              }\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","sourceOld":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : new BytesRef(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator();\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !t.startsWith(termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(delDocs, docsEnum);\n\n        final DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int chunk = docsEnum.read();\n          if (chunk <= 0) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF += chunk;\n\n          for (int i=0; i<chunk; i++) {\n            termInstances++;\n            int doc = bulkResult.docs.ints[i];\n            //System.out.println(\"    docID=\" + doc);\n            // add TNUM_OFFSET to the term number to make room for special reserved values:\n            // 0 (end term) and 1 (index into byte array follows)\n            int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n            lastTerm[doc] = termNum;\n            int val = index[doc];\n\n            if ((val & 0xff)==1) {\n              // index into byte array (actually the end of\n              // the doc-specific byte[] when building)\n              int pos = val >>> 8;\n              int ilen = vIntSize(delta);\n              byte[] arr = bytes[doc];\n              int newend = pos+ilen;\n              if (newend > arr.length) {\n                // We avoid a doubling strategy to lower memory usage.\n                // this faceting method isn't for docs with many terms.\n                // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n                // TODO: figure out what array lengths we can round up to w/o actually using more memory\n                // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n                // It should be safe to round up to the nearest 32 bits in any case.\n                int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n                byte[] newarr = new byte[newLen];\n                System.arraycopy(arr, 0, newarr, 0, pos);\n                arr = newarr;\n                bytes[doc] = newarr;\n              }\n              pos = writeInt(delta, arr, pos);\n              index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n            } else {\n              // OK, this int has data in it... find the end (a zero starting byte - not\n              // part of another number, hence not following a byte with the high bit set).\n              int ipos;\n              if (val==0) {\n                ipos=0;\n              } else if ((val & 0x0000ff80)==0) {\n                ipos=1;\n              } else if ((val & 0x00ff8000)==0) {\n                ipos=2;\n              } else if ((val & 0xff800000)==0) {\n                ipos=3;\n              } else {\n                ipos=4;\n              }\n\n              //System.out.println(\"      ipos=\" + ipos);\n\n              int endPos = writeInt(delta, tempArr, ipos);\n              //System.out.println(\"      endpos=\" + endPos);\n              if (endPos <= 4) {\n                //System.out.println(\"      fits!\");\n                // value will fit in the integer... move bytes back\n                for (int j=ipos; j<endPos; j++) {\n                  val |= (tempArr[j] & 0xff) << (j<<3);\n                }\n                index[doc] = val;\n              } else {\n                // value won't fit... move integer into byte[]\n                for (int j=0; j<ipos; j++) {\n                  tempArr[j] = (byte)val;\n                  val >>>=8;\n                }\n                // point at the end index in the byte[]\n                index[doc] = (endPos<<8) | 1;\n                bytes[doc] = tempArr;\n                tempArr = new byte[12];\n              }\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","sourceNew":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : new BytesRef(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator();\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !t.startsWith(termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(liveDocs, docsEnum);\n\n        final DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int chunk = docsEnum.read();\n          if (chunk <= 0) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF += chunk;\n\n          for (int i=0; i<chunk; i++) {\n            termInstances++;\n            int doc = bulkResult.docs.ints[i];\n            //System.out.println(\"    docID=\" + doc);\n            // add TNUM_OFFSET to the term number to make room for special reserved values:\n            // 0 (end term) and 1 (index into byte array follows)\n            int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n            lastTerm[doc] = termNum;\n            int val = index[doc];\n\n            if ((val & 0xff)==1) {\n              // index into byte array (actually the end of\n              // the doc-specific byte[] when building)\n              int pos = val >>> 8;\n              int ilen = vIntSize(delta);\n              byte[] arr = bytes[doc];\n              int newend = pos+ilen;\n              if (newend > arr.length) {\n                // We avoid a doubling strategy to lower memory usage.\n                // this faceting method isn't for docs with many terms.\n                // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n                // TODO: figure out what array lengths we can round up to w/o actually using more memory\n                // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n                // It should be safe to round up to the nearest 32 bits in any case.\n                int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n                byte[] newarr = new byte[newLen];\n                System.arraycopy(arr, 0, newarr, 0, pos);\n                arr = newarr;\n                bytes[doc] = newarr;\n              }\n              pos = writeInt(delta, arr, pos);\n              index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n            } else {\n              // OK, this int has data in it... find the end (a zero starting byte - not\n              // part of another number, hence not following a byte with the high bit set).\n              int ipos;\n              if (val==0) {\n                ipos=0;\n              } else if ((val & 0x0000ff80)==0) {\n                ipos=1;\n              } else if ((val & 0x00ff8000)==0) {\n                ipos=2;\n              } else if ((val & 0xff800000)==0) {\n                ipos=3;\n              } else {\n                ipos=4;\n              }\n\n              //System.out.println(\"      ipos=\" + ipos);\n\n              int endPos = writeInt(delta, tempArr, ipos);\n              //System.out.println(\"      endpos=\" + endPos);\n              if (endPos <= 4) {\n                //System.out.println(\"      fits!\");\n                // value will fit in the integer... move bytes back\n                for (int j=ipos; j<endPos; j++) {\n                  val |= (tempArr[j] & 0xff) << (j<<3);\n                }\n                index[doc] = val;\n              } else {\n                // value won't fit... move integer into byte[]\n                for (int j=0; j<ipos; j++) {\n                  tempArr[j] = (byte)val;\n                  val >>>=8;\n                }\n                // point at the end index in the byte[]\n                index[doc] = (endPos<<8) | 1;\n                bytes[doc] = tempArr;\n                tempArr = new byte[12];\n              }\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","sourceOld":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : new BytesRef(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator();\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seek(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !t.startsWith(termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(delDocs, docsEnum);\n\n        final DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int chunk = docsEnum.read();\n          if (chunk <= 0) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF += chunk;\n\n          for (int i=0; i<chunk; i++) {\n            termInstances++;\n            int doc = bulkResult.docs.ints[i];\n            //System.out.println(\"    docID=\" + doc);\n            // add TNUM_OFFSET to the term number to make room for special reserved values:\n            // 0 (end term) and 1 (index into byte array follows)\n            int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n            lastTerm[doc] = termNum;\n            int val = index[doc];\n\n            if ((val & 0xff)==1) {\n              // index into byte array (actually the end of\n              // the doc-specific byte[] when building)\n              int pos = val >>> 8;\n              int ilen = vIntSize(delta);\n              byte[] arr = bytes[doc];\n              int newend = pos+ilen;\n              if (newend > arr.length) {\n                // We avoid a doubling strategy to lower memory usage.\n                // this faceting method isn't for docs with many terms.\n                // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n                // TODO: figure out what array lengths we can round up to w/o actually using more memory\n                // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n                // It should be safe to round up to the nearest 32 bits in any case.\n                int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n                byte[] newarr = new byte[newLen];\n                System.arraycopy(arr, 0, newarr, 0, pos);\n                arr = newarr;\n                bytes[doc] = newarr;\n              }\n              pos = writeInt(delta, arr, pos);\n              index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n            } else {\n              // OK, this int has data in it... find the end (a zero starting byte - not\n              // part of another number, hence not following a byte with the high bit set).\n              int ipos;\n              if (val==0) {\n                ipos=0;\n              } else if ((val & 0x0000ff80)==0) {\n                ipos=1;\n              } else if ((val & 0x00ff8000)==0) {\n                ipos=2;\n              } else if ((val & 0xff800000)==0) {\n                ipos=3;\n              } else {\n                ipos=4;\n              }\n\n              //System.out.println(\"      ipos=\" + ipos);\n\n              int endPos = writeInt(delta, tempArr, ipos);\n              //System.out.println(\"      endpos=\" + endPos);\n              if (endPos <= 4) {\n                //System.out.println(\"      fits!\");\n                // value will fit in the integer... move bytes back\n                for (int j=ipos; j<endPos; j++) {\n                  val |= (tempArr[j] & 0xff) << (j<<3);\n                }\n                index[doc] = val;\n              } else {\n                // value won't fit... move integer into byte[]\n                for (int j=0; j<ipos; j++) {\n                  tempArr[j] = (byte)val;\n                  val >>>=8;\n                }\n                // point at the end index in the byte[]\n                index[doc] = (endPos<<8) | 1;\n                bytes[doc] = tempArr;\n                tempArr = new byte[12];\n              }\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3cc749c053615f5871f3b95715fe292f34e70a53","date":1321470575,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","sourceNew":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : new BytesRef(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator(null);\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !t.startsWith(termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(liveDocs, docsEnum);\n\n        final DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int chunk = docsEnum.read();\n          if (chunk <= 0) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF += chunk;\n\n          for (int i=0; i<chunk; i++) {\n            termInstances++;\n            int doc = bulkResult.docs.ints[i];\n            //System.out.println(\"    docID=\" + doc);\n            // add TNUM_OFFSET to the term number to make room for special reserved values:\n            // 0 (end term) and 1 (index into byte array follows)\n            int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n            lastTerm[doc] = termNum;\n            int val = index[doc];\n\n            if ((val & 0xff)==1) {\n              // index into byte array (actually the end of\n              // the doc-specific byte[] when building)\n              int pos = val >>> 8;\n              int ilen = vIntSize(delta);\n              byte[] arr = bytes[doc];\n              int newend = pos+ilen;\n              if (newend > arr.length) {\n                // We avoid a doubling strategy to lower memory usage.\n                // this faceting method isn't for docs with many terms.\n                // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n                // TODO: figure out what array lengths we can round up to w/o actually using more memory\n                // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n                // It should be safe to round up to the nearest 32 bits in any case.\n                int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n                byte[] newarr = new byte[newLen];\n                System.arraycopy(arr, 0, newarr, 0, pos);\n                arr = newarr;\n                bytes[doc] = newarr;\n              }\n              pos = writeInt(delta, arr, pos);\n              index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n            } else {\n              // OK, this int has data in it... find the end (a zero starting byte - not\n              // part of another number, hence not following a byte with the high bit set).\n              int ipos;\n              if (val==0) {\n                ipos=0;\n              } else if ((val & 0x0000ff80)==0) {\n                ipos=1;\n              } else if ((val & 0x00ff8000)==0) {\n                ipos=2;\n              } else if ((val & 0xff800000)==0) {\n                ipos=3;\n              } else {\n                ipos=4;\n              }\n\n              //System.out.println(\"      ipos=\" + ipos);\n\n              int endPos = writeInt(delta, tempArr, ipos);\n              //System.out.println(\"      endpos=\" + endPos);\n              if (endPos <= 4) {\n                //System.out.println(\"      fits!\");\n                // value will fit in the integer... move bytes back\n                for (int j=ipos; j<endPos; j++) {\n                  val |= (tempArr[j] & 0xff) << (j<<3);\n                }\n                index[doc] = val;\n              } else {\n                // value won't fit... move integer into byte[]\n                for (int j=0; j<ipos; j++) {\n                  tempArr[j] = (byte)val;\n                  val >>>=8;\n                }\n                // point at the end index in the byte[]\n                index[doc] = (endPos<<8) | 1;\n                bytes[doc] = tempArr;\n                tempArr = new byte[12];\n              }\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","sourceOld":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : new BytesRef(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator();\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !t.startsWith(termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(liveDocs, docsEnum);\n\n        final DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int chunk = docsEnum.read();\n          if (chunk <= 0) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF += chunk;\n\n          for (int i=0; i<chunk; i++) {\n            termInstances++;\n            int doc = bulkResult.docs.ints[i];\n            //System.out.println(\"    docID=\" + doc);\n            // add TNUM_OFFSET to the term number to make room for special reserved values:\n            // 0 (end term) and 1 (index into byte array follows)\n            int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n            lastTerm[doc] = termNum;\n            int val = index[doc];\n\n            if ((val & 0xff)==1) {\n              // index into byte array (actually the end of\n              // the doc-specific byte[] when building)\n              int pos = val >>> 8;\n              int ilen = vIntSize(delta);\n              byte[] arr = bytes[doc];\n              int newend = pos+ilen;\n              if (newend > arr.length) {\n                // We avoid a doubling strategy to lower memory usage.\n                // this faceting method isn't for docs with many terms.\n                // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n                // TODO: figure out what array lengths we can round up to w/o actually using more memory\n                // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n                // It should be safe to round up to the nearest 32 bits in any case.\n                int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n                byte[] newarr = new byte[newLen];\n                System.arraycopy(arr, 0, newarr, 0, pos);\n                arr = newarr;\n                bytes[doc] = newarr;\n              }\n              pos = writeInt(delta, arr, pos);\n              index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n            } else {\n              // OK, this int has data in it... find the end (a zero starting byte - not\n              // part of another number, hence not following a byte with the high bit set).\n              int ipos;\n              if (val==0) {\n                ipos=0;\n              } else if ((val & 0x0000ff80)==0) {\n                ipos=1;\n              } else if ((val & 0x00ff8000)==0) {\n                ipos=2;\n              } else if ((val & 0xff800000)==0) {\n                ipos=3;\n              } else {\n                ipos=4;\n              }\n\n              //System.out.println(\"      ipos=\" + ipos);\n\n              int endPos = writeInt(delta, tempArr, ipos);\n              //System.out.println(\"      endpos=\" + endPos);\n              if (endPos <= 4) {\n                //System.out.println(\"      fits!\");\n                // value will fit in the integer... move bytes back\n                for (int j=ipos; j<endPos; j++) {\n                  val |= (tempArr[j] & 0xff) << (j<<3);\n                }\n                index[doc] = val;\n              } else {\n                // value won't fit... move integer into byte[]\n                for (int j=0; j<ipos; j++) {\n                  tempArr[j] = (byte)val;\n                  val >>>=8;\n                }\n                // point at the end index in the byte[]\n                index[doc] = (endPos<<8) | 1;\n                bytes[doc] = tempArr;\n                tempArr = new byte[12];\n              }\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e6e919043fa85ee891123768dd655a98edbbf63c","date":1322225413,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","sourceNew":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : BytesRef.deepCopyOf(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator(null);\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !t.startsWith(termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(liveDocs, docsEnum);\n\n        final DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int chunk = docsEnum.read();\n          if (chunk <= 0) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF += chunk;\n\n          for (int i=0; i<chunk; i++) {\n            termInstances++;\n            int doc = bulkResult.docs.ints[i];\n            //System.out.println(\"    docID=\" + doc);\n            // add TNUM_OFFSET to the term number to make room for special reserved values:\n            // 0 (end term) and 1 (index into byte array follows)\n            int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n            lastTerm[doc] = termNum;\n            int val = index[doc];\n\n            if ((val & 0xff)==1) {\n              // index into byte array (actually the end of\n              // the doc-specific byte[] when building)\n              int pos = val >>> 8;\n              int ilen = vIntSize(delta);\n              byte[] arr = bytes[doc];\n              int newend = pos+ilen;\n              if (newend > arr.length) {\n                // We avoid a doubling strategy to lower memory usage.\n                // this faceting method isn't for docs with many terms.\n                // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n                // TODO: figure out what array lengths we can round up to w/o actually using more memory\n                // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n                // It should be safe to round up to the nearest 32 bits in any case.\n                int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n                byte[] newarr = new byte[newLen];\n                System.arraycopy(arr, 0, newarr, 0, pos);\n                arr = newarr;\n                bytes[doc] = newarr;\n              }\n              pos = writeInt(delta, arr, pos);\n              index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n            } else {\n              // OK, this int has data in it... find the end (a zero starting byte - not\n              // part of another number, hence not following a byte with the high bit set).\n              int ipos;\n              if (val==0) {\n                ipos=0;\n              } else if ((val & 0x0000ff80)==0) {\n                ipos=1;\n              } else if ((val & 0x00ff8000)==0) {\n                ipos=2;\n              } else if ((val & 0xff800000)==0) {\n                ipos=3;\n              } else {\n                ipos=4;\n              }\n\n              //System.out.println(\"      ipos=\" + ipos);\n\n              int endPos = writeInt(delta, tempArr, ipos);\n              //System.out.println(\"      endpos=\" + endPos);\n              if (endPos <= 4) {\n                //System.out.println(\"      fits!\");\n                // value will fit in the integer... move bytes back\n                for (int j=ipos; j<endPos; j++) {\n                  val |= (tempArr[j] & 0xff) << (j<<3);\n                }\n                index[doc] = val;\n              } else {\n                // value won't fit... move integer into byte[]\n                for (int j=0; j<ipos; j++) {\n                  tempArr[j] = (byte)val;\n                  val >>>=8;\n                }\n                // point at the end index in the byte[]\n                index[doc] = (endPos<<8) | 1;\n                bytes[doc] = tempArr;\n                tempArr = new byte[12];\n              }\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","sourceOld":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : new BytesRef(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator(null);\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !t.startsWith(termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(liveDocs, docsEnum);\n\n        final DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int chunk = docsEnum.read();\n          if (chunk <= 0) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF += chunk;\n\n          for (int i=0; i<chunk; i++) {\n            termInstances++;\n            int doc = bulkResult.docs.ints[i];\n            //System.out.println(\"    docID=\" + doc);\n            // add TNUM_OFFSET to the term number to make room for special reserved values:\n            // 0 (end term) and 1 (index into byte array follows)\n            int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n            lastTerm[doc] = termNum;\n            int val = index[doc];\n\n            if ((val & 0xff)==1) {\n              // index into byte array (actually the end of\n              // the doc-specific byte[] when building)\n              int pos = val >>> 8;\n              int ilen = vIntSize(delta);\n              byte[] arr = bytes[doc];\n              int newend = pos+ilen;\n              if (newend > arr.length) {\n                // We avoid a doubling strategy to lower memory usage.\n                // this faceting method isn't for docs with many terms.\n                // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n                // TODO: figure out what array lengths we can round up to w/o actually using more memory\n                // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n                // It should be safe to round up to the nearest 32 bits in any case.\n                int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n                byte[] newarr = new byte[newLen];\n                System.arraycopy(arr, 0, newarr, 0, pos);\n                arr = newarr;\n                bytes[doc] = newarr;\n              }\n              pos = writeInt(delta, arr, pos);\n              index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n            } else {\n              // OK, this int has data in it... find the end (a zero starting byte - not\n              // part of another number, hence not following a byte with the high bit set).\n              int ipos;\n              if (val==0) {\n                ipos=0;\n              } else if ((val & 0x0000ff80)==0) {\n                ipos=1;\n              } else if ((val & 0x00ff8000)==0) {\n                ipos=2;\n              } else if ((val & 0xff800000)==0) {\n                ipos=3;\n              } else {\n                ipos=4;\n              }\n\n              //System.out.println(\"      ipos=\" + ipos);\n\n              int endPos = writeInt(delta, tempArr, ipos);\n              //System.out.println(\"      endpos=\" + endPos);\n              if (endPos <= 4) {\n                //System.out.println(\"      fits!\");\n                // value will fit in the integer... move bytes back\n                for (int j=ipos; j<endPos; j++) {\n                  val |= (tempArr[j] & 0xff) << (j<<3);\n                }\n                index[doc] = val;\n              } else {\n                // value won't fit... move integer into byte[]\n                for (int j=0; j<ipos; j++) {\n                  tempArr[j] = (byte)val;\n                  val >>>=8;\n                }\n                // point at the end index in the byte[]\n                index[doc] = (endPos<<8) | 1;\n                bytes[doc] = tempArr;\n                tempArr = new byte[12];\n              }\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"61f30939a6ca0891c7b0c0f34aa43800bd4c9a15","date":1322511317,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","sourceNew":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : BytesRef.deepCopyOf(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator(null);\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !StringHelper.startsWith(t, termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(liveDocs, docsEnum);\n\n        final DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int chunk = docsEnum.read();\n          if (chunk <= 0) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF += chunk;\n\n          for (int i=0; i<chunk; i++) {\n            termInstances++;\n            int doc = bulkResult.docs.ints[i];\n            //System.out.println(\"    docID=\" + doc);\n            // add TNUM_OFFSET to the term number to make room for special reserved values:\n            // 0 (end term) and 1 (index into byte array follows)\n            int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n            lastTerm[doc] = termNum;\n            int val = index[doc];\n\n            if ((val & 0xff)==1) {\n              // index into byte array (actually the end of\n              // the doc-specific byte[] when building)\n              int pos = val >>> 8;\n              int ilen = vIntSize(delta);\n              byte[] arr = bytes[doc];\n              int newend = pos+ilen;\n              if (newend > arr.length) {\n                // We avoid a doubling strategy to lower memory usage.\n                // this faceting method isn't for docs with many terms.\n                // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n                // TODO: figure out what array lengths we can round up to w/o actually using more memory\n                // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n                // It should be safe to round up to the nearest 32 bits in any case.\n                int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n                byte[] newarr = new byte[newLen];\n                System.arraycopy(arr, 0, newarr, 0, pos);\n                arr = newarr;\n                bytes[doc] = newarr;\n              }\n              pos = writeInt(delta, arr, pos);\n              index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n            } else {\n              // OK, this int has data in it... find the end (a zero starting byte - not\n              // part of another number, hence not following a byte with the high bit set).\n              int ipos;\n              if (val==0) {\n                ipos=0;\n              } else if ((val & 0x0000ff80)==0) {\n                ipos=1;\n              } else if ((val & 0x00ff8000)==0) {\n                ipos=2;\n              } else if ((val & 0xff800000)==0) {\n                ipos=3;\n              } else {\n                ipos=4;\n              }\n\n              //System.out.println(\"      ipos=\" + ipos);\n\n              int endPos = writeInt(delta, tempArr, ipos);\n              //System.out.println(\"      endpos=\" + endPos);\n              if (endPos <= 4) {\n                //System.out.println(\"      fits!\");\n                // value will fit in the integer... move bytes back\n                for (int j=ipos; j<endPos; j++) {\n                  val |= (tempArr[j] & 0xff) << (j<<3);\n                }\n                index[doc] = val;\n              } else {\n                // value won't fit... move integer into byte[]\n                for (int j=0; j<ipos; j++) {\n                  tempArr[j] = (byte)val;\n                  val >>>=8;\n                }\n                // point at the end index in the byte[]\n                index[doc] = (endPos<<8) | 1;\n                bytes[doc] = tempArr;\n                tempArr = new byte[12];\n              }\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","sourceOld":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : BytesRef.deepCopyOf(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator(null);\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !t.startsWith(termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(liveDocs, docsEnum);\n\n        final DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int chunk = docsEnum.read();\n          if (chunk <= 0) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF += chunk;\n\n          for (int i=0; i<chunk; i++) {\n            termInstances++;\n            int doc = bulkResult.docs.ints[i];\n            //System.out.println(\"    docID=\" + doc);\n            // add TNUM_OFFSET to the term number to make room for special reserved values:\n            // 0 (end term) and 1 (index into byte array follows)\n            int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n            lastTerm[doc] = termNum;\n            int val = index[doc];\n\n            if ((val & 0xff)==1) {\n              // index into byte array (actually the end of\n              // the doc-specific byte[] when building)\n              int pos = val >>> 8;\n              int ilen = vIntSize(delta);\n              byte[] arr = bytes[doc];\n              int newend = pos+ilen;\n              if (newend > arr.length) {\n                // We avoid a doubling strategy to lower memory usage.\n                // this faceting method isn't for docs with many terms.\n                // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n                // TODO: figure out what array lengths we can round up to w/o actually using more memory\n                // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n                // It should be safe to round up to the nearest 32 bits in any case.\n                int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n                byte[] newarr = new byte[newLen];\n                System.arraycopy(arr, 0, newarr, 0, pos);\n                arr = newarr;\n                bytes[doc] = newarr;\n              }\n              pos = writeInt(delta, arr, pos);\n              index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n            } else {\n              // OK, this int has data in it... find the end (a zero starting byte - not\n              // part of another number, hence not following a byte with the high bit set).\n              int ipos;\n              if (val==0) {\n                ipos=0;\n              } else if ((val & 0x0000ff80)==0) {\n                ipos=1;\n              } else if ((val & 0x00ff8000)==0) {\n                ipos=2;\n              } else if ((val & 0xff800000)==0) {\n                ipos=3;\n              } else {\n                ipos=4;\n              }\n\n              //System.out.println(\"      ipos=\" + ipos);\n\n              int endPos = writeInt(delta, tempArr, ipos);\n              //System.out.println(\"      endpos=\" + endPos);\n              if (endPos <= 4) {\n                //System.out.println(\"      fits!\");\n                // value will fit in the integer... move bytes back\n                for (int j=ipos; j<endPos; j++) {\n                  val |= (tempArr[j] & 0xff) << (j<<3);\n                }\n                index[doc] = val;\n              } else {\n                // value won't fit... move integer into byte[]\n                for (int j=0; j<ipos; j++) {\n                  tempArr[j] = (byte)val;\n                  val >>>=8;\n                }\n                // point at the end index in the byte[]\n                index[doc] = (endPos<<8) | 1;\n                bytes[doc] = tempArr;\n                tempArr = new byte[12];\n              }\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0445bcd8433e331f296f5502fc089b336cbac3a6","date":1322630375,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","sourceNew":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : BytesRef.deepCopyOf(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator(null);\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !StringHelper.startsWith(t, termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(liveDocs, docsEnum);\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int doc = docsEnum.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF ++;\n          termInstances++;\n          \n          //System.out.println(\"    docID=\" + doc);\n          // add TNUM_OFFSET to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            //System.out.println(\"      ipos=\" + ipos);\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            //System.out.println(\"      endpos=\" + endPos);\n            if (endPos <= 4) {\n              //System.out.println(\"      fits!\");\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","sourceOld":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : BytesRef.deepCopyOf(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator(null);\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !StringHelper.startsWith(t, termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(liveDocs, docsEnum);\n\n        final DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int chunk = docsEnum.read();\n          if (chunk <= 0) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF += chunk;\n\n          for (int i=0; i<chunk; i++) {\n            termInstances++;\n            int doc = bulkResult.docs.ints[i];\n            //System.out.println(\"    docID=\" + doc);\n            // add TNUM_OFFSET to the term number to make room for special reserved values:\n            // 0 (end term) and 1 (index into byte array follows)\n            int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n            lastTerm[doc] = termNum;\n            int val = index[doc];\n\n            if ((val & 0xff)==1) {\n              // index into byte array (actually the end of\n              // the doc-specific byte[] when building)\n              int pos = val >>> 8;\n              int ilen = vIntSize(delta);\n              byte[] arr = bytes[doc];\n              int newend = pos+ilen;\n              if (newend > arr.length) {\n                // We avoid a doubling strategy to lower memory usage.\n                // this faceting method isn't for docs with many terms.\n                // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n                // TODO: figure out what array lengths we can round up to w/o actually using more memory\n                // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n                // It should be safe to round up to the nearest 32 bits in any case.\n                int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n                byte[] newarr = new byte[newLen];\n                System.arraycopy(arr, 0, newarr, 0, pos);\n                arr = newarr;\n                bytes[doc] = newarr;\n              }\n              pos = writeInt(delta, arr, pos);\n              index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n            } else {\n              // OK, this int has data in it... find the end (a zero starting byte - not\n              // part of another number, hence not following a byte with the high bit set).\n              int ipos;\n              if (val==0) {\n                ipos=0;\n              } else if ((val & 0x0000ff80)==0) {\n                ipos=1;\n              } else if ((val & 0x00ff8000)==0) {\n                ipos=2;\n              } else if ((val & 0xff800000)==0) {\n                ipos=3;\n              } else {\n                ipos=4;\n              }\n\n              //System.out.println(\"      ipos=\" + ipos);\n\n              int endPos = writeInt(delta, tempArr, ipos);\n              //System.out.println(\"      endpos=\" + endPos);\n              if (endPos <= 4) {\n                //System.out.println(\"      fits!\");\n                // value will fit in the integer... move bytes back\n                for (int j=ipos; j<endPos; j++) {\n                  val |= (tempArr[j] & 0xff) << (j<<3);\n                }\n                index[doc] = val;\n              } else {\n                // value won't fit... move integer into byte[]\n                for (int j=0; j<ipos; j++) {\n                  tempArr[j] = (byte)val;\n                  val >>>=8;\n                }\n                // point at the end index in the byte[]\n                index[doc] = (endPos<<8) | 1;\n                bytes[doc] = tempArr;\n                tempArr = new byte[12];\n              }\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"872cff1d3a554e0cd64014cd97f88d3002b0f491","date":1323024658,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","sourceNew":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : BytesRef.deepCopyOf(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator(null);\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !StringHelper.startsWith(t, termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(liveDocs, docsEnum, false);\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int doc = docsEnum.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF ++;\n          termInstances++;\n          \n          //System.out.println(\"    docID=\" + doc);\n          // add TNUM_OFFSET to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            //System.out.println(\"      ipos=\" + ipos);\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            //System.out.println(\"      endpos=\" + endPos);\n            if (endPos <= 4) {\n              //System.out.println(\"      fits!\");\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","sourceOld":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : BytesRef.deepCopyOf(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator(null);\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !StringHelper.startsWith(t, termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(liveDocs, docsEnum);\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int doc = docsEnum.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF ++;\n          termInstances++;\n          \n          //System.out.println(\"    docID=\" + doc);\n          // add TNUM_OFFSET to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            //System.out.println(\"      ipos=\" + ipos);\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            //System.out.println(\"      endpos=\" + endPos);\n            if (endPos <= 4) {\n              //System.out.println(\"      fits!\");\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","bugFix":null,"bugIntro":["02331260bb246364779cb6f04919ca47900d01bb","02331260bb246364779cb6f04919ca47900d01bb"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b65b350ca9588f9fc76ce7d6804160d06c45ff42","date":1323026297,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","sourceNew":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : BytesRef.deepCopyOf(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator(null);\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !StringHelper.startsWith(t, termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(liveDocs, docsEnum, false);\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int doc = docsEnum.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF ++;\n          termInstances++;\n          \n          //System.out.println(\"    docID=\" + doc);\n          // add TNUM_OFFSET to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            //System.out.println(\"      ipos=\" + ipos);\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            //System.out.println(\"      endpos=\" + endPos);\n            if (endPos <= 4) {\n              //System.out.println(\"      fits!\");\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","sourceOld":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : BytesRef.deepCopyOf(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator(null);\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !StringHelper.startsWith(t, termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(liveDocs, docsEnum);\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int doc = docsEnum.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF ++;\n          termInstances++;\n          \n          //System.out.println(\"    docID=\" + doc);\n          // add TNUM_OFFSET to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            //System.out.println(\"      ipos=\" + ipos);\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            //System.out.println(\"      endpos=\" + endPos);\n            if (endPos <= 4) {\n              //System.out.println(\"      fits!\");\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"59fc0e55b44c555c39d950def9414b5596c6ebe2","date":1327620010,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","sourceNew":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : BytesRef.deepCopyOf(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Fields fields = reader.fields();\n    if (fields == null) {\n      // No terms\n      return;\n    }\n    final Terms terms = fields.terms(field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator(null);\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits liveDocs = reader.getLiveDocs();\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !StringHelper.startsWith(t, termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(liveDocs, docsEnum, false);\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int doc = docsEnum.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF ++;\n          termInstances++;\n          \n          //System.out.println(\"    docID=\" + doc);\n          // add TNUM_OFFSET to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            //System.out.println(\"      ipos=\" + ipos);\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            //System.out.println(\"      endpos=\" + endPos);\n            if (endPos <= 4) {\n              //System.out.println(\"      fits!\");\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","sourceOld":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : BytesRef.deepCopyOf(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator(null);\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !StringHelper.startsWith(t, termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(liveDocs, docsEnum, false);\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int doc = docsEnum.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF ++;\n          termInstances++;\n          \n          //System.out.println(\"    docID=\" + doc);\n          // add TNUM_OFFSET to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            //System.out.println(\"      ipos=\" + ipos);\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            //System.out.println(\"      endpos=\" + endPos);\n            if (endPos <= 4) {\n              //System.out.println(\"      fits!\");\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"78a55f24d9b493c2a1cecf79f1d78279062b545b","date":1327688152,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","sourceNew":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : BytesRef.deepCopyOf(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Fields fields = reader.fields();\n    if (fields == null) {\n      // No terms\n      return;\n    }\n    final Terms terms = fields.terms(field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator(null);\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits liveDocs = reader.getLiveDocs();\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !StringHelper.startsWith(t, termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(liveDocs, docsEnum, false);\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int doc = docsEnum.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF ++;\n          termInstances++;\n          \n          //System.out.println(\"    docID=\" + doc);\n          // add TNUM_OFFSET to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            //System.out.println(\"      ipos=\" + ipos);\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            //System.out.println(\"      endpos=\" + endPos);\n            if (endPos <= 4) {\n              //System.out.println(\"      fits!\");\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","sourceOld":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : BytesRef.deepCopyOf(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator(null);\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !StringHelper.startsWith(t, termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(liveDocs, docsEnum, false);\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int doc = docsEnum.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF ++;\n          termInstances++;\n          \n          //System.out.println(\"    docID=\" + doc);\n          // add TNUM_OFFSET to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            //System.out.println(\"      ipos=\" + ipos);\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            //System.out.println(\"      endpos=\" + endPos);\n            if (endPos <= 4) {\n              //System.out.println(\"      fits!\");\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fd92b8bcc88e969302510acf77bd6970da3994c4","date":1327839530,"type":5,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(AtomicIndexReader,BytesRef).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","sourceNew":"  // Call this only once (if you subclass!)\n  protected void uninvert(final AtomicIndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : BytesRef.deepCopyOf(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Fields fields = reader.fields();\n    if (fields == null) {\n      // No terms\n      return;\n    }\n    final Terms terms = fields.terms(field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator(null);\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits liveDocs = reader.getLiveDocs();\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !StringHelper.startsWith(t, termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(liveDocs, docsEnum, false);\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int doc = docsEnum.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF ++;\n          termInstances++;\n          \n          //System.out.println(\"    docID=\" + doc);\n          // add TNUM_OFFSET to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            //System.out.println(\"      ipos=\" + ipos);\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            //System.out.println(\"      endpos=\" + endPos);\n            if (endPos <= 4) {\n              //System.out.println(\"      fits!\");\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","sourceOld":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : BytesRef.deepCopyOf(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator(null);\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !StringHelper.startsWith(t, termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(liveDocs, docsEnum, false);\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int doc = docsEnum.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF ++;\n          termInstances++;\n          \n          //System.out.println(\"    docID=\" + doc);\n          // add TNUM_OFFSET to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            //System.out.println(\"      ipos=\" + ipos);\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            //System.out.println(\"      endpos=\" + endPos);\n            if (endPos <= 4) {\n              //System.out.println(\"      fits!\");\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5cab9a86bd67202d20b6adc463008c8e982b070a","date":1327966443,"type":5,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(AtomicReader,BytesRef).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","sourceNew":"  // Call this only once (if you subclass!)\n  protected void uninvert(final AtomicReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : BytesRef.deepCopyOf(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Fields fields = reader.fields();\n    if (fields == null) {\n      // No terms\n      return;\n    }\n    final Terms terms = fields.terms(field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator(null);\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits liveDocs = reader.getLiveDocs();\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !StringHelper.startsWith(t, termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(liveDocs, docsEnum, false);\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int doc = docsEnum.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF ++;\n          termInstances++;\n          \n          //System.out.println(\"    docID=\" + doc);\n          // add TNUM_OFFSET to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            //System.out.println(\"      ipos=\" + ipos);\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            //System.out.println(\"      endpos=\" + endPos);\n            if (endPos <= 4) {\n              //System.out.println(\"      fits!\");\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","sourceOld":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : BytesRef.deepCopyOf(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Fields fields = reader.fields();\n    if (fields == null) {\n      // No terms\n      return;\n    }\n    final Terms terms = fields.terms(field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator(null);\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits liveDocs = reader.getLiveDocs();\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !StringHelper.startsWith(t, termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(liveDocs, docsEnum, false);\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int doc = docsEnum.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF ++;\n          termInstances++;\n          \n          //System.out.println(\"    docID=\" + doc);\n          // add TNUM_OFFSET to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            //System.out.println(\"      ipos=\" + ipos);\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            //System.out.println(\"      endpos=\" + endPos);\n            if (endPos <= 4) {\n              //System.out.println(\"      fits!\");\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"f20bb72b0dfa147c6f1fcd7693102c63a2714eae":["95ae76773bf2b95987d5f9c8f566ab3738953fb4"],"61f30939a6ca0891c7b0c0f34aa43800bd4c9a15":["e6e919043fa85ee891123768dd655a98edbbf63c"],"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb":["fd9cc9d77712aba3662f24632df7539ab75e3667"],"fd92b8bcc88e969302510acf77bd6970da3994c4":["872cff1d3a554e0cd64014cd97f88d3002b0f491","59fc0e55b44c555c39d950def9414b5596c6ebe2"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"fd9cc9d77712aba3662f24632df7539ab75e3667":["f8f944ac3fe3f9d40d825177507fb381d2b106b3"],"e6e919043fa85ee891123768dd655a98edbbf63c":["3cc749c053615f5871f3b95715fe292f34e70a53"],"21486a8058ee8d7503c7d7a5e55b6c3a218d0942":["0663cc678850ea2c51151f9fd217342ea35b8568"],"78a55f24d9b493c2a1cecf79f1d78279062b545b":["872cff1d3a554e0cd64014cd97f88d3002b0f491","59fc0e55b44c555c39d950def9414b5596c6ebe2"],"d4d5df8e07c035d62d982894b439322da40e0938":["45669a651c970812a680841b97a77cce06af559f","f8f944ac3fe3f9d40d825177507fb381d2b106b3"],"7a405e749df166cf8c456ac9381f77f6c99a6270":["21486a8058ee8d7503c7d7a5e55b6c3a218d0942"],"0445bcd8433e331f296f5502fc089b336cbac3a6":["61f30939a6ca0891c7b0c0f34aa43800bd4c9a15"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f8f944ac3fe3f9d40d825177507fb381d2b106b3"],"3cc749c053615f5871f3b95715fe292f34e70a53":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"0663cc678850ea2c51151f9fd217342ea35b8568":["f20bb72b0dfa147c6f1fcd7693102c63a2714eae"],"872cff1d3a554e0cd64014cd97f88d3002b0f491":["0445bcd8433e331f296f5502fc089b336cbac3a6"],"2553b00f699380c64959ccb27991289aae87be2e":["a3776dccca01c11e7046323cfad46a3b4a471233","fd9cc9d77712aba3662f24632df7539ab75e3667"],"f8f944ac3fe3f9d40d825177507fb381d2b106b3":["7a405e749df166cf8c456ac9381f77f6c99a6270"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["f8f944ac3fe3f9d40d825177507fb381d2b106b3","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["59fc0e55b44c555c39d950def9414b5596c6ebe2","fd92b8bcc88e969302510acf77bd6970da3994c4"],"817d8435e9135b756f08ce6710ab0baac51bdf88":["2553b00f699380c64959ccb27991289aae87be2e","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"b65b350ca9588f9fc76ce7d6804160d06c45ff42":["0445bcd8433e331f296f5502fc089b336cbac3a6","872cff1d3a554e0cd64014cd97f88d3002b0f491"],"a3776dccca01c11e7046323cfad46a3b4a471233":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f8f944ac3fe3f9d40d825177507fb381d2b106b3"],"59fc0e55b44c555c39d950def9414b5596c6ebe2":["872cff1d3a554e0cd64014cd97f88d3002b0f491"],"95ae76773bf2b95987d5f9c8f566ab3738953fb4":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"45669a651c970812a680841b97a77cce06af559f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","95ae76773bf2b95987d5f9c8f566ab3738953fb4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["5cab9a86bd67202d20b6adc463008c8e982b070a"]},"commit2Childs":{"f20bb72b0dfa147c6f1fcd7693102c63a2714eae":["0663cc678850ea2c51151f9fd217342ea35b8568"],"61f30939a6ca0891c7b0c0f34aa43800bd4c9a15":["0445bcd8433e331f296f5502fc089b336cbac3a6"],"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb":["3cc749c053615f5871f3b95715fe292f34e70a53","d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88"],"fd92b8bcc88e969302510acf77bd6970da3994c4":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["135621f3a0670a9394eb563224a3b76cc4dddc0f","a3776dccca01c11e7046323cfad46a3b4a471233","95ae76773bf2b95987d5f9c8f566ab3738953fb4","45669a651c970812a680841b97a77cce06af559f"],"fd9cc9d77712aba3662f24632df7539ab75e3667":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","2553b00f699380c64959ccb27991289aae87be2e"],"e6e919043fa85ee891123768dd655a98edbbf63c":["61f30939a6ca0891c7b0c0f34aa43800bd4c9a15"],"21486a8058ee8d7503c7d7a5e55b6c3a218d0942":["7a405e749df166cf8c456ac9381f77f6c99a6270"],"78a55f24d9b493c2a1cecf79f1d78279062b545b":[],"d4d5df8e07c035d62d982894b439322da40e0938":[],"7a405e749df166cf8c456ac9381f77f6c99a6270":["f8f944ac3fe3f9d40d825177507fb381d2b106b3"],"0445bcd8433e331f296f5502fc089b336cbac3a6":["872cff1d3a554e0cd64014cd97f88d3002b0f491","b65b350ca9588f9fc76ce7d6804160d06c45ff42"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":[],"3cc749c053615f5871f3b95715fe292f34e70a53":["e6e919043fa85ee891123768dd655a98edbbf63c"],"0663cc678850ea2c51151f9fd217342ea35b8568":["21486a8058ee8d7503c7d7a5e55b6c3a218d0942"],"872cff1d3a554e0cd64014cd97f88d3002b0f491":["fd92b8bcc88e969302510acf77bd6970da3994c4","78a55f24d9b493c2a1cecf79f1d78279062b545b","b65b350ca9588f9fc76ce7d6804160d06c45ff42","59fc0e55b44c555c39d950def9414b5596c6ebe2"],"2553b00f699380c64959ccb27991289aae87be2e":["817d8435e9135b756f08ce6710ab0baac51bdf88"],"f8f944ac3fe3f9d40d825177507fb381d2b106b3":["fd9cc9d77712aba3662f24632df7539ab75e3667","d4d5df8e07c035d62d982894b439322da40e0938","135621f3a0670a9394eb563224a3b76cc4dddc0f","d083e83f225b11e5fdd900e83d26ddb385b6955c","a3776dccca01c11e7046323cfad46a3b4a471233"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":[],"5cab9a86bd67202d20b6adc463008c8e982b070a":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"817d8435e9135b756f08ce6710ab0baac51bdf88":[],"b65b350ca9588f9fc76ce7d6804160d06c45ff42":[],"a3776dccca01c11e7046323cfad46a3b4a471233":["2553b00f699380c64959ccb27991289aae87be2e"],"59fc0e55b44c555c39d950def9414b5596c6ebe2":["fd92b8bcc88e969302510acf77bd6970da3994c4","78a55f24d9b493c2a1cecf79f1d78279062b545b","5cab9a86bd67202d20b6adc463008c8e982b070a"],"95ae76773bf2b95987d5f9c8f566ab3738953fb4":["f20bb72b0dfa147c6f1fcd7693102c63a2714eae","45669a651c970812a680841b97a77cce06af559f"],"45669a651c970812a680841b97a77cce06af559f":["d4d5df8e07c035d62d982894b439322da40e0938"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["78a55f24d9b493c2a1cecf79f1d78279062b545b","d4d5df8e07c035d62d982894b439322da40e0938","135621f3a0670a9394eb563224a3b76cc4dddc0f","d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88","b65b350ca9588f9fc76ce7d6804160d06c45ff42","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}