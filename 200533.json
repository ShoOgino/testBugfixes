{"path":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsReader.BlockState#doReset(int).mjava","commits":[{"id":"1f09f483a0844bb9dc34fb10380cb053aa96219b","date":1418894001,"type":1,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsReader.BlockState#doReset(int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsReader.ChunkIterator#next(int).mjava","sourceNew":"    private void doReset(int docID) throws IOException {\n      docBase = fieldsStream.readVInt();\n      final int token = fieldsStream.readVInt();\n      chunkDocs = token >>> 1;\n      if (contains(docID) == false\n          || docBase + chunkDocs > numDocs) {\n        throw new CorruptIndexException(\"Corrupted: docID=\" + docID\n            + \", docBase=\" + docBase + \", chunkDocs=\" + chunkDocs\n            + \", numDocs=\" + numDocs, fieldsStream);\n      }\n\n      sliced = (token & 1) != 0;\n\n      offsets = ArrayUtil.grow(offsets, chunkDocs + 1);\n      numStoredFields = ArrayUtil.grow(numStoredFields, chunkDocs);\n\n      if (chunkDocs == 1) {\n        numStoredFields[0] = fieldsStream.readVInt();\n        offsets[1] = fieldsStream.readVInt();\n      } else {\n        // Number of stored fields per document\n        final int bitsPerStoredFields = fieldsStream.readVInt();\n        if (bitsPerStoredFields == 0) {\n          Arrays.fill(numStoredFields, 0, chunkDocs, fieldsStream.readVInt());\n        } else if (bitsPerStoredFields > 31) {\n          throw new CorruptIndexException(\"bitsPerStoredFields=\" + bitsPerStoredFields, fieldsStream);\n        } else {\n          final PackedInts.ReaderIterator it = PackedInts.getReaderIteratorNoHeader(fieldsStream, PackedInts.Format.PACKED, packedIntsVersion, chunkDocs, bitsPerStoredFields, 1);\n          for (int i = 0; i < chunkDocs; ++i) {\n            numStoredFields[i] = (int) it.next();\n          }\n        }\n\n        // The stream encodes the length of each document and we decode\n        // it into a list of monotonically increasing offsets\n        final int bitsPerLength = fieldsStream.readVInt();\n        if (bitsPerLength == 0) {\n          final int length = fieldsStream.readVInt();\n          for (int i = 0; i < chunkDocs; ++i) {\n            offsets[1 + i] = (1 + i) * length;\n          }\n        } else if (bitsPerStoredFields > 31) {\n          throw new CorruptIndexException(\"bitsPerLength=\" + bitsPerLength, fieldsStream);\n        } else {\n          final PackedInts.ReaderIterator it = PackedInts.getReaderIteratorNoHeader(fieldsStream, PackedInts.Format.PACKED, packedIntsVersion, chunkDocs, bitsPerLength, 1);\n          for (int i = 0; i < chunkDocs; ++i) {\n            offsets[i + 1] = (int) it.next();\n          }\n          for (int i = 0; i < chunkDocs; ++i) {\n            offsets[i + 1] += offsets[i];\n          }\n        }\n\n        // Additional validation: only the empty document has a serialized length of 0\n        for (int i = 0; i < chunkDocs; ++i) {\n          final int len = offsets[i + 1] - offsets[i];\n          final int storedFields = numStoredFields[i];\n          if ((len == 0) != (storedFields == 0)) {\n            throw new CorruptIndexException(\"length=\" + len + \", numStoredFields=\" + storedFields, fieldsStream);\n          }\n        }\n\n      }\n\n      startPointer = fieldsStream.getFilePointer();\n\n      if (merging) {\n        final int totalLength = offsets[chunkDocs];\n        // decompress eagerly\n        if (sliced) {\n          bytes.offset = bytes.length = 0;\n          for (int decompressed = 0; decompressed < totalLength; ) {\n            final int toDecompress = Math.min(totalLength - decompressed, chunkSize);\n            decompressor.decompress(fieldsStream, toDecompress, 0, toDecompress, spare);\n            bytes.bytes = ArrayUtil.grow(bytes.bytes, bytes.length + spare.length);\n            System.arraycopy(spare.bytes, spare.offset, bytes.bytes, bytes.length, spare.length);\n            bytes.length += spare.length;\n            decompressed += toDecompress;\n          }\n        } else {\n          decompressor.decompress(fieldsStream, totalLength, 0, totalLength, bytes);\n        }\n        if (bytes.length != totalLength) {\n          throw new CorruptIndexException(\"Corrupted: expected chunk size = \" + totalLength + \", got \" + bytes.length, fieldsStream);\n        }\n      }\n    }\n\n","sourceOld":"    /**\n     * Go to the chunk containing the provided doc ID.\n     */\n    void next(int doc) throws IOException {\n      assert doc >= docBase + chunkDocs : doc + \" \" + docBase + \" \" + chunkDocs;\n      fieldsStream.seek(indexReader.getStartPointer(doc));\n\n      final int docBase = fieldsStream.readVInt();\n      final int chunkDocs = fieldsStream.readVInt();\n      if (docBase < this.docBase + this.chunkDocs\n          || docBase + chunkDocs > numDocs) {\n        throw new CorruptIndexException(\"Corrupted: current docBase=\" + this.docBase\n            + \", current numDocs=\" + this.chunkDocs + \", new docBase=\" + docBase\n            + \", new numDocs=\" + chunkDocs, fieldsStream);\n      }\n      this.docBase = docBase;\n      this.chunkDocs = chunkDocs;\n\n      if (chunkDocs > numStoredFields.length) {\n        final int newLength = ArrayUtil.oversize(chunkDocs, 4);\n        numStoredFields = new int[newLength];\n        lengths = new int[newLength];\n      }\n\n      if (chunkDocs == 1) {\n        numStoredFields[0] = fieldsStream.readVInt();\n        lengths[0] = fieldsStream.readVInt();\n      } else {\n        final int bitsPerStoredFields = fieldsStream.readVInt();\n        if (bitsPerStoredFields == 0) {\n          Arrays.fill(numStoredFields, 0, chunkDocs, fieldsStream.readVInt());\n        } else if (bitsPerStoredFields > 31) {\n          throw new CorruptIndexException(\"bitsPerStoredFields=\" + bitsPerStoredFields, fieldsStream);\n        } else {\n          final PackedInts.ReaderIterator it = PackedInts.getReaderIteratorNoHeader(fieldsStream, PackedInts.Format.PACKED, packedIntsVersion, chunkDocs, bitsPerStoredFields, 1);\n          for (int i = 0; i < chunkDocs; ++i) {\n            numStoredFields[i] = (int) it.next();\n          }\n        }\n\n        final int bitsPerLength = fieldsStream.readVInt();\n        if (bitsPerLength == 0) {\n          Arrays.fill(lengths, 0, chunkDocs, fieldsStream.readVInt());\n        } else if (bitsPerLength > 31) {\n          throw new CorruptIndexException(\"bitsPerLength=\" + bitsPerLength, fieldsStream);\n        } else {\n          final PackedInts.ReaderIterator it = PackedInts.getReaderIteratorNoHeader(fieldsStream, PackedInts.Format.PACKED, packedIntsVersion, chunkDocs, bitsPerLength, 1);\n          for (int i = 0; i < chunkDocs; ++i) {\n            lengths[i] = (int) it.next();\n          }\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c84d994a0fa92153d37e334f71c26ca6f6be0272","date":1600360257,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsReader.BlockState#doReset(int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsReader.BlockState#doReset(int).mjava","sourceNew":"    private void doReset(int docID) throws IOException {\n      docBase = fieldsStream.readVInt();\n      final int token = fieldsStream.readVInt();\n      chunkDocs = token >>> 1;\n      if (contains(docID) == false\n          || docBase + chunkDocs > numDocs) {\n        throw new CorruptIndexException(\"Corrupted: docID=\" + docID\n            + \", docBase=\" + docBase + \", chunkDocs=\" + chunkDocs\n            + \", numDocs=\" + numDocs, fieldsStream);\n      }\n\n      sliced = (token & 1) != 0;\n\n      offsets = ArrayUtil.grow(offsets, chunkDocs + 1);\n      numStoredFields = ArrayUtil.grow(numStoredFields, chunkDocs);\n\n      if (chunkDocs == 1) {\n        numStoredFields[0] = fieldsStream.readVInt();\n        offsets[1] = fieldsStream.readVInt();\n      } else {\n        // Number of stored fields per document\n        final int bitsPerStoredFields = fieldsStream.readVInt();\n        if (bitsPerStoredFields == 0) {\n          Arrays.fill(numStoredFields, 0, chunkDocs, fieldsStream.readVInt());\n        } else if (bitsPerStoredFields > 31) {\n          throw new CorruptIndexException(\"bitsPerStoredFields=\" + bitsPerStoredFields, fieldsStream);\n        } else {\n          final PackedInts.ReaderIterator it = PackedInts.getReaderIteratorNoHeader(fieldsStream, PackedInts.Format.PACKED, packedIntsVersion, chunkDocs, bitsPerStoredFields, 1024);\n          for (int i = 0; i < chunkDocs; ) {\n            final LongsRef next = it.next(Integer.MAX_VALUE);\n            System.arraycopy(next.longs, next.offset, numStoredFields, i, next.length);\n            i += next.length;\n          }\n        }\n\n        // The stream encodes the length of each document and we decode\n        // it into a list of monotonically increasing offsets\n        final int bitsPerLength = fieldsStream.readVInt();\n        if (bitsPerLength == 0) {\n          final int length = fieldsStream.readVInt();\n          for (int i = 0; i < chunkDocs; ++i) {\n            offsets[1 + i] = (1 + i) * length;\n          }\n        } else if (bitsPerStoredFields > 31) {\n          throw new CorruptIndexException(\"bitsPerLength=\" + bitsPerLength, fieldsStream);\n        } else {\n          final PackedInts.ReaderIterator it = PackedInts.getReaderIteratorNoHeader(fieldsStream, PackedInts.Format.PACKED, packedIntsVersion, chunkDocs, bitsPerLength, 1024);\n          for (int i = 0; i < chunkDocs; ) {\n            final LongsRef next = it.next(Integer.MAX_VALUE);\n            System.arraycopy(next.longs, next.offset, offsets, i + 1, next.length);\n            i += next.length;\n          }\n          for (int i = 0; i < chunkDocs; ++i) {\n            offsets[i + 1] += offsets[i];\n          }\n        }\n\n        // Additional validation: only the empty document has a serialized length of 0\n        for (int i = 0; i < chunkDocs; ++i) {\n          final long len = offsets[i + 1] - offsets[i];\n          final long storedFields = numStoredFields[i];\n          if ((len == 0) != (storedFields == 0)) {\n            throw new CorruptIndexException(\"length=\" + len + \", numStoredFields=\" + storedFields, fieldsStream);\n          }\n        }\n\n      }\n\n      startPointer = fieldsStream.getFilePointer();\n\n      if (merging) {\n        final int totalLength = Math.toIntExact(offsets[chunkDocs]);\n        // decompress eagerly\n        if (sliced) {\n          bytes.offset = bytes.length = 0;\n          for (int decompressed = 0; decompressed < totalLength; ) {\n            final int toDecompress = Math.min(totalLength - decompressed, chunkSize);\n            decompressor.decompress(fieldsStream, toDecompress, 0, toDecompress, spare);\n            bytes.bytes = ArrayUtil.grow(bytes.bytes, bytes.length + spare.length);\n            System.arraycopy(spare.bytes, spare.offset, bytes.bytes, bytes.length, spare.length);\n            bytes.length += spare.length;\n            decompressed += toDecompress;\n          }\n        } else {\n          decompressor.decompress(fieldsStream, totalLength, 0, totalLength, bytes);\n        }\n        if (bytes.length != totalLength) {\n          throw new CorruptIndexException(\"Corrupted: expected chunk size = \" + totalLength + \", got \" + bytes.length, fieldsStream);\n        }\n      }\n    }\n\n","sourceOld":"    private void doReset(int docID) throws IOException {\n      docBase = fieldsStream.readVInt();\n      final int token = fieldsStream.readVInt();\n      chunkDocs = token >>> 1;\n      if (contains(docID) == false\n          || docBase + chunkDocs > numDocs) {\n        throw new CorruptIndexException(\"Corrupted: docID=\" + docID\n            + \", docBase=\" + docBase + \", chunkDocs=\" + chunkDocs\n            + \", numDocs=\" + numDocs, fieldsStream);\n      }\n\n      sliced = (token & 1) != 0;\n\n      offsets = ArrayUtil.grow(offsets, chunkDocs + 1);\n      numStoredFields = ArrayUtil.grow(numStoredFields, chunkDocs);\n\n      if (chunkDocs == 1) {\n        numStoredFields[0] = fieldsStream.readVInt();\n        offsets[1] = fieldsStream.readVInt();\n      } else {\n        // Number of stored fields per document\n        final int bitsPerStoredFields = fieldsStream.readVInt();\n        if (bitsPerStoredFields == 0) {\n          Arrays.fill(numStoredFields, 0, chunkDocs, fieldsStream.readVInt());\n        } else if (bitsPerStoredFields > 31) {\n          throw new CorruptIndexException(\"bitsPerStoredFields=\" + bitsPerStoredFields, fieldsStream);\n        } else {\n          final PackedInts.ReaderIterator it = PackedInts.getReaderIteratorNoHeader(fieldsStream, PackedInts.Format.PACKED, packedIntsVersion, chunkDocs, bitsPerStoredFields, 1);\n          for (int i = 0; i < chunkDocs; ++i) {\n            numStoredFields[i] = (int) it.next();\n          }\n        }\n\n        // The stream encodes the length of each document and we decode\n        // it into a list of monotonically increasing offsets\n        final int bitsPerLength = fieldsStream.readVInt();\n        if (bitsPerLength == 0) {\n          final int length = fieldsStream.readVInt();\n          for (int i = 0; i < chunkDocs; ++i) {\n            offsets[1 + i] = (1 + i) * length;\n          }\n        } else if (bitsPerStoredFields > 31) {\n          throw new CorruptIndexException(\"bitsPerLength=\" + bitsPerLength, fieldsStream);\n        } else {\n          final PackedInts.ReaderIterator it = PackedInts.getReaderIteratorNoHeader(fieldsStream, PackedInts.Format.PACKED, packedIntsVersion, chunkDocs, bitsPerLength, 1);\n          for (int i = 0; i < chunkDocs; ++i) {\n            offsets[i + 1] = (int) it.next();\n          }\n          for (int i = 0; i < chunkDocs; ++i) {\n            offsets[i + 1] += offsets[i];\n          }\n        }\n\n        // Additional validation: only the empty document has a serialized length of 0\n        for (int i = 0; i < chunkDocs; ++i) {\n          final int len = offsets[i + 1] - offsets[i];\n          final int storedFields = numStoredFields[i];\n          if ((len == 0) != (storedFields == 0)) {\n            throw new CorruptIndexException(\"length=\" + len + \", numStoredFields=\" + storedFields, fieldsStream);\n          }\n        }\n\n      }\n\n      startPointer = fieldsStream.getFilePointer();\n\n      if (merging) {\n        final int totalLength = offsets[chunkDocs];\n        // decompress eagerly\n        if (sliced) {\n          bytes.offset = bytes.length = 0;\n          for (int decompressed = 0; decompressed < totalLength; ) {\n            final int toDecompress = Math.min(totalLength - decompressed, chunkSize);\n            decompressor.decompress(fieldsStream, toDecompress, 0, toDecompress, spare);\n            bytes.bytes = ArrayUtil.grow(bytes.bytes, bytes.length + spare.length);\n            System.arraycopy(spare.bytes, spare.offset, bytes.bytes, bytes.length, spare.length);\n            bytes.length += spare.length;\n            decompressed += toDecompress;\n          }\n        } else {\n          decompressor.decompress(fieldsStream, totalLength, 0, totalLength, bytes);\n        }\n        if (bytes.length != totalLength) {\n          throw new CorruptIndexException(\"Corrupted: expected chunk size = \" + totalLength + \", got \" + bytes.length, fieldsStream);\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"1f09f483a0844bb9dc34fb10380cb053aa96219b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"c84d994a0fa92153d37e334f71c26ca6f6be0272":["1f09f483a0844bb9dc34fb10380cb053aa96219b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["c84d994a0fa92153d37e334f71c26ca6f6be0272"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["1f09f483a0844bb9dc34fb10380cb053aa96219b"],"1f09f483a0844bb9dc34fb10380cb053aa96219b":["c84d994a0fa92153d37e334f71c26ca6f6be0272"],"c84d994a0fa92153d37e334f71c26ca6f6be0272":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}