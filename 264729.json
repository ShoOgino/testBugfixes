{"path":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testPositionIncrements().mjava","commits":[{"id":"0f080986da691a3bba7b757f43ab72cdc82b57ce","date":1273069619,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testPositionIncrements().mjava","pathOld":"lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testPositionIncrements().mjava","sourceNew":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new LargePosIncTokenFilter(\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader)),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n\n    Analyzer a3 = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        StopFilter filter = new StopFilter(TEST_VERSION_CURRENT,\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader), StandardAnalyzer.STOP_WORDS_SET);\n        filter.setEnablePositionIncrements(true);\n        return new WordDelimiterFilter(filter, \n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        new int[] { 1, 1, 0 });\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        new int[] { 2, 1, 0 });\n  }\n\n","sourceOld":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new LargePosIncTokenFilter(\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader)),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n\n    Analyzer a3 = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        StopFilter filter = new StopFilter(TEST_VERSION_CURRENT,\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader), StandardAnalyzer.STOP_WORDS_SET);\n        filter.setEnablePositionIncrements(true);\n        return new WordDelimiterFilter(filter, \n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        new int[] { 1, 1, 0 });\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        new int[] { 2, 1, 0 });\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"70e12dd4a648dadc5999dde1f0fb3a71a6ae4b01","date":1296400215,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testPositionIncrements().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testPositionIncrements().mjava","sourceNew":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new LargePosIncTokenFilter(\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader)),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n\n    Analyzer a3 = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String field, Reader reader) {\n        StopFilter filter = new StopFilter(TEST_VERSION_CURRENT,\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader), StandardAnalyzer.STOP_WORDS_SET);\n        filter.setEnablePositionIncrements(true);\n        return new WordDelimiterFilter(filter, \n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        new int[] { 1, 1, 0 });\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        new int[] { 2, 1, 0 });\n  }\n\n","sourceOld":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new LargePosIncTokenFilter(\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader)),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n\n    Analyzer a3 = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        StopFilter filter = new StopFilter(TEST_VERSION_CURRENT,\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader), StandardAnalyzer.STOP_WORDS_SET);\n        filter.setEnablePositionIncrements(true);\n        return new WordDelimiterFilter(filter, \n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        new int[] { 1, 1, 0 });\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        new int[] { 2, 1, 0 });\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testPositionIncrements().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testPositionIncrements().mjava","sourceNew":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new LargePosIncTokenFilter(\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader)),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n\n    Analyzer a3 = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String field, Reader reader) {\n        StopFilter filter = new StopFilter(TEST_VERSION_CURRENT,\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader), StandardAnalyzer.STOP_WORDS_SET);\n        filter.setEnablePositionIncrements(true);\n        return new WordDelimiterFilter(filter, \n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        new int[] { 1, 1, 0 });\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        new int[] { 2, 1, 0 });\n  }\n\n","sourceOld":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new LargePosIncTokenFilter(\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader)),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n\n    Analyzer a3 = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        StopFilter filter = new StopFilter(TEST_VERSION_CURRENT,\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader), StandardAnalyzer.STOP_WORDS_SET);\n        filter.setEnablePositionIncrements(true);\n        return new WordDelimiterFilter(filter, \n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        new int[] { 1, 1, 0 });\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        new int[] { 2, 1, 0 });\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bde51b089eb7f86171eb3406e38a274743f9b7ac","date":1298336439,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testPositionIncrements().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testPositionIncrements().mjava","sourceNew":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new LargePosIncTokenFilter(\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader)),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n\n    Analyzer a3 = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String field, Reader reader) {\n        StopFilter filter = new StopFilter(TEST_VERSION_CURRENT,\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader), StandardAnalyzer.STOP_WORDS_SET);\n        filter.setEnablePositionIncrements(true);\n        return new WordDelimiterFilter(filter, \n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        new int[] { 1, 1, 0 });\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        new int[] { 2, 1, 0 });\n  }\n\n","sourceOld":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new LargePosIncTokenFilter(\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader)),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n\n    Analyzer a3 = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        StopFilter filter = new StopFilter(TEST_VERSION_CURRENT,\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader), StandardAnalyzer.STOP_WORDS_SET);\n        filter.setEnablePositionIncrements(true);\n        return new WordDelimiterFilter(filter, \n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        new int[] { 1, 1, 0 });\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        new int[] { 2, 1, 0 });\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fcbc12aa8147f5203ca283e7252ba4280d6ffd16","date":1305663400,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testPositionIncrements().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testPositionIncrements().mjava","sourceNew":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new MockTokenizer(reader, MockTokenizer.WHITESPACE, false),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new LargePosIncTokenFilter(\n            new MockTokenizer(reader, MockTokenizer.WHITESPACE, false)),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n\n    Analyzer a3 = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String field, Reader reader) {\n        StopFilter filter = new StopFilter(TEST_VERSION_CURRENT,\n            new MockTokenizer(reader, MockTokenizer.WHITESPACE, false), StandardAnalyzer.STOP_WORDS_SET);\n        filter.setEnablePositionIncrements(true);\n        return new WordDelimiterFilter(filter, \n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        new int[] { 1, 1, 0 });\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        new int[] { 2, 1, 0 });\n  }\n\n","sourceOld":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new LargePosIncTokenFilter(\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader)),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n\n    Analyzer a3 = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String field, Reader reader) {\n        StopFilter filter = new StopFilter(TEST_VERSION_CURRENT,\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader), StandardAnalyzer.STOP_WORDS_SET);\n        filter.setEnablePositionIncrements(true);\n        return new WordDelimiterFilter(filter, \n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        new int[] { 1, 1, 0 });\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        new int[] { 2, 1, 0 });\n  }\n\n","bugFix":["8598a11db0eb9efa116ba7656c437f5bed7de0f7"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c3a8a449466c1ff7ce2274fe73dab487256964b4","date":1305735867,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testPositionIncrements().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testPositionIncrements().mjava","sourceNew":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new MockTokenizer(reader, MockTokenizer.WHITESPACE, false),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new LargePosIncTokenFilter(\n            new MockTokenizer(reader, MockTokenizer.WHITESPACE, false)),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n\n    Analyzer a3 = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String field, Reader reader) {\n        StopFilter filter = new StopFilter(TEST_VERSION_CURRENT,\n            new MockTokenizer(reader, MockTokenizer.WHITESPACE, false), StandardAnalyzer.STOP_WORDS_SET);\n        filter.setEnablePositionIncrements(true);\n        return new WordDelimiterFilter(filter, \n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        new int[] { 1, 1, 0 });\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        new int[] { 2, 1, 0 });\n  }\n\n","sourceOld":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new LargePosIncTokenFilter(\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader)),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n\n    Analyzer a3 = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String field, Reader reader) {\n        StopFilter filter = new StopFilter(TEST_VERSION_CURRENT,\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader), StandardAnalyzer.STOP_WORDS_SET);\n        filter.setEnablePositionIncrements(true);\n        return new WordDelimiterFilter(filter, \n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        new int[] { 1, 1, 0 });\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        new int[] { 2, 1, 0 });\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testPositionIncrements().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testPositionIncrements().mjava","sourceNew":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new MockTokenizer(reader, MockTokenizer.WHITESPACE, false),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new LargePosIncTokenFilter(\n            new MockTokenizer(reader, MockTokenizer.WHITESPACE, false)),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n\n    Analyzer a3 = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String field, Reader reader) {\n        StopFilter filter = new StopFilter(TEST_VERSION_CURRENT,\n            new MockTokenizer(reader, MockTokenizer.WHITESPACE, false), StandardAnalyzer.STOP_WORDS_SET);\n        filter.setEnablePositionIncrements(true);\n        return new WordDelimiterFilter(filter, \n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        new int[] { 1, 1, 0 });\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        new int[] { 2, 1, 0 });\n  }\n\n","sourceOld":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new LargePosIncTokenFilter(\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader)),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n\n    Analyzer a3 = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String field, Reader reader) {\n        StopFilter filter = new StopFilter(TEST_VERSION_CURRENT,\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader), StandardAnalyzer.STOP_WORDS_SET);\n        filter.setEnablePositionIncrements(true);\n        return new WordDelimiterFilter(filter, \n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        new int[] { 1, 1, 0 });\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        new int[] { 2, 1, 0 });\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ec1acb945fb5751735f5c9482576c8760d97b6ab","date":1315370590,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testPositionIncrements().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testPositionIncrements().mjava","sourceNew":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;\n    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new MockTokenizer(reader, MockTokenizer.WHITESPACE, false),\n            flags, protWords);\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new LargePosIncTokenFilter(\n            new MockTokenizer(reader, MockTokenizer.WHITESPACE, false)),\n            flags, protWords);\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n\n    Analyzer a3 = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String field, Reader reader) {\n        StopFilter filter = new StopFilter(TEST_VERSION_CURRENT,\n            new MockTokenizer(reader, MockTokenizer.WHITESPACE, false), StandardAnalyzer.STOP_WORDS_SET);\n        filter.setEnablePositionIncrements(true);\n        return new WordDelimiterFilter(filter, flags, protWords);\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        new int[] { 1, 1, 0 });\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        new int[] { 2, 1, 0 });\n  }\n\n","sourceOld":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new MockTokenizer(reader, MockTokenizer.WHITESPACE, false),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new LargePosIncTokenFilter(\n            new MockTokenizer(reader, MockTokenizer.WHITESPACE, false)),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n\n    Analyzer a3 = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String field, Reader reader) {\n        StopFilter filter = new StopFilter(TEST_VERSION_CURRENT,\n            new MockTokenizer(reader, MockTokenizer.WHITESPACE, false), StandardAnalyzer.STOP_WORDS_SET);\n        filter.setEnablePositionIncrements(true);\n        return new WordDelimiterFilter(filter, \n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        new int[] { 1, 1, 0 });\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        new int[] { 2, 1, 0 });\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"53ae89cd75b0acbdfb8890710c6742f3fb80e65d","date":1315806626,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testPositionIncrements().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testPositionIncrements().mjava","sourceNew":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;\n    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new MockTokenizer(reader, MockTokenizer.WHITESPACE, false),\n            flags, protWords);\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new ReusableAnalyzerBase() {\n      @Override\n      public TokenStreamComponents createComponents(String field, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new WordDelimiterFilter(\n            new LargePosIncTokenFilter(tokenizer),\n            flags, protWords));\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n\n    Analyzer a3 = new ReusableAnalyzerBase() {\n      @Override\n      public TokenStreamComponents createComponents(String field, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        StopFilter filter = new StopFilter(TEST_VERSION_CURRENT,\n            tokenizer, StandardAnalyzer.STOP_WORDS_SET);\n        filter.setEnablePositionIncrements(true);\n        return new TokenStreamComponents(tokenizer, new WordDelimiterFilter(filter, flags, protWords));\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        new int[] { 1, 1, 0 });\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        new int[] { 2, 1, 0 });\n  }\n\n","sourceOld":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;\n    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new MockTokenizer(reader, MockTokenizer.WHITESPACE, false),\n            flags, protWords);\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new LargePosIncTokenFilter(\n            new MockTokenizer(reader, MockTokenizer.WHITESPACE, false)),\n            flags, protWords);\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n\n    Analyzer a3 = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String field, Reader reader) {\n        StopFilter filter = new StopFilter(TEST_VERSION_CURRENT,\n            new MockTokenizer(reader, MockTokenizer.WHITESPACE, false), StandardAnalyzer.STOP_WORDS_SET);\n        filter.setEnablePositionIncrements(true);\n        return new WordDelimiterFilter(filter, flags, protWords);\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        new int[] { 1, 1, 0 });\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        new int[] { 2, 1, 0 });\n  }\n\n","bugFix":null,"bugIntro":["c85fa43e6918808743daa7847ba0264373af687f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"dca8d536ba2e4aab4623a172a22cc2885ec7cb3d","date":1315818042,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testPositionIncrements().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testPositionIncrements().mjava","sourceNew":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;\n    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new ReusableAnalyzerBase() {\n      @Override\n      public TokenStreamComponents createComponents(String field, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new WordDelimiterFilter(\n            tokenizer,\n            flags, protWords));\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new ReusableAnalyzerBase() {\n      @Override\n      public TokenStreamComponents createComponents(String field, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new WordDelimiterFilter(\n            new LargePosIncTokenFilter(tokenizer),\n            flags, protWords));\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n\n    Analyzer a3 = new ReusableAnalyzerBase() {\n      @Override\n      public TokenStreamComponents createComponents(String field, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        StopFilter filter = new StopFilter(TEST_VERSION_CURRENT,\n            tokenizer, StandardAnalyzer.STOP_WORDS_SET);\n        filter.setEnablePositionIncrements(true);\n        return new TokenStreamComponents(tokenizer, new WordDelimiterFilter(filter, flags, protWords));\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        new int[] { 1, 1, 0 });\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        new int[] { 2, 1, 0 });\n  }\n\n","sourceOld":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;\n    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new MockTokenizer(reader, MockTokenizer.WHITESPACE, false),\n            flags, protWords);\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new ReusableAnalyzerBase() {\n      @Override\n      public TokenStreamComponents createComponents(String field, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new WordDelimiterFilter(\n            new LargePosIncTokenFilter(tokenizer),\n            flags, protWords));\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n\n    Analyzer a3 = new ReusableAnalyzerBase() {\n      @Override\n      public TokenStreamComponents createComponents(String field, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        StopFilter filter = new StopFilter(TEST_VERSION_CURRENT,\n            tokenizer, StandardAnalyzer.STOP_WORDS_SET);\n        filter.setEnablePositionIncrements(true);\n        return new TokenStreamComponents(tokenizer, new WordDelimiterFilter(filter, flags, protWords));\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        new int[] { 1, 1, 0 });\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        new int[] { 2, 1, 0 });\n  }\n\n","bugFix":null,"bugIntro":["c85fa43e6918808743daa7847ba0264373af687f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2f49143da0a5d278a72f741432047fcfa6da996e","date":1316927425,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testPositionIncrements().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testPositionIncrements().mjava","sourceNew":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;\n    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new WordDelimiterFilter(\n            tokenizer,\n            flags, protWords));\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new WordDelimiterFilter(\n            new LargePosIncTokenFilter(tokenizer),\n            flags, protWords));\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n\n    Analyzer a3 = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        StopFilter filter = new StopFilter(TEST_VERSION_CURRENT,\n            tokenizer, StandardAnalyzer.STOP_WORDS_SET);\n        filter.setEnablePositionIncrements(true);\n        return new TokenStreamComponents(tokenizer, new WordDelimiterFilter(filter, flags, protWords));\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        new int[] { 1, 1, 0 });\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        new int[] { 2, 1, 0 });\n  }\n\n","sourceOld":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;\n    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new ReusableAnalyzerBase() {\n      @Override\n      public TokenStreamComponents createComponents(String field, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new WordDelimiterFilter(\n            tokenizer,\n            flags, protWords));\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new ReusableAnalyzerBase() {\n      @Override\n      public TokenStreamComponents createComponents(String field, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new WordDelimiterFilter(\n            new LargePosIncTokenFilter(tokenizer),\n            flags, protWords));\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n\n    Analyzer a3 = new ReusableAnalyzerBase() {\n      @Override\n      public TokenStreamComponents createComponents(String field, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        StopFilter filter = new StopFilter(TEST_VERSION_CURRENT,\n            tokenizer, StandardAnalyzer.STOP_WORDS_SET);\n        filter.setEnablePositionIncrements(true);\n        return new TokenStreamComponents(tokenizer, new WordDelimiterFilter(filter, flags, protWords));\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        new int[] { 1, 1, 0 });\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        new int[] { 2, 1, 0 });\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"888c2d6bca1edd8d9293631d6e1d188b036e0f05","date":1334076894,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testPositionIncrements().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testPositionIncrements().mjava","sourceNew":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;\n    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new WordDelimiterFilter(\n            tokenizer,\n            flags, protWords));\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        null,\n        new int[] { 1, 1 },\n        null,\n        false);\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        null,\n        new int[] { 1, 1, 1, 0 },\n        null,\n        false);\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        null,\n        new int[] { 1, 1, 1 },\n        null,\n        false);\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new WordDelimiterFilter(\n            new LargePosIncTokenFilter(tokenizer),\n            flags, protWords));\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        null,\n        new int[] { 1, 10, 1 },\n        null,\n        false);\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        null,\n        new int[] { 1, 11 },\n        null,\n        false);\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        null,\n        new int[] { 1, 11, 1, 0 },\n        null,\n        false);\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        null,\n        new int[] { 1, 11, 1 },\n        null,\n        false);\n\n    Analyzer a3 = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        StopFilter filter = new StopFilter(TEST_VERSION_CURRENT,\n            tokenizer, StandardAnalyzer.STOP_WORDS_SET);\n        filter.setEnablePositionIncrements(true);\n        return new TokenStreamComponents(tokenizer, new WordDelimiterFilter(filter, flags, protWords));\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        null,\n        new int[] { 1, 1, 0 },\n        null,\n        false);\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        null,\n        new int[] { 2, 1, 0 },\n        null,\n        false);\n  }\n\n","sourceOld":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;\n    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new WordDelimiterFilter(\n            tokenizer,\n            flags, protWords));\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new WordDelimiterFilter(\n            new LargePosIncTokenFilter(tokenizer),\n            flags, protWords));\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n\n    Analyzer a3 = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        StopFilter filter = new StopFilter(TEST_VERSION_CURRENT,\n            tokenizer, StandardAnalyzer.STOP_WORDS_SET);\n        filter.setEnablePositionIncrements(true);\n        return new TokenStreamComponents(tokenizer, new WordDelimiterFilter(filter, flags, protWords));\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        new int[] { 1, 1, 0 });\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        new int[] { 2, 1, 0 });\n  }\n\n","bugFix":null,"bugIntro":["c85fa43e6918808743daa7847ba0264373af687f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e","date":1334174049,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testPositionIncrements().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testPositionIncrements().mjava","sourceNew":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;\n    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new WordDelimiterFilter(\n            tokenizer,\n            flags, protWords));\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        null,\n        new int[] { 1, 1 },\n        null,\n        false);\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        null,\n        new int[] { 1, 1, 1, 0 },\n        null,\n        false);\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        null,\n        new int[] { 1, 1, 1 },\n        null,\n        false);\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new WordDelimiterFilter(\n            new LargePosIncTokenFilter(tokenizer),\n            flags, protWords));\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        null,\n        new int[] { 1, 10, 1 },\n        null,\n        false);\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        null,\n        new int[] { 1, 11 },\n        null,\n        false);\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        null,\n        new int[] { 1, 11, 1, 0 },\n        null,\n        false);\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        null,\n        new int[] { 1, 11, 1 },\n        null,\n        false);\n\n    Analyzer a3 = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        StopFilter filter = new StopFilter(TEST_VERSION_CURRENT,\n            tokenizer, StandardAnalyzer.STOP_WORDS_SET);\n        filter.setEnablePositionIncrements(true);\n        return new TokenStreamComponents(tokenizer, new WordDelimiterFilter(filter, flags, protWords));\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        null,\n        new int[] { 1, 1, 0 },\n        null,\n        false);\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        null,\n        new int[] { 2, 1, 0 },\n        null,\n        false);\n  }\n\n","sourceOld":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;\n    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new WordDelimiterFilter(\n            tokenizer,\n            flags, protWords));\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new WordDelimiterFilter(\n            new LargePosIncTokenFilter(tokenizer),\n            flags, protWords));\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n\n    Analyzer a3 = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        StopFilter filter = new StopFilter(TEST_VERSION_CURRENT,\n            tokenizer, StandardAnalyzer.STOP_WORDS_SET);\n        filter.setEnablePositionIncrements(true);\n        return new TokenStreamComponents(tokenizer, new WordDelimiterFilter(filter, flags, protWords));\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        new int[] { 1, 1, 0 });\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        new int[] { 2, 1, 0 });\n  }\n\n","bugFix":["f9058535b2d760062e15c60434989564a2b8302b","1d786062be6da940351591ec2372ddd0ae56bd39"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testPositionIncrements().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testPositionIncrements().mjava","sourceNew":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;\n    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new WordDelimiterFilter(\n            tokenizer,\n            flags, protWords));\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        null,\n        new int[] { 1, 1 },\n        null,\n        false);\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        null,\n        new int[] { 1, 1, 1, 0 },\n        null,\n        false);\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        null,\n        new int[] { 1, 1, 1 },\n        null,\n        false);\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new WordDelimiterFilter(\n            new LargePosIncTokenFilter(tokenizer),\n            flags, protWords));\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        null,\n        new int[] { 1, 10, 1 },\n        null,\n        false);\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        null,\n        new int[] { 1, 11 },\n        null,\n        false);\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        null,\n        new int[] { 1, 11, 1, 0 },\n        null,\n        false);\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        null,\n        new int[] { 1, 11, 1 },\n        null,\n        false);\n\n    Analyzer a3 = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        StopFilter filter = new StopFilter(TEST_VERSION_CURRENT,\n            tokenizer, StandardAnalyzer.STOP_WORDS_SET);\n        filter.setEnablePositionIncrements(true);\n        return new TokenStreamComponents(tokenizer, new WordDelimiterFilter(filter, flags, protWords));\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        null,\n        new int[] { 1, 1, 0 },\n        null,\n        false);\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        null,\n        new int[] { 2, 1, 0 },\n        null,\n        false);\n  }\n\n","sourceOld":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;\n    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new WordDelimiterFilter(\n            tokenizer,\n            flags, protWords));\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        null,\n        new int[] { 1, 1 },\n        null,\n        false);\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        null,\n        new int[] { 1, 1, 1, 0 },\n        null,\n        false);\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        null,\n        new int[] { 1, 1, 1 },\n        null,\n        false);\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new WordDelimiterFilter(\n            new LargePosIncTokenFilter(tokenizer),\n            flags, protWords));\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        null,\n        new int[] { 1, 10, 1 },\n        null,\n        false);\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        null,\n        new int[] { 1, 11 },\n        null,\n        false);\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        null,\n        new int[] { 1, 11, 1, 0 },\n        null,\n        false);\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        null,\n        new int[] { 1, 11, 1 },\n        null,\n        false);\n\n    Analyzer a3 = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        StopFilter filter = new StopFilter(TEST_VERSION_CURRENT,\n            tokenizer, StandardAnalyzer.STOP_WORDS_SET);\n        filter.setEnablePositionIncrements(true);\n        return new TokenStreamComponents(tokenizer, new WordDelimiterFilter(filter, flags, protWords));\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        null,\n        new int[] { 1, 1, 0 },\n        null,\n        false);\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        null,\n        new int[] { 2, 1, 0 },\n        null,\n        false);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"70e12dd4a648dadc5999dde1f0fb3a71a6ae4b01":["0f080986da691a3bba7b757f43ab72cdc82b57ce"],"dca8d536ba2e4aab4623a172a22cc2885ec7cb3d":["53ae89cd75b0acbdfb8890710c6742f3fb80e65d"],"0f080986da691a3bba7b757f43ab72cdc82b57ce":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"53ae89cd75b0acbdfb8890710c6742f3fb80e65d":["ec1acb945fb5751735f5c9482576c8760d97b6ab"],"888c2d6bca1edd8d9293631d6e1d188b036e0f05":["2f49143da0a5d278a72f741432047fcfa6da996e"],"fcbc12aa8147f5203ca283e7252ba4280d6ffd16":["70e12dd4a648dadc5999dde1f0fb3a71a6ae4b01"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e"],"c3a8a449466c1ff7ce2274fe73dab487256964b4":["29ef99d61cda9641b6250bf9567329a6e65f901d","fcbc12aa8147f5203ca283e7252ba4280d6ffd16"],"ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e":["2f49143da0a5d278a72f741432047fcfa6da996e","888c2d6bca1edd8d9293631d6e1d188b036e0f05"],"a3776dccca01c11e7046323cfad46a3b4a471233":["70e12dd4a648dadc5999dde1f0fb3a71a6ae4b01","fcbc12aa8147f5203ca283e7252ba4280d6ffd16"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["0f080986da691a3bba7b757f43ab72cdc82b57ce","70e12dd4a648dadc5999dde1f0fb3a71a6ae4b01"],"ec1acb945fb5751735f5c9482576c8760d97b6ab":["fcbc12aa8147f5203ca283e7252ba4280d6ffd16"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"2f49143da0a5d278a72f741432047fcfa6da996e":["dca8d536ba2e4aab4623a172a22cc2885ec7cb3d"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["0f080986da691a3bba7b757f43ab72cdc82b57ce","70e12dd4a648dadc5999dde1f0fb3a71a6ae4b01"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b89678825b68eccaf09e6ab71675fc0b0af1e099"]},"commit2Childs":{"70e12dd4a648dadc5999dde1f0fb3a71a6ae4b01":["fcbc12aa8147f5203ca283e7252ba4280d6ffd16","a3776dccca01c11e7046323cfad46a3b4a471233","29ef99d61cda9641b6250bf9567329a6e65f901d","bde51b089eb7f86171eb3406e38a274743f9b7ac"],"dca8d536ba2e4aab4623a172a22cc2885ec7cb3d":["2f49143da0a5d278a72f741432047fcfa6da996e"],"0f080986da691a3bba7b757f43ab72cdc82b57ce":["70e12dd4a648dadc5999dde1f0fb3a71a6ae4b01","29ef99d61cda9641b6250bf9567329a6e65f901d","bde51b089eb7f86171eb3406e38a274743f9b7ac"],"53ae89cd75b0acbdfb8890710c6742f3fb80e65d":["dca8d536ba2e4aab4623a172a22cc2885ec7cb3d"],"888c2d6bca1edd8d9293631d6e1d188b036e0f05":["ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e"],"fcbc12aa8147f5203ca283e7252ba4280d6ffd16":["c3a8a449466c1ff7ce2274fe73dab487256964b4","a3776dccca01c11e7046323cfad46a3b4a471233","ec1acb945fb5751735f5c9482576c8760d97b6ab"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"c3a8a449466c1ff7ce2274fe73dab487256964b4":[],"ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["c3a8a449466c1ff7ce2274fe73dab487256964b4"],"a3776dccca01c11e7046323cfad46a3b4a471233":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["0f080986da691a3bba7b757f43ab72cdc82b57ce"],"ec1acb945fb5751735f5c9482576c8760d97b6ab":["53ae89cd75b0acbdfb8890710c6742f3fb80e65d"],"2f49143da0a5d278a72f741432047fcfa6da996e":["888c2d6bca1edd8d9293631d6e1d188b036e0f05","ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["c3a8a449466c1ff7ce2274fe73dab487256964b4","a3776dccca01c11e7046323cfad46a3b4a471233","bde51b089eb7f86171eb3406e38a274743f9b7ac","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}