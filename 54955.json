{"path":"lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","commits":[{"id":"b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c","date":1399816179,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startTermsBPV;\n\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      // TODO: use Uninvert?\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n        } else {\n          startTermsBPV = 1;\n        }\n      } else {\n        startTermsBPV = 1;\n      }\n\n      MonotonicAppendingLongBuffer termOrdToBytesOffset = new MonotonicAppendingLongBuffer();\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      int termOrd = 0;\n\n      // TODO: use Uninvert?\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          termOrdToBytesOffset.add(bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            // Store 1+ ord into packed bits\n            docToTermOrd.set(docID, 1+termOrd);\n          }\n          termOrd++;\n        }\n      }\n      termOrdToBytesOffset.freeze();\n\n      // maybe an int-only impl?\n      return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset, docToTermOrd.getMutable(), termOrd);\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startTermsBPV;\n\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      // TODO: use Uninvert?\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n        } else {\n          startTermsBPV = 1;\n        }\n      } else {\n        startTermsBPV = 1;\n      }\n\n      MonotonicAppendingLongBuffer termOrdToBytesOffset = new MonotonicAppendingLongBuffer();\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      int termOrd = 0;\n\n      // TODO: use Uninvert?\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          termOrdToBytesOffset.add(bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            // Store 1+ ord into packed bits\n            docToTermOrd.set(docID, 1+termOrd);\n          }\n          termOrd++;\n        }\n      }\n      termOrdToBytesOffset.freeze();\n\n      // maybe an int-only impl?\n      return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset, docToTermOrd.getMutable(), termOrd);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"93dd449115a9247533e44bab47e8429e5dccbc6d","date":1400258396,"type":1,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startTermsBPV;\n\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      // TODO: use Uninvert?\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n        } else {\n          startTermsBPV = 1;\n        }\n      } else {\n        startTermsBPV = 1;\n      }\n\n      MonotonicAppendingLongBuffer termOrdToBytesOffset = new MonotonicAppendingLongBuffer();\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      int termOrd = 0;\n\n      // TODO: use Uninvert?\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          termOrdToBytesOffset.add(bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            // Store 1+ ord into packed bits\n            docToTermOrd.set(docID, 1+termOrd);\n          }\n          termOrd++;\n        }\n      }\n      termOrdToBytesOffset.freeze();\n\n      // maybe an int-only impl?\n      return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset, docToTermOrd.getMutable(), termOrd);\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startTermsBPV;\n\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      // TODO: use Uninvert?\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n        } else {\n          startTermsBPV = 1;\n        }\n      } else {\n        startTermsBPV = 1;\n      }\n\n      MonotonicAppendingLongBuffer termOrdToBytesOffset = new MonotonicAppendingLongBuffer();\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      int termOrd = 0;\n\n      // TODO: use Uninvert?\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          termOrdToBytesOffset.add(bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            // Store 1+ ord into packed bits\n            docToTermOrd.set(docID, 1+termOrd);\n          }\n          termOrd++;\n        }\n      }\n      termOrdToBytesOffset.freeze();\n\n      // maybe an int-only impl?\n      return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset, docToTermOrd.getMutable(), termOrd);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"56572ec06f1407c066d6b7399413178b33176cd8","date":1400495675,"type":1,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startTermsBPV;\n\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      // TODO: use Uninvert?\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n        } else {\n          startTermsBPV = 1;\n        }\n      } else {\n        startTermsBPV = 1;\n      }\n\n      MonotonicAppendingLongBuffer termOrdToBytesOffset = new MonotonicAppendingLongBuffer();\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      int termOrd = 0;\n\n      // TODO: use Uninvert?\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          termOrdToBytesOffset.add(bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            // Store 1+ ord into packed bits\n            docToTermOrd.set(docID, 1+termOrd);\n          }\n          termOrd++;\n        }\n      }\n      termOrdToBytesOffset.freeze();\n\n      // maybe an int-only impl?\n      return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset, docToTermOrd.getMutable(), termOrd);\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startTermsBPV;\n\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      // TODO: use Uninvert?\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n        } else {\n          startTermsBPV = 1;\n        }\n      } else {\n        startTermsBPV = 1;\n      }\n\n      MonotonicAppendingLongBuffer termOrdToBytesOffset = new MonotonicAppendingLongBuffer();\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      int termOrd = 0;\n\n      // TODO: use Uninvert?\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          termOrdToBytesOffset.add(bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            // Store 1+ ord into packed bits\n            docToTermOrd.set(docID, 1+termOrd);\n          }\n          termOrd++;\n        }\n      }\n      termOrdToBytesOffset.freeze();\n\n      // maybe an int-only impl?\n      return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset, docToTermOrd.getMutable(), termOrd);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e1eb6b3ce884c0b9e064e112da158013ec33cd91","date":1402692077,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Accountable createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startTermsBPV;\n\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      // TODO: use Uninvert?\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n        } else {\n          startTermsBPV = 1;\n        }\n      } else {\n        startTermsBPV = 1;\n      }\n\n      MonotonicAppendingLongBuffer termOrdToBytesOffset = new MonotonicAppendingLongBuffer();\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      int termOrd = 0;\n\n      // TODO: use Uninvert?\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          termOrdToBytesOffset.add(bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            // Store 1+ ord into packed bits\n            docToTermOrd.set(docID, 1+termOrd);\n          }\n          termOrd++;\n        }\n      }\n      termOrdToBytesOffset.freeze();\n\n      // maybe an int-only impl?\n      return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset, docToTermOrd.getMutable(), termOrd);\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startTermsBPV;\n\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      // TODO: use Uninvert?\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n        } else {\n          startTermsBPV = 1;\n        }\n      } else {\n        startTermsBPV = 1;\n      }\n\n      MonotonicAppendingLongBuffer termOrdToBytesOffset = new MonotonicAppendingLongBuffer();\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      int termOrd = 0;\n\n      // TODO: use Uninvert?\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          termOrdToBytesOffset.add(bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            // Store 1+ ord into packed bits\n            docToTermOrd.set(docID, 1+termOrd);\n          }\n          termOrd++;\n        }\n      }\n      termOrdToBytesOffset.freeze();\n\n      // maybe an int-only impl?\n      return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset, docToTermOrd.getMutable(), termOrd);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c6f080a2ab37c464dd98db173f6cbf10dc74f211","date":1402946779,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Accountable createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startTermsBPV;\n\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      // TODO: use Uninvert?\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n        } else {\n          startTermsBPV = 1;\n        }\n      } else {\n        startTermsBPV = 1;\n      }\n\n      MonotonicAppendingLongBuffer termOrdToBytesOffset = new MonotonicAppendingLongBuffer();\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      int termOrd = 0;\n\n      // TODO: use Uninvert?\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          termOrdToBytesOffset.add(bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            // Store 1+ ord into packed bits\n            docToTermOrd.set(docID, 1+termOrd);\n          }\n          termOrd++;\n        }\n      }\n      termOrdToBytesOffset.freeze();\n\n      // maybe an int-only impl?\n      return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset, docToTermOrd.getMutable(), termOrd);\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startTermsBPV;\n\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      // TODO: use Uninvert?\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n        } else {\n          startTermsBPV = 1;\n        }\n      } else {\n        startTermsBPV = 1;\n      }\n\n      MonotonicAppendingLongBuffer termOrdToBytesOffset = new MonotonicAppendingLongBuffer();\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      int termOrd = 0;\n\n      // TODO: use Uninvert?\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          termOrdToBytesOffset.add(bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            // Store 1+ ord into packed bits\n            docToTermOrd.set(docID, 1+termOrd);\n          }\n          termOrd++;\n        }\n      }\n      termOrdToBytesOffset.freeze();\n\n      // maybe an int-only impl?\n      return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset, docToTermOrd.getMutable(), termOrd);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"aae6236deecc1bf344f9c22d8d9dd09ef6701dbd","date":1404226546,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Accountable createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startTermsBPV;\n\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      // TODO: use Uninvert?\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n        } else {\n          startTermsBPV = 1;\n        }\n      } else {\n        startTermsBPV = 1;\n      }\n\n      PackedLongValues.Builder termOrdToBytesOffset = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      int termOrd = 0;\n\n      // TODO: use Uninvert?\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          termOrdToBytesOffset.add(bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            // Store 1+ ord into packed bits\n            docToTermOrd.set(docID, 1+termOrd);\n          }\n          termOrd++;\n        }\n      }\n\n      // maybe an int-only impl?\n      return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset.build(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","sourceOld":"    @Override\n    protected Accountable createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startTermsBPV;\n\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      // TODO: use Uninvert?\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n        } else {\n          startTermsBPV = 1;\n        }\n      } else {\n        startTermsBPV = 1;\n      }\n\n      MonotonicAppendingLongBuffer termOrdToBytesOffset = new MonotonicAppendingLongBuffer();\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      int termOrd = 0;\n\n      // TODO: use Uninvert?\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          termOrdToBytesOffset.add(bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            // Store 1+ ord into packed bits\n            docToTermOrd.set(docID, 1+termOrd);\n          }\n          termOrd++;\n        }\n      }\n      termOrdToBytesOffset.freeze();\n\n      // maybe an int-only impl?\n      return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset, docToTermOrd.getMutable(), termOrd);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b0b3768e97375c7a745c68f0b54710e8bedccc11","date":1406635606,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Accountable createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startTermsBPV;\n\n      // TODO: use Uninvert?\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > maxDoc) {\n            throw new IllegalStateException(\"Type mismatch: \" + key.field + \" was indexed with multiple values per document, use SORTED_SET instead\");\n          }\n\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n        } else {\n          startTermsBPV = 1;\n        }\n      } else {\n        startTermsBPV = 1;\n      }\n\n      PackedLongValues.Builder termOrdToBytesOffset = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      int termOrd = 0;\n\n      // TODO: use Uninvert?\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= maxDoc) {\n            throw new IllegalStateException(\"Type mismatch: \" + key.field + \" was indexed with multiple values per document, use SORTED_SET instead\");\n          }\n\n          termOrdToBytesOffset.add(bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            // Store 1+ ord into packed bits\n            docToTermOrd.set(docID, 1+termOrd);\n          }\n          termOrd++;\n        }\n      }\n\n      // maybe an int-only impl?\n      return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset.build(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","sourceOld":"    @Override\n    protected Accountable createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startTermsBPV;\n\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      // TODO: use Uninvert?\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n        } else {\n          startTermsBPV = 1;\n        }\n      } else {\n        startTermsBPV = 1;\n      }\n\n      PackedLongValues.Builder termOrdToBytesOffset = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      int termOrd = 0;\n\n      // TODO: use Uninvert?\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          termOrdToBytesOffset.add(bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            // Store 1+ ord into packed bits\n            docToTermOrd.set(docID, 1+termOrd);\n          }\n          termOrd++;\n        }\n      }\n\n      // maybe an int-only impl?\n      return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset.build(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","bugFix":["6312aec6ba581f919d406ceff362bef430382c31","a4d374b2bebd0d52acaa61038fbf23068620fba7"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":4,"author":"Ryan Ernst","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":null,"sourceOld":"    @Override\n    protected Accountable createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startTermsBPV;\n\n      // TODO: use Uninvert?\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > maxDoc) {\n            throw new IllegalStateException(\"Type mismatch: \" + key.field + \" was indexed with multiple values per document, use SORTED_SET instead\");\n          }\n\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n        } else {\n          startTermsBPV = 1;\n        }\n      } else {\n        startTermsBPV = 1;\n      }\n\n      PackedLongValues.Builder termOrdToBytesOffset = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      int termOrd = 0;\n\n      // TODO: use Uninvert?\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= maxDoc) {\n            throw new IllegalStateException(\"Type mismatch: \" + key.field + \" was indexed with multiple values per document, use SORTED_SET instead\");\n          }\n\n          termOrdToBytesOffset.add(bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            // Store 1+ ord into packed bits\n            docToTermOrd.set(docID, 1+termOrd);\n          }\n          termOrd++;\n        }\n      }\n\n      // maybe an int-only impl?\n      return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset.build(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"93dd449115a9247533e44bab47e8429e5dccbc6d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"b0b3768e97375c7a745c68f0b54710e8bedccc11":["aae6236deecc1bf344f9c22d8d9dd09ef6701dbd"],"c6f080a2ab37c464dd98db173f6cbf10dc74f211":["93dd449115a9247533e44bab47e8429e5dccbc6d","e1eb6b3ce884c0b9e064e112da158013ec33cd91"],"56572ec06f1407c066d6b7399413178b33176cd8":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","93dd449115a9247533e44bab47e8429e5dccbc6d"],"e1eb6b3ce884c0b9e064e112da158013ec33cd91":["93dd449115a9247533e44bab47e8429e5dccbc6d"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["b0b3768e97375c7a745c68f0b54710e8bedccc11"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"aae6236deecc1bf344f9c22d8d9dd09ef6701dbd":["e1eb6b3ce884c0b9e064e112da158013ec33cd91"]},"commit2Childs":{"93dd449115a9247533e44bab47e8429e5dccbc6d":["c6f080a2ab37c464dd98db173f6cbf10dc74f211","56572ec06f1407c066d6b7399413178b33176cd8","e1eb6b3ce884c0b9e064e112da158013ec33cd91"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["93dd449115a9247533e44bab47e8429e5dccbc6d","b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c","56572ec06f1407c066d6b7399413178b33176cd8"],"b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c":["93dd449115a9247533e44bab47e8429e5dccbc6d"],"b0b3768e97375c7a745c68f0b54710e8bedccc11":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"c6f080a2ab37c464dd98db173f6cbf10dc74f211":[],"56572ec06f1407c066d6b7399413178b33176cd8":[],"e1eb6b3ce884c0b9e064e112da158013ec33cd91":["c6f080a2ab37c464dd98db173f6cbf10dc74f211","aae6236deecc1bf344f9c22d8d9dd09ef6701dbd"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"aae6236deecc1bf344f9c22d8d9dd09ef6701dbd":["b0b3768e97375c7a745c68f0b54710e8bedccc11"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["c6f080a2ab37c464dd98db173f6cbf10dc74f211","56572ec06f1407c066d6b7399413178b33176cd8","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}