{"path":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","commits":[{"id":"38a62612cfa4e104080d89d7751a8f1a258ac335","date":1291442315,"type":0,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"/dev/null","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer()));\n      Document d = new Document();\n      d.add(new Field(\"c\", \"v\", Store.YES, Index.ANALYZED, TermVector.YES));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new RAMDirectory();\n    IndexWriterConfig conf = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    \n    assertEquals(\"Only one compound segment should exist\", 3, dir.listAll().length);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"19eab129d70239248127e04450b23fd10bf42dd0","date":1291545753,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n      Document d = new Document();\n      d.add(new Field(\"c\", \"v\", Store.YES, Index.ANALYZED, TermVector.YES));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new RAMDirectory();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    \n    assertEquals(\"Only one compound segment should exist\", 3, dir.listAll().length);\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer()));\n      Document d = new Document();\n      d.add(new Field(\"c\", \"v\", Store.YES, Index.ANALYZED, TermVector.YES));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new RAMDirectory();\n    IndexWriterConfig conf = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    \n    assertEquals(\"Only one compound segment should exist\", 3, dir.listAll().length);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4a69e5860d014751cc9329dfeb441a6d8fd1ed8e","date":1291833341,"type":0,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"/dev/null","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n      Document d = new Document();\n      d.add(new Field(\"c\", \"v\", Store.YES, Index.ANALYZED, TermVector.YES));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new RAMDirectory();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    \n    assertEquals(\"Only one compound segment should exist\", 3, dir.listAll().length);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":0,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"/dev/null","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n      Document d = new Document();\n      d.add(new Field(\"c\", \"v\", Store.YES, Index.ANALYZED, TermVector.YES));\n      w.addDocument(d);\n      w.close();\n    }\n\n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n\n    Directory dir = new RAMDirectory();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n\n    assertEquals(\"Only one compound segment should exist\", 3, dir.listAll().length);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1224a4027481acce15495b03bce9b48b93b42722","date":1300792329,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n      Document d = new Document();\n      d.add(new Field(\"c\", \"v\", Store.YES, Index.ANALYZED, TermVector.YES));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new RAMDirectory();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X, segments.gen,_Y.cfs, _Z.fnx\n    assertEquals(\"Only one compound segment should exist\", 4, dir.listAll().length);\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n      Document d = new Document();\n      d.add(new Field(\"c\", \"v\", Store.YES, Index.ANALYZED, TermVector.YES));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new RAMDirectory();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    \n    assertEquals(\"Only one compound segment should exist\", 3, dir.listAll().length);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d619839baa8ce5503e496b94a9e42ad6f079293f","date":1301309428,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n      Document d = new Document();\n      d.add(new Field(\"c\", \"v\", Store.YES, Index.ANALYZED, TermVector.YES));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new RAMDirectory();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X, segments.gen,_Y.cfs, _Z.fnx\n    assertEquals(\"Only one compound segment should exist\", 4, dir.listAll().length);\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n      Document d = new Document();\n      d.add(new Field(\"c\", \"v\", Store.YES, Index.ANALYZED, TermVector.YES));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new RAMDirectory();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    \n    assertEquals(\"Only one compound segment should exist\", 3, dir.listAll().length);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c0ef0193974807e4bddf5432a6b0287fe4d6c9df","date":1301476645,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n      Document d = new Document();\n      d.add(new Field(\"c\", \"v\", Store.YES, Index.ANALYZED, TermVector.YES));\n      w.addDocument(d);\n      w.close();\n    }\n\n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n\n    Directory dir = new RAMDirectory();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X, segments.gen,_Y.cfs, _Z.fnx\n    assertEquals(\"Only one compound segment should exist\", 4, dir.listAll().length);\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n      Document d = new Document();\n      d.add(new Field(\"c\", \"v\", Store.YES, Index.ANALYZED, TermVector.YES));\n      w.addDocument(d);\n      w.close();\n    }\n\n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n\n    Directory dir = new RAMDirectory();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n\n    assertEquals(\"Only one compound segment should exist\", 3, dir.listAll().length);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"01e5948db9a07144112d2f08f28ca2e3cd880348","date":1301759232,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n      Document d = new Document();\n      d.add(new Field(\"c\", \"v\", Store.YES, Index.ANALYZED, TermVector.YES));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new RAMDirectory();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newLogMergePolicy());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setUseCompoundFile(true);\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X, segments.gen,_Y.cfs, _Z.fnx\n    assertEquals(\"Only one compound segment should exist\", 4, dir.listAll().length);\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n      Document d = new Document();\n      d.add(new Field(\"c\", \"v\", Store.YES, Index.ANALYZED, TermVector.YES));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new RAMDirectory();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X, segments.gen,_Y.cfs, _Z.fnx\n    assertEquals(\"Only one compound segment should exist\", 4, dir.listAll().length);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"45669a651c970812a680841b97a77cce06af559f","date":1301922222,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n      Document d = new Document();\n      d.add(new Field(\"c\", \"v\", Store.YES, Index.ANALYZED, TermVector.YES));\n      w.addDocument(d);\n      w.close();\n    }\n\n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n\n    Directory dir = new RAMDirectory();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newLogMergePolicy());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setUseCompoundFile(true);\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X, segments.gen,_Y.cfs, _Z.fnx\n    assertEquals(\"Only one compound segment should exist\", 4, dir.listAll().length);\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n      Document d = new Document();\n      d.add(new Field(\"c\", \"v\", Store.YES, Index.ANALYZED, TermVector.YES));\n      w.addDocument(d);\n      w.close();\n    }\n\n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n\n    Directory dir = new RAMDirectory();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X, segments.gen,_Y.cfs, _Z.fnx\n    assertEquals(\"Only one compound segment should exist\", 4, dir.listAll().length);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f2c5f0cb44df114db4228c8f77861714b5cabaea","date":1302542431,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      Document d = new Document();\n      d.add(new Field(\"c\", \"v\", Store.YES, Index.ANALYZED, TermVector.YES));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new RAMDirectory();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setUseCompoundFile(true);\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X, segments.gen,_Y.cfs, _Z.fnx\n    assertEquals(\"Only one compound segment should exist\", 4, dir.listAll().length);\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n      Document d = new Document();\n      d.add(new Field(\"c\", \"v\", Store.YES, Index.ANALYZED, TermVector.YES));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new RAMDirectory();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newLogMergePolicy());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setUseCompoundFile(true);\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X, segments.gen,_Y.cfs, _Z.fnx\n    assertEquals(\"Only one compound segment should exist\", 4, dir.listAll().length);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"962d04139994fce5193143ef35615499a9a96d78","date":1302693744,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      Document d = new Document();\n      d.add(new Field(\"c\", \"v\", Store.YES, Index.ANALYZED, TermVector.YES));\n      w.addDocument(d);\n      w.close();\n    }\n\n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n\n    Directory dir = new RAMDirectory();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setUseCompoundFile(true);\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X, segments.gen,_Y.cfs, _Z.fnx\n    assertEquals(\"Only one compound segment should exist\", 4, dir.listAll().length);\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n      Document d = new Document();\n      d.add(new Field(\"c\", \"v\", Store.YES, Index.ANALYZED, TermVector.YES));\n      w.addDocument(d);\n      w.close();\n    }\n\n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n\n    Directory dir = new RAMDirectory();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newLogMergePolicy());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setUseCompoundFile(true);\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X, segments.gen,_Y.cfs, _Z.fnx\n    assertEquals(\"Only one compound segment should exist\", 4, dir.listAll().length);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      Document d = new Document();\n      d.add(new Field(\"c\", \"v\", Store.YES, Index.ANALYZED, TermVector.YES));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new RAMDirectory();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setUseCompoundFile(true);\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X, segments.gen,_Y.cfs, _Z.fnx\n    assertEquals(\"Only one compound segment should exist\", 4, dir.listAll().length);\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n      Document d = new Document();\n      d.add(new Field(\"c\", \"v\", Store.YES, Index.ANALYZED, TermVector.YES));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new RAMDirectory();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X, segments.gen,_Y.cfs, _Z.fnx\n    assertEquals(\"Only one compound segment should exist\", 4, dir.listAll().length);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      Document d = new Document();\n      d.add(new Field(\"c\", \"v\", Store.YES, Index.ANALYZED, TermVector.YES));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new RAMDirectory();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setUseCompoundFile(true);\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X, segments.gen,_Y.cfs, _Z.fnx\n    assertEquals(\"Only one compound segment should exist\", 4, dir.listAll().length);\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n      Document d = new Document();\n      d.add(new Field(\"c\", \"v\", Store.YES, Index.ANALYZED, TermVector.YES));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new RAMDirectory();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X, segments.gen,_Y.cfs, _Z.fnx\n    assertEquals(\"Only one compound segment should exist\", 4, dir.listAll().length);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0aab6e810b4b0d3743d6a048be0602801f4b3920","date":1308671625,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      Document d = new Document();\n      d.add(new Field(\"c\", \"v\", Store.YES, Index.ANALYZED, TermVector.YES));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new RAMDirectory();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setUseCompoundFile(true);\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X, segments.gen,_Y.cfs,_Y.cfe, _Z.fnx\n    assertEquals(\"Only one compound segment should exist\", 5, dir.listAll().length);\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      Document d = new Document();\n      d.add(new Field(\"c\", \"v\", Store.YES, Index.ANALYZED, TermVector.YES));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new RAMDirectory();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setUseCompoundFile(true);\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X, segments.gen,_Y.cfs, _Z.fnx\n    assertEquals(\"Only one compound segment should exist\", 4, dir.listAll().length);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2553b00f699380c64959ccb27991289aae87be2e","date":1309290151,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      Document d = new Document();\n      d.add(new Field(\"c\", \"v\", Store.YES, Index.ANALYZED, TermVector.YES));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new RAMDirectory();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setUseCompoundFile(true);\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X, segments.gen,_Y.cfs,_Y.cfe, _Z.fnx\n    assertEquals(\"Only one compound segment should exist\", 5, dir.listAll().length);\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      Document d = new Document();\n      d.add(new Field(\"c\", \"v\", Store.YES, Index.ANALYZED, TermVector.YES));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new RAMDirectory();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setUseCompoundFile(true);\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X, segments.gen,_Y.cfs, _Z.fnx\n    assertEquals(\"Only one compound segment should exist\", 4, dir.listAll().length);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      Document d = new Document();\n      d.add(new Field(\"c\", \"v\", Store.YES, Index.ANALYZED, TermVector.YES));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new RAMDirectory();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setUseCompoundFile(true);\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X, segments.gen,_Y.cfs,_Y.cfe, _Z.fnx\n    assertEquals(\"Only one compound segment should exist\", 5, dir.listAll().length);\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      Document d = new Document();\n      d.add(new Field(\"c\", \"v\", Store.YES, Index.ANALYZED, TermVector.YES));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new RAMDirectory();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setUseCompoundFile(true);\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X, segments.gen,_Y.cfs, _Z.fnx\n    assertEquals(\"Only one compound segment should exist\", 4, dir.listAll().length);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1509f151d7692d84fae414b2b799ac06ba60fcb4","date":1314451621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", customType, \"v\"));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new RAMDirectory();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setUseCompoundFile(true);\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X, segments.gen,_Y.cfs,_Y.cfe, _Z.fnx\n    assertEquals(\"Only one compound segment should exist\", 5, dir.listAll().length);\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      Document d = new Document();\n      d.add(new Field(\"c\", \"v\", Store.YES, Index.ANALYZED, TermVector.YES));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new RAMDirectory();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setUseCompoundFile(true);\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X, segments.gen,_Y.cfs,_Y.cfe, _Z.fnx\n    assertEquals(\"Only one compound segment should exist\", 5, dir.listAll().length);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7e4db59c6b6c10e25322cfb41c4c19d78b4298bd","date":1317197236,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new RAMDirectory();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setUseCompoundFile(true);\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X, segments.gen,_Y.cfs,_Y.cfe, _Z.fnx\n    assertEquals(\"Only one compound segment should exist\", 5, dir.listAll().length);\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", customType, \"v\"));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new RAMDirectory();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setUseCompoundFile(true);\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X, segments.gen,_Y.cfs,_Y.cfe, _Z.fnx\n    assertEquals(\"Only one compound segment should exist\", 5, dir.listAll().length);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7b91922b55d15444d554721b352861d028eb8278","date":1320421415,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new MockDirectoryWrapper(random, new RAMDirectory());\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setUseCompoundFile(true);\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.setInfoStream(VERBOSE ? System.out : null);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // segments.gen,_Y.cfs,_Y.cfe, _Z.fnx\n    assertEquals(\"Only one compound segment should exist, but got: \" + Arrays.toString(dir.listAll()), 5, dir.listAll().length);\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new RAMDirectory();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setUseCompoundFile(true);\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X, segments.gen,_Y.cfs,_Y.cfe, _Z.fnx\n    assertEquals(\"Only one compound segment should exist\", 5, dir.listAll().length);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"06584e6e98d592b34e1329b384182f368d2025e8","date":1320850353,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new MockDirectoryWrapper(random, new RAMDirectory());\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setUseCompoundFile(true);\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // segments.gen,_Y.cfs,_Y.cfe, _Z.fnx\n    assertEquals(\"Only one compound segment should exist, but got: \" + Arrays.toString(dir.listAll()), 5, dir.listAll().length);\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new MockDirectoryWrapper(random, new RAMDirectory());\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setUseCompoundFile(true);\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.setInfoStream(VERBOSE ? System.out : null);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // segments.gen,_Y.cfs,_Y.cfe, _Z.fnx\n    assertEquals(\"Only one compound segment should exist, but got: \" + Arrays.toString(dir.listAll()), 5, dir.listAll().length);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"319624eb66a10b717d3e66af448543e7dc5c479d","date":1322741556,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new MockDirectoryWrapper(random, new RAMDirectory());\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setUseCompoundFile(true);\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // segments.gen,_Y.cfs,_Y.cfe, _Z.fnx\n    assertEquals(\"Only one compound segment should exist, but got: \" + Arrays.toString(dir.listAll()), 4, dir.listAll().length);\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new MockDirectoryWrapper(random, new RAMDirectory());\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setUseCompoundFile(true);\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // segments.gen,_Y.cfs,_Y.cfe, _Z.fnx\n    assertEquals(\"Only one compound segment should exist, but got: \" + Arrays.toString(dir.listAll()), 5, dir.listAll().length);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new MockDirectoryWrapper(random, new RAMDirectory());\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setUseCompoundFile(true);\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // segments.gen,_Y.cfs,_Y.cfe, _Z.fnx\n    assertEquals(\"Only one compound segment should exist, but got: \" + Arrays.toString(dir.listAll()), 4, dir.listAll().length);\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new MockDirectoryWrapper(random, new RAMDirectory());\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setUseCompoundFile(true);\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // segments.gen,_Y.cfs,_Y.cfe, _Z.fnx\n    assertEquals(\"Only one compound segment should exist, but got: \" + Arrays.toString(dir.listAll()), 4, dir.listAll().length);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"38a62612cfa4e104080d89d7751a8f1a258ac335":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["319624eb66a10b717d3e66af448543e7dc5c479d"],"06584e6e98d592b34e1329b384182f368d2025e8":["7b91922b55d15444d554721b352861d028eb8278"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["d619839baa8ce5503e496b94a9e42ad6f079293f","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"0aab6e810b4b0d3743d6a048be0602801f4b3920":["f2c5f0cb44df114db4228c8f77861714b5cabaea"],"4a69e5860d014751cc9329dfeb441a6d8fd1ed8e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"962d04139994fce5193143ef35615499a9a96d78":["45669a651c970812a680841b97a77cce06af559f","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"2553b00f699380c64959ccb27991289aae87be2e":["a3776dccca01c11e7046323cfad46a3b4a471233","0aab6e810b4b0d3743d6a048be0602801f4b3920"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","19eab129d70239248127e04450b23fd10bf42dd0"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["f2c5f0cb44df114db4228c8f77861714b5cabaea","0aab6e810b4b0d3743d6a048be0602801f4b3920"],"01e5948db9a07144112d2f08f28ca2e3cd880348":["1224a4027481acce15495b03bce9b48b93b42722"],"7b91922b55d15444d554721b352861d028eb8278":["7e4db59c6b6c10e25322cfb41c4c19d78b4298bd"],"19eab129d70239248127e04450b23fd10bf42dd0":["38a62612cfa4e104080d89d7751a8f1a258ac335"],"c0ef0193974807e4bddf5432a6b0287fe4d6c9df":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","1224a4027481acce15495b03bce9b48b93b42722"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["01e5948db9a07144112d2f08f28ca2e3cd880348"],"d619839baa8ce5503e496b94a9e42ad6f079293f":["4a69e5860d014751cc9329dfeb441a6d8fd1ed8e","1224a4027481acce15495b03bce9b48b93b42722"],"a3776dccca01c11e7046323cfad46a3b4a471233":["1224a4027481acce15495b03bce9b48b93b42722","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"319624eb66a10b717d3e66af448543e7dc5c479d":["06584e6e98d592b34e1329b384182f368d2025e8"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["0aab6e810b4b0d3743d6a048be0602801f4b3920"],"45669a651c970812a680841b97a77cce06af559f":["c0ef0193974807e4bddf5432a6b0287fe4d6c9df","01e5948db9a07144112d2f08f28ca2e3cd880348"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"7e4db59c6b6c10e25322cfb41c4c19d78b4298bd":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"1224a4027481acce15495b03bce9b48b93b42722":["19eab129d70239248127e04450b23fd10bf42dd0"]},"commit2Childs":{"38a62612cfa4e104080d89d7751a8f1a258ac335":["19eab129d70239248127e04450b23fd10bf42dd0"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"06584e6e98d592b34e1329b384182f368d2025e8":["319624eb66a10b717d3e66af448543e7dc5c479d"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":[],"0aab6e810b4b0d3743d6a048be0602801f4b3920":["2553b00f699380c64959ccb27991289aae87be2e","d083e83f225b11e5fdd900e83d26ddb385b6955c","1509f151d7692d84fae414b2b799ac06ba60fcb4"],"4a69e5860d014751cc9329dfeb441a6d8fd1ed8e":["d619839baa8ce5503e496b94a9e42ad6f079293f"],"962d04139994fce5193143ef35615499a9a96d78":[],"2553b00f699380c64959ccb27991289aae87be2e":[],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["c0ef0193974807e4bddf5432a6b0287fe4d6c9df"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":[],"01e5948db9a07144112d2f08f28ca2e3cd880348":["f2c5f0cb44df114db4228c8f77861714b5cabaea","45669a651c970812a680841b97a77cce06af559f"],"7b91922b55d15444d554721b352861d028eb8278":["06584e6e98d592b34e1329b384182f368d2025e8"],"19eab129d70239248127e04450b23fd10bf42dd0":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","1224a4027481acce15495b03bce9b48b93b42722"],"c0ef0193974807e4bddf5432a6b0287fe4d6c9df":["45669a651c970812a680841b97a77cce06af559f"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["135621f3a0670a9394eb563224a3b76cc4dddc0f","0aab6e810b4b0d3743d6a048be0602801f4b3920","962d04139994fce5193143ef35615499a9a96d78","d083e83f225b11e5fdd900e83d26ddb385b6955c","a3776dccca01c11e7046323cfad46a3b4a471233"],"d619839baa8ce5503e496b94a9e42ad6f079293f":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"a3776dccca01c11e7046323cfad46a3b4a471233":["2553b00f699380c64959ccb27991289aae87be2e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["38a62612cfa4e104080d89d7751a8f1a258ac335","4a69e5860d014751cc9329dfeb441a6d8fd1ed8e","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"319624eb66a10b717d3e66af448543e7dc5c479d":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["7e4db59c6b6c10e25322cfb41c4c19d78b4298bd"],"45669a651c970812a680841b97a77cce06af559f":["962d04139994fce5193143ef35615499a9a96d78"],"1224a4027481acce15495b03bce9b48b93b42722":["01e5948db9a07144112d2f08f28ca2e3cd880348","c0ef0193974807e4bddf5432a6b0287fe4d6c9df","d619839baa8ce5503e496b94a9e42ad6f079293f","a3776dccca01c11e7046323cfad46a3b4a471233"],"7e4db59c6b6c10e25322cfb41c4c19d78b4298bd":["7b91922b55d15444d554721b352861d028eb8278"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["135621f3a0670a9394eb563224a3b76cc4dddc0f","962d04139994fce5193143ef35615499a9a96d78","2553b00f699380c64959ccb27991289aae87be2e","d083e83f225b11e5fdd900e83d26ddb385b6955c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}