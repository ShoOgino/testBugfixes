{"path":"solr/core/src/java/org/apache/solr/search/ExtendedAnalyzer[ExtendedDismaxQParserPlugin]#tokenStream(String,Reader).mjava","commits":[{"id":"c903c3d15906a3da96b8c0c2fb704491005fdbdb","date":1453508227,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/ExtendedAnalyzer[ExtendedDismaxQParserPlugin]#tokenStream(String,Reader).mjava","pathOld":"solr/src/java/org/apache/solr/search/ExtendedAnalyzer[ExtendedDismaxQParserPlugin]#tokenStream(String,Reader).mjava","sourceNew":"  @Override\n  public TokenStream tokenStream(String fieldName, Reader reader) {\n    if (!removeStopFilter) {\n      return queryAnalyzer.tokenStream(fieldName, reader);\n    }\n    \n    Analyzer a = map.get(fieldName);\n    if (a != null) {\n      return a.tokenStream(fieldName, reader);\n    }\n\n    FieldType ft = parser.getReq().getSchema().getFieldType(fieldName);\n    Analyzer qa = ft.getQueryAnalyzer();\n    if (!(qa instanceof TokenizerChain)) {\n      map.put(fieldName, qa);\n      return qa.tokenStream(fieldName, reader);\n    }\n    TokenizerChain tcq = (TokenizerChain)qa;\n    Analyzer ia = ft.getAnalyzer();\n    if (ia == qa || !(ia instanceof TokenizerChain)) {\n      map.put(fieldName, qa);\n      return qa.tokenStream(fieldName, reader);\n    }\n    TokenizerChain tci = (TokenizerChain)ia;\n\n    // make sure that there isn't a stop filter in the indexer\n    for (TokenFilterFactory tf : tci.getTokenFilterFactories()) {\n      if (tf instanceof StopFilterFactory) {\n        map.put(fieldName, qa);\n        return qa.tokenStream(fieldName, reader);\n      }\n    }\n\n    // now if there is a stop filter in the query analyzer, remove it\n    int stopIdx = -1;\n    TokenFilterFactory[] facs = tcq.getTokenFilterFactories();\n\n    for (int i=0; i<facs.length; i++) {\n      TokenFilterFactory tf = facs[i];\n      if (tf instanceof StopFilterFactory) {\n        stopIdx = i;\n        break;\n      }\n    }\n\n    if (stopIdx == -1) {\n      // no stop filter exists\n      map.put(fieldName, qa);\n      return qa.tokenStream(fieldName, reader);\n    }\n\n    TokenFilterFactory[] newtf = new TokenFilterFactory[facs.length-1];\n    for (int i=0,j=0; i<facs.length; i++) {\n      if (i==stopIdx) continue;\n      newtf[j++] = facs[i];\n    }\n\n    TokenizerChain newa = new TokenizerChain(tcq.getTokenizerFactory(), newtf);\n    newa.setPositionIncrementGap(tcq.getPositionIncrementGap(fieldName));\n\n    map.put(fieldName, newa);\n    return newa.tokenStream(fieldName, reader);        \n  }\n\n","sourceOld":"  @Override\n  public TokenStream tokenStream(String fieldName, Reader reader) {\n    if (!removeStopFilter) {\n      return queryAnalyzer.tokenStream(fieldName, reader);\n    }\n    \n    Analyzer a = map.get(fieldName);\n    if (a != null) {\n      return a.tokenStream(fieldName, reader);\n    }\n\n    FieldType ft = parser.getReq().getSchema().getFieldType(fieldName);\n    Analyzer qa = ft.getQueryAnalyzer();\n    if (!(qa instanceof TokenizerChain)) {\n      map.put(fieldName, qa);\n      return qa.tokenStream(fieldName, reader);\n    }\n    TokenizerChain tcq = (TokenizerChain)qa;\n    Analyzer ia = ft.getAnalyzer();\n    if (ia == qa || !(ia instanceof TokenizerChain)) {\n      map.put(fieldName, qa);\n      return qa.tokenStream(fieldName, reader);\n    }\n    TokenizerChain tci = (TokenizerChain)ia;\n\n    // make sure that there isn't a stop filter in the indexer\n    for (TokenFilterFactory tf : tci.getTokenFilterFactories()) {\n      if (tf instanceof StopFilterFactory) {\n        map.put(fieldName, qa);\n        return qa.tokenStream(fieldName, reader);\n      }\n    }\n\n    // now if there is a stop filter in the query analyzer, remove it\n    int stopIdx = -1;\n    TokenFilterFactory[] facs = tcq.getTokenFilterFactories();\n\n    for (int i=0; i<facs.length; i++) {\n      TokenFilterFactory tf = facs[i];\n      if (tf instanceof StopFilterFactory) {\n        stopIdx = i;\n        break;\n      }\n    }\n\n    if (stopIdx == -1) {\n      // no stop filter exists\n      map.put(fieldName, qa);\n      return qa.tokenStream(fieldName, reader);\n    }\n\n    TokenFilterFactory[] newtf = new TokenFilterFactory[facs.length-1];\n    for (int i=0,j=0; i<facs.length; i++) {\n      if (i==stopIdx) continue;\n      newtf[j++] = facs[i];\n    }\n\n    TokenizerChain newa = new TokenizerChain(tcq.getTokenizerFactory(), newtf);\n    newa.setPositionIncrementGap(tcq.getPositionIncrementGap(fieldName));\n\n    map.put(fieldName, newa);\n    return newa.tokenStream(fieldName, reader);        \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a258fbb26824fd104ed795e5d9033d2d040049ee","date":1453508252,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/ExtendedAnalyzer[ExtendedDismaxQParserPlugin]#tokenStream(String,Reader).mjava","pathOld":"solr/src/java/org/apache/solr/search/ExtendedAnalyzer[ExtendedDismaxQParserPlugin]#tokenStream(String,Reader).mjava","sourceNew":"  @Override\n  public TokenStream tokenStream(String fieldName, Reader reader) {\n    if (!removeStopFilter) {\n      return queryAnalyzer.tokenStream(fieldName, reader);\n    }\n    \n    Analyzer a = map.get(fieldName);\n    if (a != null) {\n      return a.tokenStream(fieldName, reader);\n    }\n\n    FieldType ft = parser.getReq().getSchema().getFieldType(fieldName);\n    Analyzer qa = ft.getQueryAnalyzer();\n    if (!(qa instanceof TokenizerChain)) {\n      map.put(fieldName, qa);\n      return qa.tokenStream(fieldName, reader);\n    }\n    TokenizerChain tcq = (TokenizerChain)qa;\n    Analyzer ia = ft.getAnalyzer();\n    if (ia == qa || !(ia instanceof TokenizerChain)) {\n      map.put(fieldName, qa);\n      return qa.tokenStream(fieldName, reader);\n    }\n    TokenizerChain tci = (TokenizerChain)ia;\n\n    // make sure that there isn't a stop filter in the indexer\n    for (TokenFilterFactory tf : tci.getTokenFilterFactories()) {\n      if (tf instanceof StopFilterFactory) {\n        map.put(fieldName, qa);\n        return qa.tokenStream(fieldName, reader);\n      }\n    }\n\n    // now if there is a stop filter in the query analyzer, remove it\n    int stopIdx = -1;\n    TokenFilterFactory[] facs = tcq.getTokenFilterFactories();\n\n    for (int i=0; i<facs.length; i++) {\n      TokenFilterFactory tf = facs[i];\n      if (tf instanceof StopFilterFactory) {\n        stopIdx = i;\n        break;\n      }\n    }\n\n    if (stopIdx == -1) {\n      // no stop filter exists\n      map.put(fieldName, qa);\n      return qa.tokenStream(fieldName, reader);\n    }\n\n    TokenFilterFactory[] newtf = new TokenFilterFactory[facs.length-1];\n    for (int i=0,j=0; i<facs.length; i++) {\n      if (i==stopIdx) continue;\n      newtf[j++] = facs[i];\n    }\n\n    TokenizerChain newa = new TokenizerChain(tcq.getTokenizerFactory(), newtf);\n    newa.setPositionIncrementGap(tcq.getPositionIncrementGap(fieldName));\n\n    map.put(fieldName, newa);\n    return newa.tokenStream(fieldName, reader);        \n  }\n\n","sourceOld":"  @Override\n  public TokenStream tokenStream(String fieldName, Reader reader) {\n    if (!removeStopFilter) {\n      return queryAnalyzer.tokenStream(fieldName, reader);\n    }\n    \n    Analyzer a = map.get(fieldName);\n    if (a != null) {\n      return a.tokenStream(fieldName, reader);\n    }\n\n    FieldType ft = parser.getReq().getSchema().getFieldType(fieldName);\n    Analyzer qa = ft.getQueryAnalyzer();\n    if (!(qa instanceof TokenizerChain)) {\n      map.put(fieldName, qa);\n      return qa.tokenStream(fieldName, reader);\n    }\n    TokenizerChain tcq = (TokenizerChain)qa;\n    Analyzer ia = ft.getAnalyzer();\n    if (ia == qa || !(ia instanceof TokenizerChain)) {\n      map.put(fieldName, qa);\n      return qa.tokenStream(fieldName, reader);\n    }\n    TokenizerChain tci = (TokenizerChain)ia;\n\n    // make sure that there isn't a stop filter in the indexer\n    for (TokenFilterFactory tf : tci.getTokenFilterFactories()) {\n      if (tf instanceof StopFilterFactory) {\n        map.put(fieldName, qa);\n        return qa.tokenStream(fieldName, reader);\n      }\n    }\n\n    // now if there is a stop filter in the query analyzer, remove it\n    int stopIdx = -1;\n    TokenFilterFactory[] facs = tcq.getTokenFilterFactories();\n\n    for (int i=0; i<facs.length; i++) {\n      TokenFilterFactory tf = facs[i];\n      if (tf instanceof StopFilterFactory) {\n        stopIdx = i;\n        break;\n      }\n    }\n\n    if (stopIdx == -1) {\n      // no stop filter exists\n      map.put(fieldName, qa);\n      return qa.tokenStream(fieldName, reader);\n    }\n\n    TokenFilterFactory[] newtf = new TokenFilterFactory[facs.length-1];\n    for (int i=0,j=0; i<facs.length; i++) {\n      if (i==stopIdx) continue;\n      newtf[j++] = facs[i];\n    }\n\n    TokenizerChain newa = new TokenizerChain(tcq.getTokenizerFactory(), newtf);\n    newa.setPositionIncrementGap(tcq.getPositionIncrementGap(fieldName));\n\n    map.put(fieldName, newa);\n    return newa.tokenStream(fieldName, reader);        \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c26f00b574427b55127e869b935845554afde1fa","date":1310252513,"type":1,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/ExtendedAnalyzer[ExtendedDismaxQParserPlugin]#tokenStream(String,Reader).mjava","pathOld":"solr/src/java/org/apache/solr/search/ExtendedAnalyzer[ExtendedDismaxQParserPlugin]#tokenStream(String,Reader).mjava","sourceNew":"  @Override\n  public TokenStream tokenStream(String fieldName, Reader reader) {\n    if (!removeStopFilter) {\n      return queryAnalyzer.tokenStream(fieldName, reader);\n    }\n    \n    Analyzer a = map.get(fieldName);\n    if (a != null) {\n      return a.tokenStream(fieldName, reader);\n    }\n\n    FieldType ft = parser.getReq().getSchema().getFieldType(fieldName);\n    Analyzer qa = ft.getQueryAnalyzer();\n    if (!(qa instanceof TokenizerChain)) {\n      map.put(fieldName, qa);\n      return qa.tokenStream(fieldName, reader);\n    }\n    TokenizerChain tcq = (TokenizerChain)qa;\n    Analyzer ia = ft.getAnalyzer();\n    if (ia == qa || !(ia instanceof TokenizerChain)) {\n      map.put(fieldName, qa);\n      return qa.tokenStream(fieldName, reader);\n    }\n    TokenizerChain tci = (TokenizerChain)ia;\n\n    // make sure that there isn't a stop filter in the indexer\n    for (TokenFilterFactory tf : tci.getTokenFilterFactories()) {\n      if (tf instanceof StopFilterFactory) {\n        map.put(fieldName, qa);\n        return qa.tokenStream(fieldName, reader);\n      }\n    }\n\n    // now if there is a stop filter in the query analyzer, remove it\n    int stopIdx = -1;\n    TokenFilterFactory[] facs = tcq.getTokenFilterFactories();\n\n    for (int i=0; i<facs.length; i++) {\n      TokenFilterFactory tf = facs[i];\n      if (tf instanceof StopFilterFactory) {\n        stopIdx = i;\n        break;\n      }\n    }\n\n    if (stopIdx == -1) {\n      // no stop filter exists\n      map.put(fieldName, qa);\n      return qa.tokenStream(fieldName, reader);\n    }\n\n    TokenFilterFactory[] newtf = new TokenFilterFactory[facs.length-1];\n    for (int i=0,j=0; i<facs.length; i++) {\n      if (i==stopIdx) continue;\n      newtf[j++] = facs[i];\n    }\n\n    TokenizerChain newa = new TokenizerChain(tcq.getTokenizerFactory(), newtf);\n    newa.setPositionIncrementGap(tcq.getPositionIncrementGap(fieldName));\n\n    map.put(fieldName, newa);\n    return newa.tokenStream(fieldName, reader);        \n  }\n\n","sourceOld":"  @Override\n  public TokenStream tokenStream(String fieldName, Reader reader) {\n    if (!removeStopFilter) {\n      return queryAnalyzer.tokenStream(fieldName, reader);\n    }\n    \n    Analyzer a = map.get(fieldName);\n    if (a != null) {\n      return a.tokenStream(fieldName, reader);\n    }\n\n    FieldType ft = parser.getReq().getSchema().getFieldType(fieldName);\n    Analyzer qa = ft.getQueryAnalyzer();\n    if (!(qa instanceof TokenizerChain)) {\n      map.put(fieldName, qa);\n      return qa.tokenStream(fieldName, reader);\n    }\n    TokenizerChain tcq = (TokenizerChain)qa;\n    Analyzer ia = ft.getAnalyzer();\n    if (ia == qa || !(ia instanceof TokenizerChain)) {\n      map.put(fieldName, qa);\n      return qa.tokenStream(fieldName, reader);\n    }\n    TokenizerChain tci = (TokenizerChain)ia;\n\n    // make sure that there isn't a stop filter in the indexer\n    for (TokenFilterFactory tf : tci.getTokenFilterFactories()) {\n      if (tf instanceof StopFilterFactory) {\n        map.put(fieldName, qa);\n        return qa.tokenStream(fieldName, reader);\n      }\n    }\n\n    // now if there is a stop filter in the query analyzer, remove it\n    int stopIdx = -1;\n    TokenFilterFactory[] facs = tcq.getTokenFilterFactories();\n\n    for (int i=0; i<facs.length; i++) {\n      TokenFilterFactory tf = facs[i];\n      if (tf instanceof StopFilterFactory) {\n        stopIdx = i;\n        break;\n      }\n    }\n\n    if (stopIdx == -1) {\n      // no stop filter exists\n      map.put(fieldName, qa);\n      return qa.tokenStream(fieldName, reader);\n    }\n\n    TokenFilterFactory[] newtf = new TokenFilterFactory[facs.length-1];\n    for (int i=0,j=0; i<facs.length; i++) {\n      if (i==stopIdx) continue;\n      newtf[j++] = facs[i];\n    }\n\n    TokenizerChain newa = new TokenizerChain(tcq.getTokenizerFactory(), newtf);\n    newa.setPositionIncrementGap(tcq.getPositionIncrementGap(fieldName));\n\n    map.put(fieldName, newa);\n    return newa.tokenStream(fieldName, reader);        \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"97a1d065fbe56cbf9eaf80cd3f8d4477203dda70","date":1315971416,"type":4,"author":"Christopher John Male","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/search/ExtendedAnalyzer[ExtendedDismaxQParserPlugin]#tokenStream(String,Reader).mjava","sourceNew":null,"sourceOld":"  @Override\n  public TokenStream tokenStream(String fieldName, Reader reader) {\n    if (!removeStopFilter) {\n      return queryAnalyzer.tokenStream(fieldName, reader);\n    }\n    \n    Analyzer a = map.get(fieldName);\n    if (a != null) {\n      return a.tokenStream(fieldName, reader);\n    }\n\n    FieldType ft = parser.getReq().getSchema().getFieldType(fieldName);\n    Analyzer qa = ft.getQueryAnalyzer();\n    if (!(qa instanceof TokenizerChain)) {\n      map.put(fieldName, qa);\n      return qa.tokenStream(fieldName, reader);\n    }\n    TokenizerChain tcq = (TokenizerChain)qa;\n    Analyzer ia = ft.getAnalyzer();\n    if (ia == qa || !(ia instanceof TokenizerChain)) {\n      map.put(fieldName, qa);\n      return qa.tokenStream(fieldName, reader);\n    }\n    TokenizerChain tci = (TokenizerChain)ia;\n\n    // make sure that there isn't a stop filter in the indexer\n    for (TokenFilterFactory tf : tci.getTokenFilterFactories()) {\n      if (tf instanceof StopFilterFactory) {\n        map.put(fieldName, qa);\n        return qa.tokenStream(fieldName, reader);\n      }\n    }\n\n    // now if there is a stop filter in the query analyzer, remove it\n    int stopIdx = -1;\n    TokenFilterFactory[] facs = tcq.getTokenFilterFactories();\n\n    for (int i=0; i<facs.length; i++) {\n      TokenFilterFactory tf = facs[i];\n      if (tf instanceof StopFilterFactory) {\n        stopIdx = i;\n        break;\n      }\n    }\n\n    if (stopIdx == -1) {\n      // no stop filter exists\n      map.put(fieldName, qa);\n      return qa.tokenStream(fieldName, reader);\n    }\n\n    TokenFilterFactory[] newtf = new TokenFilterFactory[facs.length-1];\n    for (int i=0,j=0; i<facs.length; i++) {\n      if (i==stopIdx) continue;\n      newtf[j++] = facs[i];\n    }\n\n    TokenizerChain newa = new TokenizerChain(tcq.getTokenizerFactory(), newtf);\n    newa.setPositionIncrementGap(tcq.getPositionIncrementGap(fieldName));\n\n    map.put(fieldName, newa);\n    return newa.tokenStream(fieldName, reader);        \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"c26f00b574427b55127e869b935845554afde1fa":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a258fbb26824fd104ed795e5d9033d2d040049ee":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"97a1d065fbe56cbf9eaf80cd3f8d4477203dda70":["c26f00b574427b55127e869b935845554afde1fa"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["97a1d065fbe56cbf9eaf80cd3f8d4477203dda70"]},"commit2Childs":{"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["c26f00b574427b55127e869b935845554afde1fa"],"c26f00b574427b55127e869b935845554afde1fa":["97a1d065fbe56cbf9eaf80cd3f8d4477203dda70"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c903c3d15906a3da96b8c0c2fb704491005fdbdb","c26f00b574427b55127e869b935845554afde1fa","a258fbb26824fd104ed795e5d9033d2d040049ee"],"a258fbb26824fd104ed795e5d9033d2d040049ee":[],"97a1d065fbe56cbf9eaf80cd3f8d4477203dda70":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a258fbb26824fd104ed795e5d9033d2d040049ee","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}