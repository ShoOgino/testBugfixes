{"path":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#softUpdatesConcurrently(boolean).mjava","commits":[{"id":"5ee0394b8176abd7c90a4be8c05465be1879db79","date":1522842314,"type":0,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#softUpdatesConcurrently(boolean).mjava","pathOld":"/dev/null","sourceNew":"  public void softUpdatesConcurrently(boolean mixDeletes) throws IOException, InterruptedException {\n    Directory dir = newDirectory();\n    IndexWriterConfig indexWriterConfig = newIndexWriterConfig();\n    indexWriterConfig.setSoftDeletesField(\"soft_delete\");\n    AtomicBoolean mergeAwaySoftDeletes = new AtomicBoolean(random().nextBoolean());\n    if (mixDeletes == false) {\n      indexWriterConfig.setMergePolicy(new OneMergeWrappingMergePolicy(indexWriterConfig.getMergePolicy(), towrap ->\n          new MergePolicy.OneMerge(towrap.segments) {\n            @Override\n            public CodecReader wrapForMerge(CodecReader reader) throws IOException {\n              if (mergeAwaySoftDeletes.get()) {\n                return towrap.wrapForMerge(reader);\n              } else {\n                CodecReader wrapped = towrap.wrapForMerge(reader);\n                return new FilterCodecReader(wrapped) {\n                  @Override\n                  public CacheHelper getCoreCacheHelper() {\n                    return in.getCoreCacheHelper();\n                  }\n\n                  @Override\n                  public CacheHelper getReaderCacheHelper() {\n                    return in.getReaderCacheHelper();\n                  }\n\n                  @Override\n                  public Bits getLiveDocs() {\n                    return null; // everything is live\n                  }\n\n                  @Override\n                  public int numDocs() {\n                    return maxDoc();\n                  }\n                };\n              }\n            }\n          }\n      ));\n    }\n    IndexWriter writer = new IndexWriter(dir, indexWriterConfig);\n    Thread[] threads = new Thread[2 + random().nextInt(3)];\n    CountDownLatch startLatch = new CountDownLatch(1);\n    CountDownLatch started = new CountDownLatch(threads.length);\n    boolean updateSeveralDocs = random().nextBoolean();\n    Set<String> ids = Collections.synchronizedSet(new HashSet<>());\n    for (int i = 0; i < threads.length; i++) {\n      threads[i] = new Thread(() -> {\n        try {\n          started.countDown();\n          startLatch.await();\n          for (int d = 0;  d < 100; d++) {\n            String id = String.valueOf(random().nextInt(10));\n            if (updateSeveralDocs) {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              if (mixDeletes && random().nextBoolean()) {\n                writer.updateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc));\n              } else {\n                writer.softUpdateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc),\n                    new NumericDocValuesField(\"soft_delete\", 1));\n              }\n            } else {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              if (mixDeletes && random().nextBoolean()) {\n                writer.updateDocument(new Term(\"id\", id), doc);\n              } else {\n                writer.softUpdateDocument(new Term(\"id\", id), doc,\n                    new NumericDocValuesField(\"soft_delete\", 1));\n              }\n            }\n            ids.add(id);\n          }\n        } catch (IOException | InterruptedException e) {\n          throw new AssertionError(e);\n        }\n      });\n      threads[i].start();\n    }\n    started.await();\n    startLatch.countDown();\n\n    for (int i = 0; i < threads.length; i++) {\n      threads[i].join();\n    }\n    DirectoryReader reader = DirectoryReader.open(writer);\n    IndexSearcher searcher = new IndexSearcher(reader);\n    for (String id : ids) {\n      TopDocs topDocs = searcher.search(new TermQuery(new Term(\"id\", id)), 10);\n      if (updateSeveralDocs) {\n        assertEquals(2, topDocs.totalHits);\n        assertEquals(Math.abs(topDocs.scoreDocs[0].doc - topDocs.scoreDocs[1].doc), 1);\n      } else {\n        assertEquals(1, topDocs.totalHits);\n      }\n    }\n    mergeAwaySoftDeletes.set(true);\n    writer.addDocument(new Document()); // add a dummy doc to trigger a segment here\n    writer.flush();\n    writer.forceMerge(1);\n    DirectoryReader oldReader = reader;\n    reader = DirectoryReader.openIfChanged(reader, writer);\n    if (reader != null) {\n      oldReader.close();\n      assertNotSame(oldReader, reader);\n    } else {\n      reader = oldReader;\n    }\n    for (String id : ids) {\n      if (updateSeveralDocs) {\n        assertEquals(2, reader.docFreq(new Term(\"id\", id)));\n      } else {\n        assertEquals(1, reader.docFreq(new Term(\"id\", id)));\n      }\n    }\n    IOUtils.close(reader, writer, dir);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8f2203cb8ae87188877cfbf6ad170c5738a0aad5","date":1528117512,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#softUpdatesConcurrently(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#softUpdatesConcurrently(boolean).mjava","sourceNew":"  public void softUpdatesConcurrently(boolean mixDeletes) throws IOException, InterruptedException {\n    Directory dir = newDirectory();\n    IndexWriterConfig indexWriterConfig = newIndexWriterConfig();\n    indexWriterConfig.setSoftDeletesField(\"soft_delete\");\n    AtomicBoolean mergeAwaySoftDeletes = new AtomicBoolean(random().nextBoolean());\n    if (mixDeletes == false) {\n      indexWriterConfig.setMergePolicy(new OneMergeWrappingMergePolicy(indexWriterConfig.getMergePolicy(), towrap ->\n          new MergePolicy.OneMerge(towrap.segments) {\n            @Override\n            public CodecReader wrapForMerge(CodecReader reader) throws IOException {\n              if (mergeAwaySoftDeletes.get()) {\n                return towrap.wrapForMerge(reader);\n              } else {\n                CodecReader wrapped = towrap.wrapForMerge(reader);\n                return new FilterCodecReader(wrapped) {\n                  @Override\n                  public CacheHelper getCoreCacheHelper() {\n                    return in.getCoreCacheHelper();\n                  }\n\n                  @Override\n                  public CacheHelper getReaderCacheHelper() {\n                    return in.getReaderCacheHelper();\n                  }\n\n                  @Override\n                  public Bits getLiveDocs() {\n                    return null; // everything is live\n                  }\n\n                  @Override\n                  public int numDocs() {\n                    return maxDoc();\n                  }\n                };\n              }\n            }\n          }\n      ));\n    }\n    IndexWriter writer = new IndexWriter(dir, indexWriterConfig);\n    Thread[] threads = new Thread[2 + random().nextInt(3)];\n    CountDownLatch startLatch = new CountDownLatch(1);\n    CountDownLatch started = new CountDownLatch(threads.length);\n    boolean updateSeveralDocs = random().nextBoolean();\n    Set<String> ids = Collections.synchronizedSet(new HashSet<>());\n    for (int i = 0; i < threads.length; i++) {\n      threads[i] = new Thread(() -> {\n        try {\n          started.countDown();\n          startLatch.await();\n          for (int d = 0;  d < 100; d++) {\n            String id = String.valueOf(random().nextInt(10));\n            if (updateSeveralDocs) {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              if (mixDeletes && random().nextBoolean()) {\n                writer.updateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc));\n              } else {\n                writer.softUpdateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc),\n                    new NumericDocValuesField(\"soft_delete\", 1));\n              }\n            } else {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              if (mixDeletes && random().nextBoolean()) {\n                writer.updateDocument(new Term(\"id\", id), doc);\n              } else {\n                writer.softUpdateDocument(new Term(\"id\", id), doc,\n                    new NumericDocValuesField(\"soft_delete\", 1));\n              }\n            }\n            ids.add(id);\n          }\n        } catch (IOException | InterruptedException e) {\n          throw new AssertionError(e);\n        }\n      });\n      threads[i].start();\n    }\n    started.await();\n    startLatch.countDown();\n\n    for (int i = 0; i < threads.length; i++) {\n      threads[i].join();\n    }\n    DirectoryReader reader = DirectoryReader.open(writer);\n    IndexSearcher searcher = new IndexSearcher(reader);\n    for (String id : ids) {\n      TopDocs topDocs = searcher.search(new TermQuery(new Term(\"id\", id)), 10);\n      if (updateSeveralDocs) {\n        assertEquals(2, topDocs.totalHits);\n        assertEquals(Math.abs(topDocs.scoreDocs[0].doc - topDocs.scoreDocs[1].doc), 1);\n      } else {\n        assertEquals(1, topDocs.totalHits);\n      }\n    }\n    mergeAwaySoftDeletes.set(true);\n    writer.addDocument(new Document()); // add a dummy doc to trigger a segment here\n    writer.flush();\n    writer.forceMerge(1);\n    DirectoryReader oldReader = reader;\n    reader = DirectoryReader.openIfChanged(reader, writer);\n    if (reader != null) {\n      oldReader.close();\n      assertNotSame(oldReader, reader);\n    } else {\n      reader = oldReader;\n    }\n    for (String id : ids) {\n      if (updateSeveralDocs) {\n        assertEquals(2, reader.docFreq(new Term(\"id\", id)));\n      } else {\n        assertEquals(1, reader.docFreq(new Term(\"id\", id)));\n      }\n    }\n    int numSoftDeleted = 0;\n    for (SegmentCommitInfo info : writer.segmentInfos) {\n      numSoftDeleted += info.getSoftDelCount() + info.getDelCount();\n    }\n    assertEquals(writer.maxDoc() - writer.numDocs(), numSoftDeleted);\n    writer.commit();\n    try (DirectoryReader dirReader = DirectoryReader.open(dir)) {\n      int delCount = 0;\n      for (LeafReaderContext ctx : dirReader.leaves()) {\n        SegmentCommitInfo segmentInfo = ((SegmentReader) ctx.reader()).getSegmentInfo();\n        delCount += segmentInfo.getSoftDelCount() + segmentInfo.getDelCount();\n      }\n      assertEquals(numSoftDeleted, delCount);\n    }\n    IOUtils.close(reader, writer, dir);\n  }\n\n","sourceOld":"  public void softUpdatesConcurrently(boolean mixDeletes) throws IOException, InterruptedException {\n    Directory dir = newDirectory();\n    IndexWriterConfig indexWriterConfig = newIndexWriterConfig();\n    indexWriterConfig.setSoftDeletesField(\"soft_delete\");\n    AtomicBoolean mergeAwaySoftDeletes = new AtomicBoolean(random().nextBoolean());\n    if (mixDeletes == false) {\n      indexWriterConfig.setMergePolicy(new OneMergeWrappingMergePolicy(indexWriterConfig.getMergePolicy(), towrap ->\n          new MergePolicy.OneMerge(towrap.segments) {\n            @Override\n            public CodecReader wrapForMerge(CodecReader reader) throws IOException {\n              if (mergeAwaySoftDeletes.get()) {\n                return towrap.wrapForMerge(reader);\n              } else {\n                CodecReader wrapped = towrap.wrapForMerge(reader);\n                return new FilterCodecReader(wrapped) {\n                  @Override\n                  public CacheHelper getCoreCacheHelper() {\n                    return in.getCoreCacheHelper();\n                  }\n\n                  @Override\n                  public CacheHelper getReaderCacheHelper() {\n                    return in.getReaderCacheHelper();\n                  }\n\n                  @Override\n                  public Bits getLiveDocs() {\n                    return null; // everything is live\n                  }\n\n                  @Override\n                  public int numDocs() {\n                    return maxDoc();\n                  }\n                };\n              }\n            }\n          }\n      ));\n    }\n    IndexWriter writer = new IndexWriter(dir, indexWriterConfig);\n    Thread[] threads = new Thread[2 + random().nextInt(3)];\n    CountDownLatch startLatch = new CountDownLatch(1);\n    CountDownLatch started = new CountDownLatch(threads.length);\n    boolean updateSeveralDocs = random().nextBoolean();\n    Set<String> ids = Collections.synchronizedSet(new HashSet<>());\n    for (int i = 0; i < threads.length; i++) {\n      threads[i] = new Thread(() -> {\n        try {\n          started.countDown();\n          startLatch.await();\n          for (int d = 0;  d < 100; d++) {\n            String id = String.valueOf(random().nextInt(10));\n            if (updateSeveralDocs) {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              if (mixDeletes && random().nextBoolean()) {\n                writer.updateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc));\n              } else {\n                writer.softUpdateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc),\n                    new NumericDocValuesField(\"soft_delete\", 1));\n              }\n            } else {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              if (mixDeletes && random().nextBoolean()) {\n                writer.updateDocument(new Term(\"id\", id), doc);\n              } else {\n                writer.softUpdateDocument(new Term(\"id\", id), doc,\n                    new NumericDocValuesField(\"soft_delete\", 1));\n              }\n            }\n            ids.add(id);\n          }\n        } catch (IOException | InterruptedException e) {\n          throw new AssertionError(e);\n        }\n      });\n      threads[i].start();\n    }\n    started.await();\n    startLatch.countDown();\n\n    for (int i = 0; i < threads.length; i++) {\n      threads[i].join();\n    }\n    DirectoryReader reader = DirectoryReader.open(writer);\n    IndexSearcher searcher = new IndexSearcher(reader);\n    for (String id : ids) {\n      TopDocs topDocs = searcher.search(new TermQuery(new Term(\"id\", id)), 10);\n      if (updateSeveralDocs) {\n        assertEquals(2, topDocs.totalHits);\n        assertEquals(Math.abs(topDocs.scoreDocs[0].doc - topDocs.scoreDocs[1].doc), 1);\n      } else {\n        assertEquals(1, topDocs.totalHits);\n      }\n    }\n    mergeAwaySoftDeletes.set(true);\n    writer.addDocument(new Document()); // add a dummy doc to trigger a segment here\n    writer.flush();\n    writer.forceMerge(1);\n    DirectoryReader oldReader = reader;\n    reader = DirectoryReader.openIfChanged(reader, writer);\n    if (reader != null) {\n      oldReader.close();\n      assertNotSame(oldReader, reader);\n    } else {\n      reader = oldReader;\n    }\n    for (String id : ids) {\n      if (updateSeveralDocs) {\n        assertEquals(2, reader.docFreq(new Term(\"id\", id)));\n      } else {\n        assertEquals(1, reader.docFreq(new Term(\"id\", id)));\n      }\n    }\n    IOUtils.close(reader, writer, dir);\n  }\n\n","bugFix":null,"bugIntro":["631ea3d1607299c59f33edef140ffc19a81f07a0"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f592209545c71895260367152601e9200399776d","date":1528238935,"type":3,"author":"Michael Braun","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#softUpdatesConcurrently(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#softUpdatesConcurrently(boolean).mjava","sourceNew":"  public void softUpdatesConcurrently(boolean mixDeletes) throws IOException, InterruptedException {\n    Directory dir = newDirectory();\n    IndexWriterConfig indexWriterConfig = newIndexWriterConfig();\n    indexWriterConfig.setSoftDeletesField(\"soft_delete\");\n    AtomicBoolean mergeAwaySoftDeletes = new AtomicBoolean(random().nextBoolean());\n    if (mixDeletes == false) {\n      indexWriterConfig.setMergePolicy(new OneMergeWrappingMergePolicy(indexWriterConfig.getMergePolicy(), towrap ->\n          new MergePolicy.OneMerge(towrap.segments) {\n            @Override\n            public CodecReader wrapForMerge(CodecReader reader) throws IOException {\n              if (mergeAwaySoftDeletes.get()) {\n                return towrap.wrapForMerge(reader);\n              } else {\n                CodecReader wrapped = towrap.wrapForMerge(reader);\n                return new FilterCodecReader(wrapped) {\n                  @Override\n                  public CacheHelper getCoreCacheHelper() {\n                    return in.getCoreCacheHelper();\n                  }\n\n                  @Override\n                  public CacheHelper getReaderCacheHelper() {\n                    return in.getReaderCacheHelper();\n                  }\n\n                  @Override\n                  public Bits getLiveDocs() {\n                    return null; // everything is live\n                  }\n\n                  @Override\n                  public int numDocs() {\n                    return maxDoc();\n                  }\n                };\n              }\n            }\n          }\n      ));\n    }\n    IndexWriter writer = new IndexWriter(dir, indexWriterConfig);\n    Thread[] threads = new Thread[2 + random().nextInt(3)];\n    CountDownLatch startLatch = new CountDownLatch(1);\n    CountDownLatch started = new CountDownLatch(threads.length);\n    boolean updateSeveralDocs = random().nextBoolean();\n    Set<String> ids = Collections.synchronizedSet(new HashSet<>());\n    for (int i = 0; i < threads.length; i++) {\n      threads[i] = new Thread(() -> {\n        try {\n          started.countDown();\n          startLatch.await();\n          for (int d = 0;  d < 100; d++) {\n            String id = String.valueOf(random().nextInt(10));\n            if (updateSeveralDocs) {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              if (mixDeletes && random().nextBoolean()) {\n                writer.updateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc));\n              } else {\n                writer.softUpdateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc),\n                    new NumericDocValuesField(\"soft_delete\", 1));\n              }\n            } else {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              if (mixDeletes && random().nextBoolean()) {\n                writer.updateDocument(new Term(\"id\", id), doc);\n              } else {\n                writer.softUpdateDocument(new Term(\"id\", id), doc,\n                    new NumericDocValuesField(\"soft_delete\", 1));\n              }\n            }\n            ids.add(id);\n          }\n        } catch (IOException | InterruptedException e) {\n          throw new AssertionError(e);\n        }\n      });\n      threads[i].start();\n    }\n    started.await();\n    startLatch.countDown();\n\n    for (int i = 0; i < threads.length; i++) {\n      threads[i].join();\n    }\n    DirectoryReader reader = DirectoryReader.open(writer);\n    IndexSearcher searcher = new IndexSearcher(reader);\n    for (String id : ids) {\n      TopDocs topDocs = searcher.search(new TermQuery(new Term(\"id\", id)), 10);\n      if (updateSeveralDocs) {\n        assertEquals(2, topDocs.totalHits);\n        assertEquals(Math.abs(topDocs.scoreDocs[0].doc - topDocs.scoreDocs[1].doc), 1);\n      } else {\n        assertEquals(1, topDocs.totalHits);\n      }\n    }\n    mergeAwaySoftDeletes.set(true);\n    writer.addDocument(new Document()); // add a dummy doc to trigger a segment here\n    writer.flush();\n    writer.forceMerge(1);\n    DirectoryReader oldReader = reader;\n    reader = DirectoryReader.openIfChanged(reader, writer);\n    if (reader != null) {\n      oldReader.close();\n      assertNotSame(oldReader, reader);\n    } else {\n      reader = oldReader;\n    }\n    for (String id : ids) {\n      if (updateSeveralDocs) {\n        assertEquals(2, reader.docFreq(new Term(\"id\", id)));\n      } else {\n        assertEquals(1, reader.docFreq(new Term(\"id\", id)));\n      }\n    }\n    int numSoftDeleted = 0;\n    for (SegmentCommitInfo info : writer.segmentInfos) {\n      numSoftDeleted += info.getSoftDelCount() + info.getDelCount();\n    }\n    assertEquals(writer.maxDoc() - writer.numDocs(), numSoftDeleted);\n    writer.commit();\n    try (DirectoryReader dirReader = DirectoryReader.open(dir)) {\n      int delCount = 0;\n      for (LeafReaderContext ctx : dirReader.leaves()) {\n        SegmentCommitInfo segmentInfo = ((SegmentReader) ctx.reader()).getSegmentInfo();\n        delCount += segmentInfo.getSoftDelCount() + segmentInfo.getDelCount();\n      }\n      assertEquals(numSoftDeleted, delCount);\n    }\n    IOUtils.close(reader, writer, dir);\n  }\n\n","sourceOld":"  public void softUpdatesConcurrently(boolean mixDeletes) throws IOException, InterruptedException {\n    Directory dir = newDirectory();\n    IndexWriterConfig indexWriterConfig = newIndexWriterConfig();\n    indexWriterConfig.setSoftDeletesField(\"soft_delete\");\n    AtomicBoolean mergeAwaySoftDeletes = new AtomicBoolean(random().nextBoolean());\n    if (mixDeletes == false) {\n      indexWriterConfig.setMergePolicy(new OneMergeWrappingMergePolicy(indexWriterConfig.getMergePolicy(), towrap ->\n          new MergePolicy.OneMerge(towrap.segments) {\n            @Override\n            public CodecReader wrapForMerge(CodecReader reader) throws IOException {\n              if (mergeAwaySoftDeletes.get()) {\n                return towrap.wrapForMerge(reader);\n              } else {\n                CodecReader wrapped = towrap.wrapForMerge(reader);\n                return new FilterCodecReader(wrapped) {\n                  @Override\n                  public CacheHelper getCoreCacheHelper() {\n                    return in.getCoreCacheHelper();\n                  }\n\n                  @Override\n                  public CacheHelper getReaderCacheHelper() {\n                    return in.getReaderCacheHelper();\n                  }\n\n                  @Override\n                  public Bits getLiveDocs() {\n                    return null; // everything is live\n                  }\n\n                  @Override\n                  public int numDocs() {\n                    return maxDoc();\n                  }\n                };\n              }\n            }\n          }\n      ));\n    }\n    IndexWriter writer = new IndexWriter(dir, indexWriterConfig);\n    Thread[] threads = new Thread[2 + random().nextInt(3)];\n    CountDownLatch startLatch = new CountDownLatch(1);\n    CountDownLatch started = new CountDownLatch(threads.length);\n    boolean updateSeveralDocs = random().nextBoolean();\n    Set<String> ids = Collections.synchronizedSet(new HashSet<>());\n    for (int i = 0; i < threads.length; i++) {\n      threads[i] = new Thread(() -> {\n        try {\n          started.countDown();\n          startLatch.await();\n          for (int d = 0;  d < 100; d++) {\n            String id = String.valueOf(random().nextInt(10));\n            if (updateSeveralDocs) {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              if (mixDeletes && random().nextBoolean()) {\n                writer.updateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc));\n              } else {\n                writer.softUpdateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc),\n                    new NumericDocValuesField(\"soft_delete\", 1));\n              }\n            } else {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              if (mixDeletes && random().nextBoolean()) {\n                writer.updateDocument(new Term(\"id\", id), doc);\n              } else {\n                writer.softUpdateDocument(new Term(\"id\", id), doc,\n                    new NumericDocValuesField(\"soft_delete\", 1));\n              }\n            }\n            ids.add(id);\n          }\n        } catch (IOException | InterruptedException e) {\n          throw new AssertionError(e);\n        }\n      });\n      threads[i].start();\n    }\n    started.await();\n    startLatch.countDown();\n\n    for (int i = 0; i < threads.length; i++) {\n      threads[i].join();\n    }\n    DirectoryReader reader = DirectoryReader.open(writer);\n    IndexSearcher searcher = new IndexSearcher(reader);\n    for (String id : ids) {\n      TopDocs topDocs = searcher.search(new TermQuery(new Term(\"id\", id)), 10);\n      if (updateSeveralDocs) {\n        assertEquals(2, topDocs.totalHits);\n        assertEquals(Math.abs(topDocs.scoreDocs[0].doc - topDocs.scoreDocs[1].doc), 1);\n      } else {\n        assertEquals(1, topDocs.totalHits);\n      }\n    }\n    mergeAwaySoftDeletes.set(true);\n    writer.addDocument(new Document()); // add a dummy doc to trigger a segment here\n    writer.flush();\n    writer.forceMerge(1);\n    DirectoryReader oldReader = reader;\n    reader = DirectoryReader.openIfChanged(reader, writer);\n    if (reader != null) {\n      oldReader.close();\n      assertNotSame(oldReader, reader);\n    } else {\n      reader = oldReader;\n    }\n    for (String id : ids) {\n      if (updateSeveralDocs) {\n        assertEquals(2, reader.docFreq(new Term(\"id\", id)));\n      } else {\n        assertEquals(1, reader.docFreq(new Term(\"id\", id)));\n      }\n    }\n    IOUtils.close(reader, writer, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b70042a8a492f7054d480ccdd2be9796510d4327","date":1528386658,"type":3,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#softUpdatesConcurrently(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#softUpdatesConcurrently(boolean).mjava","sourceNew":"  public void softUpdatesConcurrently(boolean mixDeletes) throws IOException, InterruptedException {\n    Directory dir = newDirectory();\n    IndexWriterConfig indexWriterConfig = newIndexWriterConfig();\n    indexWriterConfig.setSoftDeletesField(\"soft_delete\");\n    AtomicBoolean mergeAwaySoftDeletes = new AtomicBoolean(random().nextBoolean());\n    if (mixDeletes == false) {\n      indexWriterConfig.setMergePolicy(new OneMergeWrappingMergePolicy(indexWriterConfig.getMergePolicy(), towrap ->\n          new MergePolicy.OneMerge(towrap.segments) {\n            @Override\n            public CodecReader wrapForMerge(CodecReader reader) throws IOException {\n              if (mergeAwaySoftDeletes.get()) {\n                return towrap.wrapForMerge(reader);\n              } else {\n                CodecReader wrapped = towrap.wrapForMerge(reader);\n                return new FilterCodecReader(wrapped) {\n                  @Override\n                  public CacheHelper getCoreCacheHelper() {\n                    return in.getCoreCacheHelper();\n                  }\n\n                  @Override\n                  public CacheHelper getReaderCacheHelper() {\n                    return in.getReaderCacheHelper();\n                  }\n\n                  @Override\n                  public Bits getLiveDocs() {\n                    return null; // everything is live\n                  }\n\n                  @Override\n                  public int numDocs() {\n                    return maxDoc();\n                  }\n                };\n              }\n            }\n          }\n      ));\n    }\n    IndexWriter writer = new IndexWriter(dir, indexWriterConfig);\n    Thread[] threads = new Thread[2 + random().nextInt(3)];\n    CountDownLatch startLatch = new CountDownLatch(1);\n    CountDownLatch started = new CountDownLatch(threads.length);\n    boolean updateSeveralDocs = random().nextBoolean();\n    Set<String> ids = Collections.synchronizedSet(new HashSet<>());\n    for (int i = 0; i < threads.length; i++) {\n      threads[i] = new Thread(() -> {\n        try {\n          started.countDown();\n          startLatch.await();\n          for (int d = 0;  d < 100; d++) {\n            String id = String.valueOf(random().nextInt(10));\n            if (updateSeveralDocs) {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              if (mixDeletes && random().nextBoolean()) {\n                writer.updateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc));\n              } else {\n                writer.softUpdateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc),\n                    new NumericDocValuesField(\"soft_delete\", 1));\n              }\n            } else {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              if (mixDeletes && random().nextBoolean()) {\n                writer.updateDocument(new Term(\"id\", id), doc);\n              } else {\n                writer.softUpdateDocument(new Term(\"id\", id), doc,\n                    new NumericDocValuesField(\"soft_delete\", 1));\n              }\n            }\n            ids.add(id);\n          }\n        } catch (IOException | InterruptedException e) {\n          throw new AssertionError(e);\n        }\n      });\n      threads[i].start();\n    }\n    started.await();\n    startLatch.countDown();\n\n    for (int i = 0; i < threads.length; i++) {\n      threads[i].join();\n    }\n    DirectoryReader reader = DirectoryReader.open(writer);\n    IndexSearcher searcher = new IndexSearcher(reader);\n    for (String id : ids) {\n      TopDocs topDocs = searcher.search(new TermQuery(new Term(\"id\", id)), 10);\n      if (updateSeveralDocs) {\n        assertEquals(2, topDocs.totalHits);\n        assertEquals(Math.abs(topDocs.scoreDocs[0].doc - topDocs.scoreDocs[1].doc), 1);\n      } else {\n        assertEquals(1, topDocs.totalHits);\n      }\n    }\n    mergeAwaySoftDeletes.set(true);\n    writer.addDocument(new Document()); // add a dummy doc to trigger a segment here\n    writer.flush();\n    writer.forceMerge(1);\n    DirectoryReader oldReader = reader;\n    reader = DirectoryReader.openIfChanged(reader, writer);\n    if (reader != null) {\n      oldReader.close();\n      assertNotSame(oldReader, reader);\n    } else {\n      reader = oldReader;\n    }\n    for (String id : ids) {\n      if (updateSeveralDocs) {\n        assertEquals(2, reader.docFreq(new Term(\"id\", id)));\n      } else {\n        assertEquals(1, reader.docFreq(new Term(\"id\", id)));\n      }\n    }\n    int numSoftDeleted = 0;\n    for (SegmentCommitInfo info : writer.segmentInfos) {\n      numSoftDeleted += info.getSoftDelCount() + info.getDelCount();\n    }\n    assertEquals(writer.maxDoc() - writer.numDocs(), numSoftDeleted);\n    writer.commit();\n    try (DirectoryReader dirReader = DirectoryReader.open(dir)) {\n      int delCount = 0;\n      for (LeafReaderContext ctx : dirReader.leaves()) {\n        SegmentCommitInfo segmentInfo = ((SegmentReader) ctx.reader()).getSegmentInfo();\n        delCount += segmentInfo.getSoftDelCount() + segmentInfo.getDelCount();\n      }\n      assertEquals(numSoftDeleted, delCount);\n    }\n    IOUtils.close(reader, writer, dir);\n  }\n\n","sourceOld":"  public void softUpdatesConcurrently(boolean mixDeletes) throws IOException, InterruptedException {\n    Directory dir = newDirectory();\n    IndexWriterConfig indexWriterConfig = newIndexWriterConfig();\n    indexWriterConfig.setSoftDeletesField(\"soft_delete\");\n    AtomicBoolean mergeAwaySoftDeletes = new AtomicBoolean(random().nextBoolean());\n    if (mixDeletes == false) {\n      indexWriterConfig.setMergePolicy(new OneMergeWrappingMergePolicy(indexWriterConfig.getMergePolicy(), towrap ->\n          new MergePolicy.OneMerge(towrap.segments) {\n            @Override\n            public CodecReader wrapForMerge(CodecReader reader) throws IOException {\n              if (mergeAwaySoftDeletes.get()) {\n                return towrap.wrapForMerge(reader);\n              } else {\n                CodecReader wrapped = towrap.wrapForMerge(reader);\n                return new FilterCodecReader(wrapped) {\n                  @Override\n                  public CacheHelper getCoreCacheHelper() {\n                    return in.getCoreCacheHelper();\n                  }\n\n                  @Override\n                  public CacheHelper getReaderCacheHelper() {\n                    return in.getReaderCacheHelper();\n                  }\n\n                  @Override\n                  public Bits getLiveDocs() {\n                    return null; // everything is live\n                  }\n\n                  @Override\n                  public int numDocs() {\n                    return maxDoc();\n                  }\n                };\n              }\n            }\n          }\n      ));\n    }\n    IndexWriter writer = new IndexWriter(dir, indexWriterConfig);\n    Thread[] threads = new Thread[2 + random().nextInt(3)];\n    CountDownLatch startLatch = new CountDownLatch(1);\n    CountDownLatch started = new CountDownLatch(threads.length);\n    boolean updateSeveralDocs = random().nextBoolean();\n    Set<String> ids = Collections.synchronizedSet(new HashSet<>());\n    for (int i = 0; i < threads.length; i++) {\n      threads[i] = new Thread(() -> {\n        try {\n          started.countDown();\n          startLatch.await();\n          for (int d = 0;  d < 100; d++) {\n            String id = String.valueOf(random().nextInt(10));\n            if (updateSeveralDocs) {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              if (mixDeletes && random().nextBoolean()) {\n                writer.updateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc));\n              } else {\n                writer.softUpdateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc),\n                    new NumericDocValuesField(\"soft_delete\", 1));\n              }\n            } else {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              if (mixDeletes && random().nextBoolean()) {\n                writer.updateDocument(new Term(\"id\", id), doc);\n              } else {\n                writer.softUpdateDocument(new Term(\"id\", id), doc,\n                    new NumericDocValuesField(\"soft_delete\", 1));\n              }\n            }\n            ids.add(id);\n          }\n        } catch (IOException | InterruptedException e) {\n          throw new AssertionError(e);\n        }\n      });\n      threads[i].start();\n    }\n    started.await();\n    startLatch.countDown();\n\n    for (int i = 0; i < threads.length; i++) {\n      threads[i].join();\n    }\n    DirectoryReader reader = DirectoryReader.open(writer);\n    IndexSearcher searcher = new IndexSearcher(reader);\n    for (String id : ids) {\n      TopDocs topDocs = searcher.search(new TermQuery(new Term(\"id\", id)), 10);\n      if (updateSeveralDocs) {\n        assertEquals(2, topDocs.totalHits);\n        assertEquals(Math.abs(topDocs.scoreDocs[0].doc - topDocs.scoreDocs[1].doc), 1);\n      } else {\n        assertEquals(1, topDocs.totalHits);\n      }\n    }\n    mergeAwaySoftDeletes.set(true);\n    writer.addDocument(new Document()); // add a dummy doc to trigger a segment here\n    writer.flush();\n    writer.forceMerge(1);\n    DirectoryReader oldReader = reader;\n    reader = DirectoryReader.openIfChanged(reader, writer);\n    if (reader != null) {\n      oldReader.close();\n      assertNotSame(oldReader, reader);\n    } else {\n      reader = oldReader;\n    }\n    for (String id : ids) {\n      if (updateSeveralDocs) {\n        assertEquals(2, reader.docFreq(new Term(\"id\", id)));\n      } else {\n        assertEquals(1, reader.docFreq(new Term(\"id\", id)));\n      }\n    }\n    IOUtils.close(reader, writer, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4a90cc8c90aa53ddf51fbd15019989ac269514a3","date":1531845066,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#softUpdatesConcurrently(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#softUpdatesConcurrently(boolean).mjava","sourceNew":"  public void softUpdatesConcurrently(boolean mixDeletes) throws IOException, InterruptedException {\n    Directory dir = newDirectory();\n    IndexWriterConfig indexWriterConfig = newIndexWriterConfig();\n    indexWriterConfig.setSoftDeletesField(\"soft_delete\");\n    AtomicBoolean mergeAwaySoftDeletes = new AtomicBoolean(random().nextBoolean());\n    if (mixDeletes == false) {\n      indexWriterConfig.setMergePolicy(new OneMergeWrappingMergePolicy(indexWriterConfig.getMergePolicy(), towrap ->\n          new MergePolicy.OneMerge(towrap.segments) {\n            @Override\n            public CodecReader wrapForMerge(CodecReader reader) throws IOException {\n              if (mergeAwaySoftDeletes.get()) {\n                return towrap.wrapForMerge(reader);\n              } else {\n                CodecReader wrapped = towrap.wrapForMerge(reader);\n                return new FilterCodecReader(wrapped) {\n                  @Override\n                  public CacheHelper getCoreCacheHelper() {\n                    return in.getCoreCacheHelper();\n                  }\n\n                  @Override\n                  public CacheHelper getReaderCacheHelper() {\n                    return in.getReaderCacheHelper();\n                  }\n\n                  @Override\n                  public Bits getLiveDocs() {\n                    return null; // everything is live\n                  }\n\n                  @Override\n                  public int numDocs() {\n                    return maxDoc();\n                  }\n                };\n              }\n            }\n          }\n      ) {\n        @Override\n        public int numDeletesToMerge(SegmentCommitInfo info, int delCount, IOSupplier<CodecReader> readerSupplier) throws IOException {\n          if (mergeAwaySoftDeletes.get()) {\n            return super.numDeletesToMerge(info, delCount, readerSupplier);\n          } else {\n            return 0;\n          }\n        }\n      });\n    }\n    IndexWriter writer = new IndexWriter(dir, indexWriterConfig);\n    Thread[] threads = new Thread[2 + random().nextInt(3)];\n    CountDownLatch startLatch = new CountDownLatch(1);\n    CountDownLatch started = new CountDownLatch(threads.length);\n    boolean updateSeveralDocs = random().nextBoolean();\n    Set<String> ids = Collections.synchronizedSet(new HashSet<>());\n    for (int i = 0; i < threads.length; i++) {\n      threads[i] = new Thread(() -> {\n        try {\n          started.countDown();\n          startLatch.await();\n          for (int d = 0;  d < 100; d++) {\n            String id = String.valueOf(random().nextInt(10));\n            if (updateSeveralDocs) {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              if (mixDeletes && random().nextBoolean()) {\n                writer.updateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc));\n              } else {\n                writer.softUpdateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc),\n                    new NumericDocValuesField(\"soft_delete\", 1));\n              }\n            } else {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              if (mixDeletes && random().nextBoolean()) {\n                writer.updateDocument(new Term(\"id\", id), doc);\n              } else {\n                writer.softUpdateDocument(new Term(\"id\", id), doc,\n                    new NumericDocValuesField(\"soft_delete\", 1));\n              }\n            }\n            ids.add(id);\n          }\n        } catch (IOException | InterruptedException e) {\n          throw new AssertionError(e);\n        }\n      });\n      threads[i].start();\n    }\n    started.await();\n    startLatch.countDown();\n\n    for (int i = 0; i < threads.length; i++) {\n      threads[i].join();\n    }\n    DirectoryReader reader = DirectoryReader.open(writer);\n    IndexSearcher searcher = new IndexSearcher(reader);\n    for (String id : ids) {\n      TopDocs topDocs = searcher.search(new TermQuery(new Term(\"id\", id)), 10);\n      if (updateSeveralDocs) {\n        assertEquals(2, topDocs.totalHits);\n        assertEquals(Math.abs(topDocs.scoreDocs[0].doc - topDocs.scoreDocs[1].doc), 1);\n      } else {\n        assertEquals(1, topDocs.totalHits);\n      }\n    }\n    mergeAwaySoftDeletes.set(true);\n    writer.addDocument(new Document()); // add a dummy doc to trigger a segment here\n    writer.flush();\n    writer.forceMerge(1);\n    DirectoryReader oldReader = reader;\n    reader = DirectoryReader.openIfChanged(reader, writer);\n    if (reader != null) {\n      oldReader.close();\n      assertNotSame(oldReader, reader);\n    } else {\n      reader = oldReader;\n    }\n    for (String id : ids) {\n      if (updateSeveralDocs) {\n        assertEquals(2, reader.docFreq(new Term(\"id\", id)));\n      } else {\n        assertEquals(1, reader.docFreq(new Term(\"id\", id)));\n      }\n    }\n    int numSoftDeleted = 0;\n    for (SegmentCommitInfo info : writer.segmentInfos) {\n      numSoftDeleted += info.getSoftDelCount() + info.getDelCount();\n    }\n    assertEquals(writer.maxDoc() - writer.numDocs(), numSoftDeleted);\n    writer.commit();\n    try (DirectoryReader dirReader = DirectoryReader.open(dir)) {\n      int delCount = 0;\n      for (LeafReaderContext ctx : dirReader.leaves()) {\n        SegmentCommitInfo segmentInfo = ((SegmentReader) ctx.reader()).getSegmentInfo();\n        delCount += segmentInfo.getSoftDelCount() + segmentInfo.getDelCount();\n      }\n      assertEquals(numSoftDeleted, delCount);\n    }\n    IOUtils.close(reader, writer, dir);\n  }\n\n","sourceOld":"  public void softUpdatesConcurrently(boolean mixDeletes) throws IOException, InterruptedException {\n    Directory dir = newDirectory();\n    IndexWriterConfig indexWriterConfig = newIndexWriterConfig();\n    indexWriterConfig.setSoftDeletesField(\"soft_delete\");\n    AtomicBoolean mergeAwaySoftDeletes = new AtomicBoolean(random().nextBoolean());\n    if (mixDeletes == false) {\n      indexWriterConfig.setMergePolicy(new OneMergeWrappingMergePolicy(indexWriterConfig.getMergePolicy(), towrap ->\n          new MergePolicy.OneMerge(towrap.segments) {\n            @Override\n            public CodecReader wrapForMerge(CodecReader reader) throws IOException {\n              if (mergeAwaySoftDeletes.get()) {\n                return towrap.wrapForMerge(reader);\n              } else {\n                CodecReader wrapped = towrap.wrapForMerge(reader);\n                return new FilterCodecReader(wrapped) {\n                  @Override\n                  public CacheHelper getCoreCacheHelper() {\n                    return in.getCoreCacheHelper();\n                  }\n\n                  @Override\n                  public CacheHelper getReaderCacheHelper() {\n                    return in.getReaderCacheHelper();\n                  }\n\n                  @Override\n                  public Bits getLiveDocs() {\n                    return null; // everything is live\n                  }\n\n                  @Override\n                  public int numDocs() {\n                    return maxDoc();\n                  }\n                };\n              }\n            }\n          }\n      ));\n    }\n    IndexWriter writer = new IndexWriter(dir, indexWriterConfig);\n    Thread[] threads = new Thread[2 + random().nextInt(3)];\n    CountDownLatch startLatch = new CountDownLatch(1);\n    CountDownLatch started = new CountDownLatch(threads.length);\n    boolean updateSeveralDocs = random().nextBoolean();\n    Set<String> ids = Collections.synchronizedSet(new HashSet<>());\n    for (int i = 0; i < threads.length; i++) {\n      threads[i] = new Thread(() -> {\n        try {\n          started.countDown();\n          startLatch.await();\n          for (int d = 0;  d < 100; d++) {\n            String id = String.valueOf(random().nextInt(10));\n            if (updateSeveralDocs) {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              if (mixDeletes && random().nextBoolean()) {\n                writer.updateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc));\n              } else {\n                writer.softUpdateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc),\n                    new NumericDocValuesField(\"soft_delete\", 1));\n              }\n            } else {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              if (mixDeletes && random().nextBoolean()) {\n                writer.updateDocument(new Term(\"id\", id), doc);\n              } else {\n                writer.softUpdateDocument(new Term(\"id\", id), doc,\n                    new NumericDocValuesField(\"soft_delete\", 1));\n              }\n            }\n            ids.add(id);\n          }\n        } catch (IOException | InterruptedException e) {\n          throw new AssertionError(e);\n        }\n      });\n      threads[i].start();\n    }\n    started.await();\n    startLatch.countDown();\n\n    for (int i = 0; i < threads.length; i++) {\n      threads[i].join();\n    }\n    DirectoryReader reader = DirectoryReader.open(writer);\n    IndexSearcher searcher = new IndexSearcher(reader);\n    for (String id : ids) {\n      TopDocs topDocs = searcher.search(new TermQuery(new Term(\"id\", id)), 10);\n      if (updateSeveralDocs) {\n        assertEquals(2, topDocs.totalHits);\n        assertEquals(Math.abs(topDocs.scoreDocs[0].doc - topDocs.scoreDocs[1].doc), 1);\n      } else {\n        assertEquals(1, topDocs.totalHits);\n      }\n    }\n    mergeAwaySoftDeletes.set(true);\n    writer.addDocument(new Document()); // add a dummy doc to trigger a segment here\n    writer.flush();\n    writer.forceMerge(1);\n    DirectoryReader oldReader = reader;\n    reader = DirectoryReader.openIfChanged(reader, writer);\n    if (reader != null) {\n      oldReader.close();\n      assertNotSame(oldReader, reader);\n    } else {\n      reader = oldReader;\n    }\n    for (String id : ids) {\n      if (updateSeveralDocs) {\n        assertEquals(2, reader.docFreq(new Term(\"id\", id)));\n      } else {\n        assertEquals(1, reader.docFreq(new Term(\"id\", id)));\n      }\n    }\n    int numSoftDeleted = 0;\n    for (SegmentCommitInfo info : writer.segmentInfos) {\n      numSoftDeleted += info.getSoftDelCount() + info.getDelCount();\n    }\n    assertEquals(writer.maxDoc() - writer.numDocs(), numSoftDeleted);\n    writer.commit();\n    try (DirectoryReader dirReader = DirectoryReader.open(dir)) {\n      int delCount = 0;\n      for (LeafReaderContext ctx : dirReader.leaves()) {\n        SegmentCommitInfo segmentInfo = ((SegmentReader) ctx.reader()).getSegmentInfo();\n        delCount += segmentInfo.getSoftDelCount() + segmentInfo.getDelCount();\n      }\n      assertEquals(numSoftDeleted, delCount);\n    }\n    IOUtils.close(reader, writer, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","date":1531905561,"type":3,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#softUpdatesConcurrently(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#softUpdatesConcurrently(boolean).mjava","sourceNew":"  public void softUpdatesConcurrently(boolean mixDeletes) throws IOException, InterruptedException {\n    Directory dir = newDirectory();\n    IndexWriterConfig indexWriterConfig = newIndexWriterConfig();\n    indexWriterConfig.setSoftDeletesField(\"soft_delete\");\n    AtomicBoolean mergeAwaySoftDeletes = new AtomicBoolean(random().nextBoolean());\n    if (mixDeletes == false) {\n      indexWriterConfig.setMergePolicy(new OneMergeWrappingMergePolicy(indexWriterConfig.getMergePolicy(), towrap ->\n          new MergePolicy.OneMerge(towrap.segments) {\n            @Override\n            public CodecReader wrapForMerge(CodecReader reader) throws IOException {\n              if (mergeAwaySoftDeletes.get()) {\n                return towrap.wrapForMerge(reader);\n              } else {\n                CodecReader wrapped = towrap.wrapForMerge(reader);\n                return new FilterCodecReader(wrapped) {\n                  @Override\n                  public CacheHelper getCoreCacheHelper() {\n                    return in.getCoreCacheHelper();\n                  }\n\n                  @Override\n                  public CacheHelper getReaderCacheHelper() {\n                    return in.getReaderCacheHelper();\n                  }\n\n                  @Override\n                  public Bits getLiveDocs() {\n                    return null; // everything is live\n                  }\n\n                  @Override\n                  public int numDocs() {\n                    return maxDoc();\n                  }\n                };\n              }\n            }\n          }\n      ) {\n        @Override\n        public int numDeletesToMerge(SegmentCommitInfo info, int delCount, IOSupplier<CodecReader> readerSupplier) throws IOException {\n          if (mergeAwaySoftDeletes.get()) {\n            return super.numDeletesToMerge(info, delCount, readerSupplier);\n          } else {\n            return 0;\n          }\n        }\n      });\n    }\n    IndexWriter writer = new IndexWriter(dir, indexWriterConfig);\n    Thread[] threads = new Thread[2 + random().nextInt(3)];\n    CountDownLatch startLatch = new CountDownLatch(1);\n    CountDownLatch started = new CountDownLatch(threads.length);\n    boolean updateSeveralDocs = random().nextBoolean();\n    Set<String> ids = Collections.synchronizedSet(new HashSet<>());\n    for (int i = 0; i < threads.length; i++) {\n      threads[i] = new Thread(() -> {\n        try {\n          started.countDown();\n          startLatch.await();\n          for (int d = 0;  d < 100; d++) {\n            String id = String.valueOf(random().nextInt(10));\n            if (updateSeveralDocs) {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              if (mixDeletes && random().nextBoolean()) {\n                writer.updateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc));\n              } else {\n                writer.softUpdateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc),\n                    new NumericDocValuesField(\"soft_delete\", 1));\n              }\n            } else {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              if (mixDeletes && random().nextBoolean()) {\n                writer.updateDocument(new Term(\"id\", id), doc);\n              } else {\n                writer.softUpdateDocument(new Term(\"id\", id), doc,\n                    new NumericDocValuesField(\"soft_delete\", 1));\n              }\n            }\n            ids.add(id);\n          }\n        } catch (IOException | InterruptedException e) {\n          throw new AssertionError(e);\n        }\n      });\n      threads[i].start();\n    }\n    started.await();\n    startLatch.countDown();\n\n    for (int i = 0; i < threads.length; i++) {\n      threads[i].join();\n    }\n    DirectoryReader reader = DirectoryReader.open(writer);\n    IndexSearcher searcher = new IndexSearcher(reader);\n    for (String id : ids) {\n      TopDocs topDocs = searcher.search(new TermQuery(new Term(\"id\", id)), 10);\n      if (updateSeveralDocs) {\n        assertEquals(2, topDocs.totalHits);\n        assertEquals(Math.abs(topDocs.scoreDocs[0].doc - topDocs.scoreDocs[1].doc), 1);\n      } else {\n        assertEquals(1, topDocs.totalHits);\n      }\n    }\n    mergeAwaySoftDeletes.set(true);\n    writer.addDocument(new Document()); // add a dummy doc to trigger a segment here\n    writer.flush();\n    writer.forceMerge(1);\n    DirectoryReader oldReader = reader;\n    reader = DirectoryReader.openIfChanged(reader, writer);\n    if (reader != null) {\n      oldReader.close();\n      assertNotSame(oldReader, reader);\n    } else {\n      reader = oldReader;\n    }\n    for (String id : ids) {\n      if (updateSeveralDocs) {\n        assertEquals(2, reader.docFreq(new Term(\"id\", id)));\n      } else {\n        assertEquals(1, reader.docFreq(new Term(\"id\", id)));\n      }\n    }\n    int numSoftDeleted = 0;\n    for (SegmentCommitInfo info : writer.segmentInfos) {\n      numSoftDeleted += info.getSoftDelCount() + info.getDelCount();\n    }\n    assertEquals(writer.maxDoc() - writer.numDocs(), numSoftDeleted);\n    writer.commit();\n    try (DirectoryReader dirReader = DirectoryReader.open(dir)) {\n      int delCount = 0;\n      for (LeafReaderContext ctx : dirReader.leaves()) {\n        SegmentCommitInfo segmentInfo = ((SegmentReader) ctx.reader()).getSegmentInfo();\n        delCount += segmentInfo.getSoftDelCount() + segmentInfo.getDelCount();\n      }\n      assertEquals(numSoftDeleted, delCount);\n    }\n    IOUtils.close(reader, writer, dir);\n  }\n\n","sourceOld":"  public void softUpdatesConcurrently(boolean mixDeletes) throws IOException, InterruptedException {\n    Directory dir = newDirectory();\n    IndexWriterConfig indexWriterConfig = newIndexWriterConfig();\n    indexWriterConfig.setSoftDeletesField(\"soft_delete\");\n    AtomicBoolean mergeAwaySoftDeletes = new AtomicBoolean(random().nextBoolean());\n    if (mixDeletes == false) {\n      indexWriterConfig.setMergePolicy(new OneMergeWrappingMergePolicy(indexWriterConfig.getMergePolicy(), towrap ->\n          new MergePolicy.OneMerge(towrap.segments) {\n            @Override\n            public CodecReader wrapForMerge(CodecReader reader) throws IOException {\n              if (mergeAwaySoftDeletes.get()) {\n                return towrap.wrapForMerge(reader);\n              } else {\n                CodecReader wrapped = towrap.wrapForMerge(reader);\n                return new FilterCodecReader(wrapped) {\n                  @Override\n                  public CacheHelper getCoreCacheHelper() {\n                    return in.getCoreCacheHelper();\n                  }\n\n                  @Override\n                  public CacheHelper getReaderCacheHelper() {\n                    return in.getReaderCacheHelper();\n                  }\n\n                  @Override\n                  public Bits getLiveDocs() {\n                    return null; // everything is live\n                  }\n\n                  @Override\n                  public int numDocs() {\n                    return maxDoc();\n                  }\n                };\n              }\n            }\n          }\n      ));\n    }\n    IndexWriter writer = new IndexWriter(dir, indexWriterConfig);\n    Thread[] threads = new Thread[2 + random().nextInt(3)];\n    CountDownLatch startLatch = new CountDownLatch(1);\n    CountDownLatch started = new CountDownLatch(threads.length);\n    boolean updateSeveralDocs = random().nextBoolean();\n    Set<String> ids = Collections.synchronizedSet(new HashSet<>());\n    for (int i = 0; i < threads.length; i++) {\n      threads[i] = new Thread(() -> {\n        try {\n          started.countDown();\n          startLatch.await();\n          for (int d = 0;  d < 100; d++) {\n            String id = String.valueOf(random().nextInt(10));\n            if (updateSeveralDocs) {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              if (mixDeletes && random().nextBoolean()) {\n                writer.updateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc));\n              } else {\n                writer.softUpdateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc),\n                    new NumericDocValuesField(\"soft_delete\", 1));\n              }\n            } else {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              if (mixDeletes && random().nextBoolean()) {\n                writer.updateDocument(new Term(\"id\", id), doc);\n              } else {\n                writer.softUpdateDocument(new Term(\"id\", id), doc,\n                    new NumericDocValuesField(\"soft_delete\", 1));\n              }\n            }\n            ids.add(id);\n          }\n        } catch (IOException | InterruptedException e) {\n          throw new AssertionError(e);\n        }\n      });\n      threads[i].start();\n    }\n    started.await();\n    startLatch.countDown();\n\n    for (int i = 0; i < threads.length; i++) {\n      threads[i].join();\n    }\n    DirectoryReader reader = DirectoryReader.open(writer);\n    IndexSearcher searcher = new IndexSearcher(reader);\n    for (String id : ids) {\n      TopDocs topDocs = searcher.search(new TermQuery(new Term(\"id\", id)), 10);\n      if (updateSeveralDocs) {\n        assertEquals(2, topDocs.totalHits);\n        assertEquals(Math.abs(topDocs.scoreDocs[0].doc - topDocs.scoreDocs[1].doc), 1);\n      } else {\n        assertEquals(1, topDocs.totalHits);\n      }\n    }\n    mergeAwaySoftDeletes.set(true);\n    writer.addDocument(new Document()); // add a dummy doc to trigger a segment here\n    writer.flush();\n    writer.forceMerge(1);\n    DirectoryReader oldReader = reader;\n    reader = DirectoryReader.openIfChanged(reader, writer);\n    if (reader != null) {\n      oldReader.close();\n      assertNotSame(oldReader, reader);\n    } else {\n      reader = oldReader;\n    }\n    for (String id : ids) {\n      if (updateSeveralDocs) {\n        assertEquals(2, reader.docFreq(new Term(\"id\", id)));\n      } else {\n        assertEquals(1, reader.docFreq(new Term(\"id\", id)));\n      }\n    }\n    int numSoftDeleted = 0;\n    for (SegmentCommitInfo info : writer.segmentInfos) {\n      numSoftDeleted += info.getSoftDelCount() + info.getDelCount();\n    }\n    assertEquals(writer.maxDoc() - writer.numDocs(), numSoftDeleted);\n    writer.commit();\n    try (DirectoryReader dirReader = DirectoryReader.open(dir)) {\n      int delCount = 0;\n      for (LeafReaderContext ctx : dirReader.leaves()) {\n        SegmentCommitInfo segmentInfo = ((SegmentReader) ctx.reader()).getSegmentInfo();\n        delCount += segmentInfo.getSoftDelCount() + segmentInfo.getDelCount();\n      }\n      assertEquals(numSoftDeleted, delCount);\n    }\n    IOUtils.close(reader, writer, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"631ea3d1607299c59f33edef140ffc19a81f07a0","date":1532450367,"type":3,"author":"Nhat Nguyen","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#softUpdatesConcurrently(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#softUpdatesConcurrently(boolean).mjava","sourceNew":"  public void softUpdatesConcurrently(boolean mixDeletes) throws IOException, InterruptedException {\n    Directory dir = newDirectory();\n    IndexWriterConfig indexWriterConfig = newIndexWriterConfig();\n    indexWriterConfig.setSoftDeletesField(\"soft_delete\");\n    AtomicBoolean mergeAwaySoftDeletes = new AtomicBoolean(random().nextBoolean());\n    if (mixDeletes == false) {\n      indexWriterConfig.setMergePolicy(new OneMergeWrappingMergePolicy(indexWriterConfig.getMergePolicy(), towrap ->\n          new MergePolicy.OneMerge(towrap.segments) {\n            @Override\n            public CodecReader wrapForMerge(CodecReader reader) throws IOException {\n              if (mergeAwaySoftDeletes.get()) {\n                return towrap.wrapForMerge(reader);\n              } else {\n                CodecReader wrapped = towrap.wrapForMerge(reader);\n                return new FilterCodecReader(wrapped) {\n                  @Override\n                  public CacheHelper getCoreCacheHelper() {\n                    return in.getCoreCacheHelper();\n                  }\n\n                  @Override\n                  public CacheHelper getReaderCacheHelper() {\n                    return in.getReaderCacheHelper();\n                  }\n\n                  @Override\n                  public Bits getLiveDocs() {\n                    return null; // everything is live\n                  }\n\n                  @Override\n                  public int numDocs() {\n                    return maxDoc();\n                  }\n                };\n              }\n            }\n          }\n      ) {\n        @Override\n        public int numDeletesToMerge(SegmentCommitInfo info, int delCount, IOSupplier<CodecReader> readerSupplier) throws IOException {\n          if (mergeAwaySoftDeletes.get()) {\n            return super.numDeletesToMerge(info, delCount, readerSupplier);\n          } else {\n            return 0;\n          }\n        }\n      });\n    }\n    IndexWriter writer = new IndexWriter(dir, indexWriterConfig);\n    Thread[] threads = new Thread[2 + random().nextInt(3)];\n    CountDownLatch startLatch = new CountDownLatch(1);\n    CountDownLatch started = new CountDownLatch(threads.length);\n    boolean updateSeveralDocs = random().nextBoolean();\n    Set<String> ids = Collections.synchronizedSet(new HashSet<>());\n    for (int i = 0; i < threads.length; i++) {\n      threads[i] = new Thread(() -> {\n        try {\n          started.countDown();\n          startLatch.await();\n          for (int d = 0;  d < 100; d++) {\n            String id = String.valueOf(random().nextInt(10));\n            if (updateSeveralDocs) {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              if (mixDeletes && random().nextBoolean()) {\n                writer.updateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc));\n              } else {\n                writer.softUpdateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc),\n                    new NumericDocValuesField(\"soft_delete\", 1));\n              }\n            } else {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              if (mixDeletes && random().nextBoolean()) {\n                writer.updateDocument(new Term(\"id\", id), doc);\n              } else {\n                writer.softUpdateDocument(new Term(\"id\", id), doc,\n                    new NumericDocValuesField(\"soft_delete\", 1));\n              }\n            }\n            ids.add(id);\n          }\n        } catch (IOException | InterruptedException e) {\n          throw new AssertionError(e);\n        }\n      });\n      threads[i].start();\n    }\n    started.await();\n    startLatch.countDown();\n\n    for (int i = 0; i < threads.length; i++) {\n      threads[i].join();\n    }\n    DirectoryReader reader = DirectoryReader.open(writer);\n    IndexSearcher searcher = new IndexSearcher(reader);\n    for (String id : ids) {\n      TopDocs topDocs = searcher.search(new TermQuery(new Term(\"id\", id)), 10);\n      if (updateSeveralDocs) {\n        assertEquals(2, topDocs.totalHits);\n        assertEquals(Math.abs(topDocs.scoreDocs[0].doc - topDocs.scoreDocs[1].doc), 1);\n      } else {\n        assertEquals(1, topDocs.totalHits);\n      }\n    }\n    mergeAwaySoftDeletes.set(true);\n    writer.addDocument(new Document()); // add a dummy doc to trigger a segment here\n    writer.flush();\n    writer.forceMerge(1);\n    DirectoryReader oldReader = reader;\n    reader = DirectoryReader.openIfChanged(reader, writer);\n    if (reader != null) {\n      oldReader.close();\n      assertNotSame(oldReader, reader);\n    } else {\n      reader = oldReader;\n    }\n    for (String id : ids) {\n      if (updateSeveralDocs) {\n        assertEquals(2, reader.docFreq(new Term(\"id\", id)));\n      } else {\n        assertEquals(1, reader.docFreq(new Term(\"id\", id)));\n      }\n    }\n    int numSoftDeleted = 0;\n    for (SegmentCommitInfo info : writer.cloneSegmentInfos()) {\n      numSoftDeleted += info.getSoftDelCount() + info.getDelCount();\n    }\n    assertEquals(writer.maxDoc() - writer.numDocs(), numSoftDeleted);\n    writer.commit();\n    try (DirectoryReader dirReader = DirectoryReader.open(dir)) {\n      int delCount = 0;\n      for (LeafReaderContext ctx : dirReader.leaves()) {\n        SegmentCommitInfo segmentInfo = ((SegmentReader) ctx.reader()).getSegmentInfo();\n        delCount += segmentInfo.getSoftDelCount() + segmentInfo.getDelCount();\n      }\n      assertEquals(numSoftDeleted, delCount);\n    }\n    IOUtils.close(reader, writer, dir);\n  }\n\n","sourceOld":"  public void softUpdatesConcurrently(boolean mixDeletes) throws IOException, InterruptedException {\n    Directory dir = newDirectory();\n    IndexWriterConfig indexWriterConfig = newIndexWriterConfig();\n    indexWriterConfig.setSoftDeletesField(\"soft_delete\");\n    AtomicBoolean mergeAwaySoftDeletes = new AtomicBoolean(random().nextBoolean());\n    if (mixDeletes == false) {\n      indexWriterConfig.setMergePolicy(new OneMergeWrappingMergePolicy(indexWriterConfig.getMergePolicy(), towrap ->\n          new MergePolicy.OneMerge(towrap.segments) {\n            @Override\n            public CodecReader wrapForMerge(CodecReader reader) throws IOException {\n              if (mergeAwaySoftDeletes.get()) {\n                return towrap.wrapForMerge(reader);\n              } else {\n                CodecReader wrapped = towrap.wrapForMerge(reader);\n                return new FilterCodecReader(wrapped) {\n                  @Override\n                  public CacheHelper getCoreCacheHelper() {\n                    return in.getCoreCacheHelper();\n                  }\n\n                  @Override\n                  public CacheHelper getReaderCacheHelper() {\n                    return in.getReaderCacheHelper();\n                  }\n\n                  @Override\n                  public Bits getLiveDocs() {\n                    return null; // everything is live\n                  }\n\n                  @Override\n                  public int numDocs() {\n                    return maxDoc();\n                  }\n                };\n              }\n            }\n          }\n      ) {\n        @Override\n        public int numDeletesToMerge(SegmentCommitInfo info, int delCount, IOSupplier<CodecReader> readerSupplier) throws IOException {\n          if (mergeAwaySoftDeletes.get()) {\n            return super.numDeletesToMerge(info, delCount, readerSupplier);\n          } else {\n            return 0;\n          }\n        }\n      });\n    }\n    IndexWriter writer = new IndexWriter(dir, indexWriterConfig);\n    Thread[] threads = new Thread[2 + random().nextInt(3)];\n    CountDownLatch startLatch = new CountDownLatch(1);\n    CountDownLatch started = new CountDownLatch(threads.length);\n    boolean updateSeveralDocs = random().nextBoolean();\n    Set<String> ids = Collections.synchronizedSet(new HashSet<>());\n    for (int i = 0; i < threads.length; i++) {\n      threads[i] = new Thread(() -> {\n        try {\n          started.countDown();\n          startLatch.await();\n          for (int d = 0;  d < 100; d++) {\n            String id = String.valueOf(random().nextInt(10));\n            if (updateSeveralDocs) {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              if (mixDeletes && random().nextBoolean()) {\n                writer.updateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc));\n              } else {\n                writer.softUpdateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc),\n                    new NumericDocValuesField(\"soft_delete\", 1));\n              }\n            } else {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              if (mixDeletes && random().nextBoolean()) {\n                writer.updateDocument(new Term(\"id\", id), doc);\n              } else {\n                writer.softUpdateDocument(new Term(\"id\", id), doc,\n                    new NumericDocValuesField(\"soft_delete\", 1));\n              }\n            }\n            ids.add(id);\n          }\n        } catch (IOException | InterruptedException e) {\n          throw new AssertionError(e);\n        }\n      });\n      threads[i].start();\n    }\n    started.await();\n    startLatch.countDown();\n\n    for (int i = 0; i < threads.length; i++) {\n      threads[i].join();\n    }\n    DirectoryReader reader = DirectoryReader.open(writer);\n    IndexSearcher searcher = new IndexSearcher(reader);\n    for (String id : ids) {\n      TopDocs topDocs = searcher.search(new TermQuery(new Term(\"id\", id)), 10);\n      if (updateSeveralDocs) {\n        assertEquals(2, topDocs.totalHits);\n        assertEquals(Math.abs(topDocs.scoreDocs[0].doc - topDocs.scoreDocs[1].doc), 1);\n      } else {\n        assertEquals(1, topDocs.totalHits);\n      }\n    }\n    mergeAwaySoftDeletes.set(true);\n    writer.addDocument(new Document()); // add a dummy doc to trigger a segment here\n    writer.flush();\n    writer.forceMerge(1);\n    DirectoryReader oldReader = reader;\n    reader = DirectoryReader.openIfChanged(reader, writer);\n    if (reader != null) {\n      oldReader.close();\n      assertNotSame(oldReader, reader);\n    } else {\n      reader = oldReader;\n    }\n    for (String id : ids) {\n      if (updateSeveralDocs) {\n        assertEquals(2, reader.docFreq(new Term(\"id\", id)));\n      } else {\n        assertEquals(1, reader.docFreq(new Term(\"id\", id)));\n      }\n    }\n    int numSoftDeleted = 0;\n    for (SegmentCommitInfo info : writer.segmentInfos) {\n      numSoftDeleted += info.getSoftDelCount() + info.getDelCount();\n    }\n    assertEquals(writer.maxDoc() - writer.numDocs(), numSoftDeleted);\n    writer.commit();\n    try (DirectoryReader dirReader = DirectoryReader.open(dir)) {\n      int delCount = 0;\n      for (LeafReaderContext ctx : dirReader.leaves()) {\n        SegmentCommitInfo segmentInfo = ((SegmentReader) ctx.reader()).getSegmentInfo();\n        delCount += segmentInfo.getSoftDelCount() + segmentInfo.getDelCount();\n      }\n      assertEquals(numSoftDeleted, delCount);\n    }\n    IOUtils.close(reader, writer, dir);\n  }\n\n","bugFix":["8f2203cb8ae87188877cfbf6ad170c5738a0aad5"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"578a4d73d90ecd838846cc32bf1098aaa262b524","date":1532504076,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#softUpdatesConcurrently(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#softUpdatesConcurrently(boolean).mjava","sourceNew":"  public void softUpdatesConcurrently(boolean mixDeletes) throws IOException, InterruptedException {\n    Directory dir = newDirectory();\n    IndexWriterConfig indexWriterConfig = newIndexWriterConfig();\n    indexWriterConfig.setSoftDeletesField(\"soft_delete\");\n    AtomicBoolean mergeAwaySoftDeletes = new AtomicBoolean(random().nextBoolean());\n    if (mixDeletes == false) {\n      indexWriterConfig.setMergePolicy(new OneMergeWrappingMergePolicy(indexWriterConfig.getMergePolicy(), towrap ->\n          new MergePolicy.OneMerge(towrap.segments) {\n            @Override\n            public CodecReader wrapForMerge(CodecReader reader) throws IOException {\n              if (mergeAwaySoftDeletes.get()) {\n                return towrap.wrapForMerge(reader);\n              } else {\n                CodecReader wrapped = towrap.wrapForMerge(reader);\n                return new FilterCodecReader(wrapped) {\n                  @Override\n                  public CacheHelper getCoreCacheHelper() {\n                    return in.getCoreCacheHelper();\n                  }\n\n                  @Override\n                  public CacheHelper getReaderCacheHelper() {\n                    return in.getReaderCacheHelper();\n                  }\n\n                  @Override\n                  public Bits getLiveDocs() {\n                    return null; // everything is live\n                  }\n\n                  @Override\n                  public int numDocs() {\n                    return maxDoc();\n                  }\n                };\n              }\n            }\n          }\n      ) {\n        @Override\n        public int numDeletesToMerge(SegmentCommitInfo info, int delCount, IOSupplier<CodecReader> readerSupplier) throws IOException {\n          if (mergeAwaySoftDeletes.get()) {\n            return super.numDeletesToMerge(info, delCount, readerSupplier);\n          } else {\n            return 0;\n          }\n        }\n      });\n    }\n    IndexWriter writer = new IndexWriter(dir, indexWriterConfig);\n    Thread[] threads = new Thread[2 + random().nextInt(3)];\n    CountDownLatch startLatch = new CountDownLatch(1);\n    CountDownLatch started = new CountDownLatch(threads.length);\n    boolean updateSeveralDocs = random().nextBoolean();\n    Set<String> ids = Collections.synchronizedSet(new HashSet<>());\n    for (int i = 0; i < threads.length; i++) {\n      threads[i] = new Thread(() -> {\n        try {\n          started.countDown();\n          startLatch.await();\n          for (int d = 0;  d < 100; d++) {\n            String id = String.valueOf(random().nextInt(10));\n            if (updateSeveralDocs) {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              if (mixDeletes && random().nextBoolean()) {\n                writer.updateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc));\n              } else {\n                writer.softUpdateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc),\n                    new NumericDocValuesField(\"soft_delete\", 1));\n              }\n            } else {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              if (mixDeletes && random().nextBoolean()) {\n                writer.updateDocument(new Term(\"id\", id), doc);\n              } else {\n                writer.softUpdateDocument(new Term(\"id\", id), doc,\n                    new NumericDocValuesField(\"soft_delete\", 1));\n              }\n            }\n            ids.add(id);\n          }\n        } catch (IOException | InterruptedException e) {\n          throw new AssertionError(e);\n        }\n      });\n      threads[i].start();\n    }\n    started.await();\n    startLatch.countDown();\n\n    for (int i = 0; i < threads.length; i++) {\n      threads[i].join();\n    }\n    DirectoryReader reader = DirectoryReader.open(writer);\n    IndexSearcher searcher = new IndexSearcher(reader);\n    for (String id : ids) {\n      TopDocs topDocs = searcher.search(new TermQuery(new Term(\"id\", id)), 10);\n      if (updateSeveralDocs) {\n        assertEquals(2, topDocs.totalHits);\n        assertEquals(Math.abs(topDocs.scoreDocs[0].doc - topDocs.scoreDocs[1].doc), 1);\n      } else {\n        assertEquals(1, topDocs.totalHits);\n      }\n    }\n    if (mixDeletes == false) {\n      for (LeafReaderContext context : reader.leaves()) {\n        LeafReader leaf = context.reader();\n        assertNull(((SegmentReader) leaf).getHardLiveDocs());\n      }\n    }\n    mergeAwaySoftDeletes.set(true);\n    writer.addDocument(new Document()); // add a dummy doc to trigger a segment here\n    writer.flush();\n    writer.forceMerge(1);\n    DirectoryReader oldReader = reader;\n    reader = DirectoryReader.openIfChanged(reader, writer);\n    if (reader != null) {\n      oldReader.close();\n      assertNotSame(oldReader, reader);\n    } else {\n      reader = oldReader;\n    }\n    for (String id : ids) {\n      if (updateSeveralDocs) {\n        assertEquals(2, reader.docFreq(new Term(\"id\", id)));\n      } else {\n        assertEquals(1, reader.docFreq(new Term(\"id\", id)));\n      }\n    }\n    int numSoftDeleted = 0;\n    for (SegmentCommitInfo info : writer.cloneSegmentInfos()) {\n      numSoftDeleted += info.getSoftDelCount() + info.getDelCount();\n    }\n    assertEquals(writer.maxDoc() - writer.numDocs(), numSoftDeleted);\n    writer.commit();\n    try (DirectoryReader dirReader = DirectoryReader.open(dir)) {\n      int delCount = 0;\n      for (LeafReaderContext ctx : dirReader.leaves()) {\n        SegmentCommitInfo segmentInfo = ((SegmentReader) ctx.reader()).getSegmentInfo();\n        delCount += segmentInfo.getSoftDelCount() + segmentInfo.getDelCount();\n      }\n      assertEquals(numSoftDeleted, delCount);\n    }\n    IOUtils.close(reader, writer, dir);\n  }\n\n","sourceOld":"  public void softUpdatesConcurrently(boolean mixDeletes) throws IOException, InterruptedException {\n    Directory dir = newDirectory();\n    IndexWriterConfig indexWriterConfig = newIndexWriterConfig();\n    indexWriterConfig.setSoftDeletesField(\"soft_delete\");\n    AtomicBoolean mergeAwaySoftDeletes = new AtomicBoolean(random().nextBoolean());\n    if (mixDeletes == false) {\n      indexWriterConfig.setMergePolicy(new OneMergeWrappingMergePolicy(indexWriterConfig.getMergePolicy(), towrap ->\n          new MergePolicy.OneMerge(towrap.segments) {\n            @Override\n            public CodecReader wrapForMerge(CodecReader reader) throws IOException {\n              if (mergeAwaySoftDeletes.get()) {\n                return towrap.wrapForMerge(reader);\n              } else {\n                CodecReader wrapped = towrap.wrapForMerge(reader);\n                return new FilterCodecReader(wrapped) {\n                  @Override\n                  public CacheHelper getCoreCacheHelper() {\n                    return in.getCoreCacheHelper();\n                  }\n\n                  @Override\n                  public CacheHelper getReaderCacheHelper() {\n                    return in.getReaderCacheHelper();\n                  }\n\n                  @Override\n                  public Bits getLiveDocs() {\n                    return null; // everything is live\n                  }\n\n                  @Override\n                  public int numDocs() {\n                    return maxDoc();\n                  }\n                };\n              }\n            }\n          }\n      ) {\n        @Override\n        public int numDeletesToMerge(SegmentCommitInfo info, int delCount, IOSupplier<CodecReader> readerSupplier) throws IOException {\n          if (mergeAwaySoftDeletes.get()) {\n            return super.numDeletesToMerge(info, delCount, readerSupplier);\n          } else {\n            return 0;\n          }\n        }\n      });\n    }\n    IndexWriter writer = new IndexWriter(dir, indexWriterConfig);\n    Thread[] threads = new Thread[2 + random().nextInt(3)];\n    CountDownLatch startLatch = new CountDownLatch(1);\n    CountDownLatch started = new CountDownLatch(threads.length);\n    boolean updateSeveralDocs = random().nextBoolean();\n    Set<String> ids = Collections.synchronizedSet(new HashSet<>());\n    for (int i = 0; i < threads.length; i++) {\n      threads[i] = new Thread(() -> {\n        try {\n          started.countDown();\n          startLatch.await();\n          for (int d = 0;  d < 100; d++) {\n            String id = String.valueOf(random().nextInt(10));\n            if (updateSeveralDocs) {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              if (mixDeletes && random().nextBoolean()) {\n                writer.updateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc));\n              } else {\n                writer.softUpdateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc),\n                    new NumericDocValuesField(\"soft_delete\", 1));\n              }\n            } else {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              if (mixDeletes && random().nextBoolean()) {\n                writer.updateDocument(new Term(\"id\", id), doc);\n              } else {\n                writer.softUpdateDocument(new Term(\"id\", id), doc,\n                    new NumericDocValuesField(\"soft_delete\", 1));\n              }\n            }\n            ids.add(id);\n          }\n        } catch (IOException | InterruptedException e) {\n          throw new AssertionError(e);\n        }\n      });\n      threads[i].start();\n    }\n    started.await();\n    startLatch.countDown();\n\n    for (int i = 0; i < threads.length; i++) {\n      threads[i].join();\n    }\n    DirectoryReader reader = DirectoryReader.open(writer);\n    IndexSearcher searcher = new IndexSearcher(reader);\n    for (String id : ids) {\n      TopDocs topDocs = searcher.search(new TermQuery(new Term(\"id\", id)), 10);\n      if (updateSeveralDocs) {\n        assertEquals(2, topDocs.totalHits);\n        assertEquals(Math.abs(topDocs.scoreDocs[0].doc - topDocs.scoreDocs[1].doc), 1);\n      } else {\n        assertEquals(1, topDocs.totalHits);\n      }\n    }\n    mergeAwaySoftDeletes.set(true);\n    writer.addDocument(new Document()); // add a dummy doc to trigger a segment here\n    writer.flush();\n    writer.forceMerge(1);\n    DirectoryReader oldReader = reader;\n    reader = DirectoryReader.openIfChanged(reader, writer);\n    if (reader != null) {\n      oldReader.close();\n      assertNotSame(oldReader, reader);\n    } else {\n      reader = oldReader;\n    }\n    for (String id : ids) {\n      if (updateSeveralDocs) {\n        assertEquals(2, reader.docFreq(new Term(\"id\", id)));\n      } else {\n        assertEquals(1, reader.docFreq(new Term(\"id\", id)));\n      }\n    }\n    int numSoftDeleted = 0;\n    for (SegmentCommitInfo info : writer.cloneSegmentInfos()) {\n      numSoftDeleted += info.getSoftDelCount() + info.getDelCount();\n    }\n    assertEquals(writer.maxDoc() - writer.numDocs(), numSoftDeleted);\n    writer.commit();\n    try (DirectoryReader dirReader = DirectoryReader.open(dir)) {\n      int delCount = 0;\n      for (LeafReaderContext ctx : dirReader.leaves()) {\n        SegmentCommitInfo segmentInfo = ((SegmentReader) ctx.reader()).getSegmentInfo();\n        delCount += segmentInfo.getSoftDelCount() + segmentInfo.getDelCount();\n      }\n      assertEquals(numSoftDeleted, delCount);\n    }\n    IOUtils.close(reader, writer, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"83788ad129a5154d5c6562c4e8ce3db48793aada","date":1532961485,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#softUpdatesConcurrently(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#softUpdatesConcurrently(boolean).mjava","sourceNew":"  public void softUpdatesConcurrently(boolean mixDeletes) throws IOException, InterruptedException {\n    Directory dir = newDirectory();\n    IndexWriterConfig indexWriterConfig = newIndexWriterConfig();\n    indexWriterConfig.setSoftDeletesField(\"soft_delete\");\n    AtomicBoolean mergeAwaySoftDeletes = new AtomicBoolean(random().nextBoolean());\n    if (mixDeletes == false) {\n      indexWriterConfig.setMergePolicy(new OneMergeWrappingMergePolicy(indexWriterConfig.getMergePolicy(), towrap ->\n          new MergePolicy.OneMerge(towrap.segments) {\n            @Override\n            public CodecReader wrapForMerge(CodecReader reader) throws IOException {\n              if (mergeAwaySoftDeletes.get()) {\n                return towrap.wrapForMerge(reader);\n              } else {\n                CodecReader wrapped = towrap.wrapForMerge(reader);\n                return new FilterCodecReader(wrapped) {\n                  @Override\n                  public CacheHelper getCoreCacheHelper() {\n                    return in.getCoreCacheHelper();\n                  }\n\n                  @Override\n                  public CacheHelper getReaderCacheHelper() {\n                    return in.getReaderCacheHelper();\n                  }\n\n                  @Override\n                  public Bits getLiveDocs() {\n                    return null; // everything is live\n                  }\n\n                  @Override\n                  public int numDocs() {\n                    return maxDoc();\n                  }\n                };\n              }\n            }\n          }\n      ) {\n        @Override\n        public int numDeletesToMerge(SegmentCommitInfo info, int delCount, IOSupplier<CodecReader> readerSupplier) throws IOException {\n          if (mergeAwaySoftDeletes.get()) {\n            return super.numDeletesToMerge(info, delCount, readerSupplier);\n          } else {\n            return 0;\n          }\n        }\n      });\n    }\n    IndexWriter writer = new IndexWriter(dir, indexWriterConfig);\n    Thread[] threads = new Thread[2 + random().nextInt(3)];\n    CountDownLatch startLatch = new CountDownLatch(1);\n    CountDownLatch started = new CountDownLatch(threads.length);\n    boolean updateSeveralDocs = random().nextBoolean();\n    Set<String> ids = Collections.synchronizedSet(new HashSet<>());\n    for (int i = 0; i < threads.length; i++) {\n      threads[i] = new Thread(() -> {\n        try {\n          started.countDown();\n          startLatch.await();\n          for (int d = 0;  d < 100; d++) {\n            String id = String.valueOf(random().nextInt(10));\n            if (updateSeveralDocs) {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              if (mixDeletes && random().nextBoolean()) {\n                writer.updateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc));\n              } else {\n                writer.softUpdateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc),\n                    new NumericDocValuesField(\"soft_delete\", 1));\n              }\n            } else {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              if (mixDeletes && random().nextBoolean()) {\n                writer.updateDocument(new Term(\"id\", id), doc);\n              } else {\n                writer.softUpdateDocument(new Term(\"id\", id), doc,\n                    new NumericDocValuesField(\"soft_delete\", 1));\n              }\n            }\n            ids.add(id);\n          }\n        } catch (IOException | InterruptedException e) {\n          throw new AssertionError(e);\n        }\n      });\n      threads[i].start();\n    }\n    started.await();\n    startLatch.countDown();\n\n    for (int i = 0; i < threads.length; i++) {\n      threads[i].join();\n    }\n    DirectoryReader reader = DirectoryReader.open(writer);\n    IndexSearcher searcher = new IndexSearcher(reader);\n    for (String id : ids) {\n      TopDocs topDocs = searcher.search(new TermQuery(new Term(\"id\", id)), 10);\n      if (updateSeveralDocs) {\n        assertEquals(2, topDocs.totalHits.value);\n        assertEquals(Math.abs(topDocs.scoreDocs[0].doc - topDocs.scoreDocs[1].doc), 1);\n      } else {\n        assertEquals(1, topDocs.totalHits.value);\n      }\n    }\n    if (mixDeletes == false) {\n      for (LeafReaderContext context : reader.leaves()) {\n        LeafReader leaf = context.reader();\n        assertNull(((SegmentReader) leaf).getHardLiveDocs());\n      }\n    }\n    mergeAwaySoftDeletes.set(true);\n    writer.addDocument(new Document()); // add a dummy doc to trigger a segment here\n    writer.flush();\n    writer.forceMerge(1);\n    DirectoryReader oldReader = reader;\n    reader = DirectoryReader.openIfChanged(reader, writer);\n    if (reader != null) {\n      oldReader.close();\n      assertNotSame(oldReader, reader);\n    } else {\n      reader = oldReader;\n    }\n    for (String id : ids) {\n      if (updateSeveralDocs) {\n        assertEquals(2, reader.docFreq(new Term(\"id\", id)));\n      } else {\n        assertEquals(1, reader.docFreq(new Term(\"id\", id)));\n      }\n    }\n    int numSoftDeleted = 0;\n    for (SegmentCommitInfo info : writer.cloneSegmentInfos()) {\n      numSoftDeleted += info.getSoftDelCount() + info.getDelCount();\n    }\n    assertEquals(writer.maxDoc() - writer.numDocs(), numSoftDeleted);\n    writer.commit();\n    try (DirectoryReader dirReader = DirectoryReader.open(dir)) {\n      int delCount = 0;\n      for (LeafReaderContext ctx : dirReader.leaves()) {\n        SegmentCommitInfo segmentInfo = ((SegmentReader) ctx.reader()).getSegmentInfo();\n        delCount += segmentInfo.getSoftDelCount() + segmentInfo.getDelCount();\n      }\n      assertEquals(numSoftDeleted, delCount);\n    }\n    IOUtils.close(reader, writer, dir);\n  }\n\n","sourceOld":"  public void softUpdatesConcurrently(boolean mixDeletes) throws IOException, InterruptedException {\n    Directory dir = newDirectory();\n    IndexWriterConfig indexWriterConfig = newIndexWriterConfig();\n    indexWriterConfig.setSoftDeletesField(\"soft_delete\");\n    AtomicBoolean mergeAwaySoftDeletes = new AtomicBoolean(random().nextBoolean());\n    if (mixDeletes == false) {\n      indexWriterConfig.setMergePolicy(new OneMergeWrappingMergePolicy(indexWriterConfig.getMergePolicy(), towrap ->\n          new MergePolicy.OneMerge(towrap.segments) {\n            @Override\n            public CodecReader wrapForMerge(CodecReader reader) throws IOException {\n              if (mergeAwaySoftDeletes.get()) {\n                return towrap.wrapForMerge(reader);\n              } else {\n                CodecReader wrapped = towrap.wrapForMerge(reader);\n                return new FilterCodecReader(wrapped) {\n                  @Override\n                  public CacheHelper getCoreCacheHelper() {\n                    return in.getCoreCacheHelper();\n                  }\n\n                  @Override\n                  public CacheHelper getReaderCacheHelper() {\n                    return in.getReaderCacheHelper();\n                  }\n\n                  @Override\n                  public Bits getLiveDocs() {\n                    return null; // everything is live\n                  }\n\n                  @Override\n                  public int numDocs() {\n                    return maxDoc();\n                  }\n                };\n              }\n            }\n          }\n      ) {\n        @Override\n        public int numDeletesToMerge(SegmentCommitInfo info, int delCount, IOSupplier<CodecReader> readerSupplier) throws IOException {\n          if (mergeAwaySoftDeletes.get()) {\n            return super.numDeletesToMerge(info, delCount, readerSupplier);\n          } else {\n            return 0;\n          }\n        }\n      });\n    }\n    IndexWriter writer = new IndexWriter(dir, indexWriterConfig);\n    Thread[] threads = new Thread[2 + random().nextInt(3)];\n    CountDownLatch startLatch = new CountDownLatch(1);\n    CountDownLatch started = new CountDownLatch(threads.length);\n    boolean updateSeveralDocs = random().nextBoolean();\n    Set<String> ids = Collections.synchronizedSet(new HashSet<>());\n    for (int i = 0; i < threads.length; i++) {\n      threads[i] = new Thread(() -> {\n        try {\n          started.countDown();\n          startLatch.await();\n          for (int d = 0;  d < 100; d++) {\n            String id = String.valueOf(random().nextInt(10));\n            if (updateSeveralDocs) {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              if (mixDeletes && random().nextBoolean()) {\n                writer.updateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc));\n              } else {\n                writer.softUpdateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc),\n                    new NumericDocValuesField(\"soft_delete\", 1));\n              }\n            } else {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              if (mixDeletes && random().nextBoolean()) {\n                writer.updateDocument(new Term(\"id\", id), doc);\n              } else {\n                writer.softUpdateDocument(new Term(\"id\", id), doc,\n                    new NumericDocValuesField(\"soft_delete\", 1));\n              }\n            }\n            ids.add(id);\n          }\n        } catch (IOException | InterruptedException e) {\n          throw new AssertionError(e);\n        }\n      });\n      threads[i].start();\n    }\n    started.await();\n    startLatch.countDown();\n\n    for (int i = 0; i < threads.length; i++) {\n      threads[i].join();\n    }\n    DirectoryReader reader = DirectoryReader.open(writer);\n    IndexSearcher searcher = new IndexSearcher(reader);\n    for (String id : ids) {\n      TopDocs topDocs = searcher.search(new TermQuery(new Term(\"id\", id)), 10);\n      if (updateSeveralDocs) {\n        assertEquals(2, topDocs.totalHits);\n        assertEquals(Math.abs(topDocs.scoreDocs[0].doc - topDocs.scoreDocs[1].doc), 1);\n      } else {\n        assertEquals(1, topDocs.totalHits);\n      }\n    }\n    if (mixDeletes == false) {\n      for (LeafReaderContext context : reader.leaves()) {\n        LeafReader leaf = context.reader();\n        assertNull(((SegmentReader) leaf).getHardLiveDocs());\n      }\n    }\n    mergeAwaySoftDeletes.set(true);\n    writer.addDocument(new Document()); // add a dummy doc to trigger a segment here\n    writer.flush();\n    writer.forceMerge(1);\n    DirectoryReader oldReader = reader;\n    reader = DirectoryReader.openIfChanged(reader, writer);\n    if (reader != null) {\n      oldReader.close();\n      assertNotSame(oldReader, reader);\n    } else {\n      reader = oldReader;\n    }\n    for (String id : ids) {\n      if (updateSeveralDocs) {\n        assertEquals(2, reader.docFreq(new Term(\"id\", id)));\n      } else {\n        assertEquals(1, reader.docFreq(new Term(\"id\", id)));\n      }\n    }\n    int numSoftDeleted = 0;\n    for (SegmentCommitInfo info : writer.cloneSegmentInfos()) {\n      numSoftDeleted += info.getSoftDelCount() + info.getDelCount();\n    }\n    assertEquals(writer.maxDoc() - writer.numDocs(), numSoftDeleted);\n    writer.commit();\n    try (DirectoryReader dirReader = DirectoryReader.open(dir)) {\n      int delCount = 0;\n      for (LeafReaderContext ctx : dirReader.leaves()) {\n        SegmentCommitInfo segmentInfo = ((SegmentReader) ctx.reader()).getSegmentInfo();\n        delCount += segmentInfo.getSoftDelCount() + segmentInfo.getDelCount();\n      }\n      assertEquals(numSoftDeleted, delCount);\n    }\n    IOUtils.close(reader, writer, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"feb4029567b43f074ed7b6eb8fb126d355075dfd","date":1544812585,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#softUpdatesConcurrently(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#softUpdatesConcurrently(boolean).mjava","sourceNew":"  public void softUpdatesConcurrently(boolean mixDeletes) throws IOException, InterruptedException {\n    Directory dir = newDirectory();\n    IndexWriterConfig indexWriterConfig = newIndexWriterConfig();\n    indexWriterConfig.setSoftDeletesField(\"soft_delete\");\n    AtomicBoolean mergeAwaySoftDeletes = new AtomicBoolean(random().nextBoolean());\n    if (mixDeletes == false) {\n      indexWriterConfig.setMergePolicy(new OneMergeWrappingMergePolicy(indexWriterConfig.getMergePolicy(), towrap ->\n          new MergePolicy.OneMerge(towrap.segments) {\n            @Override\n            public CodecReader wrapForMerge(CodecReader reader) throws IOException {\n              if (mergeAwaySoftDeletes.get()) {\n                return towrap.wrapForMerge(reader);\n              } else {\n                CodecReader wrapped = towrap.wrapForMerge(reader);\n                return new FilterCodecReader(wrapped) {\n                  @Override\n                  public CacheHelper getCoreCacheHelper() {\n                    return in.getCoreCacheHelper();\n                  }\n\n                  @Override\n                  public CacheHelper getReaderCacheHelper() {\n                    return in.getReaderCacheHelper();\n                  }\n\n                  @Override\n                  public Bits getLiveDocs() {\n                    return null; // everything is live\n                  }\n\n                  @Override\n                  public int numDocs() {\n                    return maxDoc();\n                  }\n                };\n              }\n            }\n          }\n      ) {\n        @Override\n        public int numDeletesToMerge(SegmentCommitInfo info, int delCount, IOSupplier<CodecReader> readerSupplier) throws IOException {\n          if (mergeAwaySoftDeletes.get()) {\n            return super.numDeletesToMerge(info, delCount, readerSupplier);\n          } else {\n            return 0;\n          }\n        }\n      });\n    }\n    IndexWriter writer = new IndexWriter(dir, indexWriterConfig);\n    Thread[] threads = new Thread[2 + random().nextInt(3)];\n    CountDownLatch startLatch = new CountDownLatch(1);\n    CountDownLatch started = new CountDownLatch(threads.length);\n    boolean updateSeveralDocs = random().nextBoolean();\n    Set<String> ids = Collections.synchronizedSet(new HashSet<>());\n    for (int i = 0; i < threads.length; i++) {\n      threads[i] = new Thread(() -> {\n        try {\n          started.countDown();\n          startLatch.await();\n          for (int d = 0;  d < 100; d++) {\n            String id = String.valueOf(random().nextInt(10));\n            if (updateSeveralDocs) {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              if (mixDeletes && random().nextBoolean()) {\n                writer.updateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc));\n              } else {\n                writer.softUpdateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc),\n                    new NumericDocValuesField(\"soft_delete\", 1));\n              }\n            } else {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              if (mixDeletes && random().nextBoolean()) {\n                writer.updateDocument(new Term(\"id\", id), doc);\n              } else {\n                writer.softUpdateDocument(new Term(\"id\", id), doc,\n                    new NumericDocValuesField(\"soft_delete\", 1));\n              }\n            }\n            ids.add(id);\n          }\n        } catch (IOException | InterruptedException e) {\n          throw new AssertionError(e);\n        }\n      });\n      threads[i].start();\n    }\n    started.await();\n    startLatch.countDown();\n\n    for (int i = 0; i < threads.length; i++) {\n      threads[i].join();\n    }\n    DirectoryReader reader = DirectoryReader.open(writer);\n    IndexSearcher searcher = new IndexSearcher(reader);\n    for (String id : ids) {\n      TopDocs topDocs = searcher.search(new TermQuery(new Term(\"id\", id)), 10);\n      if (updateSeveralDocs) {\n        assertEquals(2, topDocs.totalHits.value);\n        assertEquals(Math.abs(topDocs.scoreDocs[0].doc - topDocs.scoreDocs[1].doc), 1);\n      } else {\n        assertEquals(1, topDocs.totalHits.value);\n      }\n    }\n    if (mixDeletes == false) {\n      for (LeafReaderContext context : reader.leaves()) {\n        LeafReader leaf = context.reader();\n        assertNull(((SegmentReader) leaf).getHardLiveDocs());\n      }\n    }\n    mergeAwaySoftDeletes.set(true);\n    writer.addDocument(new Document()); // add a dummy doc to trigger a segment here\n    writer.flush();\n    writer.forceMerge(1);\n    DirectoryReader oldReader = reader;\n    reader = DirectoryReader.openIfChanged(reader, writer);\n    if (reader != null) {\n      oldReader.close();\n      assertNotSame(oldReader, reader);\n    } else {\n      reader = oldReader;\n    }\n    for (String id : ids) {\n      if (updateSeveralDocs) {\n        assertEquals(2, reader.docFreq(new Term(\"id\", id)));\n      } else {\n        assertEquals(1, reader.docFreq(new Term(\"id\", id)));\n      }\n    }\n    int numSoftDeleted = 0;\n    for (SegmentCommitInfo info : writer.cloneSegmentInfos()) {\n      numSoftDeleted += info.getSoftDelCount() + info.getDelCount();\n    }\n    IndexWriter.DocStats docStats = writer.getDocStats();\n    assertEquals(docStats.maxDoc - docStats.numDocs, numSoftDeleted);\n    writer.commit();\n    try (DirectoryReader dirReader = DirectoryReader.open(dir)) {\n      int delCount = 0;\n      for (LeafReaderContext ctx : dirReader.leaves()) {\n        SegmentCommitInfo segmentInfo = ((SegmentReader) ctx.reader()).getSegmentInfo();\n        delCount += segmentInfo.getSoftDelCount() + segmentInfo.getDelCount();\n      }\n      assertEquals(numSoftDeleted, delCount);\n    }\n    IOUtils.close(reader, writer, dir);\n  }\n\n","sourceOld":"  public void softUpdatesConcurrently(boolean mixDeletes) throws IOException, InterruptedException {\n    Directory dir = newDirectory();\n    IndexWriterConfig indexWriterConfig = newIndexWriterConfig();\n    indexWriterConfig.setSoftDeletesField(\"soft_delete\");\n    AtomicBoolean mergeAwaySoftDeletes = new AtomicBoolean(random().nextBoolean());\n    if (mixDeletes == false) {\n      indexWriterConfig.setMergePolicy(new OneMergeWrappingMergePolicy(indexWriterConfig.getMergePolicy(), towrap ->\n          new MergePolicy.OneMerge(towrap.segments) {\n            @Override\n            public CodecReader wrapForMerge(CodecReader reader) throws IOException {\n              if (mergeAwaySoftDeletes.get()) {\n                return towrap.wrapForMerge(reader);\n              } else {\n                CodecReader wrapped = towrap.wrapForMerge(reader);\n                return new FilterCodecReader(wrapped) {\n                  @Override\n                  public CacheHelper getCoreCacheHelper() {\n                    return in.getCoreCacheHelper();\n                  }\n\n                  @Override\n                  public CacheHelper getReaderCacheHelper() {\n                    return in.getReaderCacheHelper();\n                  }\n\n                  @Override\n                  public Bits getLiveDocs() {\n                    return null; // everything is live\n                  }\n\n                  @Override\n                  public int numDocs() {\n                    return maxDoc();\n                  }\n                };\n              }\n            }\n          }\n      ) {\n        @Override\n        public int numDeletesToMerge(SegmentCommitInfo info, int delCount, IOSupplier<CodecReader> readerSupplier) throws IOException {\n          if (mergeAwaySoftDeletes.get()) {\n            return super.numDeletesToMerge(info, delCount, readerSupplier);\n          } else {\n            return 0;\n          }\n        }\n      });\n    }\n    IndexWriter writer = new IndexWriter(dir, indexWriterConfig);\n    Thread[] threads = new Thread[2 + random().nextInt(3)];\n    CountDownLatch startLatch = new CountDownLatch(1);\n    CountDownLatch started = new CountDownLatch(threads.length);\n    boolean updateSeveralDocs = random().nextBoolean();\n    Set<String> ids = Collections.synchronizedSet(new HashSet<>());\n    for (int i = 0; i < threads.length; i++) {\n      threads[i] = new Thread(() -> {\n        try {\n          started.countDown();\n          startLatch.await();\n          for (int d = 0;  d < 100; d++) {\n            String id = String.valueOf(random().nextInt(10));\n            if (updateSeveralDocs) {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              if (mixDeletes && random().nextBoolean()) {\n                writer.updateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc));\n              } else {\n                writer.softUpdateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc),\n                    new NumericDocValuesField(\"soft_delete\", 1));\n              }\n            } else {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              if (mixDeletes && random().nextBoolean()) {\n                writer.updateDocument(new Term(\"id\", id), doc);\n              } else {\n                writer.softUpdateDocument(new Term(\"id\", id), doc,\n                    new NumericDocValuesField(\"soft_delete\", 1));\n              }\n            }\n            ids.add(id);\n          }\n        } catch (IOException | InterruptedException e) {\n          throw new AssertionError(e);\n        }\n      });\n      threads[i].start();\n    }\n    started.await();\n    startLatch.countDown();\n\n    for (int i = 0; i < threads.length; i++) {\n      threads[i].join();\n    }\n    DirectoryReader reader = DirectoryReader.open(writer);\n    IndexSearcher searcher = new IndexSearcher(reader);\n    for (String id : ids) {\n      TopDocs topDocs = searcher.search(new TermQuery(new Term(\"id\", id)), 10);\n      if (updateSeveralDocs) {\n        assertEquals(2, topDocs.totalHits.value);\n        assertEquals(Math.abs(topDocs.scoreDocs[0].doc - topDocs.scoreDocs[1].doc), 1);\n      } else {\n        assertEquals(1, topDocs.totalHits.value);\n      }\n    }\n    if (mixDeletes == false) {\n      for (LeafReaderContext context : reader.leaves()) {\n        LeafReader leaf = context.reader();\n        assertNull(((SegmentReader) leaf).getHardLiveDocs());\n      }\n    }\n    mergeAwaySoftDeletes.set(true);\n    writer.addDocument(new Document()); // add a dummy doc to trigger a segment here\n    writer.flush();\n    writer.forceMerge(1);\n    DirectoryReader oldReader = reader;\n    reader = DirectoryReader.openIfChanged(reader, writer);\n    if (reader != null) {\n      oldReader.close();\n      assertNotSame(oldReader, reader);\n    } else {\n      reader = oldReader;\n    }\n    for (String id : ids) {\n      if (updateSeveralDocs) {\n        assertEquals(2, reader.docFreq(new Term(\"id\", id)));\n      } else {\n        assertEquals(1, reader.docFreq(new Term(\"id\", id)));\n      }\n    }\n    int numSoftDeleted = 0;\n    for (SegmentCommitInfo info : writer.cloneSegmentInfos()) {\n      numSoftDeleted += info.getSoftDelCount() + info.getDelCount();\n    }\n    assertEquals(writer.maxDoc() - writer.numDocs(), numSoftDeleted);\n    writer.commit();\n    try (DirectoryReader dirReader = DirectoryReader.open(dir)) {\n      int delCount = 0;\n      for (LeafReaderContext ctx : dirReader.leaves()) {\n        SegmentCommitInfo segmentInfo = ((SegmentReader) ctx.reader()).getSegmentInfo();\n        delCount += segmentInfo.getSoftDelCount() + segmentInfo.getDelCount();\n      }\n      assertEquals(numSoftDeleted, delCount);\n    }\n    IOUtils.close(reader, writer, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":["b70042a8a492f7054d480ccdd2be9796510d4327","4a90cc8c90aa53ddf51fbd15019989ac269514a3"],"631ea3d1607299c59f33edef140ffc19a81f07a0":["4a90cc8c90aa53ddf51fbd15019989ac269514a3"],"feb4029567b43f074ed7b6eb8fb126d355075dfd":["83788ad129a5154d5c6562c4e8ce3db48793aada"],"5ee0394b8176abd7c90a4be8c05465be1879db79":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b70042a8a492f7054d480ccdd2be9796510d4327":["5ee0394b8176abd7c90a4be8c05465be1879db79","8f2203cb8ae87188877cfbf6ad170c5738a0aad5"],"4a90cc8c90aa53ddf51fbd15019989ac269514a3":["8f2203cb8ae87188877cfbf6ad170c5738a0aad5"],"83788ad129a5154d5c6562c4e8ce3db48793aada":["578a4d73d90ecd838846cc32bf1098aaa262b524"],"8f2203cb8ae87188877cfbf6ad170c5738a0aad5":["5ee0394b8176abd7c90a4be8c05465be1879db79"],"578a4d73d90ecd838846cc32bf1098aaa262b524":["631ea3d1607299c59f33edef140ffc19a81f07a0"],"f592209545c71895260367152601e9200399776d":["5ee0394b8176abd7c90a4be8c05465be1879db79","8f2203cb8ae87188877cfbf6ad170c5738a0aad5"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["feb4029567b43f074ed7b6eb8fb126d355075dfd"]},"commit2Childs":{"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":[],"631ea3d1607299c59f33edef140ffc19a81f07a0":["578a4d73d90ecd838846cc32bf1098aaa262b524"],"feb4029567b43f074ed7b6eb8fb126d355075dfd":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"b70042a8a492f7054d480ccdd2be9796510d4327":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5"],"5ee0394b8176abd7c90a4be8c05465be1879db79":["b70042a8a492f7054d480ccdd2be9796510d4327","8f2203cb8ae87188877cfbf6ad170c5738a0aad5","f592209545c71895260367152601e9200399776d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["5ee0394b8176abd7c90a4be8c05465be1879db79"],"4a90cc8c90aa53ddf51fbd15019989ac269514a3":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","631ea3d1607299c59f33edef140ffc19a81f07a0"],"83788ad129a5154d5c6562c4e8ce3db48793aada":["feb4029567b43f074ed7b6eb8fb126d355075dfd"],"8f2203cb8ae87188877cfbf6ad170c5738a0aad5":["b70042a8a492f7054d480ccdd2be9796510d4327","4a90cc8c90aa53ddf51fbd15019989ac269514a3","f592209545c71895260367152601e9200399776d"],"578a4d73d90ecd838846cc32bf1098aaa262b524":["83788ad129a5154d5c6562c4e8ce3db48793aada"],"f592209545c71895260367152601e9200399776d":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","f592209545c71895260367152601e9200399776d","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}