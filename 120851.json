{"path":"solr/src/java/org/apache/solr/spelling/suggest/fst/FSTLookup#build(TermFreqIterator).mjava","commits":[{"id":"47ac88233e9e3019faa24a5184425ad60c23d70f","date":1302779803,"type":0,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/spelling/suggest/fst/FSTLookup#build(TermFreqIterator).mjava","pathOld":"/dev/null","sourceNew":"  /* */\n  @Override\n  public void build(TermFreqIterator tfit) throws IOException {\n    // Buffer the input because we will need it twice: for calculating\n    // weights distribution and for the actual automata building.\n    List<Entry> entries = Lists.newArrayList();\n    while (tfit.hasNext()) {\n      String term = tfit.next();\n      char [] termChars = new char [term.length() + 1]; // add padding for weight.\n      for (int i = 0; i < term.length(); i++)\n        termChars[i + 1] = term.charAt(i);\n      entries.add(new Entry(termChars, tfit.freq()));\n    }\n\n    // Distribute weights into at most N buckets. This is a form of discretization to\n    // limit the number of possible weights so that they can be efficiently encoded in the\n    // automaton.\n    //\n    // It is assumed the distribution of weights is _linear_ so proportional division \n    // of [min, max] range will be enough here. Other approaches could be to sort \n    // weights and divide into proportional ranges.\n    if (entries.size() > 0) {\n      redistributeWeightsProportionalMinMax(entries, buckets);\n      encodeWeightPrefix(entries);\n    }\n\n    // Build the automaton (includes input sorting) and cache root arcs in order from the highest,\n    // to the lowest weight.\n    this.automaton = buildAutomaton(entries);\n    cacheRootArcs();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1eb46686a27187e42311e77666a2c7026f461ebc","date":1302858020,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/spelling/suggest/fst/FSTLookup#build(TermFreqIterator).mjava","pathOld":"/dev/null","sourceNew":"  /* */\n  @Override\n  public void build(TermFreqIterator tfit) throws IOException {\n    // Buffer the input because we will need it twice: for calculating\n    // weights distribution and for the actual automata building.\n    List<Entry> entries = Lists.newArrayList();\n    while (tfit.hasNext()) {\n      String term = tfit.next();\n      char [] termChars = new char [term.length() + 1]; // add padding for weight.\n      for (int i = 0; i < term.length(); i++)\n        termChars[i + 1] = term.charAt(i);\n      entries.add(new Entry(termChars, tfit.freq()));\n    }\n\n    // Distribute weights into at most N buckets. This is a form of discretization to\n    // limit the number of possible weights so that they can be efficiently encoded in the\n    // automaton.\n    //\n    // It is assumed the distribution of weights is _linear_ so proportional division \n    // of [min, max] range will be enough here. Other approaches could be to sort \n    // weights and divide into proportional ranges.\n    if (entries.size() > 0) {\n      redistributeWeightsProportionalMinMax(entries, buckets);\n      encodeWeightPrefix(entries);\n    }\n\n    // Build the automaton (includes input sorting) and cache root arcs in order from the highest,\n    // to the lowest weight.\n    this.automaton = buildAutomaton(entries);\n    cacheRootArcs();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/spelling/suggest/fst/FSTLookup#build(TermFreqIterator).mjava","pathOld":"/dev/null","sourceNew":"  /* */\n  @Override\n  public void build(TermFreqIterator tfit) throws IOException {\n    // Buffer the input because we will need it twice: for calculating\n    // weights distribution and for the actual automata building.\n    List<Entry> entries = Lists.newArrayList();\n    while (tfit.hasNext()) {\n      String term = tfit.next();\n      char [] termChars = new char [term.length() + 1]; // add padding for weight.\n      for (int i = 0; i < term.length(); i++)\n        termChars[i + 1] = term.charAt(i);\n      entries.add(new Entry(termChars, tfit.freq()));\n    }\n\n    // Distribute weights into at most N buckets. This is a form of discretization to\n    // limit the number of possible weights so that they can be efficiently encoded in the\n    // automaton.\n    //\n    // It is assumed the distribution of weights is _linear_ so proportional division \n    // of [min, max] range will be enough here. Other approaches could be to sort \n    // weights and divide into proportional ranges.\n    if (entries.size() > 0) {\n      redistributeWeightsProportionalMinMax(entries, buckets);\n      encodeWeightPrefix(entries);\n    }\n\n    // Build the automaton (includes input sorting) and cache root arcs in order from the highest,\n    // to the lowest weight.\n    this.automaton = buildAutomaton(entries);\n    cacheRootArcs();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":0,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/spelling/suggest/fst/FSTLookup#build(TermFreqIterator).mjava","pathOld":"/dev/null","sourceNew":"  /* */\n  @Override\n  public void build(TermFreqIterator tfit) throws IOException {\n    // Buffer the input because we will need it twice: for calculating\n    // weights distribution and for the actual automata building.\n    List<Entry> entries = Lists.newArrayList();\n    while (tfit.hasNext()) {\n      String term = tfit.next();\n      char [] termChars = new char [term.length() + 1]; // add padding for weight.\n      for (int i = 0; i < term.length(); i++)\n        termChars[i + 1] = term.charAt(i);\n      entries.add(new Entry(termChars, tfit.freq()));\n    }\n\n    // Distribute weights into at most N buckets. This is a form of discretization to\n    // limit the number of possible weights so that they can be efficiently encoded in the\n    // automaton.\n    //\n    // It is assumed the distribution of weights is _linear_ so proportional division \n    // of [min, max] range will be enough here. Other approaches could be to sort \n    // weights and divide into proportional ranges.\n    if (entries.size() > 0) {\n      redistributeWeightsProportionalMinMax(entries, buckets);\n      encodeWeightPrefix(entries);\n    }\n\n    // Build the automaton (includes input sorting) and cache root arcs in order from the highest,\n    // to the lowest weight.\n    this.automaton = buildAutomaton(entries);\n    cacheRootArcs();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e230a61047bc041516c811baa08a7174d6f8322a","date":1306175633,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"modules/suggest/src/java/org/apache/lucene/search/suggest/fst/FSTLookup#build(TermFreqIterator).mjava","pathOld":"solr/src/java/org/apache/solr/spelling/suggest/fst/FSTLookup#build(TermFreqIterator).mjava","sourceNew":"  /* */\n  @Override\n  public void build(TermFreqIterator tfit) throws IOException {\n    // Buffer the input because we will need it twice: for calculating\n    // weights distribution and for the actual automata building.\n    List<Entry> entries = new ArrayList<Entry>();\n    while (tfit.hasNext()) {\n      String term = tfit.next();\n      char [] termChars = new char [term.length() + 1]; // add padding for weight.\n      for (int i = 0; i < term.length(); i++)\n        termChars[i + 1] = term.charAt(i);\n      entries.add(new Entry(termChars, tfit.freq()));\n    }\n\n    // Distribute weights into at most N buckets. This is a form of discretization to\n    // limit the number of possible weights so that they can be efficiently encoded in the\n    // automaton.\n    //\n    // It is assumed the distribution of weights is _linear_ so proportional division \n    // of [min, max] range will be enough here. Other approaches could be to sort \n    // weights and divide into proportional ranges.\n    if (entries.size() > 0) {\n      redistributeWeightsProportionalMinMax(entries, buckets);\n      encodeWeightPrefix(entries);\n    }\n\n    // Build the automaton (includes input sorting) and cache root arcs in order from the highest,\n    // to the lowest weight.\n    this.automaton = buildAutomaton(entries);\n    cacheRootArcs();\n  }\n\n","sourceOld":"  /* */\n  @Override\n  public void build(TermFreqIterator tfit) throws IOException {\n    // Buffer the input because we will need it twice: for calculating\n    // weights distribution and for the actual automata building.\n    List<Entry> entries = Lists.newArrayList();\n    while (tfit.hasNext()) {\n      String term = tfit.next();\n      char [] termChars = new char [term.length() + 1]; // add padding for weight.\n      for (int i = 0; i < term.length(); i++)\n        termChars[i + 1] = term.charAt(i);\n      entries.add(new Entry(termChars, tfit.freq()));\n    }\n\n    // Distribute weights into at most N buckets. This is a form of discretization to\n    // limit the number of possible weights so that they can be efficiently encoded in the\n    // automaton.\n    //\n    // It is assumed the distribution of weights is _linear_ so proportional division \n    // of [min, max] range will be enough here. Other approaches could be to sort \n    // weights and divide into proportional ranges.\n    if (entries.size() > 0) {\n      redistributeWeightsProportionalMinMax(entries, buckets);\n      encodeWeightPrefix(entries);\n    }\n\n    // Build the automaton (includes input sorting) and cache root arcs in order from the highest,\n    // to the lowest weight.\n    this.automaton = buildAutomaton(entries);\n    cacheRootArcs();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","date":1306767085,"type":5,"author":"Steven Rowe","isMerge":true,"pathNew":"modules/suggest/src/java/org/apache/lucene/search/suggest/fst/FSTLookup#build(TermFreqIterator).mjava","pathOld":"solr/src/java/org/apache/solr/spelling/suggest/fst/FSTLookup#build(TermFreqIterator).mjava","sourceNew":"  /* */\n  @Override\n  public void build(TermFreqIterator tfit) throws IOException {\n    // Buffer the input because we will need it twice: for calculating\n    // weights distribution and for the actual automata building.\n    List<Entry> entries = new ArrayList<Entry>();\n    while (tfit.hasNext()) {\n      String term = tfit.next();\n      char [] termChars = new char [term.length() + 1]; // add padding for weight.\n      for (int i = 0; i < term.length(); i++)\n        termChars[i + 1] = term.charAt(i);\n      entries.add(new Entry(termChars, tfit.freq()));\n    }\n\n    // Distribute weights into at most N buckets. This is a form of discretization to\n    // limit the number of possible weights so that they can be efficiently encoded in the\n    // automaton.\n    //\n    // It is assumed the distribution of weights is _linear_ so proportional division \n    // of [min, max] range will be enough here. Other approaches could be to sort \n    // weights and divide into proportional ranges.\n    if (entries.size() > 0) {\n      redistributeWeightsProportionalMinMax(entries, buckets);\n      encodeWeightPrefix(entries);\n    }\n\n    // Build the automaton (includes input sorting) and cache root arcs in order from the highest,\n    // to the lowest weight.\n    this.automaton = buildAutomaton(entries);\n    cacheRootArcs();\n  }\n\n","sourceOld":"  /* */\n  @Override\n  public void build(TermFreqIterator tfit) throws IOException {\n    // Buffer the input because we will need it twice: for calculating\n    // weights distribution and for the actual automata building.\n    List<Entry> entries = Lists.newArrayList();\n    while (tfit.hasNext()) {\n      String term = tfit.next();\n      char [] termChars = new char [term.length() + 1]; // add padding for weight.\n      for (int i = 0; i < term.length(); i++)\n        termChars[i + 1] = term.charAt(i);\n      entries.add(new Entry(termChars, tfit.freq()));\n    }\n\n    // Distribute weights into at most N buckets. This is a form of discretization to\n    // limit the number of possible weights so that they can be efficiently encoded in the\n    // automaton.\n    //\n    // It is assumed the distribution of weights is _linear_ so proportional division \n    // of [min, max] range will be enough here. Other approaches could be to sort \n    // weights and divide into proportional ranges.\n    if (entries.size() > 0) {\n      redistributeWeightsProportionalMinMax(entries, buckets);\n      encodeWeightPrefix(entries);\n    }\n\n    // Build the automaton (includes input sorting) and cache root arcs in order from the highest,\n    // to the lowest weight.\n    this.automaton = buildAutomaton(entries);\n    cacheRootArcs();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2e10cb22a8bdb44339e282925a29182bb2f3174d","date":1306841137,"type":5,"author":"Simon Willnauer","isMerge":true,"pathNew":"modules/suggest/src/java/org/apache/lucene/search/suggest/fst/FSTLookup#build(TermFreqIterator).mjava","pathOld":"solr/src/java/org/apache/solr/spelling/suggest/fst/FSTLookup#build(TermFreqIterator).mjava","sourceNew":"  /* */\n  @Override\n  public void build(TermFreqIterator tfit) throws IOException {\n    // Buffer the input because we will need it twice: for calculating\n    // weights distribution and for the actual automata building.\n    List<Entry> entries = new ArrayList<Entry>();\n    while (tfit.hasNext()) {\n      String term = tfit.next();\n      char [] termChars = new char [term.length() + 1]; // add padding for weight.\n      for (int i = 0; i < term.length(); i++)\n        termChars[i + 1] = term.charAt(i);\n      entries.add(new Entry(termChars, tfit.freq()));\n    }\n\n    // Distribute weights into at most N buckets. This is a form of discretization to\n    // limit the number of possible weights so that they can be efficiently encoded in the\n    // automaton.\n    //\n    // It is assumed the distribution of weights is _linear_ so proportional division \n    // of [min, max] range will be enough here. Other approaches could be to sort \n    // weights and divide into proportional ranges.\n    if (entries.size() > 0) {\n      redistributeWeightsProportionalMinMax(entries, buckets);\n      encodeWeightPrefix(entries);\n    }\n\n    // Build the automaton (includes input sorting) and cache root arcs in order from the highest,\n    // to the lowest weight.\n    this.automaton = buildAutomaton(entries);\n    cacheRootArcs();\n  }\n\n","sourceOld":"  /* */\n  @Override\n  public void build(TermFreqIterator tfit) throws IOException {\n    // Buffer the input because we will need it twice: for calculating\n    // weights distribution and for the actual automata building.\n    List<Entry> entries = Lists.newArrayList();\n    while (tfit.hasNext()) {\n      String term = tfit.next();\n      char [] termChars = new char [term.length() + 1]; // add padding for weight.\n      for (int i = 0; i < term.length(); i++)\n        termChars[i + 1] = term.charAt(i);\n      entries.add(new Entry(termChars, tfit.freq()));\n    }\n\n    // Distribute weights into at most N buckets. This is a form of discretization to\n    // limit the number of possible weights so that they can be efficiently encoded in the\n    // automaton.\n    //\n    // It is assumed the distribution of weights is _linear_ so proportional division \n    // of [min, max] range will be enough here. Other approaches could be to sort \n    // weights and divide into proportional ranges.\n    if (entries.size() > 0) {\n      redistributeWeightsProportionalMinMax(entries, buckets);\n      encodeWeightPrefix(entries);\n    }\n\n    // Build the automaton (includes input sorting) and cache root arcs in order from the highest,\n    // to the lowest weight.\n    this.automaton = buildAutomaton(entries);\n    cacheRootArcs();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"47ac88233e9e3019faa24a5184425ad60c23d70f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"1eb46686a27187e42311e77666a2c7026f461ebc":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","47ac88233e9e3019faa24a5184425ad60c23d70f"],"e230a61047bc041516c811baa08a7174d6f8322a":["47ac88233e9e3019faa24a5184425ad60c23d70f"],"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91":["a3776dccca01c11e7046323cfad46a3b4a471233","e230a61047bc041516c811baa08a7174d6f8322a"],"a3776dccca01c11e7046323cfad46a3b4a471233":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","47ac88233e9e3019faa24a5184425ad60c23d70f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","47ac88233e9e3019faa24a5184425ad60c23d70f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["e230a61047bc041516c811baa08a7174d6f8322a"],"2e10cb22a8bdb44339e282925a29182bb2f3174d":["135621f3a0670a9394eb563224a3b76cc4dddc0f","e230a61047bc041516c811baa08a7174d6f8322a"]},"commit2Childs":{"47ac88233e9e3019faa24a5184425ad60c23d70f":["1eb46686a27187e42311e77666a2c7026f461ebc","e230a61047bc041516c811baa08a7174d6f8322a","a3776dccca01c11e7046323cfad46a3b4a471233","135621f3a0670a9394eb563224a3b76cc4dddc0f"],"1eb46686a27187e42311e77666a2c7026f461ebc":[],"e230a61047bc041516c811baa08a7174d6f8322a":["5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","cd5edd1f2b162a5cfa08efd17851a07373a96817","2e10cb22a8bdb44339e282925a29182bb2f3174d"],"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91":[],"a3776dccca01c11e7046323cfad46a3b4a471233":["5128b7b3b73fedff05fdc5ea2e6be53c1020bb91"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["47ac88233e9e3019faa24a5184425ad60c23d70f","1eb46686a27187e42311e77666a2c7026f461ebc","a3776dccca01c11e7046323cfad46a3b4a471233","135621f3a0670a9394eb563224a3b76cc4dddc0f"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["2e10cb22a8bdb44339e282925a29182bb2f3174d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"2e10cb22a8bdb44339e282925a29182bb2f3174d":[]},"heads":["1eb46686a27187e42311e77666a2c7026f461ebc","5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","cd5edd1f2b162a5cfa08efd17851a07373a96817","2e10cb22a8bdb44339e282925a29182bb2f3174d"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}