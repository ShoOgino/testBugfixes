{"path":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListNC(Query,DocSet,Sort,int,int,int).mjava","commits":[{"id":"0c3e228bf650e96f3002a8fb73dd0c13d55af077","date":1138253849,"type":0,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListNC(Query,DocSet,Sort,int,int,int).mjava","pathOld":"/dev/null","sourceNew":"  private DocList getDocListNC(Query query, DocSet filter, Sort lsort, int offset, int len, int flags) throws IOException {\n    final int lastDocRequested = offset+len;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final DocSet filt = filter;\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n      final int[] numHits = new int[1];\n\n      searcher.search(query, new HitCollector() {\n        public void collect(int doc, float score) {\n          if (filt!=null && !filt.exists(doc)) return;\n          numHits[0]++;\n          if (score > topscore[0]) topscore[0]=score;\n        }\n      }\n      );\n\n      nDocsReturned=0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else if (lsort != null) {\n      // can't use TopDocs if there is a sort since it\n      // will do automatic score normalization.\n      // NOTE: this changed late in Lucene 1.9\n\n      final DocSet filt = filter;\n      final PublicFieldSortedHitQueue hq = new PublicFieldSortedHitQueue(reader, lsort.getSort(), offset+len);\n\n      searcher.search(query, new HitCollector() {\n        public void collect(int doc, float score) {\n          if (filt!=null && !filt.exists(doc)) return;\n          hq.insert(new FieldDoc(doc, score));\n        }\n      }\n      );\n\n      totalHits = hq.getTotalHits();\n      maxScore = totalHits>0 ? hq.getMaxScore() : 0.0f;\n\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (flags&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        FieldDoc fieldDoc = (FieldDoc)hq.pop();\n        // fillFields is the point where score normalization happens\n        // hq.fillFields(fieldDoc)\n        ids[i] = fieldDoc.doc;\n        if (scores != null) scores[i] = fieldDoc.score;\n      }\n    } else {\n      // No Sort specified (sort by score descending)\n      // This case could be done with TopDocs, but would currently require\n      // getting a BitSet filter from a DocSet which may be inefficient.\n\n      final DocSet filt = filter;\n      final ScorePriorityQueue hq = new ScorePriorityQueue(lastDocRequested);\n      final int[] numHits = new int[1];\n      searcher.search(query, new HitCollector() {\n        float minScore=Float.NEGATIVE_INFINITY;  // minimum score in the priority queue\n        public void collect(int doc, float score) {\n          if (filt!=null && !filt.exists(doc)) return;\n          if (numHits[0]++ < lastDocRequested || score >= minScore) {\n            // if docs are always delivered in order, we could use \"score>minScore\"\n            // but might BooleanScorer14 might still be used and deliver docs out-of-order?\n            hq.insert(new ScoreDoc(doc, score));\n            minScore = ((ScoreDoc)hq.top()).score;\n          }\n        }\n      }\n      );\n\n      totalHits = numHits[0];\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (flags&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      ScoreDoc sdoc =null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        sdoc = (ScoreDoc)hq.pop();\n        ids[i] = sdoc.doc;\n        if (scores != null) scores[i] = sdoc.score;\n      }\n      maxScore = sdoc ==null ? 0.0f : sdoc.score;\n    }\n\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned) - offset;\n    if (sliceLen < 0) sliceLen=0;\n    return new DocSlice(offset,sliceLen,ids,scores,totalHits,maxScore);\n\n\n\n    /**************** older implementation using TopDocs *******************\n\n\n      Filter lfilter=null;\n      if (filter != null) {\n        final BitSet bits = filter.getBits();   // avoid if possible\n        lfilter = new Filter() {\n          public BitSet bits(IndexReader reader)  {\n            return bits;\n          }\n        };\n      }\n\n      int lastDocRequested=offset+len;\n\n      // lucene doesn't allow 0 to be passed for nDocs\n      if (lastDocRequested==0) lastDocRequested=1;\n\n      // TopFieldDocs sortedDocs;  // use TopDocs so both versions can use it\n      TopDocs sortedDocs;\n      if (lsort!=null) {\n         sortedDocs = searcher.search(query, lfilter, lastDocRequested, lsort);\n      } else {\n         sortedDocs = searcher.search(query, lfilter, lastDocRequested);\n      }\n\n      int nDocsReturned = sortedDocs.scoreDocs.length;\n      int[] docs = new int[nDocsReturned];\n      for (int i=0; i<nDocsReturned; i++) {\n        docs[i] = sortedDocs.scoreDocs[i].doc;\n      }\n      float[] scores=null;\n      float maxScore=0.0f;\n      if ((flags & GET_SCORES) != 0) {\n        scores = new float[nDocsReturned];\n        for (int i=0; i<nDocsReturned; i++) {\n          scores[i] = sortedDocs.scoreDocs[i].score;\n        }\n        if (nDocsReturned>0) {\n          maxScore=sortedDocs.scoreDocs[0].score;\n        }\n      }\n      int sliceLen = Math.min(offset+len,nDocsReturned) - offset;\n      if (sliceLen < 0) sliceLen=0;\n      return new DocSlice(offset,sliceLen,docs,scores,sortedDocs.totalHits, maxScore);\n\n    **********************************************************************************/\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["30c238dad8c4234f556cd28cd22ff426247e70c4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6079dc4b996dfa326ce75a4fdd0422a4db7720c8","date":1139243061,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListNC(Query,DocSet,Sort,int,int,int).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListNC(Query,DocSet,Sort,int,int,int).mjava","sourceNew":"  private DocList getDocListNC(Query query, DocSet filter, Sort lsort, int offset, int len, int flags) throws IOException {\n    final int lastDocRequested = offset+len;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final DocSet filt = filter;\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n      final int[] numHits = new int[1];\n\n      searcher.search(query, new HitCollector() {\n        public void collect(int doc, float score) {\n          if (filt!=null && !filt.exists(doc)) return;\n          numHits[0]++;\n          if (score > topscore[0]) topscore[0]=score;\n        }\n      }\n      );\n\n      nDocsReturned=0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else if (lsort != null) {\n      // can't use TopDocs if there is a sort since it\n      // will do automatic score normalization.\n      // NOTE: this changed late in Lucene 1.9\n\n      final DocSet filt = filter;\n      final int[] numHits = new int[1];\n      final FieldSortedHitQueue hq = new FieldSortedHitQueue(reader, lsort.getSort(), offset+len);\n\n      searcher.search(query, new HitCollector() {\n        public void collect(int doc, float score) {\n          if (filt!=null && !filt.exists(doc)) return;\n          numHits[0]++;\n          hq.insert(new FieldDoc(doc, score));\n        }\n      }\n      );\n\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? hq.getMaxScore() : 0.0f;\n\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (flags&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        FieldDoc fieldDoc = (FieldDoc)hq.pop();\n        // fillFields is the point where score normalization happens\n        // hq.fillFields(fieldDoc)\n        ids[i] = fieldDoc.doc;\n        if (scores != null) scores[i] = fieldDoc.score;\n      }\n    } else {\n      // No Sort specified (sort by score descending)\n      // This case could be done with TopDocs, but would currently require\n      // getting a BitSet filter from a DocSet which may be inefficient.\n\n      final DocSet filt = filter;\n      final ScorePriorityQueue hq = new ScorePriorityQueue(lastDocRequested);\n      final int[] numHits = new int[1];\n      searcher.search(query, new HitCollector() {\n        float minScore=Float.NEGATIVE_INFINITY;  // minimum score in the priority queue\n        public void collect(int doc, float score) {\n          if (filt!=null && !filt.exists(doc)) return;\n          if (numHits[0]++ < lastDocRequested || score >= minScore) {\n            // if docs are always delivered in order, we could use \"score>minScore\"\n            // but might BooleanScorer14 might still be used and deliver docs out-of-order?\n            hq.insert(new ScoreDoc(doc, score));\n            minScore = ((ScoreDoc)hq.top()).score;\n          }\n        }\n      }\n      );\n\n      totalHits = numHits[0];\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (flags&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      ScoreDoc sdoc =null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        sdoc = (ScoreDoc)hq.pop();\n        ids[i] = sdoc.doc;\n        if (scores != null) scores[i] = sdoc.score;\n      }\n      maxScore = sdoc ==null ? 0.0f : sdoc.score;\n    }\n\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned) - offset;\n    if (sliceLen < 0) sliceLen=0;\n    return new DocSlice(offset,sliceLen,ids,scores,totalHits,maxScore);\n\n\n\n    /**************** older implementation using TopDocs *******************\n\n\n      Filter lfilter=null;\n      if (filter != null) {\n        final BitSet bits = filter.getBits();   // avoid if possible\n        lfilter = new Filter() {\n          public BitSet bits(IndexReader reader)  {\n            return bits;\n          }\n        };\n      }\n\n      int lastDocRequested=offset+len;\n\n      // lucene doesn't allow 0 to be passed for nDocs\n      if (lastDocRequested==0) lastDocRequested=1;\n\n      // TopFieldDocs sortedDocs;  // use TopDocs so both versions can use it\n      TopDocs sortedDocs;\n      if (lsort!=null) {\n         sortedDocs = searcher.search(query, lfilter, lastDocRequested, lsort);\n      } else {\n         sortedDocs = searcher.search(query, lfilter, lastDocRequested);\n      }\n\n      int nDocsReturned = sortedDocs.scoreDocs.length;\n      int[] docs = new int[nDocsReturned];\n      for (int i=0; i<nDocsReturned; i++) {\n        docs[i] = sortedDocs.scoreDocs[i].doc;\n      }\n      float[] scores=null;\n      float maxScore=0.0f;\n      if ((flags & GET_SCORES) != 0) {\n        scores = new float[nDocsReturned];\n        for (int i=0; i<nDocsReturned; i++) {\n          scores[i] = sortedDocs.scoreDocs[i].score;\n        }\n        if (nDocsReturned>0) {\n          maxScore=sortedDocs.scoreDocs[0].score;\n        }\n      }\n      int sliceLen = Math.min(offset+len,nDocsReturned) - offset;\n      if (sliceLen < 0) sliceLen=0;\n      return new DocSlice(offset,sliceLen,docs,scores,sortedDocs.totalHits, maxScore);\n\n    **********************************************************************************/\n\n  }\n\n","sourceOld":"  private DocList getDocListNC(Query query, DocSet filter, Sort lsort, int offset, int len, int flags) throws IOException {\n    final int lastDocRequested = offset+len;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final DocSet filt = filter;\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n      final int[] numHits = new int[1];\n\n      searcher.search(query, new HitCollector() {\n        public void collect(int doc, float score) {\n          if (filt!=null && !filt.exists(doc)) return;\n          numHits[0]++;\n          if (score > topscore[0]) topscore[0]=score;\n        }\n      }\n      );\n\n      nDocsReturned=0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else if (lsort != null) {\n      // can't use TopDocs if there is a sort since it\n      // will do automatic score normalization.\n      // NOTE: this changed late in Lucene 1.9\n\n      final DocSet filt = filter;\n      final PublicFieldSortedHitQueue hq = new PublicFieldSortedHitQueue(reader, lsort.getSort(), offset+len);\n\n      searcher.search(query, new HitCollector() {\n        public void collect(int doc, float score) {\n          if (filt!=null && !filt.exists(doc)) return;\n          hq.insert(new FieldDoc(doc, score));\n        }\n      }\n      );\n\n      totalHits = hq.getTotalHits();\n      maxScore = totalHits>0 ? hq.getMaxScore() : 0.0f;\n\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (flags&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        FieldDoc fieldDoc = (FieldDoc)hq.pop();\n        // fillFields is the point where score normalization happens\n        // hq.fillFields(fieldDoc)\n        ids[i] = fieldDoc.doc;\n        if (scores != null) scores[i] = fieldDoc.score;\n      }\n    } else {\n      // No Sort specified (sort by score descending)\n      // This case could be done with TopDocs, but would currently require\n      // getting a BitSet filter from a DocSet which may be inefficient.\n\n      final DocSet filt = filter;\n      final ScorePriorityQueue hq = new ScorePriorityQueue(lastDocRequested);\n      final int[] numHits = new int[1];\n      searcher.search(query, new HitCollector() {\n        float minScore=Float.NEGATIVE_INFINITY;  // minimum score in the priority queue\n        public void collect(int doc, float score) {\n          if (filt!=null && !filt.exists(doc)) return;\n          if (numHits[0]++ < lastDocRequested || score >= minScore) {\n            // if docs are always delivered in order, we could use \"score>minScore\"\n            // but might BooleanScorer14 might still be used and deliver docs out-of-order?\n            hq.insert(new ScoreDoc(doc, score));\n            minScore = ((ScoreDoc)hq.top()).score;\n          }\n        }\n      }\n      );\n\n      totalHits = numHits[0];\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (flags&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      ScoreDoc sdoc =null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        sdoc = (ScoreDoc)hq.pop();\n        ids[i] = sdoc.doc;\n        if (scores != null) scores[i] = sdoc.score;\n      }\n      maxScore = sdoc ==null ? 0.0f : sdoc.score;\n    }\n\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned) - offset;\n    if (sliceLen < 0) sliceLen=0;\n    return new DocSlice(offset,sliceLen,ids,scores,totalHits,maxScore);\n\n\n\n    /**************** older implementation using TopDocs *******************\n\n\n      Filter lfilter=null;\n      if (filter != null) {\n        final BitSet bits = filter.getBits();   // avoid if possible\n        lfilter = new Filter() {\n          public BitSet bits(IndexReader reader)  {\n            return bits;\n          }\n        };\n      }\n\n      int lastDocRequested=offset+len;\n\n      // lucene doesn't allow 0 to be passed for nDocs\n      if (lastDocRequested==0) lastDocRequested=1;\n\n      // TopFieldDocs sortedDocs;  // use TopDocs so both versions can use it\n      TopDocs sortedDocs;\n      if (lsort!=null) {\n         sortedDocs = searcher.search(query, lfilter, lastDocRequested, lsort);\n      } else {\n         sortedDocs = searcher.search(query, lfilter, lastDocRequested);\n      }\n\n      int nDocsReturned = sortedDocs.scoreDocs.length;\n      int[] docs = new int[nDocsReturned];\n      for (int i=0; i<nDocsReturned; i++) {\n        docs[i] = sortedDocs.scoreDocs[i].doc;\n      }\n      float[] scores=null;\n      float maxScore=0.0f;\n      if ((flags & GET_SCORES) != 0) {\n        scores = new float[nDocsReturned];\n        for (int i=0; i<nDocsReturned; i++) {\n          scores[i] = sortedDocs.scoreDocs[i].score;\n        }\n        if (nDocsReturned>0) {\n          maxScore=sortedDocs.scoreDocs[0].score;\n        }\n      }\n      int sliceLen = Math.min(offset+len,nDocsReturned) - offset;\n      if (sliceLen < 0) sliceLen=0;\n      return new DocSlice(offset,sliceLen,docs,scores,sortedDocs.totalHits, maxScore);\n\n    **********************************************************************************/\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"73efb6b41b441344b4deb2b802aeda0e232871ce","date":1164206336,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListNC(Query,DocSet,Sort,int,int,int).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListNC(Query,DocSet,Sort,int,int,int).mjava","sourceNew":"  private DocList getDocListNC(Query query, DocSet filter, Sort lsort, int offset, int len, int flags) throws IOException {\n    final int lastDocRequested = offset+len;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final DocSet filt = filter;\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n      final int[] numHits = new int[1];\n\n      searcher.search(query, new HitCollector() {\n        public void collect(int doc, float score) {\n          if (filt!=null && !filt.exists(doc)) return;\n          numHits[0]++;\n          if (score > topscore[0]) topscore[0]=score;\n        }\n      }\n      );\n\n      nDocsReturned=0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else if (lsort != null) {\n      // can't use TopDocs if there is a sort since it\n      // will do automatic score normalization.\n      // NOTE: this changed late in Lucene 1.9\n\n      final DocSet filt = filter;\n      final int[] numHits = new int[1];\n      final FieldSortedHitQueue hq = new FieldSortedHitQueue(reader, lsort.getSort(), offset+len);\n\n      searcher.search(query, new HitCollector() {\n        public void collect(int doc, float score) {\n          if (filt!=null && !filt.exists(doc)) return;\n          numHits[0]++;\n          hq.insert(new FieldDoc(doc, score));\n        }\n      }\n      );\n\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? hq.getMaxScore() : 0.0f;\n\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (flags&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        FieldDoc fieldDoc = (FieldDoc)hq.pop();\n        // fillFields is the point where score normalization happens\n        // hq.fillFields(fieldDoc)\n        ids[i] = fieldDoc.doc;\n        if (scores != null) scores[i] = fieldDoc.score;\n      }\n    } else {\n      // No Sort specified (sort by score descending)\n      // This case could be done with TopDocs, but would currently require\n      // getting a BitSet filter from a DocSet which may be inefficient.\n\n      final DocSet filt = filter;\n      final ScorePriorityQueue hq = new ScorePriorityQueue(lastDocRequested);\n      final int[] numHits = new int[1];\n      searcher.search(query, new HitCollector() {\n        float minScore=Float.NEGATIVE_INFINITY;  // minimum score in the priority queue\n        public void collect(int doc, float score) {\n          if (filt!=null && !filt.exists(doc)) return;\n          if (numHits[0]++ < lastDocRequested || score >= minScore) {\n            // TODO: if docs are always delivered in order, we could use \"score>minScore\"\n            // instead of \"score>=minScore\" and avoid tiebreaking scores\n            // in the priority queue.\n            // but might BooleanScorer14 might still be used and deliver docs out-of-order?\n            hq.insert(new ScoreDoc(doc, score));\n            minScore = ((ScoreDoc)hq.top()).score;\n          }\n        }\n      }\n      );\n\n      totalHits = numHits[0];\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (flags&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      ScoreDoc sdoc =null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        sdoc = (ScoreDoc)hq.pop();\n        ids[i] = sdoc.doc;\n        if (scores != null) scores[i] = sdoc.score;\n      }\n      maxScore = sdoc ==null ? 0.0f : sdoc.score;\n    }\n\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned) - offset;\n    if (sliceLen < 0) sliceLen=0;\n    return new DocSlice(offset,sliceLen,ids,scores,totalHits,maxScore);\n\n\n\n    /**************** older implementation using TopDocs *******************\n\n\n      Filter lfilter=null;\n      if (filter != null) {\n        final BitSet bits = filter.getBits();   // avoid if possible\n        lfilter = new Filter() {\n          public BitSet bits(IndexReader reader)  {\n            return bits;\n          }\n        };\n      }\n\n      int lastDocRequested=offset+len;\n\n      // lucene doesn't allow 0 to be passed for nDocs\n      if (lastDocRequested==0) lastDocRequested=1;\n\n      // TopFieldDocs sortedDocs;  // use TopDocs so both versions can use it\n      TopDocs sortedDocs;\n      if (lsort!=null) {\n         sortedDocs = searcher.search(query, lfilter, lastDocRequested, lsort);\n      } else {\n         sortedDocs = searcher.search(query, lfilter, lastDocRequested);\n      }\n\n      int nDocsReturned = sortedDocs.scoreDocs.length;\n      int[] docs = new int[nDocsReturned];\n      for (int i=0; i<nDocsReturned; i++) {\n        docs[i] = sortedDocs.scoreDocs[i].doc;\n      }\n      float[] scores=null;\n      float maxScore=0.0f;\n      if ((flags & GET_SCORES) != 0) {\n        scores = new float[nDocsReturned];\n        for (int i=0; i<nDocsReturned; i++) {\n          scores[i] = sortedDocs.scoreDocs[i].score;\n        }\n        if (nDocsReturned>0) {\n          maxScore=sortedDocs.scoreDocs[0].score;\n        }\n      }\n      int sliceLen = Math.min(offset+len,nDocsReturned) - offset;\n      if (sliceLen < 0) sliceLen=0;\n      return new DocSlice(offset,sliceLen,docs,scores,sortedDocs.totalHits, maxScore);\n\n    **********************************************************************************/\n\n  }\n\n","sourceOld":"  private DocList getDocListNC(Query query, DocSet filter, Sort lsort, int offset, int len, int flags) throws IOException {\n    final int lastDocRequested = offset+len;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final DocSet filt = filter;\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n      final int[] numHits = new int[1];\n\n      searcher.search(query, new HitCollector() {\n        public void collect(int doc, float score) {\n          if (filt!=null && !filt.exists(doc)) return;\n          numHits[0]++;\n          if (score > topscore[0]) topscore[0]=score;\n        }\n      }\n      );\n\n      nDocsReturned=0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else if (lsort != null) {\n      // can't use TopDocs if there is a sort since it\n      // will do automatic score normalization.\n      // NOTE: this changed late in Lucene 1.9\n\n      final DocSet filt = filter;\n      final int[] numHits = new int[1];\n      final FieldSortedHitQueue hq = new FieldSortedHitQueue(reader, lsort.getSort(), offset+len);\n\n      searcher.search(query, new HitCollector() {\n        public void collect(int doc, float score) {\n          if (filt!=null && !filt.exists(doc)) return;\n          numHits[0]++;\n          hq.insert(new FieldDoc(doc, score));\n        }\n      }\n      );\n\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? hq.getMaxScore() : 0.0f;\n\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (flags&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        FieldDoc fieldDoc = (FieldDoc)hq.pop();\n        // fillFields is the point where score normalization happens\n        // hq.fillFields(fieldDoc)\n        ids[i] = fieldDoc.doc;\n        if (scores != null) scores[i] = fieldDoc.score;\n      }\n    } else {\n      // No Sort specified (sort by score descending)\n      // This case could be done with TopDocs, but would currently require\n      // getting a BitSet filter from a DocSet which may be inefficient.\n\n      final DocSet filt = filter;\n      final ScorePriorityQueue hq = new ScorePriorityQueue(lastDocRequested);\n      final int[] numHits = new int[1];\n      searcher.search(query, new HitCollector() {\n        float minScore=Float.NEGATIVE_INFINITY;  // minimum score in the priority queue\n        public void collect(int doc, float score) {\n          if (filt!=null && !filt.exists(doc)) return;\n          if (numHits[0]++ < lastDocRequested || score >= minScore) {\n            // if docs are always delivered in order, we could use \"score>minScore\"\n            // but might BooleanScorer14 might still be used and deliver docs out-of-order?\n            hq.insert(new ScoreDoc(doc, score));\n            minScore = ((ScoreDoc)hq.top()).score;\n          }\n        }\n      }\n      );\n\n      totalHits = numHits[0];\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (flags&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      ScoreDoc sdoc =null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        sdoc = (ScoreDoc)hq.pop();\n        ids[i] = sdoc.doc;\n        if (scores != null) scores[i] = sdoc.score;\n      }\n      maxScore = sdoc ==null ? 0.0f : sdoc.score;\n    }\n\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned) - offset;\n    if (sliceLen < 0) sliceLen=0;\n    return new DocSlice(offset,sliceLen,ids,scores,totalHits,maxScore);\n\n\n\n    /**************** older implementation using TopDocs *******************\n\n\n      Filter lfilter=null;\n      if (filter != null) {\n        final BitSet bits = filter.getBits();   // avoid if possible\n        lfilter = new Filter() {\n          public BitSet bits(IndexReader reader)  {\n            return bits;\n          }\n        };\n      }\n\n      int lastDocRequested=offset+len;\n\n      // lucene doesn't allow 0 to be passed for nDocs\n      if (lastDocRequested==0) lastDocRequested=1;\n\n      // TopFieldDocs sortedDocs;  // use TopDocs so both versions can use it\n      TopDocs sortedDocs;\n      if (lsort!=null) {\n         sortedDocs = searcher.search(query, lfilter, lastDocRequested, lsort);\n      } else {\n         sortedDocs = searcher.search(query, lfilter, lastDocRequested);\n      }\n\n      int nDocsReturned = sortedDocs.scoreDocs.length;\n      int[] docs = new int[nDocsReturned];\n      for (int i=0; i<nDocsReturned; i++) {\n        docs[i] = sortedDocs.scoreDocs[i].doc;\n      }\n      float[] scores=null;\n      float maxScore=0.0f;\n      if ((flags & GET_SCORES) != 0) {\n        scores = new float[nDocsReturned];\n        for (int i=0; i<nDocsReturned; i++) {\n          scores[i] = sortedDocs.scoreDocs[i].score;\n        }\n        if (nDocsReturned>0) {\n          maxScore=sortedDocs.scoreDocs[0].score;\n        }\n      }\n      int sliceLen = Math.min(offset+len,nDocsReturned) - offset;\n      if (sliceLen < 0) sliceLen=0;\n      return new DocSlice(offset,sliceLen,docs,scores,sortedDocs.totalHits, maxScore);\n\n    **********************************************************************************/\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b1940b60224897131cf61bb615e02af1b26558c8","date":1169501002,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListNC(Query,DocSet,Sort,int,int,int).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListNC(Query,DocSet,Sort,int,int,int).mjava","sourceNew":"  private DocList getDocListNC(Query query, DocSet filter, Sort lsort, int offset, int len, int flags) throws IOException {\n    final int lastDocRequested = offset+len;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n\n    query = QueryUtils.makeQueryable(query);\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final DocSet filt = filter;\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n      final int[] numHits = new int[1];\n\n      searcher.search(query, new HitCollector() {\n        public void collect(int doc, float score) {\n          if (filt!=null && !filt.exists(doc)) return;\n          numHits[0]++;\n          if (score > topscore[0]) topscore[0]=score;\n        }\n      }\n      );\n\n      nDocsReturned=0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else if (lsort != null) {\n      // can't use TopDocs if there is a sort since it\n      // will do automatic score normalization.\n      // NOTE: this changed late in Lucene 1.9\n\n      final DocSet filt = filter;\n      final int[] numHits = new int[1];\n      final FieldSortedHitQueue hq = new FieldSortedHitQueue(reader, lsort.getSort(), offset+len);\n\n      searcher.search(query, new HitCollector() {\n        public void collect(int doc, float score) {\n          if (filt!=null && !filt.exists(doc)) return;\n          numHits[0]++;\n          hq.insert(new FieldDoc(doc, score));\n        }\n      }\n      );\n\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? hq.getMaxScore() : 0.0f;\n\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (flags&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        FieldDoc fieldDoc = (FieldDoc)hq.pop();\n        // fillFields is the point where score normalization happens\n        // hq.fillFields(fieldDoc)\n        ids[i] = fieldDoc.doc;\n        if (scores != null) scores[i] = fieldDoc.score;\n      }\n    } else {\n      // No Sort specified (sort by score descending)\n      // This case could be done with TopDocs, but would currently require\n      // getting a BitSet filter from a DocSet which may be inefficient.\n\n      final DocSet filt = filter;\n      final ScorePriorityQueue hq = new ScorePriorityQueue(lastDocRequested);\n      final int[] numHits = new int[1];\n      searcher.search(query, new HitCollector() {\n        float minScore=Float.NEGATIVE_INFINITY;  // minimum score in the priority queue\n        public void collect(int doc, float score) {\n          if (filt!=null && !filt.exists(doc)) return;\n          if (numHits[0]++ < lastDocRequested || score >= minScore) {\n            // TODO: if docs are always delivered in order, we could use \"score>minScore\"\n            // instead of \"score>=minScore\" and avoid tiebreaking scores\n            // in the priority queue.\n            // but might BooleanScorer14 might still be used and deliver docs out-of-order?\n            hq.insert(new ScoreDoc(doc, score));\n            minScore = ((ScoreDoc)hq.top()).score;\n          }\n        }\n      }\n      );\n\n      totalHits = numHits[0];\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (flags&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      ScoreDoc sdoc =null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        sdoc = (ScoreDoc)hq.pop();\n        ids[i] = sdoc.doc;\n        if (scores != null) scores[i] = sdoc.score;\n      }\n      maxScore = sdoc ==null ? 0.0f : sdoc.score;\n    }\n\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned) - offset;\n    if (sliceLen < 0) sliceLen=0;\n    return new DocSlice(offset,sliceLen,ids,scores,totalHits,maxScore);\n\n\n\n    /**************** older implementation using TopDocs *******************\n\n\n      Filter lfilter=null;\n      if (filter != null) {\n        final BitSet bits = filter.getBits();   // avoid if possible\n        lfilter = new Filter() {\n          public BitSet bits(IndexReader reader)  {\n            return bits;\n          }\n        };\n      }\n\n      int lastDocRequested=offset+len;\n\n      // lucene doesn't allow 0 to be passed for nDocs\n      if (lastDocRequested==0) lastDocRequested=1;\n\n      // TopFieldDocs sortedDocs;  // use TopDocs so both versions can use it\n      TopDocs sortedDocs;\n      if (lsort!=null) {\n         sortedDocs = searcher.search(query, lfilter, lastDocRequested, lsort);\n      } else {\n         sortedDocs = searcher.search(query, lfilter, lastDocRequested);\n      }\n\n      int nDocsReturned = sortedDocs.scoreDocs.length;\n      int[] docs = new int[nDocsReturned];\n      for (int i=0; i<nDocsReturned; i++) {\n        docs[i] = sortedDocs.scoreDocs[i].doc;\n      }\n      float[] scores=null;\n      float maxScore=0.0f;\n      if ((flags & GET_SCORES) != 0) {\n        scores = new float[nDocsReturned];\n        for (int i=0; i<nDocsReturned; i++) {\n          scores[i] = sortedDocs.scoreDocs[i].score;\n        }\n        if (nDocsReturned>0) {\n          maxScore=sortedDocs.scoreDocs[0].score;\n        }\n      }\n      int sliceLen = Math.min(offset+len,nDocsReturned) - offset;\n      if (sliceLen < 0) sliceLen=0;\n      return new DocSlice(offset,sliceLen,docs,scores,sortedDocs.totalHits, maxScore);\n\n    **********************************************************************************/\n\n  }\n\n","sourceOld":"  private DocList getDocListNC(Query query, DocSet filter, Sort lsort, int offset, int len, int flags) throws IOException {\n    final int lastDocRequested = offset+len;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final DocSet filt = filter;\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n      final int[] numHits = new int[1];\n\n      searcher.search(query, new HitCollector() {\n        public void collect(int doc, float score) {\n          if (filt!=null && !filt.exists(doc)) return;\n          numHits[0]++;\n          if (score > topscore[0]) topscore[0]=score;\n        }\n      }\n      );\n\n      nDocsReturned=0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else if (lsort != null) {\n      // can't use TopDocs if there is a sort since it\n      // will do automatic score normalization.\n      // NOTE: this changed late in Lucene 1.9\n\n      final DocSet filt = filter;\n      final int[] numHits = new int[1];\n      final FieldSortedHitQueue hq = new FieldSortedHitQueue(reader, lsort.getSort(), offset+len);\n\n      searcher.search(query, new HitCollector() {\n        public void collect(int doc, float score) {\n          if (filt!=null && !filt.exists(doc)) return;\n          numHits[0]++;\n          hq.insert(new FieldDoc(doc, score));\n        }\n      }\n      );\n\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? hq.getMaxScore() : 0.0f;\n\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (flags&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        FieldDoc fieldDoc = (FieldDoc)hq.pop();\n        // fillFields is the point where score normalization happens\n        // hq.fillFields(fieldDoc)\n        ids[i] = fieldDoc.doc;\n        if (scores != null) scores[i] = fieldDoc.score;\n      }\n    } else {\n      // No Sort specified (sort by score descending)\n      // This case could be done with TopDocs, but would currently require\n      // getting a BitSet filter from a DocSet which may be inefficient.\n\n      final DocSet filt = filter;\n      final ScorePriorityQueue hq = new ScorePriorityQueue(lastDocRequested);\n      final int[] numHits = new int[1];\n      searcher.search(query, new HitCollector() {\n        float minScore=Float.NEGATIVE_INFINITY;  // minimum score in the priority queue\n        public void collect(int doc, float score) {\n          if (filt!=null && !filt.exists(doc)) return;\n          if (numHits[0]++ < lastDocRequested || score >= minScore) {\n            // TODO: if docs are always delivered in order, we could use \"score>minScore\"\n            // instead of \"score>=minScore\" and avoid tiebreaking scores\n            // in the priority queue.\n            // but might BooleanScorer14 might still be used and deliver docs out-of-order?\n            hq.insert(new ScoreDoc(doc, score));\n            minScore = ((ScoreDoc)hq.top()).score;\n          }\n        }\n      }\n      );\n\n      totalHits = numHits[0];\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (flags&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      ScoreDoc sdoc =null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        sdoc = (ScoreDoc)hq.pop();\n        ids[i] = sdoc.doc;\n        if (scores != null) scores[i] = sdoc.score;\n      }\n      maxScore = sdoc ==null ? 0.0f : sdoc.score;\n    }\n\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned) - offset;\n    if (sliceLen < 0) sliceLen=0;\n    return new DocSlice(offset,sliceLen,ids,scores,totalHits,maxScore);\n\n\n\n    /**************** older implementation using TopDocs *******************\n\n\n      Filter lfilter=null;\n      if (filter != null) {\n        final BitSet bits = filter.getBits();   // avoid if possible\n        lfilter = new Filter() {\n          public BitSet bits(IndexReader reader)  {\n            return bits;\n          }\n        };\n      }\n\n      int lastDocRequested=offset+len;\n\n      // lucene doesn't allow 0 to be passed for nDocs\n      if (lastDocRequested==0) lastDocRequested=1;\n\n      // TopFieldDocs sortedDocs;  // use TopDocs so both versions can use it\n      TopDocs sortedDocs;\n      if (lsort!=null) {\n         sortedDocs = searcher.search(query, lfilter, lastDocRequested, lsort);\n      } else {\n         sortedDocs = searcher.search(query, lfilter, lastDocRequested);\n      }\n\n      int nDocsReturned = sortedDocs.scoreDocs.length;\n      int[] docs = new int[nDocsReturned];\n      for (int i=0; i<nDocsReturned; i++) {\n        docs[i] = sortedDocs.scoreDocs[i].doc;\n      }\n      float[] scores=null;\n      float maxScore=0.0f;\n      if ((flags & GET_SCORES) != 0) {\n        scores = new float[nDocsReturned];\n        for (int i=0; i<nDocsReturned; i++) {\n          scores[i] = sortedDocs.scoreDocs[i].score;\n        }\n        if (nDocsReturned>0) {\n          maxScore=sortedDocs.scoreDocs[0].score;\n        }\n      }\n      int sliceLen = Math.min(offset+len,nDocsReturned) - offset;\n      if (sliceLen < 0) sliceLen=0;\n      return new DocSlice(offset,sliceLen,docs,scores,sortedDocs.totalHits, maxScore);\n\n    **********************************************************************************/\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"30c238dad8c4234f556cd28cd22ff426247e70c4","date":1195490330,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListNC(Query,DocSet,Sort,int,int,int).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListNC(Query,DocSet,Sort,int,int,int).mjava","sourceNew":"  private DocList getDocListNC(Query query, DocSet filter, Sort lsort, int offset, int len, int flags) throws IOException {\n    int last = offset+len;\n    if (last < 0 || last > maxDoc()) last=maxDoc();\n    final int lastDocRequested = last;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n\n    query = QueryUtils.makeQueryable(query);\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final DocSet filt = filter;\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n      final int[] numHits = new int[1];\n\n      searcher.search(query, new HitCollector() {\n        public void collect(int doc, float score) {\n          if (filt!=null && !filt.exists(doc)) return;\n          numHits[0]++;\n          if (score > topscore[0]) topscore[0]=score;\n        }\n      }\n      );\n\n      nDocsReturned=0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else if (lsort != null) {\n      // can't use TopDocs if there is a sort since it\n      // will do automatic score normalization.\n      // NOTE: this changed late in Lucene 1.9\n\n      final DocSet filt = filter;\n      final int[] numHits = new int[1];\n      final FieldSortedHitQueue hq = new FieldSortedHitQueue(reader, lsort.getSort(), offset+len);\n\n      searcher.search(query, new HitCollector() {\n        public void collect(int doc, float score) {\n          if (filt!=null && !filt.exists(doc)) return;\n          numHits[0]++;\n          hq.insert(new FieldDoc(doc, score));\n        }\n      }\n      );\n\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? hq.getMaxScore() : 0.0f;\n\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (flags&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        FieldDoc fieldDoc = (FieldDoc)hq.pop();\n        // fillFields is the point where score normalization happens\n        // hq.fillFields(fieldDoc)\n        ids[i] = fieldDoc.doc;\n        if (scores != null) scores[i] = fieldDoc.score;\n      }\n    } else {\n      // No Sort specified (sort by score descending)\n      // This case could be done with TopDocs, but would currently require\n      // getting a BitSet filter from a DocSet which may be inefficient.\n\n      final DocSet filt = filter;\n      final ScorePriorityQueue hq = new ScorePriorityQueue(lastDocRequested);\n      final int[] numHits = new int[1];\n      searcher.search(query, new HitCollector() {\n        float minScore=Float.NEGATIVE_INFINITY;  // minimum score in the priority queue\n        public void collect(int doc, float score) {\n          if (filt!=null && !filt.exists(doc)) return;\n          if (numHits[0]++ < lastDocRequested || score >= minScore) {\n            // TODO: if docs are always delivered in order, we could use \"score>minScore\"\n            // instead of \"score>=minScore\" and avoid tiebreaking scores\n            // in the priority queue.\n            // but might BooleanScorer14 might still be used and deliver docs out-of-order?\n            hq.insert(new ScoreDoc(doc, score));\n            minScore = ((ScoreDoc)hq.top()).score;\n          }\n        }\n      }\n      );\n\n      totalHits = numHits[0];\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (flags&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      ScoreDoc sdoc =null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        sdoc = (ScoreDoc)hq.pop();\n        ids[i] = sdoc.doc;\n        if (scores != null) scores[i] = sdoc.score;\n      }\n      maxScore = sdoc ==null ? 0.0f : sdoc.score;\n    }\n\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned) - offset;\n    if (sliceLen < 0) sliceLen=0;\n    return new DocSlice(offset,sliceLen,ids,scores,totalHits,maxScore);\n\n\n\n    /**************** older implementation using TopDocs *******************\n\n\n      Filter lfilter=null;\n      if (filter != null) {\n        final BitSet bits = filter.getBits();   // avoid if possible\n        lfilter = new Filter() {\n          public BitSet bits(IndexReader reader)  {\n            return bits;\n          }\n        };\n      }\n\n      int lastDocRequested=offset+len;\n\n      // lucene doesn't allow 0 to be passed for nDocs\n      if (lastDocRequested==0) lastDocRequested=1;\n\n      // TopFieldDocs sortedDocs;  // use TopDocs so both versions can use it\n      TopDocs sortedDocs;\n      if (lsort!=null) {\n         sortedDocs = searcher.search(query, lfilter, lastDocRequested, lsort);\n      } else {\n         sortedDocs = searcher.search(query, lfilter, lastDocRequested);\n      }\n\n      int nDocsReturned = sortedDocs.scoreDocs.length;\n      int[] docs = new int[nDocsReturned];\n      for (int i=0; i<nDocsReturned; i++) {\n        docs[i] = sortedDocs.scoreDocs[i].doc;\n      }\n      float[] scores=null;\n      float maxScore=0.0f;\n      if ((flags & GET_SCORES) != 0) {\n        scores = new float[nDocsReturned];\n        for (int i=0; i<nDocsReturned; i++) {\n          scores[i] = sortedDocs.scoreDocs[i].score;\n        }\n        if (nDocsReturned>0) {\n          maxScore=sortedDocs.scoreDocs[0].score;\n        }\n      }\n      int sliceLen = Math.min(offset+len,nDocsReturned) - offset;\n      if (sliceLen < 0) sliceLen=0;\n      return new DocSlice(offset,sliceLen,docs,scores,sortedDocs.totalHits, maxScore);\n\n    **********************************************************************************/\n\n  }\n\n","sourceOld":"  private DocList getDocListNC(Query query, DocSet filter, Sort lsort, int offset, int len, int flags) throws IOException {\n    final int lastDocRequested = offset+len;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n\n    query = QueryUtils.makeQueryable(query);\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final DocSet filt = filter;\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n      final int[] numHits = new int[1];\n\n      searcher.search(query, new HitCollector() {\n        public void collect(int doc, float score) {\n          if (filt!=null && !filt.exists(doc)) return;\n          numHits[0]++;\n          if (score > topscore[0]) topscore[0]=score;\n        }\n      }\n      );\n\n      nDocsReturned=0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else if (lsort != null) {\n      // can't use TopDocs if there is a sort since it\n      // will do automatic score normalization.\n      // NOTE: this changed late in Lucene 1.9\n\n      final DocSet filt = filter;\n      final int[] numHits = new int[1];\n      final FieldSortedHitQueue hq = new FieldSortedHitQueue(reader, lsort.getSort(), offset+len);\n\n      searcher.search(query, new HitCollector() {\n        public void collect(int doc, float score) {\n          if (filt!=null && !filt.exists(doc)) return;\n          numHits[0]++;\n          hq.insert(new FieldDoc(doc, score));\n        }\n      }\n      );\n\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? hq.getMaxScore() : 0.0f;\n\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (flags&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        FieldDoc fieldDoc = (FieldDoc)hq.pop();\n        // fillFields is the point where score normalization happens\n        // hq.fillFields(fieldDoc)\n        ids[i] = fieldDoc.doc;\n        if (scores != null) scores[i] = fieldDoc.score;\n      }\n    } else {\n      // No Sort specified (sort by score descending)\n      // This case could be done with TopDocs, but would currently require\n      // getting a BitSet filter from a DocSet which may be inefficient.\n\n      final DocSet filt = filter;\n      final ScorePriorityQueue hq = new ScorePriorityQueue(lastDocRequested);\n      final int[] numHits = new int[1];\n      searcher.search(query, new HitCollector() {\n        float minScore=Float.NEGATIVE_INFINITY;  // minimum score in the priority queue\n        public void collect(int doc, float score) {\n          if (filt!=null && !filt.exists(doc)) return;\n          if (numHits[0]++ < lastDocRequested || score >= minScore) {\n            // TODO: if docs are always delivered in order, we could use \"score>minScore\"\n            // instead of \"score>=minScore\" and avoid tiebreaking scores\n            // in the priority queue.\n            // but might BooleanScorer14 might still be used and deliver docs out-of-order?\n            hq.insert(new ScoreDoc(doc, score));\n            minScore = ((ScoreDoc)hq.top()).score;\n          }\n        }\n      }\n      );\n\n      totalHits = numHits[0];\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (flags&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      ScoreDoc sdoc =null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        sdoc = (ScoreDoc)hq.pop();\n        ids[i] = sdoc.doc;\n        if (scores != null) scores[i] = sdoc.score;\n      }\n      maxScore = sdoc ==null ? 0.0f : sdoc.score;\n    }\n\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned) - offset;\n    if (sliceLen < 0) sliceLen=0;\n    return new DocSlice(offset,sliceLen,ids,scores,totalHits,maxScore);\n\n\n\n    /**************** older implementation using TopDocs *******************\n\n\n      Filter lfilter=null;\n      if (filter != null) {\n        final BitSet bits = filter.getBits();   // avoid if possible\n        lfilter = new Filter() {\n          public BitSet bits(IndexReader reader)  {\n            return bits;\n          }\n        };\n      }\n\n      int lastDocRequested=offset+len;\n\n      // lucene doesn't allow 0 to be passed for nDocs\n      if (lastDocRequested==0) lastDocRequested=1;\n\n      // TopFieldDocs sortedDocs;  // use TopDocs so both versions can use it\n      TopDocs sortedDocs;\n      if (lsort!=null) {\n         sortedDocs = searcher.search(query, lfilter, lastDocRequested, lsort);\n      } else {\n         sortedDocs = searcher.search(query, lfilter, lastDocRequested);\n      }\n\n      int nDocsReturned = sortedDocs.scoreDocs.length;\n      int[] docs = new int[nDocsReturned];\n      for (int i=0; i<nDocsReturned; i++) {\n        docs[i] = sortedDocs.scoreDocs[i].doc;\n      }\n      float[] scores=null;\n      float maxScore=0.0f;\n      if ((flags & GET_SCORES) != 0) {\n        scores = new float[nDocsReturned];\n        for (int i=0; i<nDocsReturned; i++) {\n          scores[i] = sortedDocs.scoreDocs[i].score;\n        }\n        if (nDocsReturned>0) {\n          maxScore=sortedDocs.scoreDocs[0].score;\n        }\n      }\n      int sliceLen = Math.min(offset+len,nDocsReturned) - offset;\n      if (sliceLen < 0) sliceLen=0;\n      return new DocSlice(offset,sliceLen,docs,scores,sortedDocs.totalHits, maxScore);\n\n    **********************************************************************************/\n\n  }\n\n","bugFix":["0c3e228bf650e96f3002a8fb73dd0c13d55af077"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"db25c1f61b5ae826f10777da6551a832703967d5","date":1215306972,"type":5,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListNC(QueryResult,QueryCommand).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListNC(Query,DocSet,Sort,int,int,int).mjava","sourceNew":"  private void getDocListNC(QueryResult qr,QueryCommand cmd) throws IOException {\n    //Parameters: cmd.getQuery(),theFilt,cmd.getSort(),0,supersetMaxDoc,cmd.getFlags(),cmd.getTimeAllowed(),responseHeader);\n    //Query query, DocSet filter, Sort lsort, int offset, int len, int flags, long timeAllowed, NamedList<Object> responseHeader\n    DocSet filter = cmd.getFilter()!=null ? cmd.getFilter() : getDocSet(cmd.getFilterList());\n    final long timeAllowed = cmd.getTimeAllowed();\n    int len = cmd.getSupersetMaxDoc();\n    int last = len;\n    if (last < 0 || last > maxDoc()) last=maxDoc();\n    final int lastDocRequested = last;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n\n    Query query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final DocSet filt = filter;\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n      final int[] numHits = new int[1];\n\n      HitCollector hc = new HitCollector() {\n        public void collect(int doc, float score) {\n          if (filt!=null && !filt.exists(doc)) return;\n          numHits[0]++;\n          if (score > topscore[0]) topscore[0]=score;\n        }\n      };\n      if( timeAllowed > 0 ) {\n        hc = new TimeLimitedCollector( hc, timeAllowed );\n      }\n      try {\n        searcher.search(query, hc );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warning( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      nDocsReturned=0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else if (cmd.getSort() != null) {\n      // can't use TopDocs if there is a sort since it\n      // will do automatic score normalization.\n      // NOTE: this changed late in Lucene 1.9\n\n      final DocSet filt = filter;\n      final int[] numHits = new int[1];\n      final FieldSortedHitQueue hq = new FieldSortedHitQueue(reader, cmd.getSort().getSort(), len);\n\n      HitCollector hc = new HitCollector() {\n        public void collect(int doc, float score) {\n          if (filt!=null && !filt.exists(doc)) return;\n          numHits[0]++;\n          hq.insert(new FieldDoc(doc, score));\n        }\n      };\n      if( timeAllowed > 0 ) {\n        hc = new TimeLimitedCollector( hc, timeAllowed );\n      }\n      try {\n        searcher.search(query, hc );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warning( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? hq.getMaxScore() : 0.0f;\n\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        FieldDoc fieldDoc = (FieldDoc)hq.pop();\n        // fillFields is the point where score normalization happens\n        // hq.fillFields(fieldDoc)\n        ids[i] = fieldDoc.doc;\n        if (scores != null) scores[i] = fieldDoc.score;\n      }\n    } else {\n      // No Sort specified (sort by score descending)\n      // This case could be done with TopDocs, but would currently require\n      // getting a BitSet filter from a DocSet which may be inefficient.\n\n      final DocSet filt = filter;\n      final ScorePriorityQueue hq = new ScorePriorityQueue(lastDocRequested);\n      final int[] numHits = new int[1];\n      HitCollector hc = new HitCollector() {\n        float minScore=Float.NEGATIVE_INFINITY;  // minimum score in the priority queue\n        public void collect(int doc, float score) {\n          if (filt!=null && !filt.exists(doc)) return;\n          if (numHits[0]++ < lastDocRequested || score >= minScore) {\n            // TODO: if docs are always delivered in order, we could use \"score>minScore\"\n            // instead of \"score>=minScore\" and avoid tiebreaking scores\n            // in the priority queue.\n            // but might BooleanScorer14 might still be used and deliver docs out-of-order?\n            hq.insert(new ScoreDoc(doc, score));\n            minScore = ((ScoreDoc)hq.top()).score;\n          }\n        }\n      };\n      if( timeAllowed > 0 ) {\n        hc = new TimeLimitedCollector( hc, timeAllowed );\n      }\n      try {\n        searcher.search(query, hc );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warning( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      totalHits = numHits[0];\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      ScoreDoc sdoc =null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        sdoc = (ScoreDoc)hq.pop();\n        ids[i] = sdoc.doc;\n        if (scores != null) scores[i] = sdoc.score;\n      }\n      maxScore = sdoc ==null ? 0.0f : sdoc.score;\n    }\n\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned);\n    if (sliceLen < 0) sliceLen=0;\n    qr.setDocList(new DocSlice(0,sliceLen,ids,scores,totalHits,maxScore));\n\n\n\n    /**************** older implementation using TopDocs *******************\n\n\n      Filter lfilter=null;\n      if (filter != null) {\n        final BitSet bits = filter.getBits();   // avoid if possible\n        lfilter = new Filter() {\n          public BitSet bits(IndexReader reader)  {\n            return bits;\n          }\n        };\n      }\n\n      int lastDocRequested=offset+len;\n\n      // lucene doesn't allow 0 to be passed for nDocs\n      if (lastDocRequested==0) lastDocRequested=1;\n\n      // TopFieldDocs sortedDocs;  // use TopDocs so both versions can use it\n      TopDocs sortedDocs;\n      if (lsort!=null) {\n         sortedDocs = searcher.search(query, lfilter, lastDocRequested, lsort);\n      } else {\n         sortedDocs = searcher.search(query, lfilter, lastDocRequested);\n      }\n\n      int nDocsReturned = sortedDocs.scoreDocs.length;\n      int[] docs = new int[nDocsReturned];\n      for (int i=0; i<nDocsReturned; i++) {\n        docs[i] = sortedDocs.scoreDocs[i].doc;\n      }\n      float[] scores=null;\n      float maxScore=0.0f;\n      if ((flags & GET_SCORES) != 0) {\n        scores = new float[nDocsReturned];\n        for (int i=0; i<nDocsReturned; i++) {\n          scores[i] = sortedDocs.scoreDocs[i].score;\n        }\n        if (nDocsReturned>0) {\n          maxScore=sortedDocs.scoreDocs[0].score;\n        }\n      }\n      int sliceLen = Math.min(offset+len,nDocsReturned) - offset;\n      if (sliceLen < 0) sliceLen=0;\n      return new DocSlice(offset,sliceLen,docs,scores,sortedDocs.totalHits, maxScore);\n\n    **********************************************************************************/\n\n  }\n\n","sourceOld":"  private DocList getDocListNC(Query query, DocSet filter, Sort lsort, int offset, int len, int flags) throws IOException {\n    int last = offset+len;\n    if (last < 0 || last > maxDoc()) last=maxDoc();\n    final int lastDocRequested = last;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n\n    query = QueryUtils.makeQueryable(query);\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final DocSet filt = filter;\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n      final int[] numHits = new int[1];\n\n      searcher.search(query, new HitCollector() {\n        public void collect(int doc, float score) {\n          if (filt!=null && !filt.exists(doc)) return;\n          numHits[0]++;\n          if (score > topscore[0]) topscore[0]=score;\n        }\n      }\n      );\n\n      nDocsReturned=0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else if (lsort != null) {\n      // can't use TopDocs if there is a sort since it\n      // will do automatic score normalization.\n      // NOTE: this changed late in Lucene 1.9\n\n      final DocSet filt = filter;\n      final int[] numHits = new int[1];\n      final FieldSortedHitQueue hq = new FieldSortedHitQueue(reader, lsort.getSort(), offset+len);\n\n      searcher.search(query, new HitCollector() {\n        public void collect(int doc, float score) {\n          if (filt!=null && !filt.exists(doc)) return;\n          numHits[0]++;\n          hq.insert(new FieldDoc(doc, score));\n        }\n      }\n      );\n\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? hq.getMaxScore() : 0.0f;\n\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (flags&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        FieldDoc fieldDoc = (FieldDoc)hq.pop();\n        // fillFields is the point where score normalization happens\n        // hq.fillFields(fieldDoc)\n        ids[i] = fieldDoc.doc;\n        if (scores != null) scores[i] = fieldDoc.score;\n      }\n    } else {\n      // No Sort specified (sort by score descending)\n      // This case could be done with TopDocs, but would currently require\n      // getting a BitSet filter from a DocSet which may be inefficient.\n\n      final DocSet filt = filter;\n      final ScorePriorityQueue hq = new ScorePriorityQueue(lastDocRequested);\n      final int[] numHits = new int[1];\n      searcher.search(query, new HitCollector() {\n        float minScore=Float.NEGATIVE_INFINITY;  // minimum score in the priority queue\n        public void collect(int doc, float score) {\n          if (filt!=null && !filt.exists(doc)) return;\n          if (numHits[0]++ < lastDocRequested || score >= minScore) {\n            // TODO: if docs are always delivered in order, we could use \"score>minScore\"\n            // instead of \"score>=minScore\" and avoid tiebreaking scores\n            // in the priority queue.\n            // but might BooleanScorer14 might still be used and deliver docs out-of-order?\n            hq.insert(new ScoreDoc(doc, score));\n            minScore = ((ScoreDoc)hq.top()).score;\n          }\n        }\n      }\n      );\n\n      totalHits = numHits[0];\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (flags&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      ScoreDoc sdoc =null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        sdoc = (ScoreDoc)hq.pop();\n        ids[i] = sdoc.doc;\n        if (scores != null) scores[i] = sdoc.score;\n      }\n      maxScore = sdoc ==null ? 0.0f : sdoc.score;\n    }\n\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned) - offset;\n    if (sliceLen < 0) sliceLen=0;\n    return new DocSlice(offset,sliceLen,ids,scores,totalHits,maxScore);\n\n\n\n    /**************** older implementation using TopDocs *******************\n\n\n      Filter lfilter=null;\n      if (filter != null) {\n        final BitSet bits = filter.getBits();   // avoid if possible\n        lfilter = new Filter() {\n          public BitSet bits(IndexReader reader)  {\n            return bits;\n          }\n        };\n      }\n\n      int lastDocRequested=offset+len;\n\n      // lucene doesn't allow 0 to be passed for nDocs\n      if (lastDocRequested==0) lastDocRequested=1;\n\n      // TopFieldDocs sortedDocs;  // use TopDocs so both versions can use it\n      TopDocs sortedDocs;\n      if (lsort!=null) {\n         sortedDocs = searcher.search(query, lfilter, lastDocRequested, lsort);\n      } else {\n         sortedDocs = searcher.search(query, lfilter, lastDocRequested);\n      }\n\n      int nDocsReturned = sortedDocs.scoreDocs.length;\n      int[] docs = new int[nDocsReturned];\n      for (int i=0; i<nDocsReturned; i++) {\n        docs[i] = sortedDocs.scoreDocs[i].doc;\n      }\n      float[] scores=null;\n      float maxScore=0.0f;\n      if ((flags & GET_SCORES) != 0) {\n        scores = new float[nDocsReturned];\n        for (int i=0; i<nDocsReturned; i++) {\n          scores[i] = sortedDocs.scoreDocs[i].score;\n        }\n        if (nDocsReturned>0) {\n          maxScore=sortedDocs.scoreDocs[0].score;\n        }\n      }\n      int sliceLen = Math.min(offset+len,nDocsReturned) - offset;\n      if (sliceLen < 0) sliceLen=0;\n      return new DocSlice(offset,sliceLen,docs,scores,sortedDocs.totalHits, maxScore);\n\n    **********************************************************************************/\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"30c238dad8c4234f556cd28cd22ff426247e70c4":["b1940b60224897131cf61bb615e02af1b26558c8"],"73efb6b41b441344b4deb2b802aeda0e232871ce":["6079dc4b996dfa326ce75a4fdd0422a4db7720c8"],"0c3e228bf650e96f3002a8fb73dd0c13d55af077":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b1940b60224897131cf61bb615e02af1b26558c8":["73efb6b41b441344b4deb2b802aeda0e232871ce"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":[],"db25c1f61b5ae826f10777da6551a832703967d5":["30c238dad8c4234f556cd28cd22ff426247e70c4"],"6079dc4b996dfa326ce75a4fdd0422a4db7720c8":["0c3e228bf650e96f3002a8fb73dd0c13d55af077"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"30c238dad8c4234f556cd28cd22ff426247e70c4":["db25c1f61b5ae826f10777da6551a832703967d5"],"73efb6b41b441344b4deb2b802aeda0e232871ce":["b1940b60224897131cf61bb615e02af1b26558c8"],"0c3e228bf650e96f3002a8fb73dd0c13d55af077":["6079dc4b996dfa326ce75a4fdd0422a4db7720c8"],"b1940b60224897131cf61bb615e02af1b26558c8":["30c238dad8c4234f556cd28cd22ff426247e70c4"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":["0c3e228bf650e96f3002a8fb73dd0c13d55af077"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"db25c1f61b5ae826f10777da6551a832703967d5":[],"6079dc4b996dfa326ce75a4fdd0422a4db7720c8":["73efb6b41b441344b4deb2b802aeda0e232871ce"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["db25c1f61b5ae826f10777da6551a832703967d5","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}