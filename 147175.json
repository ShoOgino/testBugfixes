{"path":"lucene/core/src/test/org/apache/lucene/index/TestPostingsFormat#buildIndex(Directory,IndexOptions,boolean).mjava","commits":[{"id":"e885d2b1e112b1d9db6a2dae82b3b493dfba1df1","date":1342716838,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsFormat#buildIndex(Directory,IndexOptions,boolean).mjava","pathOld":"/dev/null","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", 1+maxDocID, false, Codec.getDefault(), null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // nocommit use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[random().nextInt(1+maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValues.Type.FIXED_INTS_8,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         32, null, IOContext.DEFAULT);\n\n    FieldsConsumer fieldsConsumer = Codec.getDefault().postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,List<Posting>> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field);\n      }\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDocID+1);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        List<Posting> postings = termEnt.getValue();\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.size());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docCount = 0;\n        for(Posting posting : postings) {\n          if (VERBOSE) {\n            System.out.println(\"    \" + docCount + \": docID=\" + posting.docID + \" freq=\" + posting.positions.size());\n          }\n          postingsConsumer.startDoc(posting.docID, posting.positions.size());\n          seenDocs.set(posting.docID);\n          if (doPos) {\n            totalTF += posting.positions.size();\n            for(Position pos : posting.positions) {\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos.position + \" payload=\" + (pos.payload == null ? \"null\" : pos.payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos.position);\n                }\n              }\n              postingsConsumer.addPosition(pos.position, (doPayloads && pos.payload != null) ? new BytesRef(pos.payload) : null, pos.startOffset, pos.endOffset);\n            }\n          } else if (doFreq) {\n            totalTF += posting.positions.size();\n          } else {\n            totalTF++;\n          }\n          docCount++;\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.size(), totalTF));\n        sumTotalTF += totalTF;\n        sumDF += postings.size();\n      }\n\n      termsConsumer.finish(sumTotalTF, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT, 1);\n\n    return Codec.getDefault().postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a434b20f16e875530b6c86775a3efaf148056c3b","date":1342877476,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsFormat#buildIndex(Directory,IndexOptions,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsFormat#buildIndex(Directory,IndexOptions,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", 1+maxDocID, false, Codec.getDefault(), null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // nocommit use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValues.Type.FIXED_INTS_8,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         32, null, IOContext.DEFAULT);\n\n    FieldsConsumer fieldsConsumer = Codec.getDefault().postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,List<Posting>> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDocID+1);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        List<Posting> postings = termEnt.getValue();\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.size());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docCount = 0;\n        for(Posting posting : postings) {\n          if (VERBOSE) {\n            System.out.println(\"    \" + docCount + \": docID=\" + posting.docID + \" freq=\" + posting.positions.size());\n          }\n          postingsConsumer.startDoc(posting.docID, posting.positions.size());\n          seenDocs.set(posting.docID);\n          if (doPos) {\n            totalTF += posting.positions.size();\n            for(Position pos : posting.positions) {\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos.position + \" payload=\" + (pos.payload == null ? \"null\" : pos.payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos.position);\n                }\n              }\n              postingsConsumer.addPosition(pos.position, (doPayloads && pos.payload != null) ? new BytesRef(pos.payload) : null, pos.startOffset, pos.endOffset);\n            }\n          } else if (doFreq) {\n            totalTF += posting.positions.size();\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n          docCount++;\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.size(), totalTF));\n        sumTotalTF += totalTF;\n        sumDF += postings.size();\n      }\n\n      termsConsumer.finish(sumTotalTF, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT, 1);\n\n    return Codec.getDefault().postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", 1+maxDocID, false, Codec.getDefault(), null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // nocommit use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[random().nextInt(1+maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValues.Type.FIXED_INTS_8,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         32, null, IOContext.DEFAULT);\n\n    FieldsConsumer fieldsConsumer = Codec.getDefault().postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,List<Posting>> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field);\n      }\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDocID+1);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        List<Posting> postings = termEnt.getValue();\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.size());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docCount = 0;\n        for(Posting posting : postings) {\n          if (VERBOSE) {\n            System.out.println(\"    \" + docCount + \": docID=\" + posting.docID + \" freq=\" + posting.positions.size());\n          }\n          postingsConsumer.startDoc(posting.docID, posting.positions.size());\n          seenDocs.set(posting.docID);\n          if (doPos) {\n            totalTF += posting.positions.size();\n            for(Position pos : posting.positions) {\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos.position + \" payload=\" + (pos.payload == null ? \"null\" : pos.payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos.position);\n                }\n              }\n              postingsConsumer.addPosition(pos.position, (doPayloads && pos.payload != null) ? new BytesRef(pos.payload) : null, pos.startOffset, pos.endOffset);\n            }\n          } else if (doFreq) {\n            totalTF += posting.positions.size();\n          } else {\n            totalTF++;\n          }\n          docCount++;\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.size(), totalTF));\n        sumTotalTF += totalTF;\n        sumDF += postings.size();\n      }\n\n      termsConsumer.finish(sumTotalTF, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT, 1);\n\n    return Codec.getDefault().postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ddf662c415c0d0ad543e5314fcdf8396cd2f1b8d","date":1343058759,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsFormat#buildIndex(Directory,IndexOptions,boolean).mjava","pathOld":"/dev/null","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", 1+maxDocID, false, Codec.getDefault(), null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValues.Type.FIXED_INTS_8,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         32, null, IOContext.DEFAULT);\n\n    FieldsConsumer fieldsConsumer = Codec.getDefault().postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,List<Posting>> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDocID+1);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        List<Posting> postings = termEnt.getValue();\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.size());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docCount = 0;\n        for(Posting posting : postings) {\n          if (VERBOSE) {\n            System.out.println(\"    \" + docCount + \": docID=\" + posting.docID + \" freq=\" + posting.positions.size());\n          }\n          postingsConsumer.startDoc(posting.docID, posting.positions.size());\n          seenDocs.set(posting.docID);\n          if (doPos) {\n            totalTF += posting.positions.size();\n            for(Position pos : posting.positions) {\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos.position + \" payload=\" + (pos.payload == null ? \"null\" : pos.payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos.position);\n                }\n              }\n              postingsConsumer.addPosition(pos.position, (doPayloads && pos.payload != null) ? new BytesRef(pos.payload) : null, pos.startOffset, pos.endOffset);\n            }\n          } else if (doFreq) {\n            totalTF += posting.positions.size();\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n          docCount++;\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.size(), totalTF));\n        sumTotalTF += totalTF;\n        sumDF += postings.size();\n      }\n\n      termsConsumer.finish(sumTotalTF, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT, 1);\n\n    return Codec.getDefault().postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["29e23e367cc757f42cdfce2bcbf21e68cd209cda","d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7ff9989ce725b0ab3b15c933e10601d526791d40","date":1343059104,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsFormat#buildIndex(Directory,IndexOptions,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsFormat#buildIndex(Directory,IndexOptions,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", 1+maxDocID, false, Codec.getDefault(), null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValues.Type.FIXED_INTS_8,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         32, null, IOContext.DEFAULT);\n\n    FieldsConsumer fieldsConsumer = Codec.getDefault().postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,List<Posting>> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      boolean doOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDocID+1);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        List<Posting> postings = termEnt.getValue();\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.size());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docCount = 0;\n        for(Posting posting : postings) {\n          if (VERBOSE) {\n            System.out.println(\"    \" + docCount + \": docID=\" + posting.docID + \" freq=\" + posting.positions.size());\n          }\n          postingsConsumer.startDoc(posting.docID, posting.positions.size());\n          seenDocs.set(posting.docID);\n          if (doPos) {\n            totalTF += posting.positions.size();\n            for(Position pos : posting.positions) {\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos.position + \" payload=\" + (pos.payload == null ? \"null\" : pos.payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos.position);\n                }\n              }\n              postingsConsumer.addPosition(pos.position, (doPayloads && pos.payload != null) ? new BytesRef(pos.payload) : null, doOffsets ? pos.startOffset : -1, doOffsets ? pos.endOffset : -1);\n            }\n          } else if (doFreq) {\n            totalTF += posting.positions.size();\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n          docCount++;\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.size(), totalTF));\n        sumTotalTF += totalTF;\n        sumDF += postings.size();\n      }\n\n      termsConsumer.finish(sumTotalTF, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT, 1);\n\n    return Codec.getDefault().postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", 1+maxDocID, false, Codec.getDefault(), null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValues.Type.FIXED_INTS_8,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         32, null, IOContext.DEFAULT);\n\n    FieldsConsumer fieldsConsumer = Codec.getDefault().postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,List<Posting>> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDocID+1);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        List<Posting> postings = termEnt.getValue();\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.size());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docCount = 0;\n        for(Posting posting : postings) {\n          if (VERBOSE) {\n            System.out.println(\"    \" + docCount + \": docID=\" + posting.docID + \" freq=\" + posting.positions.size());\n          }\n          postingsConsumer.startDoc(posting.docID, posting.positions.size());\n          seenDocs.set(posting.docID);\n          if (doPos) {\n            totalTF += posting.positions.size();\n            for(Position pos : posting.positions) {\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos.position + \" payload=\" + (pos.payload == null ? \"null\" : pos.payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos.position);\n                }\n              }\n              postingsConsumer.addPosition(pos.position, (doPayloads && pos.payload != null) ? new BytesRef(pos.payload) : null, pos.startOffset, pos.endOffset);\n            }\n          } else if (doFreq) {\n            totalTF += posting.positions.size();\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n          docCount++;\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.size(), totalTF));\n        sumTotalTF += totalTF;\n        sumDF += postings.size();\n      }\n\n      termsConsumer.finish(sumTotalTF, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT, 1);\n\n    return Codec.getDefault().postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"094f0d273d15943ff2daa367b891b16c672f66f1","date":1343063629,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsFormat#buildIndex(Directory,IndexOptions,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsFormat#buildIndex(Directory,IndexOptions,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", 1+maxDocID, false, Codec.getDefault(), null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValues.Type.FIXED_INTS_8,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         32, null, IOContext.DEFAULT);\n\n    FieldsConsumer fieldsConsumer = Codec.getDefault().postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,List<Posting>> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      boolean doOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDocID+1);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        List<Posting> postings = termEnt.getValue();\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.size());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docCount = 0;\n        for(Posting posting : postings) {\n          if (VERBOSE) {\n            System.out.println(\"    \" + docCount + \": docID=\" + posting.docID + \" freq=\" + posting.positions.size());\n          }\n          postingsConsumer.startDoc(posting.docID, posting.positions.size());\n          seenDocs.set(posting.docID);\n          if (doPos) {\n            totalTF += posting.positions.size();\n            for(Position pos : posting.positions) {\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos.position + \" payload=\" + (pos.payload == null ? \"null\" : pos.payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos.position);\n                }\n              }\n              postingsConsumer.addPosition(pos.position, (doPayloads && pos.payload != null) ? new BytesRef(pos.payload) : null, doOffsets ? pos.startOffset : -1, doOffsets ? pos.endOffset : -1);\n            }\n          } else if (doFreq) {\n            totalTF += posting.positions.size();\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n          docCount++;\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.size(), totalTF));\n        sumTotalTF += totalTF;\n        sumDF += postings.size();\n      }\n\n      termsConsumer.finish(sumTotalTF, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT, 1);\n\n    return Codec.getDefault().postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", 1+maxDocID, false, Codec.getDefault(), null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // nocommit use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValues.Type.FIXED_INTS_8,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         32, null, IOContext.DEFAULT);\n\n    FieldsConsumer fieldsConsumer = Codec.getDefault().postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,List<Posting>> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDocID+1);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        List<Posting> postings = termEnt.getValue();\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.size());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docCount = 0;\n        for(Posting posting : postings) {\n          if (VERBOSE) {\n            System.out.println(\"    \" + docCount + \": docID=\" + posting.docID + \" freq=\" + posting.positions.size());\n          }\n          postingsConsumer.startDoc(posting.docID, posting.positions.size());\n          seenDocs.set(posting.docID);\n          if (doPos) {\n            totalTF += posting.positions.size();\n            for(Position pos : posting.positions) {\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos.position + \" payload=\" + (pos.payload == null ? \"null\" : pos.payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos.position);\n                }\n              }\n              postingsConsumer.addPosition(pos.position, (doPayloads && pos.payload != null) ? new BytesRef(pos.payload) : null, pos.startOffset, pos.endOffset);\n            }\n          } else if (doFreq) {\n            totalTF += posting.positions.size();\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n          docCount++;\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.size(), totalTF));\n        sumTotalTF += totalTF;\n        sumDF += postings.size();\n      }\n\n      termsConsumer.finish(sumTotalTF, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT, 1);\n\n    return Codec.getDefault().postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"29e23e367cc757f42cdfce2bcbf21e68cd209cda","date":1343071560,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsFormat#buildIndex(Directory,IndexOptions,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsFormat#buildIndex(Directory,IndexOptions,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", 1+maxDocID, false, Codec.getDefault(), null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValues.Type.FIXED_INTS_8,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         32, null, IOContext.DEFAULT);\n\n    FieldsConsumer fieldsConsumer = Codec.getDefault().postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,List<Posting>> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      boolean doOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDocID+1);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        List<Posting> postings = termEnt.getValue();\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.size());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docCount = 0;\n        for(Posting posting : postings) {\n          if (VERBOSE) {\n            System.out.println(\"    \" + docCount + \": docID=\" + posting.docID + \" freq=\" + posting.positions.size());\n          }\n          postingsConsumer.startDoc(posting.docID, posting.positions.size());\n          seenDocs.set(posting.docID);\n          if (doPos) {\n            totalTF += posting.positions.size();\n            for(Position pos : posting.positions) {\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos.position + \" payload=\" + (pos.payload == null ? \"null\" : pos.payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos.position);\n                }\n              }\n              postingsConsumer.addPosition(pos.position, (doPayloads && pos.payload != null) ? new BytesRef(pos.payload) : null, doOffsets ? pos.startOffset : -1, doOffsets ? pos.endOffset : -1);\n            }\n          } else if (doFreq) {\n            totalTF += posting.positions.size();\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n          docCount++;\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.size(), doFreq ? totalTF : -1));\n        sumTotalTF += totalTF;\n        sumDF += postings.size();\n      }\n\n      termsConsumer.finish(doFreq ? sumTotalTF : -1, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT, 1);\n\n    return Codec.getDefault().postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", 1+maxDocID, false, Codec.getDefault(), null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValues.Type.FIXED_INTS_8,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         32, null, IOContext.DEFAULT);\n\n    FieldsConsumer fieldsConsumer = Codec.getDefault().postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,List<Posting>> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      boolean doOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDocID+1);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        List<Posting> postings = termEnt.getValue();\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.size());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docCount = 0;\n        for(Posting posting : postings) {\n          if (VERBOSE) {\n            System.out.println(\"    \" + docCount + \": docID=\" + posting.docID + \" freq=\" + posting.positions.size());\n          }\n          postingsConsumer.startDoc(posting.docID, posting.positions.size());\n          seenDocs.set(posting.docID);\n          if (doPos) {\n            totalTF += posting.positions.size();\n            for(Position pos : posting.positions) {\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos.position + \" payload=\" + (pos.payload == null ? \"null\" : pos.payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos.position);\n                }\n              }\n              postingsConsumer.addPosition(pos.position, (doPayloads && pos.payload != null) ? new BytesRef(pos.payload) : null, doOffsets ? pos.startOffset : -1, doOffsets ? pos.endOffset : -1);\n            }\n          } else if (doFreq) {\n            totalTF += posting.positions.size();\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n          docCount++;\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.size(), totalTF));\n        sumTotalTF += totalTF;\n        sumDF += postings.size();\n      }\n\n      termsConsumer.finish(sumTotalTF, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT, 1);\n\n    return Codec.getDefault().postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":["ddf662c415c0d0ad543e5314fcdf8396cd2f1b8d"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"55c178897422fc01a257353a67f2ee23f1c82403","date":1343076368,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsFormat#buildIndex(Directory,IndexOptions,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsFormat#buildIndex(Directory,IndexOptions,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", 1+maxDocID, false, Codec.getDefault(), null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValues.Type.FIXED_INTS_8,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         32, null, IOContext.DEFAULT);\n\n    FieldsConsumer fieldsConsumer = Codec.getDefault().postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,List<Posting>> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      boolean doOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDocID+1);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        List<Posting> postings = termEnt.getValue();\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.size());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docCount = 0;\n        for(Posting posting : postings) {\n          if (VERBOSE) {\n            System.out.println(\"    \" + docCount + \": docID=\" + posting.docID + \" freq=\" + posting.positions.size());\n          }\n          postingsConsumer.startDoc(posting.docID, doFreq ? posting.positions.size() : -1);\n          seenDocs.set(posting.docID);\n          if (doPos) {\n            totalTF += posting.positions.size();\n            for(Position pos : posting.positions) {\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos.position + \" payload=\" + (pos.payload == null ? \"null\" : pos.payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos.position);\n                }\n              }\n              postingsConsumer.addPosition(pos.position, (doPayloads && pos.payload != null) ? new BytesRef(pos.payload) : null, doOffsets ? pos.startOffset : -1, doOffsets ? pos.endOffset : -1);\n            }\n          } else if (doFreq) {\n            totalTF += posting.positions.size();\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n          docCount++;\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.size(), doFreq ? totalTF : -1));\n        sumTotalTF += totalTF;\n        sumDF += postings.size();\n      }\n\n      termsConsumer.finish(doFreq ? sumTotalTF : -1, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT, 1);\n\n    return Codec.getDefault().postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", 1+maxDocID, false, Codec.getDefault(), null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValues.Type.FIXED_INTS_8,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         32, null, IOContext.DEFAULT);\n\n    FieldsConsumer fieldsConsumer = Codec.getDefault().postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,List<Posting>> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      boolean doOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDocID+1);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        List<Posting> postings = termEnt.getValue();\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.size());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docCount = 0;\n        for(Posting posting : postings) {\n          if (VERBOSE) {\n            System.out.println(\"    \" + docCount + \": docID=\" + posting.docID + \" freq=\" + posting.positions.size());\n          }\n          postingsConsumer.startDoc(posting.docID, posting.positions.size());\n          seenDocs.set(posting.docID);\n          if (doPos) {\n            totalTF += posting.positions.size();\n            for(Position pos : posting.positions) {\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos.position + \" payload=\" + (pos.payload == null ? \"null\" : pos.payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos.position);\n                }\n              }\n              postingsConsumer.addPosition(pos.position, (doPayloads && pos.payload != null) ? new BytesRef(pos.payload) : null, doOffsets ? pos.startOffset : -1, doOffsets ? pos.endOffset : -1);\n            }\n          } else if (doFreq) {\n            totalTF += posting.positions.size();\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n          docCount++;\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.size(), doFreq ? totalTF : -1));\n        sumTotalTF += totalTF;\n        sumDF += postings.size();\n      }\n\n      termsConsumer.finish(doFreq ? sumTotalTF : -1, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT, 1);\n\n    return Codec.getDefault().postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"57635ff388fa1bee703f3b892a86a3e48975576a","date":1343077051,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsFormat#buildIndex(Directory,IndexOptions,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsFormat#buildIndex(Directory,IndexOptions,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", 1+maxDocID, false, Codec.getDefault(), null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValues.Type.FIXED_INTS_8,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         32, null, IOContext.DEFAULT);\n\n    FieldsConsumer fieldsConsumer = Codec.getDefault().postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,List<Posting>> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      boolean doOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDocID+1);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        List<Posting> postings = termEnt.getValue();\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.size());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docCount = 0;\n        for(Posting posting : postings) {\n          if (VERBOSE) {\n            System.out.println(\"    \" + docCount + \": docID=\" + posting.docID + \" freq=\" + posting.positions.size());\n          }\n          postingsConsumer.startDoc(posting.docID, doFreq ? posting.positions.size() : -1);\n          seenDocs.set(posting.docID);\n          if (doPos) {\n            totalTF += posting.positions.size();\n            for(Position pos : posting.positions) {\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos.position + \" payload=\" + (pos.payload == null ? \"null\" : pos.payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos.position);\n                }\n              }\n              postingsConsumer.addPosition(pos.position, (doPayloads && pos.payload != null) ? new BytesRef(pos.payload) : null, doOffsets ? pos.startOffset : -1, doOffsets ? pos.endOffset : -1);\n            }\n          } else if (doFreq) {\n            totalTF += posting.positions.size();\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n          docCount++;\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.size(), doFreq ? totalTF : -1));\n        sumTotalTF += totalTF;\n        sumDF += postings.size();\n      }\n\n      termsConsumer.finish(doFreq ? sumTotalTF : -1, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT, 1);\n\n    return Codec.getDefault().postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", 1+maxDocID, false, Codec.getDefault(), null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValues.Type.FIXED_INTS_8,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         32, null, IOContext.DEFAULT);\n\n    FieldsConsumer fieldsConsumer = Codec.getDefault().postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,List<Posting>> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      boolean doOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDocID+1);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        List<Posting> postings = termEnt.getValue();\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.size());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docCount = 0;\n        for(Posting posting : postings) {\n          if (VERBOSE) {\n            System.out.println(\"    \" + docCount + \": docID=\" + posting.docID + \" freq=\" + posting.positions.size());\n          }\n          postingsConsumer.startDoc(posting.docID, posting.positions.size());\n          seenDocs.set(posting.docID);\n          if (doPos) {\n            totalTF += posting.positions.size();\n            for(Position pos : posting.positions) {\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos.position + \" payload=\" + (pos.payload == null ? \"null\" : pos.payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos.position);\n                }\n              }\n              postingsConsumer.addPosition(pos.position, (doPayloads && pos.payload != null) ? new BytesRef(pos.payload) : null, doOffsets ? pos.startOffset : -1, doOffsets ? pos.endOffset : -1);\n            }\n          } else if (doFreq) {\n            totalTF += posting.positions.size();\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n          docCount++;\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.size(), totalTF));\n        sumTotalTF += totalTF;\n        sumDF += postings.size();\n      }\n\n      termsConsumer.finish(sumTotalTF, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT, 1);\n\n    return Codec.getDefault().postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"aba371508186796cc6151d8223a5b4e16d02e26e","date":1343474871,"type":0,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsFormat#buildIndex(Directory,IndexOptions,boolean).mjava","pathOld":"/dev/null","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", 1+maxDocID, false, Codec.getDefault(), null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValues.Type.FIXED_INTS_8,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         32, null, IOContext.DEFAULT);\n\n    FieldsConsumer fieldsConsumer = Codec.getDefault().postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,List<Posting>> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      boolean doOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDocID+1);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        List<Posting> postings = termEnt.getValue();\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.size());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docCount = 0;\n        for(Posting posting : postings) {\n          if (VERBOSE) {\n            System.out.println(\"    \" + docCount + \": docID=\" + posting.docID + \" freq=\" + posting.positions.size());\n          }\n          postingsConsumer.startDoc(posting.docID, doFreq ? posting.positions.size() : -1);\n          seenDocs.set(posting.docID);\n          if (doPos) {\n            totalTF += posting.positions.size();\n            for(Position pos : posting.positions) {\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos.position + \" payload=\" + (pos.payload == null ? \"null\" : pos.payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos.position);\n                }\n              }\n              postingsConsumer.addPosition(pos.position, (doPayloads && pos.payload != null) ? new BytesRef(pos.payload) : null, doOffsets ? pos.startOffset : -1, doOffsets ? pos.endOffset : -1);\n            }\n          } else if (doFreq) {\n            totalTF += posting.positions.size();\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n          docCount++;\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.size(), doFreq ? totalTF : -1));\n        sumTotalTF += totalTF;\n        sumDF += postings.size();\n      }\n\n      termsConsumer.finish(doFreq ? sumTotalTF : -1, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT, 1);\n\n    return Codec.getDefault().postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b43cc463de57963524b3835202575c1662c9e927","date":1346784739,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsFormat#buildIndex(Directory,IndexOptions,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsFormat#buildIndex(Directory,IndexOptions,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", 1+maxDocID, false, Codec.getDefault(), null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValues.Type.FIXED_INTS_8,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  RamUsageEstimator.sizeOf(fields)/4;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         32, null, new IOContext(new FlushInfo(maxDocID, bytes)));\n    FieldsConsumer fieldsConsumer = Codec.getDefault().postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,List<Posting>> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      boolean doOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDocID+1);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        List<Posting> postings = termEnt.getValue();\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.size());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docCount = 0;\n        for(Posting posting : postings) {\n          if (VERBOSE) {\n            System.out.println(\"    \" + docCount + \": docID=\" + posting.docID + \" freq=\" + posting.positions.size());\n          }\n          postingsConsumer.startDoc(posting.docID, doFreq ? posting.positions.size() : -1);\n          seenDocs.set(posting.docID);\n          if (doPos) {\n            totalTF += posting.positions.size();\n            for(Position pos : posting.positions) {\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos.position + \" payload=\" + (pos.payload == null ? \"null\" : pos.payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos.position);\n                }\n              }\n              postingsConsumer.addPosition(pos.position, (doPayloads && pos.payload != null) ? new BytesRef(pos.payload) : null, doOffsets ? pos.startOffset : -1, doOffsets ? pos.endOffset : -1);\n            }\n          } else if (doFreq) {\n            totalTF += posting.positions.size();\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n          docCount++;\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.size(), doFreq ? totalTF : -1));\n        sumTotalTF += totalTF;\n        sumDF += postings.size();\n      }\n\n      termsConsumer.finish(doFreq ? sumTotalTF : -1, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT, 1);\n\n    return Codec.getDefault().postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", 1+maxDocID, false, Codec.getDefault(), null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValues.Type.FIXED_INTS_8,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         32, null, IOContext.DEFAULT);\n\n    FieldsConsumer fieldsConsumer = Codec.getDefault().postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,List<Posting>> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      boolean doOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDocID+1);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        List<Posting> postings = termEnt.getValue();\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.size());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docCount = 0;\n        for(Posting posting : postings) {\n          if (VERBOSE) {\n            System.out.println(\"    \" + docCount + \": docID=\" + posting.docID + \" freq=\" + posting.positions.size());\n          }\n          postingsConsumer.startDoc(posting.docID, doFreq ? posting.positions.size() : -1);\n          seenDocs.set(posting.docID);\n          if (doPos) {\n            totalTF += posting.positions.size();\n            for(Position pos : posting.positions) {\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos.position + \" payload=\" + (pos.payload == null ? \"null\" : pos.payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos.position);\n                }\n              }\n              postingsConsumer.addPosition(pos.position, (doPayloads && pos.payload != null) ? new BytesRef(pos.payload) : null, doOffsets ? pos.startOffset : -1, doOffsets ? pos.endOffset : -1);\n            }\n          } else if (doFreq) {\n            totalTF += posting.positions.size();\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n          docCount++;\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.size(), doFreq ? totalTF : -1));\n        sumTotalTF += totalTF;\n        sumDF += postings.size();\n      }\n\n      termsConsumer.finish(doFreq ? sumTotalTF : -1, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT, 1);\n\n    return Codec.getDefault().postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8649cef98eb6b4eda0113896873934b1dcd745bb","date":1346856421,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsFormat#buildIndex(Directory,IndexOptions,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsFormat#buildIndex(Directory,IndexOptions,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", 1+maxDocID, false, Codec.getDefault(), null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValues.Type.FIXED_INTS_8,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         32, null, new IOContext(new FlushInfo(maxDocID, bytes)));\n    FieldsConsumer fieldsConsumer = Codec.getDefault().postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,List<Posting>> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      boolean doOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDocID+1);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        List<Posting> postings = termEnt.getValue();\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.size());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docCount = 0;\n        for(Posting posting : postings) {\n          if (VERBOSE) {\n            System.out.println(\"    \" + docCount + \": docID=\" + posting.docID + \" freq=\" + posting.positions.size());\n          }\n          postingsConsumer.startDoc(posting.docID, doFreq ? posting.positions.size() : -1);\n          seenDocs.set(posting.docID);\n          if (doPos) {\n            totalTF += posting.positions.size();\n            for(Position pos : posting.positions) {\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos.position + \" payload=\" + (pos.payload == null ? \"null\" : pos.payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos.position);\n                }\n              }\n              postingsConsumer.addPosition(pos.position, (doPayloads && pos.payload != null) ? new BytesRef(pos.payload) : null, doOffsets ? pos.startOffset : -1, doOffsets ? pos.endOffset : -1);\n            }\n          } else if (doFreq) {\n            totalTF += posting.positions.size();\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n          docCount++;\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.size(), doFreq ? totalTF : -1));\n        sumTotalTF += totalTF;\n        sumDF += postings.size();\n      }\n\n      termsConsumer.finish(doFreq ? sumTotalTF : -1, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT, 1);\n\n    return Codec.getDefault().postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", 1+maxDocID, false, Codec.getDefault(), null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValues.Type.FIXED_INTS_8,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  RamUsageEstimator.sizeOf(fields)/4;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         32, null, new IOContext(new FlushInfo(maxDocID, bytes)));\n    FieldsConsumer fieldsConsumer = Codec.getDefault().postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,List<Posting>> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      boolean doOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDocID+1);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        List<Posting> postings = termEnt.getValue();\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.size());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docCount = 0;\n        for(Posting posting : postings) {\n          if (VERBOSE) {\n            System.out.println(\"    \" + docCount + \": docID=\" + posting.docID + \" freq=\" + posting.positions.size());\n          }\n          postingsConsumer.startDoc(posting.docID, doFreq ? posting.positions.size() : -1);\n          seenDocs.set(posting.docID);\n          if (doPos) {\n            totalTF += posting.positions.size();\n            for(Position pos : posting.positions) {\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos.position + \" payload=\" + (pos.payload == null ? \"null\" : pos.payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos.position);\n                }\n              }\n              postingsConsumer.addPosition(pos.position, (doPayloads && pos.payload != null) ? new BytesRef(pos.payload) : null, doOffsets ? pos.startOffset : -1, doOffsets ? pos.endOffset : -1);\n            }\n          } else if (doFreq) {\n            totalTF += posting.positions.size();\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n          docCount++;\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.size(), doFreq ? totalTF : -1));\n        sumTotalTF += totalTF;\n        sumDF += postings.size();\n      }\n\n      termsConsumer.finish(doFreq ? sumTotalTF : -1, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT, 1);\n\n    return Codec.getDefault().postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"95323da8eca89d45766013f5b300a865a5ac7dfb","date":1348933777,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsFormat#buildIndex(Directory,IndexOptions,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", 1+maxDocID, false, codec, null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValues.Type.FIXED_INTS_8,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         32, null, new IOContext(new FlushInfo(maxDocID, bytes)));\n    FieldsConsumer fieldsConsumer = codec.postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,List<Posting>> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      boolean doOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDocID+1);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        List<Posting> postings = termEnt.getValue();\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.size());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docCount = 0;\n        for(Posting posting : postings) {\n          if (VERBOSE) {\n            System.out.println(\"    \" + docCount + \": docID=\" + posting.docID + \" freq=\" + posting.positions.size());\n          }\n          postingsConsumer.startDoc(posting.docID, doFreq ? posting.positions.size() : -1);\n          seenDocs.set(posting.docID);\n          if (doPos) {\n            totalTF += posting.positions.size();\n            for(Position pos : posting.positions) {\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos.position + \" payload=\" + (pos.payload == null ? \"null\" : pos.payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos.position);\n                }\n              }\n              postingsConsumer.addPosition(pos.position, (doPayloads && pos.payload != null) ? new BytesRef(pos.payload) : null, doOffsets ? pos.startOffset : -1, doOffsets ? pos.endOffset : -1);\n            }\n          } else if (doFreq) {\n            totalTF += posting.positions.size();\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n          docCount++;\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.size(), doFreq ? totalTF : -1));\n        sumTotalTF += totalTF;\n        sumDF += postings.size();\n      }\n\n      termsConsumer.finish(doFreq ? sumTotalTF : -1, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT, 1);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", 1+maxDocID, false, Codec.getDefault(), null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValues.Type.FIXED_INTS_8,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         32, null, new IOContext(new FlushInfo(maxDocID, bytes)));\n    FieldsConsumer fieldsConsumer = Codec.getDefault().postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,List<Posting>> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      boolean doOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDocID+1);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        List<Posting> postings = termEnt.getValue();\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.size());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docCount = 0;\n        for(Posting posting : postings) {\n          if (VERBOSE) {\n            System.out.println(\"    \" + docCount + \": docID=\" + posting.docID + \" freq=\" + posting.positions.size());\n          }\n          postingsConsumer.startDoc(posting.docID, doFreq ? posting.positions.size() : -1);\n          seenDocs.set(posting.docID);\n          if (doPos) {\n            totalTF += posting.positions.size();\n            for(Position pos : posting.positions) {\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos.position + \" payload=\" + (pos.payload == null ? \"null\" : pos.payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos.position);\n                }\n              }\n              postingsConsumer.addPosition(pos.position, (doPayloads && pos.payload != null) ? new BytesRef(pos.payload) : null, doOffsets ? pos.startOffset : -1, doOffsets ? pos.endOffset : -1);\n            }\n          } else if (doFreq) {\n            totalTF += posting.positions.size();\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n          docCount++;\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.size(), doFreq ? totalTF : -1));\n        sumTotalTF += totalTF;\n        sumDF += postings.size();\n      }\n\n      termsConsumer.finish(doFreq ? sumTotalTF : -1, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT, 1);\n\n    return Codec.getDefault().postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"55c178897422fc01a257353a67f2ee23f1c82403":["29e23e367cc757f42cdfce2bcbf21e68cd209cda"],"ddf662c415c0d0ad543e5314fcdf8396cd2f1b8d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"95323da8eca89d45766013f5b300a865a5ac7dfb":["8649cef98eb6b4eda0113896873934b1dcd745bb"],"29e23e367cc757f42cdfce2bcbf21e68cd209cda":["7ff9989ce725b0ab3b15c933e10601d526791d40"],"8649cef98eb6b4eda0113896873934b1dcd745bb":["b43cc463de57963524b3835202575c1662c9e927"],"57635ff388fa1bee703f3b892a86a3e48975576a":["094f0d273d15943ff2daa367b891b16c672f66f1","55c178897422fc01a257353a67f2ee23f1c82403"],"a434b20f16e875530b6c86775a3efaf148056c3b":["e885d2b1e112b1d9db6a2dae82b3b493dfba1df1"],"b43cc463de57963524b3835202575c1662c9e927":["55c178897422fc01a257353a67f2ee23f1c82403"],"094f0d273d15943ff2daa367b891b16c672f66f1":["a434b20f16e875530b6c86775a3efaf148056c3b","7ff9989ce725b0ab3b15c933e10601d526791d40"],"aba371508186796cc6151d8223a5b4e16d02e26e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","55c178897422fc01a257353a67f2ee23f1c82403"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"7ff9989ce725b0ab3b15c933e10601d526791d40":["ddf662c415c0d0ad543e5314fcdf8396cd2f1b8d"],"e885d2b1e112b1d9db6a2dae82b3b493dfba1df1":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["95323da8eca89d45766013f5b300a865a5ac7dfb"]},"commit2Childs":{"55c178897422fc01a257353a67f2ee23f1c82403":["57635ff388fa1bee703f3b892a86a3e48975576a","b43cc463de57963524b3835202575c1662c9e927","aba371508186796cc6151d8223a5b4e16d02e26e"],"ddf662c415c0d0ad543e5314fcdf8396cd2f1b8d":["7ff9989ce725b0ab3b15c933e10601d526791d40"],"95323da8eca89d45766013f5b300a865a5ac7dfb":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"29e23e367cc757f42cdfce2bcbf21e68cd209cda":["55c178897422fc01a257353a67f2ee23f1c82403"],"8649cef98eb6b4eda0113896873934b1dcd745bb":["95323da8eca89d45766013f5b300a865a5ac7dfb"],"57635ff388fa1bee703f3b892a86a3e48975576a":[],"a434b20f16e875530b6c86775a3efaf148056c3b":["094f0d273d15943ff2daa367b891b16c672f66f1"],"b43cc463de57963524b3835202575c1662c9e927":["8649cef98eb6b4eda0113896873934b1dcd745bb"],"094f0d273d15943ff2daa367b891b16c672f66f1":["57635ff388fa1bee703f3b892a86a3e48975576a"],"aba371508186796cc6151d8223a5b4e16d02e26e":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["ddf662c415c0d0ad543e5314fcdf8396cd2f1b8d","aba371508186796cc6151d8223a5b4e16d02e26e","e885d2b1e112b1d9db6a2dae82b3b493dfba1df1"],"7ff9989ce725b0ab3b15c933e10601d526791d40":["29e23e367cc757f42cdfce2bcbf21e68cd209cda","094f0d273d15943ff2daa367b891b16c672f66f1"],"e885d2b1e112b1d9db6a2dae82b3b493dfba1df1":["a434b20f16e875530b6c86775a3efaf148056c3b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["57635ff388fa1bee703f3b892a86a3e48975576a","aba371508186796cc6151d8223a5b4e16d02e26e","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}