{"path":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#merge(MergeState).mjava","commits":[{"id":"eda61b1e90b490cc5837200e04c02639a0d272c7","date":1358795519,"type":0,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#merge(MergeState).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];\n      CompressingTermVectorsReader matchingVectorsReader = null;\n      if (matchingSegmentReader != null) {\n        final TermVectorsReader vectorsReader = matchingSegmentReader.getTermVectorsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingTermVectorsReader\n        if (vectorsReader != null && vectorsReader instanceof CompressingTermVectorsReader) {\n          matchingVectorsReader = (CompressingTermVectorsReader) vectorsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingVectorsReader == null\n          || matchingVectorsReader.getCompressionMode() != compressionMode\n          || matchingVectorsReader.getChunkSize() != chunkSize\n          || matchingVectorsReader.getPackedIntsVersion() != PackedInts.VERSION_CURRENT) {\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          final Fields vectors = reader.getTermVectors(i);\n          addAllDocVectors(vectors, mergeState);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        final CompressingStoredFieldsIndexReader index = matchingVectorsReader.getIndex();\n        final IndexInput vectorsStream = matchingVectorsReader.getVectorsStream();\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; ) {\n          if (pendingDocs.isEmpty()\n              && (i == 0 || index.getStartPointer(i - 1) < index.getStartPointer(i))) { // start of a chunk\n            final long startPointer = index.getStartPointer(i);\n            vectorsStream.seek(startPointer);\n            final int docBase = vectorsStream.readVInt();\n            final int chunkDocs = vectorsStream.readVInt();\n            assert docBase + chunkDocs <= matchingSegmentReader.maxDoc();\n            if (docBase + chunkDocs < matchingSegmentReader.maxDoc()\n                && nextDeletedDoc(docBase, liveDocs, docBase + chunkDocs) == docBase + chunkDocs) {\n              final long chunkEnd = index.getStartPointer(docBase + chunkDocs);\n              final long chunkLength = chunkEnd - vectorsStream.getFilePointer();\n              indexWriter.writeIndex(chunkDocs, this.vectorsStream.getFilePointer());\n              this.vectorsStream.writeVInt(docCount);\n              this.vectorsStream.writeVInt(chunkDocs);\n              this.vectorsStream.copyBytes(vectorsStream, chunkLength);\n              docCount += chunkDocs;\n              this.numDocs += chunkDocs;\n              mergeState.checkAbort.work(300 * chunkDocs);\n              i = nextLiveDoc(docBase + chunkDocs, liveDocs, maxDoc);\n            } else {\n              for (; i < docBase + chunkDocs; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n                final Fields vectors = reader.getTermVectors(i);\n                addAllDocVectors(vectors, mergeState);\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } else {\n            final Fields vectors = reader.getTermVectors(i);\n            addAllDocVectors(vectors, mergeState);\n            ++docCount;\n            mergeState.checkAbort.work(300);\n            i = nextLiveDoc(i + 1, liveDocs, maxDoc);\n          }\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["b88448324d3a96c5842455dabea63450b697b58f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"07155cdd910937cdf6877e48884d5782845c8b8b","date":1358796205,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#merge(MergeState).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];\n      CompressingTermVectorsReader matchingVectorsReader = null;\n      if (matchingSegmentReader != null) {\n        final TermVectorsReader vectorsReader = matchingSegmentReader.getTermVectorsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingTermVectorsReader\n        if (vectorsReader != null && vectorsReader instanceof CompressingTermVectorsReader) {\n          matchingVectorsReader = (CompressingTermVectorsReader) vectorsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingVectorsReader == null\n          || matchingVectorsReader.getCompressionMode() != compressionMode\n          || matchingVectorsReader.getChunkSize() != chunkSize\n          || matchingVectorsReader.getPackedIntsVersion() != PackedInts.VERSION_CURRENT) {\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          final Fields vectors = reader.getTermVectors(i);\n          addAllDocVectors(vectors, mergeState);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        final CompressingStoredFieldsIndexReader index = matchingVectorsReader.getIndex();\n        final IndexInput vectorsStream = matchingVectorsReader.getVectorsStream();\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; ) {\n          if (pendingDocs.isEmpty()\n              && (i == 0 || index.getStartPointer(i - 1) < index.getStartPointer(i))) { // start of a chunk\n            final long startPointer = index.getStartPointer(i);\n            vectorsStream.seek(startPointer);\n            final int docBase = vectorsStream.readVInt();\n            final int chunkDocs = vectorsStream.readVInt();\n            assert docBase + chunkDocs <= matchingSegmentReader.maxDoc();\n            if (docBase + chunkDocs < matchingSegmentReader.maxDoc()\n                && nextDeletedDoc(docBase, liveDocs, docBase + chunkDocs) == docBase + chunkDocs) {\n              final long chunkEnd = index.getStartPointer(docBase + chunkDocs);\n              final long chunkLength = chunkEnd - vectorsStream.getFilePointer();\n              indexWriter.writeIndex(chunkDocs, this.vectorsStream.getFilePointer());\n              this.vectorsStream.writeVInt(docCount);\n              this.vectorsStream.writeVInt(chunkDocs);\n              this.vectorsStream.copyBytes(vectorsStream, chunkLength);\n              docCount += chunkDocs;\n              this.numDocs += chunkDocs;\n              mergeState.checkAbort.work(300 * chunkDocs);\n              i = nextLiveDoc(docBase + chunkDocs, liveDocs, maxDoc);\n            } else {\n              for (; i < docBase + chunkDocs; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n                final Fields vectors = reader.getTermVectors(i);\n                addAllDocVectors(vectors, mergeState);\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } else {\n            final Fields vectors = reader.getTermVectors(i);\n            addAllDocVectors(vectors, mergeState);\n            ++docCount;\n            mergeState.checkAbort.work(300);\n            i = nextLiveDoc(i + 1, liveDocs, maxDoc);\n          }\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1db0e36bd56f06d5830e980340e7fe58353a44f7","date":1397233249,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];\n      CompressingTermVectorsReader matchingVectorsReader = null;\n      if (matchingSegmentReader != null) {\n        final TermVectorsReader vectorsReader = matchingSegmentReader.getTermVectorsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingTermVectorsReader\n        if (vectorsReader != null && vectorsReader instanceof CompressingTermVectorsReader) {\n          matchingVectorsReader = (CompressingTermVectorsReader) vectorsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingVectorsReader == null\n          || matchingVectorsReader.getVersion() != VERSION_CURRENT\n          || matchingVectorsReader.getCompressionMode() != compressionMode\n          || matchingVectorsReader.getChunkSize() != chunkSize\n          || matchingVectorsReader.getPackedIntsVersion() != PackedInts.VERSION_CURRENT) {\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          final Fields vectors = reader.getTermVectors(i);\n          addAllDocVectors(vectors, mergeState);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        final CompressingStoredFieldsIndexReader index = matchingVectorsReader.getIndex();\n        final IndexInput vectorsStreamOrig = matchingVectorsReader.getVectorsStream();\n        vectorsStreamOrig.seek(0);\n        final ChecksumIndexInput vectorsStream = new BufferedChecksumIndexInput(vectorsStreamOrig.clone());\n        \n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; ) {\n          // We make sure to move the checksum input in any case, otherwise the final\n          // integrity check might need to read the whole file a second time\n          final long startPointer = index.getStartPointer(i);\n          if (startPointer > vectorsStream.getFilePointer()) {\n            vectorsStream.seek(startPointer);\n          }\n          if (pendingDocs.isEmpty()\n              && (i == 0 || index.getStartPointer(i - 1) < startPointer)) { // start of a chunk\n            final int docBase = vectorsStream.readVInt();\n            final int chunkDocs = vectorsStream.readVInt();\n            assert docBase + chunkDocs <= matchingSegmentReader.maxDoc();\n            if (docBase + chunkDocs < matchingSegmentReader.maxDoc()\n                && nextDeletedDoc(docBase, liveDocs, docBase + chunkDocs) == docBase + chunkDocs) {\n              final long chunkEnd = index.getStartPointer(docBase + chunkDocs);\n              final long chunkLength = chunkEnd - vectorsStream.getFilePointer();\n              indexWriter.writeIndex(chunkDocs, this.vectorsStream.getFilePointer());\n              this.vectorsStream.writeVInt(docCount);\n              this.vectorsStream.writeVInt(chunkDocs);\n              this.vectorsStream.copyBytes(vectorsStream, chunkLength);\n              docCount += chunkDocs;\n              this.numDocs += chunkDocs;\n              mergeState.checkAbort.work(300 * chunkDocs);\n              i = nextLiveDoc(docBase + chunkDocs, liveDocs, maxDoc);\n            } else {\n              for (; i < docBase + chunkDocs; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n                final Fields vectors = reader.getTermVectors(i);\n                addAllDocVectors(vectors, mergeState);\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } else {\n            final Fields vectors = reader.getTermVectors(i);\n            addAllDocVectors(vectors, mergeState);\n            ++docCount;\n            mergeState.checkAbort.work(300);\n            i = nextLiveDoc(i + 1, liveDocs, maxDoc);\n          }\n        }\n        \n        vectorsStream.seek(vectorsStream.length() - CodecUtil.footerLength());\n        CodecUtil.checkFooter(vectorsStream);\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];\n      CompressingTermVectorsReader matchingVectorsReader = null;\n      if (matchingSegmentReader != null) {\n        final TermVectorsReader vectorsReader = matchingSegmentReader.getTermVectorsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingTermVectorsReader\n        if (vectorsReader != null && vectorsReader instanceof CompressingTermVectorsReader) {\n          matchingVectorsReader = (CompressingTermVectorsReader) vectorsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingVectorsReader == null\n          || matchingVectorsReader.getCompressionMode() != compressionMode\n          || matchingVectorsReader.getChunkSize() != chunkSize\n          || matchingVectorsReader.getPackedIntsVersion() != PackedInts.VERSION_CURRENT) {\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          final Fields vectors = reader.getTermVectors(i);\n          addAllDocVectors(vectors, mergeState);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        final CompressingStoredFieldsIndexReader index = matchingVectorsReader.getIndex();\n        final IndexInput vectorsStream = matchingVectorsReader.getVectorsStream();\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; ) {\n          if (pendingDocs.isEmpty()\n              && (i == 0 || index.getStartPointer(i - 1) < index.getStartPointer(i))) { // start of a chunk\n            final long startPointer = index.getStartPointer(i);\n            vectorsStream.seek(startPointer);\n            final int docBase = vectorsStream.readVInt();\n            final int chunkDocs = vectorsStream.readVInt();\n            assert docBase + chunkDocs <= matchingSegmentReader.maxDoc();\n            if (docBase + chunkDocs < matchingSegmentReader.maxDoc()\n                && nextDeletedDoc(docBase, liveDocs, docBase + chunkDocs) == docBase + chunkDocs) {\n              final long chunkEnd = index.getStartPointer(docBase + chunkDocs);\n              final long chunkLength = chunkEnd - vectorsStream.getFilePointer();\n              indexWriter.writeIndex(chunkDocs, this.vectorsStream.getFilePointer());\n              this.vectorsStream.writeVInt(docCount);\n              this.vectorsStream.writeVInt(chunkDocs);\n              this.vectorsStream.copyBytes(vectorsStream, chunkLength);\n              docCount += chunkDocs;\n              this.numDocs += chunkDocs;\n              mergeState.checkAbort.work(300 * chunkDocs);\n              i = nextLiveDoc(docBase + chunkDocs, liveDocs, maxDoc);\n            } else {\n              for (; i < docBase + chunkDocs; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n                final Fields vectors = reader.getTermVectors(i);\n                addAllDocVectors(vectors, mergeState);\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } else {\n            final Fields vectors = reader.getTermVectors(i);\n            addAllDocVectors(vectors, mergeState);\n            ++docCount;\n            mergeState.checkAbort.work(300);\n            i = nextLiveDoc(i + 1, liveDocs, maxDoc);\n          }\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":["b88448324d3a96c5842455dabea63450b697b58f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"22a2e66dfda83847e80095b8693c660742ab3e9c","date":1408628796,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = matching.matchingSegmentReaders[idx++];\n      CompressingTermVectorsReader matchingVectorsReader = null;\n      if (matchingSegmentReader != null) {\n        final TermVectorsReader vectorsReader = matchingSegmentReader.getTermVectorsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingTermVectorsReader\n        if (vectorsReader != null && vectorsReader instanceof CompressingTermVectorsReader) {\n          matchingVectorsReader = (CompressingTermVectorsReader) vectorsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingVectorsReader == null\n          || matchingVectorsReader.getVersion() != VERSION_CURRENT\n          || matchingVectorsReader.getCompressionMode() != compressionMode\n          || matchingVectorsReader.getChunkSize() != chunkSize\n          || matchingVectorsReader.getPackedIntsVersion() != PackedInts.VERSION_CURRENT) {\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          final Fields vectors = reader.getTermVectors(i);\n          addAllDocVectors(vectors, mergeState);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        final CompressingStoredFieldsIndexReader index = matchingVectorsReader.getIndex();\n        final IndexInput vectorsStreamOrig = matchingVectorsReader.getVectorsStream();\n        vectorsStreamOrig.seek(0);\n        final ChecksumIndexInput vectorsStream = new BufferedChecksumIndexInput(vectorsStreamOrig.clone());\n        \n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; ) {\n          // We make sure to move the checksum input in any case, otherwise the final\n          // integrity check might need to read the whole file a second time\n          final long startPointer = index.getStartPointer(i);\n          if (startPointer > vectorsStream.getFilePointer()) {\n            vectorsStream.seek(startPointer);\n          }\n          if (pendingDocs.isEmpty()\n              && (i == 0 || index.getStartPointer(i - 1) < startPointer)) { // start of a chunk\n            final int docBase = vectorsStream.readVInt();\n            final int chunkDocs = vectorsStream.readVInt();\n            assert docBase + chunkDocs <= matchingSegmentReader.maxDoc();\n            if (docBase + chunkDocs < matchingSegmentReader.maxDoc()\n                && nextDeletedDoc(docBase, liveDocs, docBase + chunkDocs) == docBase + chunkDocs) {\n              final long chunkEnd = index.getStartPointer(docBase + chunkDocs);\n              final long chunkLength = chunkEnd - vectorsStream.getFilePointer();\n              indexWriter.writeIndex(chunkDocs, this.vectorsStream.getFilePointer());\n              this.vectorsStream.writeVInt(docCount);\n              this.vectorsStream.writeVInt(chunkDocs);\n              this.vectorsStream.copyBytes(vectorsStream, chunkLength);\n              docCount += chunkDocs;\n              this.numDocs += chunkDocs;\n              mergeState.checkAbort.work(300 * chunkDocs);\n              i = nextLiveDoc(docBase + chunkDocs, liveDocs, maxDoc);\n            } else {\n              for (; i < docBase + chunkDocs; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n                final Fields vectors = reader.getTermVectors(i);\n                addAllDocVectors(vectors, mergeState);\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } else {\n            final Fields vectors = reader.getTermVectors(i);\n            addAllDocVectors(vectors, mergeState);\n            ++docCount;\n            mergeState.checkAbort.work(300);\n            i = nextLiveDoc(i + 1, liveDocs, maxDoc);\n          }\n        }\n        \n        vectorsStream.seek(vectorsStream.length() - CodecUtil.footerLength());\n        CodecUtil.checkFooter(vectorsStream);\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];\n      CompressingTermVectorsReader matchingVectorsReader = null;\n      if (matchingSegmentReader != null) {\n        final TermVectorsReader vectorsReader = matchingSegmentReader.getTermVectorsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingTermVectorsReader\n        if (vectorsReader != null && vectorsReader instanceof CompressingTermVectorsReader) {\n          matchingVectorsReader = (CompressingTermVectorsReader) vectorsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingVectorsReader == null\n          || matchingVectorsReader.getVersion() != VERSION_CURRENT\n          || matchingVectorsReader.getCompressionMode() != compressionMode\n          || matchingVectorsReader.getChunkSize() != chunkSize\n          || matchingVectorsReader.getPackedIntsVersion() != PackedInts.VERSION_CURRENT) {\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          final Fields vectors = reader.getTermVectors(i);\n          addAllDocVectors(vectors, mergeState);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        final CompressingStoredFieldsIndexReader index = matchingVectorsReader.getIndex();\n        final IndexInput vectorsStreamOrig = matchingVectorsReader.getVectorsStream();\n        vectorsStreamOrig.seek(0);\n        final ChecksumIndexInput vectorsStream = new BufferedChecksumIndexInput(vectorsStreamOrig.clone());\n        \n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; ) {\n          // We make sure to move the checksum input in any case, otherwise the final\n          // integrity check might need to read the whole file a second time\n          final long startPointer = index.getStartPointer(i);\n          if (startPointer > vectorsStream.getFilePointer()) {\n            vectorsStream.seek(startPointer);\n          }\n          if (pendingDocs.isEmpty()\n              && (i == 0 || index.getStartPointer(i - 1) < startPointer)) { // start of a chunk\n            final int docBase = vectorsStream.readVInt();\n            final int chunkDocs = vectorsStream.readVInt();\n            assert docBase + chunkDocs <= matchingSegmentReader.maxDoc();\n            if (docBase + chunkDocs < matchingSegmentReader.maxDoc()\n                && nextDeletedDoc(docBase, liveDocs, docBase + chunkDocs) == docBase + chunkDocs) {\n              final long chunkEnd = index.getStartPointer(docBase + chunkDocs);\n              final long chunkLength = chunkEnd - vectorsStream.getFilePointer();\n              indexWriter.writeIndex(chunkDocs, this.vectorsStream.getFilePointer());\n              this.vectorsStream.writeVInt(docCount);\n              this.vectorsStream.writeVInt(chunkDocs);\n              this.vectorsStream.copyBytes(vectorsStream, chunkLength);\n              docCount += chunkDocs;\n              this.numDocs += chunkDocs;\n              mergeState.checkAbort.work(300 * chunkDocs);\n              i = nextLiveDoc(docBase + chunkDocs, liveDocs, maxDoc);\n            } else {\n              for (; i < docBase + chunkDocs; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n                final Fields vectors = reader.getTermVectors(i);\n                addAllDocVectors(vectors, mergeState);\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } else {\n            final Fields vectors = reader.getTermVectors(i);\n            addAllDocVectors(vectors, mergeState);\n            ++docCount;\n            mergeState.checkAbort.work(300);\n            i = nextLiveDoc(i + 1, liveDocs, maxDoc);\n          }\n        }\n        \n        vectorsStream.seek(vectorsStream.length() - CodecUtil.footerLength());\n        CodecUtil.checkFooter(vectorsStream);\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (LeafReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = matching.matchingSegmentReaders[idx++];\n      CompressingTermVectorsReader matchingVectorsReader = null;\n      if (matchingSegmentReader != null) {\n        final TermVectorsReader vectorsReader = matchingSegmentReader.getTermVectorsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingTermVectorsReader\n        if (vectorsReader != null && vectorsReader instanceof CompressingTermVectorsReader) {\n          matchingVectorsReader = (CompressingTermVectorsReader) vectorsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingVectorsReader == null\n          || matchingVectorsReader.getVersion() != VERSION_CURRENT\n          || matchingVectorsReader.getCompressionMode() != compressionMode\n          || matchingVectorsReader.getChunkSize() != chunkSize\n          || matchingVectorsReader.getPackedIntsVersion() != PackedInts.VERSION_CURRENT) {\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          final Fields vectors = reader.getTermVectors(i);\n          addAllDocVectors(vectors, mergeState);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        final CompressingStoredFieldsIndexReader index = matchingVectorsReader.getIndex();\n        final IndexInput vectorsStreamOrig = matchingVectorsReader.getVectorsStream();\n        vectorsStreamOrig.seek(0);\n        final ChecksumIndexInput vectorsStream = new BufferedChecksumIndexInput(vectorsStreamOrig.clone());\n        \n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; ) {\n          // We make sure to move the checksum input in any case, otherwise the final\n          // integrity check might need to read the whole file a second time\n          final long startPointer = index.getStartPointer(i);\n          if (startPointer > vectorsStream.getFilePointer()) {\n            vectorsStream.seek(startPointer);\n          }\n          if (pendingDocs.isEmpty()\n              && (i == 0 || index.getStartPointer(i - 1) < startPointer)) { // start of a chunk\n            final int docBase = vectorsStream.readVInt();\n            final int chunkDocs = vectorsStream.readVInt();\n            assert docBase + chunkDocs <= matchingSegmentReader.maxDoc();\n            if (docBase + chunkDocs < matchingSegmentReader.maxDoc()\n                && nextDeletedDoc(docBase, liveDocs, docBase + chunkDocs) == docBase + chunkDocs) {\n              final long chunkEnd = index.getStartPointer(docBase + chunkDocs);\n              final long chunkLength = chunkEnd - vectorsStream.getFilePointer();\n              indexWriter.writeIndex(chunkDocs, this.vectorsStream.getFilePointer());\n              this.vectorsStream.writeVInt(docCount);\n              this.vectorsStream.writeVInt(chunkDocs);\n              this.vectorsStream.copyBytes(vectorsStream, chunkLength);\n              docCount += chunkDocs;\n              this.numDocs += chunkDocs;\n              mergeState.checkAbort.work(300 * chunkDocs);\n              i = nextLiveDoc(docBase + chunkDocs, liveDocs, maxDoc);\n            } else {\n              for (; i < docBase + chunkDocs; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n                final Fields vectors = reader.getTermVectors(i);\n                addAllDocVectors(vectors, mergeState);\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } else {\n            final Fields vectors = reader.getTermVectors(i);\n            addAllDocVectors(vectors, mergeState);\n            ++docCount;\n            mergeState.checkAbort.work(300);\n            i = nextLiveDoc(i + 1, liveDocs, maxDoc);\n          }\n        }\n        \n        vectorsStream.seek(vectorsStream.length() - CodecUtil.footerLength());\n        CodecUtil.checkFooter(vectorsStream);\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = matching.matchingSegmentReaders[idx++];\n      CompressingTermVectorsReader matchingVectorsReader = null;\n      if (matchingSegmentReader != null) {\n        final TermVectorsReader vectorsReader = matchingSegmentReader.getTermVectorsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingTermVectorsReader\n        if (vectorsReader != null && vectorsReader instanceof CompressingTermVectorsReader) {\n          matchingVectorsReader = (CompressingTermVectorsReader) vectorsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingVectorsReader == null\n          || matchingVectorsReader.getVersion() != VERSION_CURRENT\n          || matchingVectorsReader.getCompressionMode() != compressionMode\n          || matchingVectorsReader.getChunkSize() != chunkSize\n          || matchingVectorsReader.getPackedIntsVersion() != PackedInts.VERSION_CURRENT) {\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          final Fields vectors = reader.getTermVectors(i);\n          addAllDocVectors(vectors, mergeState);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        final CompressingStoredFieldsIndexReader index = matchingVectorsReader.getIndex();\n        final IndexInput vectorsStreamOrig = matchingVectorsReader.getVectorsStream();\n        vectorsStreamOrig.seek(0);\n        final ChecksumIndexInput vectorsStream = new BufferedChecksumIndexInput(vectorsStreamOrig.clone());\n        \n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; ) {\n          // We make sure to move the checksum input in any case, otherwise the final\n          // integrity check might need to read the whole file a second time\n          final long startPointer = index.getStartPointer(i);\n          if (startPointer > vectorsStream.getFilePointer()) {\n            vectorsStream.seek(startPointer);\n          }\n          if (pendingDocs.isEmpty()\n              && (i == 0 || index.getStartPointer(i - 1) < startPointer)) { // start of a chunk\n            final int docBase = vectorsStream.readVInt();\n            final int chunkDocs = vectorsStream.readVInt();\n            assert docBase + chunkDocs <= matchingSegmentReader.maxDoc();\n            if (docBase + chunkDocs < matchingSegmentReader.maxDoc()\n                && nextDeletedDoc(docBase, liveDocs, docBase + chunkDocs) == docBase + chunkDocs) {\n              final long chunkEnd = index.getStartPointer(docBase + chunkDocs);\n              final long chunkLength = chunkEnd - vectorsStream.getFilePointer();\n              indexWriter.writeIndex(chunkDocs, this.vectorsStream.getFilePointer());\n              this.vectorsStream.writeVInt(docCount);\n              this.vectorsStream.writeVInt(chunkDocs);\n              this.vectorsStream.copyBytes(vectorsStream, chunkLength);\n              docCount += chunkDocs;\n              this.numDocs += chunkDocs;\n              mergeState.checkAbort.work(300 * chunkDocs);\n              i = nextLiveDoc(docBase + chunkDocs, liveDocs, maxDoc);\n            } else {\n              for (; i < docBase + chunkDocs; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n                final Fields vectors = reader.getTermVectors(i);\n                addAllDocVectors(vectors, mergeState);\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } else {\n            final Fields vectors = reader.getTermVectors(i);\n            addAllDocVectors(vectors, mergeState);\n            ++docCount;\n            mergeState.checkAbort.work(300);\n            i = nextLiveDoc(i + 1, liveDocs, maxDoc);\n          }\n        }\n        \n        vectorsStream.seek(vectorsStream.length() - CodecUtil.footerLength());\n        CodecUtil.checkFooter(vectorsStream);\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2131047ecceac64b54ba70feec3d26bbd7e483d7","date":1411862069,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n\n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      CompressingTermVectorsReader matchingVectorsReader = null;\n      final TermVectorsReader vectorsReader = mergeState.termVectorsReaders[readerIndex];\n      if (matching.matchingReaders[readerIndex]) {\n        // we can only bulk-copy if the matching reader is also a CompressingTermVectorsReader\n        if (vectorsReader != null && vectorsReader instanceof CompressingTermVectorsReader) {\n          matchingVectorsReader = (CompressingTermVectorsReader) vectorsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      if (matchingVectorsReader == null\n          || matchingVectorsReader.getVersion() != VERSION_CURRENT\n          || matchingVectorsReader.getCompressionMode() != compressionMode\n          || matchingVectorsReader.getChunkSize() != chunkSize\n          || matchingVectorsReader.getPackedIntsVersion() != PackedInts.VERSION_CURRENT) {\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          Fields vectors;\n          if (vectorsReader == null) {\n            vectors = null;\n          } else {\n            vectors = vectorsReader.get(i);\n          }\n          addAllDocVectors(vectors, mergeState);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        final CompressingStoredFieldsIndexReader index = matchingVectorsReader.getIndex();\n        final IndexInput vectorsStreamOrig = matchingVectorsReader.getVectorsStream();\n        vectorsStreamOrig.seek(0);\n        final ChecksumIndexInput vectorsStream = new BufferedChecksumIndexInput(vectorsStreamOrig.clone());\n        \n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; ) {\n          // We make sure to move the checksum input in any case, otherwise the final\n          // integrity check might need to read the whole file a second time\n          final long startPointer = index.getStartPointer(i);\n          if (startPointer > vectorsStream.getFilePointer()) {\n            vectorsStream.seek(startPointer);\n          }\n          if (pendingDocs.isEmpty()\n              && (i == 0 || index.getStartPointer(i - 1) < startPointer)) { // start of a chunk\n            final int docBase = vectorsStream.readVInt();\n            final int chunkDocs = vectorsStream.readVInt();\n            assert docBase + chunkDocs <= maxDoc;\n            if (docBase + chunkDocs < maxDoc\n                && nextDeletedDoc(docBase, liveDocs, docBase + chunkDocs) == docBase + chunkDocs) {\n              final long chunkEnd = index.getStartPointer(docBase + chunkDocs);\n              final long chunkLength = chunkEnd - vectorsStream.getFilePointer();\n              indexWriter.writeIndex(chunkDocs, this.vectorsStream.getFilePointer());\n              this.vectorsStream.writeVInt(docCount);\n              this.vectorsStream.writeVInt(chunkDocs);\n              this.vectorsStream.copyBytes(vectorsStream, chunkLength);\n              docCount += chunkDocs;\n              this.numDocs += chunkDocs;\n              mergeState.checkAbort.work(300 * chunkDocs);\n              i = nextLiveDoc(docBase + chunkDocs, liveDocs, maxDoc);\n            } else {\n              for (; i < docBase + chunkDocs; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n                Fields vectors;\n                if (vectorsReader == null) {\n                  vectors = null;\n                } else {\n                  vectors = vectorsReader.get(i);\n                }\n                addAllDocVectors(vectors, mergeState);\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } else {\n            Fields vectors;\n            if (vectorsReader == null) {\n              vectors = null;\n            } else {\n              vectors = vectorsReader.get(i);\n            }\n            addAllDocVectors(vectors, mergeState);\n            ++docCount;\n            mergeState.checkAbort.work(300);\n            i = nextLiveDoc(i + 1, liveDocs, maxDoc);\n          }\n        }\n        \n        vectorsStream.seek(vectorsStream.length() - CodecUtil.footerLength());\n        CodecUtil.checkFooter(vectorsStream);\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (LeafReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = matching.matchingSegmentReaders[idx++];\n      CompressingTermVectorsReader matchingVectorsReader = null;\n      if (matchingSegmentReader != null) {\n        final TermVectorsReader vectorsReader = matchingSegmentReader.getTermVectorsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingTermVectorsReader\n        if (vectorsReader != null && vectorsReader instanceof CompressingTermVectorsReader) {\n          matchingVectorsReader = (CompressingTermVectorsReader) vectorsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingVectorsReader == null\n          || matchingVectorsReader.getVersion() != VERSION_CURRENT\n          || matchingVectorsReader.getCompressionMode() != compressionMode\n          || matchingVectorsReader.getChunkSize() != chunkSize\n          || matchingVectorsReader.getPackedIntsVersion() != PackedInts.VERSION_CURRENT) {\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          final Fields vectors = reader.getTermVectors(i);\n          addAllDocVectors(vectors, mergeState);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        final CompressingStoredFieldsIndexReader index = matchingVectorsReader.getIndex();\n        final IndexInput vectorsStreamOrig = matchingVectorsReader.getVectorsStream();\n        vectorsStreamOrig.seek(0);\n        final ChecksumIndexInput vectorsStream = new BufferedChecksumIndexInput(vectorsStreamOrig.clone());\n        \n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; ) {\n          // We make sure to move the checksum input in any case, otherwise the final\n          // integrity check might need to read the whole file a second time\n          final long startPointer = index.getStartPointer(i);\n          if (startPointer > vectorsStream.getFilePointer()) {\n            vectorsStream.seek(startPointer);\n          }\n          if (pendingDocs.isEmpty()\n              && (i == 0 || index.getStartPointer(i - 1) < startPointer)) { // start of a chunk\n            final int docBase = vectorsStream.readVInt();\n            final int chunkDocs = vectorsStream.readVInt();\n            assert docBase + chunkDocs <= matchingSegmentReader.maxDoc();\n            if (docBase + chunkDocs < matchingSegmentReader.maxDoc()\n                && nextDeletedDoc(docBase, liveDocs, docBase + chunkDocs) == docBase + chunkDocs) {\n              final long chunkEnd = index.getStartPointer(docBase + chunkDocs);\n              final long chunkLength = chunkEnd - vectorsStream.getFilePointer();\n              indexWriter.writeIndex(chunkDocs, this.vectorsStream.getFilePointer());\n              this.vectorsStream.writeVInt(docCount);\n              this.vectorsStream.writeVInt(chunkDocs);\n              this.vectorsStream.copyBytes(vectorsStream, chunkLength);\n              docCount += chunkDocs;\n              this.numDocs += chunkDocs;\n              mergeState.checkAbort.work(300 * chunkDocs);\n              i = nextLiveDoc(docBase + chunkDocs, liveDocs, maxDoc);\n            } else {\n              for (; i < docBase + chunkDocs; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n                final Fields vectors = reader.getTermVectors(i);\n                addAllDocVectors(vectors, mergeState);\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } else {\n            final Fields vectors = reader.getTermVectors(i);\n            addAllDocVectors(vectors, mergeState);\n            ++docCount;\n            mergeState.checkAbort.work(300);\n            i = nextLiveDoc(i + 1, liveDocs, maxDoc);\n          }\n        }\n        \n        vectorsStream.seek(vectorsStream.length() - CodecUtil.footerLength());\n        CodecUtil.checkFooter(vectorsStream);\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":["b88448324d3a96c5842455dabea63450b697b58f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a661533765521d46fcaf46aee272301b6afb6376","date":1411919137,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n\n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      CompressingTermVectorsReader matchingVectorsReader = null;\n      final TermVectorsReader vectorsReader = mergeState.termVectorsReaders[readerIndex];\n      if (matching.matchingReaders[readerIndex]) {\n        // we can only bulk-copy if the matching reader is also a CompressingTermVectorsReader\n        if (vectorsReader != null && vectorsReader instanceof CompressingTermVectorsReader) {\n          matchingVectorsReader = (CompressingTermVectorsReader) vectorsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      if (matchingVectorsReader == null\n          || matchingVectorsReader.getVersion() != VERSION_CURRENT\n          || matchingVectorsReader.getCompressionMode() != compressionMode\n          || matchingVectorsReader.getChunkSize() != chunkSize\n          || matchingVectorsReader.getPackedIntsVersion() != PackedInts.VERSION_CURRENT) {\n        // naive merge...\n        if (vectorsReader != null) {\n          vectorsReader.checkIntegrity();\n        }\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          Fields vectors;\n          if (vectorsReader == null) {\n            vectors = null;\n          } else {\n            vectors = vectorsReader.get(i);\n          }\n          addAllDocVectors(vectors, mergeState);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        final CompressingStoredFieldsIndexReader index = matchingVectorsReader.getIndex();\n        final IndexInput vectorsStreamOrig = matchingVectorsReader.getVectorsStream();\n        vectorsStreamOrig.seek(0);\n        final ChecksumIndexInput vectorsStream = new BufferedChecksumIndexInput(vectorsStreamOrig.clone());\n        \n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; ) {\n          // We make sure to move the checksum input in any case, otherwise the final\n          // integrity check might need to read the whole file a second time\n          final long startPointer = index.getStartPointer(i);\n          if (startPointer > vectorsStream.getFilePointer()) {\n            vectorsStream.seek(startPointer);\n          }\n          if (pendingDocs.isEmpty()\n              && (i == 0 || index.getStartPointer(i - 1) < startPointer)) { // start of a chunk\n            final int docBase = vectorsStream.readVInt();\n            final int chunkDocs = vectorsStream.readVInt();\n            assert docBase + chunkDocs <= maxDoc;\n            if (docBase + chunkDocs < maxDoc\n                && nextDeletedDoc(docBase, liveDocs, docBase + chunkDocs) == docBase + chunkDocs) {\n              final long chunkEnd = index.getStartPointer(docBase + chunkDocs);\n              final long chunkLength = chunkEnd - vectorsStream.getFilePointer();\n              indexWriter.writeIndex(chunkDocs, this.vectorsStream.getFilePointer());\n              this.vectorsStream.writeVInt(docCount);\n              this.vectorsStream.writeVInt(chunkDocs);\n              this.vectorsStream.copyBytes(vectorsStream, chunkLength);\n              docCount += chunkDocs;\n              this.numDocs += chunkDocs;\n              mergeState.checkAbort.work(300 * chunkDocs);\n              i = nextLiveDoc(docBase + chunkDocs, liveDocs, maxDoc);\n            } else {\n              for (; i < docBase + chunkDocs; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n                Fields vectors;\n                if (vectorsReader == null) {\n                  vectors = null;\n                } else {\n                  vectors = vectorsReader.get(i);\n                }\n                addAllDocVectors(vectors, mergeState);\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } else {\n            Fields vectors;\n            if (vectorsReader == null) {\n              vectors = null;\n            } else {\n              vectors = vectorsReader.get(i);\n            }\n            addAllDocVectors(vectors, mergeState);\n            ++docCount;\n            mergeState.checkAbort.work(300);\n            i = nextLiveDoc(i + 1, liveDocs, maxDoc);\n          }\n        }\n        \n        vectorsStream.seek(vectorsStream.length() - CodecUtil.footerLength());\n        CodecUtil.checkFooter(vectorsStream);\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n\n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      CompressingTermVectorsReader matchingVectorsReader = null;\n      final TermVectorsReader vectorsReader = mergeState.termVectorsReaders[readerIndex];\n      if (matching.matchingReaders[readerIndex]) {\n        // we can only bulk-copy if the matching reader is also a CompressingTermVectorsReader\n        if (vectorsReader != null && vectorsReader instanceof CompressingTermVectorsReader) {\n          matchingVectorsReader = (CompressingTermVectorsReader) vectorsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      if (matchingVectorsReader == null\n          || matchingVectorsReader.getVersion() != VERSION_CURRENT\n          || matchingVectorsReader.getCompressionMode() != compressionMode\n          || matchingVectorsReader.getChunkSize() != chunkSize\n          || matchingVectorsReader.getPackedIntsVersion() != PackedInts.VERSION_CURRENT) {\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          Fields vectors;\n          if (vectorsReader == null) {\n            vectors = null;\n          } else {\n            vectors = vectorsReader.get(i);\n          }\n          addAllDocVectors(vectors, mergeState);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        final CompressingStoredFieldsIndexReader index = matchingVectorsReader.getIndex();\n        final IndexInput vectorsStreamOrig = matchingVectorsReader.getVectorsStream();\n        vectorsStreamOrig.seek(0);\n        final ChecksumIndexInput vectorsStream = new BufferedChecksumIndexInput(vectorsStreamOrig.clone());\n        \n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; ) {\n          // We make sure to move the checksum input in any case, otherwise the final\n          // integrity check might need to read the whole file a second time\n          final long startPointer = index.getStartPointer(i);\n          if (startPointer > vectorsStream.getFilePointer()) {\n            vectorsStream.seek(startPointer);\n          }\n          if (pendingDocs.isEmpty()\n              && (i == 0 || index.getStartPointer(i - 1) < startPointer)) { // start of a chunk\n            final int docBase = vectorsStream.readVInt();\n            final int chunkDocs = vectorsStream.readVInt();\n            assert docBase + chunkDocs <= maxDoc;\n            if (docBase + chunkDocs < maxDoc\n                && nextDeletedDoc(docBase, liveDocs, docBase + chunkDocs) == docBase + chunkDocs) {\n              final long chunkEnd = index.getStartPointer(docBase + chunkDocs);\n              final long chunkLength = chunkEnd - vectorsStream.getFilePointer();\n              indexWriter.writeIndex(chunkDocs, this.vectorsStream.getFilePointer());\n              this.vectorsStream.writeVInt(docCount);\n              this.vectorsStream.writeVInt(chunkDocs);\n              this.vectorsStream.copyBytes(vectorsStream, chunkLength);\n              docCount += chunkDocs;\n              this.numDocs += chunkDocs;\n              mergeState.checkAbort.work(300 * chunkDocs);\n              i = nextLiveDoc(docBase + chunkDocs, liveDocs, maxDoc);\n            } else {\n              for (; i < docBase + chunkDocs; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n                Fields vectors;\n                if (vectorsReader == null) {\n                  vectors = null;\n                } else {\n                  vectors = vectorsReader.get(i);\n                }\n                addAllDocVectors(vectors, mergeState);\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } else {\n            Fields vectors;\n            if (vectorsReader == null) {\n              vectors = null;\n            } else {\n              vectors = vectorsReader.get(i);\n            }\n            addAllDocVectors(vectors, mergeState);\n            ++docCount;\n            mergeState.checkAbort.work(300);\n            i = nextLiveDoc(i + 1, liveDocs, maxDoc);\n          }\n        }\n        \n        vectorsStream.seek(vectorsStream.length() - CodecUtil.footerLength());\n        CodecUtil.checkFooter(vectorsStream);\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9bb9a29a5e71a90295f175df8919802993142c9a","date":1412517673,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n\n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      CompressingTermVectorsReader matchingVectorsReader = null;\n      final TermVectorsReader vectorsReader = mergeState.termVectorsReaders[readerIndex];\n      if (matching.matchingReaders[readerIndex]) {\n        // we can only bulk-copy if the matching reader is also a CompressingTermVectorsReader\n        if (vectorsReader != null && vectorsReader instanceof CompressingTermVectorsReader) {\n          matchingVectorsReader = (CompressingTermVectorsReader) vectorsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      if (matchingVectorsReader == null\n          || matchingVectorsReader.getVersion() != VERSION_CURRENT\n          || matchingVectorsReader.getCompressionMode() != compressionMode\n          || matchingVectorsReader.getChunkSize() != chunkSize\n          || matchingVectorsReader.getPackedIntsVersion() != PackedInts.VERSION_CURRENT) {\n        // naive merge...\n        if (vectorsReader != null) {\n          vectorsReader.checkIntegrity();\n        }\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          Fields vectors;\n          if (vectorsReader == null) {\n            vectors = null;\n          } else {\n            vectors = vectorsReader.get(i);\n          }\n          addAllDocVectors(vectors, mergeState);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        final CompressingStoredFieldsIndexReader index = matchingVectorsReader.getIndex();\n        final IndexInput vectorsStreamOrig = matchingVectorsReader.getVectorsStream();\n        vectorsStreamOrig.seek(0);\n        final ChecksumIndexInput vectorsStream = new BufferedChecksumIndexInput(vectorsStreamOrig.clone());\n        \n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; ) {\n          // We make sure to move the checksum input in any case, otherwise the final\n          // integrity check might need to read the whole file a second time\n          final long startPointer = index.getStartPointer(i);\n          if (startPointer > vectorsStream.getFilePointer()) {\n            vectorsStream.seek(startPointer);\n          }\n          if (pendingDocs.isEmpty()\n              && (i == 0 || index.getStartPointer(i - 1) < startPointer)) { // start of a chunk\n            final int docBase = vectorsStream.readVInt();\n            final int chunkDocs = vectorsStream.readVInt();\n            assert docBase + chunkDocs <= maxDoc;\n            if (docBase + chunkDocs < maxDoc\n                && nextDeletedDoc(docBase, liveDocs, docBase + chunkDocs) == docBase + chunkDocs) {\n              final long chunkEnd = index.getStartPointer(docBase + chunkDocs);\n              final long chunkLength = chunkEnd - vectorsStream.getFilePointer();\n              indexWriter.writeIndex(chunkDocs, this.vectorsStream.getFilePointer());\n              this.vectorsStream.writeVInt(docCount);\n              this.vectorsStream.writeVInt(chunkDocs);\n              this.vectorsStream.copyBytes(vectorsStream, chunkLength);\n              docCount += chunkDocs;\n              this.numDocs += chunkDocs;\n              mergeState.checkAbort.work(300 * chunkDocs);\n              i = nextLiveDoc(docBase + chunkDocs, liveDocs, maxDoc);\n            } else {\n              for (; i < docBase + chunkDocs; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n                Fields vectors;\n                if (vectorsReader == null) {\n                  vectors = null;\n                } else {\n                  vectors = vectorsReader.get(i);\n                }\n                addAllDocVectors(vectors, mergeState);\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } else {\n            Fields vectors;\n            if (vectorsReader == null) {\n              vectors = null;\n            } else {\n              vectors = vectorsReader.get(i);\n            }\n            addAllDocVectors(vectors, mergeState);\n            ++docCount;\n            mergeState.checkAbort.work(300);\n            i = nextLiveDoc(i + 1, liveDocs, maxDoc);\n          }\n        }\n        \n        vectorsStream.seek(vectorsStream.length() - CodecUtil.footerLength());\n        CodecUtil.checkFooter(vectorsStream);\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (LeafReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = matching.matchingSegmentReaders[idx++];\n      CompressingTermVectorsReader matchingVectorsReader = null;\n      if (matchingSegmentReader != null) {\n        final TermVectorsReader vectorsReader = matchingSegmentReader.getTermVectorsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingTermVectorsReader\n        if (vectorsReader != null && vectorsReader instanceof CompressingTermVectorsReader) {\n          matchingVectorsReader = (CompressingTermVectorsReader) vectorsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingVectorsReader == null\n          || matchingVectorsReader.getVersion() != VERSION_CURRENT\n          || matchingVectorsReader.getCompressionMode() != compressionMode\n          || matchingVectorsReader.getChunkSize() != chunkSize\n          || matchingVectorsReader.getPackedIntsVersion() != PackedInts.VERSION_CURRENT) {\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          final Fields vectors = reader.getTermVectors(i);\n          addAllDocVectors(vectors, mergeState);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        final CompressingStoredFieldsIndexReader index = matchingVectorsReader.getIndex();\n        final IndexInput vectorsStreamOrig = matchingVectorsReader.getVectorsStream();\n        vectorsStreamOrig.seek(0);\n        final ChecksumIndexInput vectorsStream = new BufferedChecksumIndexInput(vectorsStreamOrig.clone());\n        \n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; ) {\n          // We make sure to move the checksum input in any case, otherwise the final\n          // integrity check might need to read the whole file a second time\n          final long startPointer = index.getStartPointer(i);\n          if (startPointer > vectorsStream.getFilePointer()) {\n            vectorsStream.seek(startPointer);\n          }\n          if (pendingDocs.isEmpty()\n              && (i == 0 || index.getStartPointer(i - 1) < startPointer)) { // start of a chunk\n            final int docBase = vectorsStream.readVInt();\n            final int chunkDocs = vectorsStream.readVInt();\n            assert docBase + chunkDocs <= matchingSegmentReader.maxDoc();\n            if (docBase + chunkDocs < matchingSegmentReader.maxDoc()\n                && nextDeletedDoc(docBase, liveDocs, docBase + chunkDocs) == docBase + chunkDocs) {\n              final long chunkEnd = index.getStartPointer(docBase + chunkDocs);\n              final long chunkLength = chunkEnd - vectorsStream.getFilePointer();\n              indexWriter.writeIndex(chunkDocs, this.vectorsStream.getFilePointer());\n              this.vectorsStream.writeVInt(docCount);\n              this.vectorsStream.writeVInt(chunkDocs);\n              this.vectorsStream.copyBytes(vectorsStream, chunkLength);\n              docCount += chunkDocs;\n              this.numDocs += chunkDocs;\n              mergeState.checkAbort.work(300 * chunkDocs);\n              i = nextLiveDoc(docBase + chunkDocs, liveDocs, maxDoc);\n            } else {\n              for (; i < docBase + chunkDocs; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n                final Fields vectors = reader.getTermVectors(i);\n                addAllDocVectors(vectors, mergeState);\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } else {\n            final Fields vectors = reader.getTermVectors(i);\n            addAllDocVectors(vectors, mergeState);\n            ++docCount;\n            mergeState.checkAbort.work(300);\n            i = nextLiveDoc(i + 1, liveDocs, maxDoc);\n          }\n        }\n        \n        vectorsStream.seek(vectorsStream.length() - CodecUtil.footerLength());\n        CodecUtil.checkFooter(vectorsStream);\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5faf65b6692f15cca0f87bf8666c87899afc619f","date":1420468108,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n\n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      CompressingTermVectorsReader matchingVectorsReader = null;\n      final TermVectorsReader vectorsReader = mergeState.termVectorsReaders[readerIndex];\n      if (matching.matchingReaders[readerIndex]) {\n        // we can only bulk-copy if the matching reader is also a CompressingTermVectorsReader\n        if (vectorsReader != null && vectorsReader instanceof CompressingTermVectorsReader) {\n          matchingVectorsReader = (CompressingTermVectorsReader) vectorsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      if (matchingVectorsReader == null\n          || matchingVectorsReader.getVersion() != VERSION_CURRENT\n          || matchingVectorsReader.getCompressionMode() != compressionMode\n          || matchingVectorsReader.getChunkSize() != chunkSize\n          || matchingVectorsReader.getPackedIntsVersion() != PackedInts.VERSION_CURRENT) {\n        // naive merge...\n        if (vectorsReader != null) {\n          vectorsReader.checkIntegrity();\n        }\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          Fields vectors;\n          if (vectorsReader == null) {\n            vectors = null;\n          } else {\n            vectors = vectorsReader.get(i);\n          }\n          addAllDocVectors(vectors, mergeState);\n          ++docCount;\n        }\n      } else {\n        final CompressingStoredFieldsIndexReader index = matchingVectorsReader.getIndex();\n        final IndexInput vectorsStreamOrig = matchingVectorsReader.getVectorsStream();\n        vectorsStreamOrig.seek(0);\n        final ChecksumIndexInput vectorsStream = new BufferedChecksumIndexInput(vectorsStreamOrig.clone());\n        \n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; ) {\n          // We make sure to move the checksum input in any case, otherwise the final\n          // integrity check might need to read the whole file a second time\n          final long startPointer = index.getStartPointer(i);\n          if (startPointer > vectorsStream.getFilePointer()) {\n            vectorsStream.seek(startPointer);\n          }\n          if (pendingDocs.isEmpty()\n              && (i == 0 || index.getStartPointer(i - 1) < startPointer)) { // start of a chunk\n            final int docBase = vectorsStream.readVInt();\n            final int chunkDocs = vectorsStream.readVInt();\n            assert docBase + chunkDocs <= maxDoc;\n            if (docBase + chunkDocs < maxDoc\n                && nextDeletedDoc(docBase, liveDocs, docBase + chunkDocs) == docBase + chunkDocs) {\n              final long chunkEnd = index.getStartPointer(docBase + chunkDocs);\n              final long chunkLength = chunkEnd - vectorsStream.getFilePointer();\n              indexWriter.writeIndex(chunkDocs, this.vectorsStream.getFilePointer());\n              this.vectorsStream.writeVInt(docCount);\n              this.vectorsStream.writeVInt(chunkDocs);\n              this.vectorsStream.copyBytes(vectorsStream, chunkLength);\n              docCount += chunkDocs;\n              this.numDocs += chunkDocs;\n              i = nextLiveDoc(docBase + chunkDocs, liveDocs, maxDoc);\n            } else {\n              for (; i < docBase + chunkDocs; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n                Fields vectors;\n                if (vectorsReader == null) {\n                  vectors = null;\n                } else {\n                  vectors = vectorsReader.get(i);\n                }\n                addAllDocVectors(vectors, mergeState);\n                ++docCount;\n              }\n            }\n          } else {\n            Fields vectors;\n            if (vectorsReader == null) {\n              vectors = null;\n            } else {\n              vectors = vectorsReader.get(i);\n            }\n            addAllDocVectors(vectors, mergeState);\n            ++docCount;\n            i = nextLiveDoc(i + 1, liveDocs, maxDoc);\n          }\n        }\n        \n        vectorsStream.seek(vectorsStream.length() - CodecUtil.footerLength());\n        CodecUtil.checkFooter(vectorsStream);\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n\n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      CompressingTermVectorsReader matchingVectorsReader = null;\n      final TermVectorsReader vectorsReader = mergeState.termVectorsReaders[readerIndex];\n      if (matching.matchingReaders[readerIndex]) {\n        // we can only bulk-copy if the matching reader is also a CompressingTermVectorsReader\n        if (vectorsReader != null && vectorsReader instanceof CompressingTermVectorsReader) {\n          matchingVectorsReader = (CompressingTermVectorsReader) vectorsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      if (matchingVectorsReader == null\n          || matchingVectorsReader.getVersion() != VERSION_CURRENT\n          || matchingVectorsReader.getCompressionMode() != compressionMode\n          || matchingVectorsReader.getChunkSize() != chunkSize\n          || matchingVectorsReader.getPackedIntsVersion() != PackedInts.VERSION_CURRENT) {\n        // naive merge...\n        if (vectorsReader != null) {\n          vectorsReader.checkIntegrity();\n        }\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          Fields vectors;\n          if (vectorsReader == null) {\n            vectors = null;\n          } else {\n            vectors = vectorsReader.get(i);\n          }\n          addAllDocVectors(vectors, mergeState);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        final CompressingStoredFieldsIndexReader index = matchingVectorsReader.getIndex();\n        final IndexInput vectorsStreamOrig = matchingVectorsReader.getVectorsStream();\n        vectorsStreamOrig.seek(0);\n        final ChecksumIndexInput vectorsStream = new BufferedChecksumIndexInput(vectorsStreamOrig.clone());\n        \n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; ) {\n          // We make sure to move the checksum input in any case, otherwise the final\n          // integrity check might need to read the whole file a second time\n          final long startPointer = index.getStartPointer(i);\n          if (startPointer > vectorsStream.getFilePointer()) {\n            vectorsStream.seek(startPointer);\n          }\n          if (pendingDocs.isEmpty()\n              && (i == 0 || index.getStartPointer(i - 1) < startPointer)) { // start of a chunk\n            final int docBase = vectorsStream.readVInt();\n            final int chunkDocs = vectorsStream.readVInt();\n            assert docBase + chunkDocs <= maxDoc;\n            if (docBase + chunkDocs < maxDoc\n                && nextDeletedDoc(docBase, liveDocs, docBase + chunkDocs) == docBase + chunkDocs) {\n              final long chunkEnd = index.getStartPointer(docBase + chunkDocs);\n              final long chunkLength = chunkEnd - vectorsStream.getFilePointer();\n              indexWriter.writeIndex(chunkDocs, this.vectorsStream.getFilePointer());\n              this.vectorsStream.writeVInt(docCount);\n              this.vectorsStream.writeVInt(chunkDocs);\n              this.vectorsStream.copyBytes(vectorsStream, chunkLength);\n              docCount += chunkDocs;\n              this.numDocs += chunkDocs;\n              mergeState.checkAbort.work(300 * chunkDocs);\n              i = nextLiveDoc(docBase + chunkDocs, liveDocs, maxDoc);\n            } else {\n              for (; i < docBase + chunkDocs; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n                Fields vectors;\n                if (vectorsReader == null) {\n                  vectors = null;\n                } else {\n                  vectors = vectorsReader.get(i);\n                }\n                addAllDocVectors(vectors, mergeState);\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } else {\n            Fields vectors;\n            if (vectorsReader == null) {\n              vectors = null;\n            } else {\n              vectors = vectorsReader.get(i);\n            }\n            addAllDocVectors(vectors, mergeState);\n            ++docCount;\n            mergeState.checkAbort.work(300);\n            i = nextLiveDoc(i + 1, liveDocs, maxDoc);\n          }\n        }\n        \n        vectorsStream.seek(vectorsStream.length() - CodecUtil.footerLength());\n        CodecUtil.checkFooter(vectorsStream);\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b88448324d3a96c5842455dabea63450b697b58f","date":1421779050,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n\n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      CompressingTermVectorsReader matchingVectorsReader = null;\n      final TermVectorsReader vectorsReader = mergeState.termVectorsReaders[readerIndex];\n      if (matching.matchingReaders[readerIndex]) {\n        // we can only bulk-copy if the matching reader is also a CompressingTermVectorsReader\n        if (vectorsReader != null && vectorsReader instanceof CompressingTermVectorsReader) {\n          matchingVectorsReader = (CompressingTermVectorsReader) vectorsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n      \n      if (matchingVectorsReader != null &&\n          matchingVectorsReader.getCompressionMode() == compressionMode &&\n          matchingVectorsReader.getChunkSize() == chunkSize &&\n          matchingVectorsReader.getVersion() == VERSION_CURRENT && \n          matchingVectorsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n          BULK_MERGE_ENABLED &&\n          liveDocs == null &&\n          !tooDirty(matchingVectorsReader)) {\n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        matchingVectorsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (!pendingDocs.isEmpty()) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the vectors index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingVectorsReader.getVectorsStream();\n        CompressingStoredFieldsIndexReader index = matchingVectorsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int bufferedDocs = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          indexWriter.writeIndex(bufferedDocs, vectorsStream.getFilePointer());\n          vectorsStream.writeVInt(docCount); // rebase\n          vectorsStream.writeVInt(bufferedDocs);\n          docID += bufferedDocs;\n          docCount += bufferedDocs;\n          numDocs += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingVectorsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          vectorsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingVectorsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingVectorsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingVectorsReader.getNumChunks();\n        numDirtyChunks += matchingVectorsReader.getNumDirtyChunks();\n      } else {        \n        // naive merge...\n        if (vectorsReader != null) {\n          vectorsReader.checkIntegrity();\n        }\n        for (int i = 0; i < maxDoc; i++) {\n          if (liveDocs != null && liveDocs.get(i) == false) {\n            continue;\n          }\n          Fields vectors;\n          if (vectorsReader == null) {\n            vectors = null;\n          } else {\n            vectors = vectorsReader.get(i);\n          }\n          addAllDocVectors(vectors, mergeState);\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n\n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      CompressingTermVectorsReader matchingVectorsReader = null;\n      final TermVectorsReader vectorsReader = mergeState.termVectorsReaders[readerIndex];\n      if (matching.matchingReaders[readerIndex]) {\n        // we can only bulk-copy if the matching reader is also a CompressingTermVectorsReader\n        if (vectorsReader != null && vectorsReader instanceof CompressingTermVectorsReader) {\n          matchingVectorsReader = (CompressingTermVectorsReader) vectorsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      if (matchingVectorsReader == null\n          || matchingVectorsReader.getVersion() != VERSION_CURRENT\n          || matchingVectorsReader.getCompressionMode() != compressionMode\n          || matchingVectorsReader.getChunkSize() != chunkSize\n          || matchingVectorsReader.getPackedIntsVersion() != PackedInts.VERSION_CURRENT) {\n        // naive merge...\n        if (vectorsReader != null) {\n          vectorsReader.checkIntegrity();\n        }\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          Fields vectors;\n          if (vectorsReader == null) {\n            vectors = null;\n          } else {\n            vectors = vectorsReader.get(i);\n          }\n          addAllDocVectors(vectors, mergeState);\n          ++docCount;\n        }\n      } else {\n        final CompressingStoredFieldsIndexReader index = matchingVectorsReader.getIndex();\n        final IndexInput vectorsStreamOrig = matchingVectorsReader.getVectorsStream();\n        vectorsStreamOrig.seek(0);\n        final ChecksumIndexInput vectorsStream = new BufferedChecksumIndexInput(vectorsStreamOrig.clone());\n        \n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; ) {\n          // We make sure to move the checksum input in any case, otherwise the final\n          // integrity check might need to read the whole file a second time\n          final long startPointer = index.getStartPointer(i);\n          if (startPointer > vectorsStream.getFilePointer()) {\n            vectorsStream.seek(startPointer);\n          }\n          if (pendingDocs.isEmpty()\n              && (i == 0 || index.getStartPointer(i - 1) < startPointer)) { // start of a chunk\n            final int docBase = vectorsStream.readVInt();\n            final int chunkDocs = vectorsStream.readVInt();\n            assert docBase + chunkDocs <= maxDoc;\n            if (docBase + chunkDocs < maxDoc\n                && nextDeletedDoc(docBase, liveDocs, docBase + chunkDocs) == docBase + chunkDocs) {\n              final long chunkEnd = index.getStartPointer(docBase + chunkDocs);\n              final long chunkLength = chunkEnd - vectorsStream.getFilePointer();\n              indexWriter.writeIndex(chunkDocs, this.vectorsStream.getFilePointer());\n              this.vectorsStream.writeVInt(docCount);\n              this.vectorsStream.writeVInt(chunkDocs);\n              this.vectorsStream.copyBytes(vectorsStream, chunkLength);\n              docCount += chunkDocs;\n              this.numDocs += chunkDocs;\n              i = nextLiveDoc(docBase + chunkDocs, liveDocs, maxDoc);\n            } else {\n              for (; i < docBase + chunkDocs; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n                Fields vectors;\n                if (vectorsReader == null) {\n                  vectors = null;\n                } else {\n                  vectors = vectorsReader.get(i);\n                }\n                addAllDocVectors(vectors, mergeState);\n                ++docCount;\n              }\n            }\n          } else {\n            Fields vectors;\n            if (vectorsReader == null) {\n              vectors = null;\n            } else {\n              vectors = vectorsReader.get(i);\n            }\n            addAllDocVectors(vectors, mergeState);\n            ++docCount;\n            i = nextLiveDoc(i + 1, liveDocs, maxDoc);\n          }\n        }\n        \n        vectorsStream.seek(vectorsStream.length() - CodecUtil.footerLength());\n        CodecUtil.checkFooter(vectorsStream);\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":["2131047ecceac64b54ba70feec3d26bbd7e483d7","eda61b1e90b490cc5837200e04c02639a0d272c7","1db0e36bd56f06d5830e980340e7fe58353a44f7"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fbe8fc0e68a5e2e7acce82ba880a982bd15cfab8","date":1462567286,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    if (mergeState.segmentInfo.getIndexSort() != null) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      return super.merge(mergeState);\n    }\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n\n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      CompressingTermVectorsReader matchingVectorsReader = null;\n      final TermVectorsReader vectorsReader = mergeState.termVectorsReaders[readerIndex];\n      if (matching.matchingReaders[readerIndex]) {\n        // we can only bulk-copy if the matching reader is also a CompressingTermVectorsReader\n        if (vectorsReader != null && vectorsReader instanceof CompressingTermVectorsReader) {\n          matchingVectorsReader = (CompressingTermVectorsReader) vectorsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n      \n      if (matchingVectorsReader != null &&\n          matchingVectorsReader.getCompressionMode() == compressionMode &&\n          matchingVectorsReader.getChunkSize() == chunkSize &&\n          matchingVectorsReader.getVersion() == VERSION_CURRENT && \n          matchingVectorsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n          BULK_MERGE_ENABLED &&\n          liveDocs == null &&\n          !tooDirty(matchingVectorsReader)) {\n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        matchingVectorsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (!pendingDocs.isEmpty()) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the vectors index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingVectorsReader.getVectorsStream();\n        CompressingStoredFieldsIndexReader index = matchingVectorsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int bufferedDocs = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          indexWriter.writeIndex(bufferedDocs, vectorsStream.getFilePointer());\n          vectorsStream.writeVInt(docCount); // rebase\n          vectorsStream.writeVInt(bufferedDocs);\n          docID += bufferedDocs;\n          docCount += bufferedDocs;\n          numDocs += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingVectorsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          vectorsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingVectorsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingVectorsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingVectorsReader.getNumChunks();\n        numDirtyChunks += matchingVectorsReader.getNumDirtyChunks();\n      } else {        \n        // naive merge...\n        if (vectorsReader != null) {\n          vectorsReader.checkIntegrity();\n        }\n        for (int i = 0; i < maxDoc; i++) {\n          if (liveDocs != null && liveDocs.get(i) == false) {\n            continue;\n          }\n          Fields vectors;\n          if (vectorsReader == null) {\n            vectors = null;\n          } else {\n            vectors = vectorsReader.get(i);\n          }\n          addAllDocVectors(vectors, mergeState);\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n\n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      CompressingTermVectorsReader matchingVectorsReader = null;\n      final TermVectorsReader vectorsReader = mergeState.termVectorsReaders[readerIndex];\n      if (matching.matchingReaders[readerIndex]) {\n        // we can only bulk-copy if the matching reader is also a CompressingTermVectorsReader\n        if (vectorsReader != null && vectorsReader instanceof CompressingTermVectorsReader) {\n          matchingVectorsReader = (CompressingTermVectorsReader) vectorsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n      \n      if (matchingVectorsReader != null &&\n          matchingVectorsReader.getCompressionMode() == compressionMode &&\n          matchingVectorsReader.getChunkSize() == chunkSize &&\n          matchingVectorsReader.getVersion() == VERSION_CURRENT && \n          matchingVectorsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n          BULK_MERGE_ENABLED &&\n          liveDocs == null &&\n          !tooDirty(matchingVectorsReader)) {\n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        matchingVectorsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (!pendingDocs.isEmpty()) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the vectors index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingVectorsReader.getVectorsStream();\n        CompressingStoredFieldsIndexReader index = matchingVectorsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int bufferedDocs = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          indexWriter.writeIndex(bufferedDocs, vectorsStream.getFilePointer());\n          vectorsStream.writeVInt(docCount); // rebase\n          vectorsStream.writeVInt(bufferedDocs);\n          docID += bufferedDocs;\n          docCount += bufferedDocs;\n          numDocs += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingVectorsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          vectorsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingVectorsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingVectorsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingVectorsReader.getNumChunks();\n        numDirtyChunks += matchingVectorsReader.getNumDirtyChunks();\n      } else {        \n        // naive merge...\n        if (vectorsReader != null) {\n          vectorsReader.checkIntegrity();\n        }\n        for (int i = 0; i < maxDoc; i++) {\n          if (liveDocs != null && liveDocs.get(i) == false) {\n            continue;\n          }\n          Fields vectors;\n          if (vectorsReader == null) {\n            vectors = null;\n          } else {\n            vectors = vectorsReader.get(i);\n          }\n          addAllDocVectors(vectors, mergeState);\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3d33e731a93d4b57e662ff094f64f94a745422d4","date":1463128289,"type":3,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    if (mergeState.segmentInfo.getIndexSort() != null) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      return super.merge(mergeState);\n    }\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n\n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      CompressingTermVectorsReader matchingVectorsReader = null;\n      final TermVectorsReader vectorsReader = mergeState.termVectorsReaders[readerIndex];\n      if (matching.matchingReaders[readerIndex]) {\n        // we can only bulk-copy if the matching reader is also a CompressingTermVectorsReader\n        if (vectorsReader != null && vectorsReader instanceof CompressingTermVectorsReader) {\n          matchingVectorsReader = (CompressingTermVectorsReader) vectorsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n      \n      if (matchingVectorsReader != null &&\n          matchingVectorsReader.getCompressionMode() == compressionMode &&\n          matchingVectorsReader.getChunkSize() == chunkSize &&\n          matchingVectorsReader.getVersion() == VERSION_CURRENT && \n          matchingVectorsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n          BULK_MERGE_ENABLED &&\n          liveDocs == null &&\n          !tooDirty(matchingVectorsReader)) {\n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        matchingVectorsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (!pendingDocs.isEmpty()) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the vectors index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingVectorsReader.getVectorsStream();\n        CompressingStoredFieldsIndexReader index = matchingVectorsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int bufferedDocs = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          indexWriter.writeIndex(bufferedDocs, vectorsStream.getFilePointer());\n          vectorsStream.writeVInt(docCount); // rebase\n          vectorsStream.writeVInt(bufferedDocs);\n          docID += bufferedDocs;\n          docCount += bufferedDocs;\n          numDocs += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingVectorsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          vectorsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingVectorsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingVectorsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingVectorsReader.getNumChunks();\n        numDirtyChunks += matchingVectorsReader.getNumDirtyChunks();\n      } else {        \n        // naive merge...\n        if (vectorsReader != null) {\n          vectorsReader.checkIntegrity();\n        }\n        for (int i = 0; i < maxDoc; i++) {\n          if (liveDocs != null && liveDocs.get(i) == false) {\n            continue;\n          }\n          Fields vectors;\n          if (vectorsReader == null) {\n            vectors = null;\n          } else {\n            vectors = vectorsReader.get(i);\n          }\n          addAllDocVectors(vectors, mergeState);\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n\n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      CompressingTermVectorsReader matchingVectorsReader = null;\n      final TermVectorsReader vectorsReader = mergeState.termVectorsReaders[readerIndex];\n      if (matching.matchingReaders[readerIndex]) {\n        // we can only bulk-copy if the matching reader is also a CompressingTermVectorsReader\n        if (vectorsReader != null && vectorsReader instanceof CompressingTermVectorsReader) {\n          matchingVectorsReader = (CompressingTermVectorsReader) vectorsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n      \n      if (matchingVectorsReader != null &&\n          matchingVectorsReader.getCompressionMode() == compressionMode &&\n          matchingVectorsReader.getChunkSize() == chunkSize &&\n          matchingVectorsReader.getVersion() == VERSION_CURRENT && \n          matchingVectorsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n          BULK_MERGE_ENABLED &&\n          liveDocs == null &&\n          !tooDirty(matchingVectorsReader)) {\n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        matchingVectorsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (!pendingDocs.isEmpty()) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the vectors index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingVectorsReader.getVectorsStream();\n        CompressingStoredFieldsIndexReader index = matchingVectorsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int bufferedDocs = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          indexWriter.writeIndex(bufferedDocs, vectorsStream.getFilePointer());\n          vectorsStream.writeVInt(docCount); // rebase\n          vectorsStream.writeVInt(bufferedDocs);\n          docID += bufferedDocs;\n          docCount += bufferedDocs;\n          numDocs += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingVectorsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          vectorsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingVectorsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingVectorsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingVectorsReader.getNumChunks();\n        numDirtyChunks += matchingVectorsReader.getNumDirtyChunks();\n      } else {        \n        // naive merge...\n        if (vectorsReader != null) {\n          vectorsReader.checkIntegrity();\n        }\n        for (int i = 0; i < maxDoc; i++) {\n          if (liveDocs != null && liveDocs.get(i) == false) {\n            continue;\n          }\n          Fields vectors;\n          if (vectorsReader == null) {\n            vectors = null;\n          } else {\n            vectors = vectorsReader.get(i);\n          }\n          addAllDocVectors(vectors, mergeState);\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0ad30c6a479e764150a3316e57263319775f1df2","date":1463395403,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    if (mergeState.segmentInfo.getIndexSort() != null) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      return super.merge(mergeState);\n    }\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n\n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      CompressingTermVectorsReader matchingVectorsReader = null;\n      final TermVectorsReader vectorsReader = mergeState.termVectorsReaders[readerIndex];\n      if (matching.matchingReaders[readerIndex]) {\n        // we can only bulk-copy if the matching reader is also a CompressingTermVectorsReader\n        if (vectorsReader != null && vectorsReader instanceof CompressingTermVectorsReader) {\n          matchingVectorsReader = (CompressingTermVectorsReader) vectorsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n      \n      if (matchingVectorsReader != null &&\n          matchingVectorsReader.getCompressionMode() == compressionMode &&\n          matchingVectorsReader.getChunkSize() == chunkSize &&\n          matchingVectorsReader.getVersion() == VERSION_CURRENT && \n          matchingVectorsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n          BULK_MERGE_ENABLED &&\n          liveDocs == null &&\n          !tooDirty(matchingVectorsReader)) {\n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        matchingVectorsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (!pendingDocs.isEmpty()) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the vectors index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingVectorsReader.getVectorsStream();\n        CompressingStoredFieldsIndexReader index = matchingVectorsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int bufferedDocs = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          indexWriter.writeIndex(bufferedDocs, vectorsStream.getFilePointer());\n          vectorsStream.writeVInt(docCount); // rebase\n          vectorsStream.writeVInt(bufferedDocs);\n          docID += bufferedDocs;\n          docCount += bufferedDocs;\n          numDocs += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingVectorsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          vectorsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingVectorsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingVectorsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingVectorsReader.getNumChunks();\n        numDirtyChunks += matchingVectorsReader.getNumDirtyChunks();\n      } else {        \n        // naive merge...\n        if (vectorsReader != null) {\n          vectorsReader.checkIntegrity();\n        }\n        for (int i = 0; i < maxDoc; i++) {\n          if (liveDocs != null && liveDocs.get(i) == false) {\n            continue;\n          }\n          Fields vectors;\n          if (vectorsReader == null) {\n            vectors = null;\n          } else {\n            vectors = vectorsReader.get(i);\n          }\n          addAllDocVectors(vectors, mergeState);\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n\n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      CompressingTermVectorsReader matchingVectorsReader = null;\n      final TermVectorsReader vectorsReader = mergeState.termVectorsReaders[readerIndex];\n      if (matching.matchingReaders[readerIndex]) {\n        // we can only bulk-copy if the matching reader is also a CompressingTermVectorsReader\n        if (vectorsReader != null && vectorsReader instanceof CompressingTermVectorsReader) {\n          matchingVectorsReader = (CompressingTermVectorsReader) vectorsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n      \n      if (matchingVectorsReader != null &&\n          matchingVectorsReader.getCompressionMode() == compressionMode &&\n          matchingVectorsReader.getChunkSize() == chunkSize &&\n          matchingVectorsReader.getVersion() == VERSION_CURRENT && \n          matchingVectorsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n          BULK_MERGE_ENABLED &&\n          liveDocs == null &&\n          !tooDirty(matchingVectorsReader)) {\n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        matchingVectorsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (!pendingDocs.isEmpty()) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the vectors index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingVectorsReader.getVectorsStream();\n        CompressingStoredFieldsIndexReader index = matchingVectorsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int bufferedDocs = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          indexWriter.writeIndex(bufferedDocs, vectorsStream.getFilePointer());\n          vectorsStream.writeVInt(docCount); // rebase\n          vectorsStream.writeVInt(bufferedDocs);\n          docID += bufferedDocs;\n          docCount += bufferedDocs;\n          numDocs += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingVectorsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          vectorsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingVectorsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingVectorsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingVectorsReader.getNumChunks();\n        numDirtyChunks += matchingVectorsReader.getNumDirtyChunks();\n      } else {        \n        // naive merge...\n        if (vectorsReader != null) {\n          vectorsReader.checkIntegrity();\n        }\n        for (int i = 0; i < maxDoc; i++) {\n          if (liveDocs != null && liveDocs.get(i) == false) {\n            continue;\n          }\n          Fields vectors;\n          if (vectorsReader == null) {\n            vectors = null;\n          } else {\n            vectors = vectorsReader.get(i);\n          }\n          addAllDocVectors(vectors, mergeState);\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d470c8182e92b264680e34081b75e70a9f2b3c89","date":1463985353,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    if (mergeState.segmentInfo.getIndexSort() != null) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      return super.merge(mergeState);\n    }\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n\n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      CompressingTermVectorsReader matchingVectorsReader = null;\n      final TermVectorsReader vectorsReader = mergeState.termVectorsReaders[readerIndex];\n      if (matching.matchingReaders[readerIndex]) {\n        // we can only bulk-copy if the matching reader is also a CompressingTermVectorsReader\n        if (vectorsReader != null && vectorsReader instanceof CompressingTermVectorsReader) {\n          matchingVectorsReader = (CompressingTermVectorsReader) vectorsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n      \n      if (matchingVectorsReader != null &&\n          matchingVectorsReader.getCompressionMode() == compressionMode &&\n          matchingVectorsReader.getChunkSize() == chunkSize &&\n          matchingVectorsReader.getVersion() == VERSION_CURRENT && \n          matchingVectorsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n          BULK_MERGE_ENABLED &&\n          liveDocs == null &&\n          !tooDirty(matchingVectorsReader)) {\n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        matchingVectorsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (!pendingDocs.isEmpty()) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the vectors index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingVectorsReader.getVectorsStream();\n        CompressingStoredFieldsIndexReader index = matchingVectorsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int bufferedDocs = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          indexWriter.writeIndex(bufferedDocs, vectorsStream.getFilePointer());\n          vectorsStream.writeVInt(docCount); // rebase\n          vectorsStream.writeVInt(bufferedDocs);\n          docID += bufferedDocs;\n          docCount += bufferedDocs;\n          numDocs += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingVectorsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          vectorsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingVectorsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingVectorsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingVectorsReader.getNumChunks();\n        numDirtyChunks += matchingVectorsReader.getNumDirtyChunks();\n      } else {        \n        // naive merge...\n        if (vectorsReader != null) {\n          vectorsReader.checkIntegrity();\n        }\n        for (int i = 0; i < maxDoc; i++) {\n          if (liveDocs != null && liveDocs.get(i) == false) {\n            continue;\n          }\n          Fields vectors;\n          if (vectorsReader == null) {\n            vectors = null;\n          } else {\n            vectors = vectorsReader.get(i);\n          }\n          addAllDocVectors(vectors, mergeState);\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n\n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      CompressingTermVectorsReader matchingVectorsReader = null;\n      final TermVectorsReader vectorsReader = mergeState.termVectorsReaders[readerIndex];\n      if (matching.matchingReaders[readerIndex]) {\n        // we can only bulk-copy if the matching reader is also a CompressingTermVectorsReader\n        if (vectorsReader != null && vectorsReader instanceof CompressingTermVectorsReader) {\n          matchingVectorsReader = (CompressingTermVectorsReader) vectorsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n      \n      if (matchingVectorsReader != null &&\n          matchingVectorsReader.getCompressionMode() == compressionMode &&\n          matchingVectorsReader.getChunkSize() == chunkSize &&\n          matchingVectorsReader.getVersion() == VERSION_CURRENT && \n          matchingVectorsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n          BULK_MERGE_ENABLED &&\n          liveDocs == null &&\n          !tooDirty(matchingVectorsReader)) {\n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        matchingVectorsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (!pendingDocs.isEmpty()) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the vectors index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingVectorsReader.getVectorsStream();\n        CompressingStoredFieldsIndexReader index = matchingVectorsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int bufferedDocs = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          indexWriter.writeIndex(bufferedDocs, vectorsStream.getFilePointer());\n          vectorsStream.writeVInt(docCount); // rebase\n          vectorsStream.writeVInt(bufferedDocs);\n          docID += bufferedDocs;\n          docCount += bufferedDocs;\n          numDocs += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingVectorsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          vectorsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingVectorsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingVectorsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingVectorsReader.getNumChunks();\n        numDirtyChunks += matchingVectorsReader.getNumDirtyChunks();\n      } else {        \n        // naive merge...\n        if (vectorsReader != null) {\n          vectorsReader.checkIntegrity();\n        }\n        for (int i = 0; i < maxDoc; i++) {\n          if (liveDocs != null && liveDocs.get(i) == false) {\n            continue;\n          }\n          Fields vectors;\n          if (vectorsReader == null) {\n            vectors = null;\n          } else {\n            vectors = vectorsReader.get(i);\n          }\n          addAllDocVectors(vectors, mergeState);\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    if (mergeState.segmentInfo.getIndexSort() != null) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      return super.merge(mergeState);\n    }\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n\n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      CompressingTermVectorsReader matchingVectorsReader = null;\n      final TermVectorsReader vectorsReader = mergeState.termVectorsReaders[readerIndex];\n      if (matching.matchingReaders[readerIndex]) {\n        // we can only bulk-copy if the matching reader is also a CompressingTermVectorsReader\n        if (vectorsReader != null && vectorsReader instanceof CompressingTermVectorsReader) {\n          matchingVectorsReader = (CompressingTermVectorsReader) vectorsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n      \n      if (matchingVectorsReader != null &&\n          matchingVectorsReader.getCompressionMode() == compressionMode &&\n          matchingVectorsReader.getChunkSize() == chunkSize &&\n          matchingVectorsReader.getVersion() == VERSION_CURRENT && \n          matchingVectorsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n          BULK_MERGE_ENABLED &&\n          liveDocs == null &&\n          !tooDirty(matchingVectorsReader)) {\n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        matchingVectorsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (!pendingDocs.isEmpty()) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the vectors index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingVectorsReader.getVectorsStream();\n        CompressingStoredFieldsIndexReader index = matchingVectorsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int bufferedDocs = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          indexWriter.writeIndex(bufferedDocs, vectorsStream.getFilePointer());\n          vectorsStream.writeVInt(docCount); // rebase\n          vectorsStream.writeVInt(bufferedDocs);\n          docID += bufferedDocs;\n          docCount += bufferedDocs;\n          numDocs += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingVectorsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          vectorsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingVectorsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingVectorsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingVectorsReader.getNumChunks();\n        numDirtyChunks += matchingVectorsReader.getNumDirtyChunks();\n      } else {        \n        // naive merge...\n        if (vectorsReader != null) {\n          vectorsReader.checkIntegrity();\n        }\n        for (int i = 0; i < maxDoc; i++) {\n          if (liveDocs != null && liveDocs.get(i) == false) {\n            continue;\n          }\n          Fields vectors;\n          if (vectorsReader == null) {\n            vectors = null;\n          } else {\n            vectors = vectorsReader.get(i);\n          }\n          addAllDocVectors(vectors, mergeState);\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n\n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      CompressingTermVectorsReader matchingVectorsReader = null;\n      final TermVectorsReader vectorsReader = mergeState.termVectorsReaders[readerIndex];\n      if (matching.matchingReaders[readerIndex]) {\n        // we can only bulk-copy if the matching reader is also a CompressingTermVectorsReader\n        if (vectorsReader != null && vectorsReader instanceof CompressingTermVectorsReader) {\n          matchingVectorsReader = (CompressingTermVectorsReader) vectorsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n      \n      if (matchingVectorsReader != null &&\n          matchingVectorsReader.getCompressionMode() == compressionMode &&\n          matchingVectorsReader.getChunkSize() == chunkSize &&\n          matchingVectorsReader.getVersion() == VERSION_CURRENT && \n          matchingVectorsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n          BULK_MERGE_ENABLED &&\n          liveDocs == null &&\n          !tooDirty(matchingVectorsReader)) {\n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        matchingVectorsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (!pendingDocs.isEmpty()) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the vectors index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingVectorsReader.getVectorsStream();\n        CompressingStoredFieldsIndexReader index = matchingVectorsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int bufferedDocs = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          indexWriter.writeIndex(bufferedDocs, vectorsStream.getFilePointer());\n          vectorsStream.writeVInt(docCount); // rebase\n          vectorsStream.writeVInt(bufferedDocs);\n          docID += bufferedDocs;\n          docCount += bufferedDocs;\n          numDocs += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingVectorsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          vectorsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingVectorsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingVectorsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingVectorsReader.getNumChunks();\n        numDirtyChunks += matchingVectorsReader.getNumDirtyChunks();\n      } else {        \n        // naive merge...\n        if (vectorsReader != null) {\n          vectorsReader.checkIntegrity();\n        }\n        for (int i = 0; i < maxDoc; i++) {\n          if (liveDocs != null && liveDocs.get(i) == false) {\n            continue;\n          }\n          Fields vectors;\n          if (vectorsReader == null) {\n            vectors = null;\n          } else {\n            vectors = vectorsReader.get(i);\n          }\n          addAllDocVectors(vectors, mergeState);\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"653128722fb3b4713ac331c621491a93f34a4a22","date":1479841816,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    if (mergeState.needsIndexSort) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      return super.merge(mergeState);\n    }\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n\n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      CompressingTermVectorsReader matchingVectorsReader = null;\n      final TermVectorsReader vectorsReader = mergeState.termVectorsReaders[readerIndex];\n      if (matching.matchingReaders[readerIndex]) {\n        // we can only bulk-copy if the matching reader is also a CompressingTermVectorsReader\n        if (vectorsReader != null && vectorsReader instanceof CompressingTermVectorsReader) {\n          matchingVectorsReader = (CompressingTermVectorsReader) vectorsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n      \n      if (matchingVectorsReader != null &&\n          matchingVectorsReader.getCompressionMode() == compressionMode &&\n          matchingVectorsReader.getChunkSize() == chunkSize &&\n          matchingVectorsReader.getVersion() == VERSION_CURRENT && \n          matchingVectorsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n          BULK_MERGE_ENABLED &&\n          liveDocs == null &&\n          !tooDirty(matchingVectorsReader)) {\n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        matchingVectorsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (!pendingDocs.isEmpty()) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the vectors index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingVectorsReader.getVectorsStream();\n        CompressingStoredFieldsIndexReader index = matchingVectorsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int bufferedDocs = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          indexWriter.writeIndex(bufferedDocs, vectorsStream.getFilePointer());\n          vectorsStream.writeVInt(docCount); // rebase\n          vectorsStream.writeVInt(bufferedDocs);\n          docID += bufferedDocs;\n          docCount += bufferedDocs;\n          numDocs += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingVectorsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          vectorsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingVectorsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingVectorsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingVectorsReader.getNumChunks();\n        numDirtyChunks += matchingVectorsReader.getNumDirtyChunks();\n      } else {        \n        // naive merge...\n        if (vectorsReader != null) {\n          vectorsReader.checkIntegrity();\n        }\n        for (int i = 0; i < maxDoc; i++) {\n          if (liveDocs != null && liveDocs.get(i) == false) {\n            continue;\n          }\n          Fields vectors;\n          if (vectorsReader == null) {\n            vectors = null;\n          } else {\n            vectors = vectorsReader.get(i);\n          }\n          addAllDocVectors(vectors, mergeState);\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    if (mergeState.segmentInfo.getIndexSort() != null) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      return super.merge(mergeState);\n    }\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n\n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      CompressingTermVectorsReader matchingVectorsReader = null;\n      final TermVectorsReader vectorsReader = mergeState.termVectorsReaders[readerIndex];\n      if (matching.matchingReaders[readerIndex]) {\n        // we can only bulk-copy if the matching reader is also a CompressingTermVectorsReader\n        if (vectorsReader != null && vectorsReader instanceof CompressingTermVectorsReader) {\n          matchingVectorsReader = (CompressingTermVectorsReader) vectorsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n      \n      if (matchingVectorsReader != null &&\n          matchingVectorsReader.getCompressionMode() == compressionMode &&\n          matchingVectorsReader.getChunkSize() == chunkSize &&\n          matchingVectorsReader.getVersion() == VERSION_CURRENT && \n          matchingVectorsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n          BULK_MERGE_ENABLED &&\n          liveDocs == null &&\n          !tooDirty(matchingVectorsReader)) {\n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        matchingVectorsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (!pendingDocs.isEmpty()) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the vectors index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingVectorsReader.getVectorsStream();\n        CompressingStoredFieldsIndexReader index = matchingVectorsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int bufferedDocs = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          indexWriter.writeIndex(bufferedDocs, vectorsStream.getFilePointer());\n          vectorsStream.writeVInt(docCount); // rebase\n          vectorsStream.writeVInt(bufferedDocs);\n          docID += bufferedDocs;\n          docCount += bufferedDocs;\n          numDocs += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingVectorsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          vectorsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingVectorsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingVectorsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingVectorsReader.getNumChunks();\n        numDirtyChunks += matchingVectorsReader.getNumDirtyChunks();\n      } else {        \n        // naive merge...\n        if (vectorsReader != null) {\n          vectorsReader.checkIntegrity();\n        }\n        for (int i = 0; i < maxDoc; i++) {\n          if (liveDocs != null && liveDocs.get(i) == false) {\n            continue;\n          }\n          Fields vectors;\n          if (vectorsReader == null) {\n            vectors = null;\n          } else {\n            vectors = vectorsReader.get(i);\n          }\n          addAllDocVectors(vectors, mergeState);\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"727bb765ff2542275f6d31f67be18d7104bae148","date":1480353976,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    if (mergeState.needsIndexSort) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      return super.merge(mergeState);\n    }\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n\n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      CompressingTermVectorsReader matchingVectorsReader = null;\n      final TermVectorsReader vectorsReader = mergeState.termVectorsReaders[readerIndex];\n      if (matching.matchingReaders[readerIndex]) {\n        // we can only bulk-copy if the matching reader is also a CompressingTermVectorsReader\n        if (vectorsReader != null && vectorsReader instanceof CompressingTermVectorsReader) {\n          matchingVectorsReader = (CompressingTermVectorsReader) vectorsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n      \n      if (matchingVectorsReader != null &&\n          matchingVectorsReader.getCompressionMode() == compressionMode &&\n          matchingVectorsReader.getChunkSize() == chunkSize &&\n          matchingVectorsReader.getVersion() == VERSION_CURRENT && \n          matchingVectorsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n          BULK_MERGE_ENABLED &&\n          liveDocs == null &&\n          !tooDirty(matchingVectorsReader)) {\n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        matchingVectorsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (!pendingDocs.isEmpty()) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the vectors index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingVectorsReader.getVectorsStream();\n        CompressingStoredFieldsIndexReader index = matchingVectorsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int bufferedDocs = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          indexWriter.writeIndex(bufferedDocs, vectorsStream.getFilePointer());\n          vectorsStream.writeVInt(docCount); // rebase\n          vectorsStream.writeVInt(bufferedDocs);\n          docID += bufferedDocs;\n          docCount += bufferedDocs;\n          numDocs += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingVectorsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          vectorsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingVectorsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingVectorsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingVectorsReader.getNumChunks();\n        numDirtyChunks += matchingVectorsReader.getNumDirtyChunks();\n      } else {        \n        // naive merge...\n        if (vectorsReader != null) {\n          vectorsReader.checkIntegrity();\n        }\n        for (int i = 0; i < maxDoc; i++) {\n          if (liveDocs != null && liveDocs.get(i) == false) {\n            continue;\n          }\n          Fields vectors;\n          if (vectorsReader == null) {\n            vectors = null;\n          } else {\n            vectors = vectorsReader.get(i);\n          }\n          addAllDocVectors(vectors, mergeState);\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    if (mergeState.segmentInfo.getIndexSort() != null) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      return super.merge(mergeState);\n    }\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n\n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      CompressingTermVectorsReader matchingVectorsReader = null;\n      final TermVectorsReader vectorsReader = mergeState.termVectorsReaders[readerIndex];\n      if (matching.matchingReaders[readerIndex]) {\n        // we can only bulk-copy if the matching reader is also a CompressingTermVectorsReader\n        if (vectorsReader != null && vectorsReader instanceof CompressingTermVectorsReader) {\n          matchingVectorsReader = (CompressingTermVectorsReader) vectorsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n      \n      if (matchingVectorsReader != null &&\n          matchingVectorsReader.getCompressionMode() == compressionMode &&\n          matchingVectorsReader.getChunkSize() == chunkSize &&\n          matchingVectorsReader.getVersion() == VERSION_CURRENT && \n          matchingVectorsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n          BULK_MERGE_ENABLED &&\n          liveDocs == null &&\n          !tooDirty(matchingVectorsReader)) {\n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        matchingVectorsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (!pendingDocs.isEmpty()) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the vectors index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingVectorsReader.getVectorsStream();\n        CompressingStoredFieldsIndexReader index = matchingVectorsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int bufferedDocs = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          indexWriter.writeIndex(bufferedDocs, vectorsStream.getFilePointer());\n          vectorsStream.writeVInt(docCount); // rebase\n          vectorsStream.writeVInt(bufferedDocs);\n          docID += bufferedDocs;\n          docCount += bufferedDocs;\n          numDocs += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingVectorsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          vectorsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingVectorsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingVectorsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingVectorsReader.getNumChunks();\n        numDirtyChunks += matchingVectorsReader.getNumDirtyChunks();\n      } else {        \n        // naive merge...\n        if (vectorsReader != null) {\n          vectorsReader.checkIntegrity();\n        }\n        for (int i = 0; i < maxDoc; i++) {\n          if (liveDocs != null && liveDocs.get(i) == false) {\n            continue;\n          }\n          Fields vectors;\n          if (vectorsReader == null) {\n            vectors = null;\n          } else {\n            vectors = vectorsReader.get(i);\n          }\n          addAllDocVectors(vectors, mergeState);\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"70a4487b07c49a1861c05720e04624826ecbe9fa","date":1580924108,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    if (mergeState.needsIndexSort) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      return super.merge(mergeState);\n    }\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n\n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      CompressingTermVectorsReader matchingVectorsReader = null;\n      final TermVectorsReader vectorsReader = mergeState.termVectorsReaders[readerIndex];\n      if (matching.matchingReaders[readerIndex]) {\n        // we can only bulk-copy if the matching reader is also a CompressingTermVectorsReader\n        if (vectorsReader != null && vectorsReader instanceof CompressingTermVectorsReader) {\n          matchingVectorsReader = (CompressingTermVectorsReader) vectorsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n      \n      if (matchingVectorsReader != null &&\n          matchingVectorsReader.getCompressionMode() == compressionMode &&\n          matchingVectorsReader.getChunkSize() == chunkSize &&\n          matchingVectorsReader.getVersion() == VERSION_CURRENT && \n          matchingVectorsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n          BULK_MERGE_ENABLED &&\n          liveDocs == null &&\n          !tooDirty(matchingVectorsReader)) {\n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        matchingVectorsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (!pendingDocs.isEmpty()) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the vectors index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingVectorsReader.getVectorsStream();\n        FieldsIndex index = matchingVectorsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int bufferedDocs = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          indexWriter.writeIndex(bufferedDocs, vectorsStream.getFilePointer());\n          vectorsStream.writeVInt(docCount); // rebase\n          vectorsStream.writeVInt(bufferedDocs);\n          docID += bufferedDocs;\n          docCount += bufferedDocs;\n          numDocs += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingVectorsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          vectorsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingVectorsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingVectorsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingVectorsReader.getNumChunks();\n        numDirtyChunks += matchingVectorsReader.getNumDirtyChunks();\n      } else {        \n        // naive merge...\n        if (vectorsReader != null) {\n          vectorsReader.checkIntegrity();\n        }\n        for (int i = 0; i < maxDoc; i++) {\n          if (liveDocs != null && liveDocs.get(i) == false) {\n            continue;\n          }\n          Fields vectors;\n          if (vectorsReader == null) {\n            vectors = null;\n          } else {\n            vectors = vectorsReader.get(i);\n          }\n          addAllDocVectors(vectors, mergeState);\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    if (mergeState.needsIndexSort) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      return super.merge(mergeState);\n    }\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n\n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      CompressingTermVectorsReader matchingVectorsReader = null;\n      final TermVectorsReader vectorsReader = mergeState.termVectorsReaders[readerIndex];\n      if (matching.matchingReaders[readerIndex]) {\n        // we can only bulk-copy if the matching reader is also a CompressingTermVectorsReader\n        if (vectorsReader != null && vectorsReader instanceof CompressingTermVectorsReader) {\n          matchingVectorsReader = (CompressingTermVectorsReader) vectorsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n      \n      if (matchingVectorsReader != null &&\n          matchingVectorsReader.getCompressionMode() == compressionMode &&\n          matchingVectorsReader.getChunkSize() == chunkSize &&\n          matchingVectorsReader.getVersion() == VERSION_CURRENT && \n          matchingVectorsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n          BULK_MERGE_ENABLED &&\n          liveDocs == null &&\n          !tooDirty(matchingVectorsReader)) {\n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        matchingVectorsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (!pendingDocs.isEmpty()) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the vectors index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingVectorsReader.getVectorsStream();\n        CompressingStoredFieldsIndexReader index = matchingVectorsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int bufferedDocs = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          indexWriter.writeIndex(bufferedDocs, vectorsStream.getFilePointer());\n          vectorsStream.writeVInt(docCount); // rebase\n          vectorsStream.writeVInt(bufferedDocs);\n          docID += bufferedDocs;\n          docCount += bufferedDocs;\n          numDocs += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingVectorsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          vectorsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingVectorsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingVectorsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingVectorsReader.getNumChunks();\n        numDirtyChunks += matchingVectorsReader.getNumDirtyChunks();\n      } else {        \n        // naive merge...\n        if (vectorsReader != null) {\n          vectorsReader.checkIntegrity();\n        }\n        for (int i = 0; i < maxDoc; i++) {\n          if (liveDocs != null && liveDocs.get(i) == false) {\n            continue;\n          }\n          Fields vectors;\n          if (vectorsReader == null) {\n            vectors = null;\n          } else {\n            vectors = vectorsReader.get(i);\n          }\n          addAllDocVectors(vectors, mergeState);\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"45264aed0cfa8a8a55ae1292b0e336d29cd88401","date":1600361948,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    if (mergeState.needsIndexSort) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      return super.merge(mergeState);\n    }\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n\n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      CompressingTermVectorsReader matchingVectorsReader = null;\n      final TermVectorsReader vectorsReader = mergeState.termVectorsReaders[readerIndex];\n      if (matching.matchingReaders[readerIndex]) {\n        // we can only bulk-copy if the matching reader is also a CompressingTermVectorsReader\n        if (vectorsReader != null && vectorsReader instanceof CompressingTermVectorsReader) {\n          matchingVectorsReader = (CompressingTermVectorsReader) vectorsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n      \n      if (matchingVectorsReader != null &&\n          matchingVectorsReader.getCompressionMode() == compressionMode &&\n          matchingVectorsReader.getChunkSize() == chunkSize &&\n          matchingVectorsReader.getVersion() == VERSION_CURRENT && \n          matchingVectorsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n          BULK_MERGE_ENABLED &&\n          liveDocs == null &&\n          !tooDirty(matchingVectorsReader)) {\n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        matchingVectorsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (!pendingDocs.isEmpty()) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the vectors index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingVectorsReader.getVectorsStream();\n        FieldsIndex index = matchingVectorsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int bufferedDocs = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          indexWriter.writeIndex(bufferedDocs, vectorsStream.getFilePointer());\n          vectorsStream.writeVInt(docCount); // rebase\n          vectorsStream.writeVInt(bufferedDocs);\n          docID += bufferedDocs;\n          docCount += bufferedDocs;\n          numDocs += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingVectorsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          vectorsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingVectorsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingVectorsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numDirtyChunks += matchingVectorsReader.getNumDirtyChunks();\n        numDirtyDocs += matchingVectorsReader.getNumDirtyDocs();\n      } else {        \n        // naive merge...\n        if (vectorsReader != null) {\n          vectorsReader.checkIntegrity();\n        }\n        for (int i = 0; i < maxDoc; i++) {\n          if (liveDocs != null && liveDocs.get(i) == false) {\n            continue;\n          }\n          Fields vectors;\n          if (vectorsReader == null) {\n            vectors = null;\n          } else {\n            vectors = vectorsReader.get(i);\n          }\n          addAllDocVectors(vectors, mergeState);\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    if (mergeState.needsIndexSort) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      return super.merge(mergeState);\n    }\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n\n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      CompressingTermVectorsReader matchingVectorsReader = null;\n      final TermVectorsReader vectorsReader = mergeState.termVectorsReaders[readerIndex];\n      if (matching.matchingReaders[readerIndex]) {\n        // we can only bulk-copy if the matching reader is also a CompressingTermVectorsReader\n        if (vectorsReader != null && vectorsReader instanceof CompressingTermVectorsReader) {\n          matchingVectorsReader = (CompressingTermVectorsReader) vectorsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n      \n      if (matchingVectorsReader != null &&\n          matchingVectorsReader.getCompressionMode() == compressionMode &&\n          matchingVectorsReader.getChunkSize() == chunkSize &&\n          matchingVectorsReader.getVersion() == VERSION_CURRENT && \n          matchingVectorsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n          BULK_MERGE_ENABLED &&\n          liveDocs == null &&\n          !tooDirty(matchingVectorsReader)) {\n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        matchingVectorsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (!pendingDocs.isEmpty()) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the vectors index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingVectorsReader.getVectorsStream();\n        FieldsIndex index = matchingVectorsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int bufferedDocs = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          indexWriter.writeIndex(bufferedDocs, vectorsStream.getFilePointer());\n          vectorsStream.writeVInt(docCount); // rebase\n          vectorsStream.writeVInt(bufferedDocs);\n          docID += bufferedDocs;\n          docCount += bufferedDocs;\n          numDocs += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingVectorsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          vectorsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingVectorsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingVectorsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingVectorsReader.getNumChunks();\n        numDirtyChunks += matchingVectorsReader.getNumDirtyChunks();\n      } else {        \n        // naive merge...\n        if (vectorsReader != null) {\n          vectorsReader.checkIntegrity();\n        }\n        for (int i = 0; i < maxDoc; i++) {\n          if (liveDocs != null && liveDocs.get(i) == false) {\n            continue;\n          }\n          Fields vectors;\n          if (vectorsReader == null) {\n            vectors = null;\n          } else {\n            vectors = vectorsReader.get(i);\n          }\n          addAllDocVectors(vectors, mergeState);\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"eda61b1e90b490cc5837200e04c02639a0d272c7":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a661533765521d46fcaf46aee272301b6afb6376":["2131047ecceac64b54ba70feec3d26bbd7e483d7"],"1db0e36bd56f06d5830e980340e7fe58353a44f7":["eda61b1e90b490cc5837200e04c02639a0d272c7"],"0ad30c6a479e764150a3316e57263319775f1df2":["b88448324d3a96c5842455dabea63450b697b58f","3d33e731a93d4b57e662ff094f64f94a745422d4"],"07155cdd910937cdf6877e48884d5782845c8b8b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","eda61b1e90b490cc5837200e04c02639a0d272c7"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["22a2e66dfda83847e80095b8693c660742ab3e9c"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["b88448324d3a96c5842455dabea63450b697b58f","d470c8182e92b264680e34081b75e70a9f2b3c89"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["b88448324d3a96c5842455dabea63450b697b58f","0ad30c6a479e764150a3316e57263319775f1df2"],"70a4487b07c49a1861c05720e04624826ecbe9fa":["653128722fb3b4713ac331c621491a93f34a4a22"],"727bb765ff2542275f6d31f67be18d7104bae148":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","653128722fb3b4713ac331c621491a93f34a4a22"],"b88448324d3a96c5842455dabea63450b697b58f":["5faf65b6692f15cca0f87bf8666c87899afc619f"],"9bb9a29a5e71a90295f175df8919802993142c9a":["c9fb5f46e264daf5ba3860defe623a89d202dd87","a661533765521d46fcaf46aee272301b6afb6376"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"2131047ecceac64b54ba70feec3d26bbd7e483d7":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"22a2e66dfda83847e80095b8693c660742ab3e9c":["1db0e36bd56f06d5830e980340e7fe58353a44f7"],"45264aed0cfa8a8a55ae1292b0e336d29cd88401":["70a4487b07c49a1861c05720e04624826ecbe9fa"],"5faf65b6692f15cca0f87bf8666c87899afc619f":["9bb9a29a5e71a90295f175df8919802993142c9a"],"3d33e731a93d4b57e662ff094f64f94a745422d4":["b88448324d3a96c5842455dabea63450b697b58f","fbe8fc0e68a5e2e7acce82ba880a982bd15cfab8"],"653128722fb3b4713ac331c621491a93f34a4a22":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"fbe8fc0e68a5e2e7acce82ba880a982bd15cfab8":["b88448324d3a96c5842455dabea63450b697b58f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["45264aed0cfa8a8a55ae1292b0e336d29cd88401"]},"commit2Childs":{"eda61b1e90b490cc5837200e04c02639a0d272c7":["1db0e36bd56f06d5830e980340e7fe58353a44f7","07155cdd910937cdf6877e48884d5782845c8b8b"],"a661533765521d46fcaf46aee272301b6afb6376":["9bb9a29a5e71a90295f175df8919802993142c9a"],"1db0e36bd56f06d5830e980340e7fe58353a44f7":["22a2e66dfda83847e80095b8693c660742ab3e9c"],"0ad30c6a479e764150a3316e57263319775f1df2":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"07155cdd910937cdf6877e48884d5782845c8b8b":[],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["9bb9a29a5e71a90295f175df8919802993142c9a","2131047ecceac64b54ba70feec3d26bbd7e483d7"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["727bb765ff2542275f6d31f67be18d7104bae148"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","653128722fb3b4713ac331c621491a93f34a4a22"],"70a4487b07c49a1861c05720e04624826ecbe9fa":["45264aed0cfa8a8a55ae1292b0e336d29cd88401"],"727bb765ff2542275f6d31f67be18d7104bae148":[],"b88448324d3a96c5842455dabea63450b697b58f":["0ad30c6a479e764150a3316e57263319775f1df2","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","d470c8182e92b264680e34081b75e70a9f2b3c89","3d33e731a93d4b57e662ff094f64f94a745422d4","fbe8fc0e68a5e2e7acce82ba880a982bd15cfab8"],"9bb9a29a5e71a90295f175df8919802993142c9a":["5faf65b6692f15cca0f87bf8666c87899afc619f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["eda61b1e90b490cc5837200e04c02639a0d272c7","07155cdd910937cdf6877e48884d5782845c8b8b"],"2131047ecceac64b54ba70feec3d26bbd7e483d7":["a661533765521d46fcaf46aee272301b6afb6376"],"22a2e66dfda83847e80095b8693c660742ab3e9c":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"5faf65b6692f15cca0f87bf8666c87899afc619f":["b88448324d3a96c5842455dabea63450b697b58f"],"45264aed0cfa8a8a55ae1292b0e336d29cd88401":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"3d33e731a93d4b57e662ff094f64f94a745422d4":["0ad30c6a479e764150a3316e57263319775f1df2"],"653128722fb3b4713ac331c621491a93f34a4a22":["70a4487b07c49a1861c05720e04624826ecbe9fa","727bb765ff2542275f6d31f67be18d7104bae148"],"fbe8fc0e68a5e2e7acce82ba880a982bd15cfab8":["3d33e731a93d4b57e662ff094f64f94a745422d4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["07155cdd910937cdf6877e48884d5782845c8b8b","727bb765ff2542275f6d31f67be18d7104bae148","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}