{"path":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(Terms,boolean).mjava","commits":[{"id":"3cc749c053615f5871f3b95715fe292f34e70a53","date":1321470575,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(Terms,boolean).mjava","pathOld":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","sourceNew":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(Terms tpv,\n      boolean tokenPositionsGuaranteedContiguous) \n  throws IOException {\n    if (!tokenPositionsGuaranteedContiguous && hasPositions(tpv)) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    TermsEnum termsEnum = tpv.iterator(null);\n    int totalTokens = 0;\n    while(termsEnum.next() != null) {\n      totalTokens += (int) termsEnum.totalTermFreq();\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    termsEnum = tpv.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while ((text = termsEnum.next()) != null) {\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum);\n      if (dpEnum == null || (!dpEnum.attributes().hasAttribute(OffsetAttribute.class))) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n\n      final String term = text.utf8ToString();\n\n      final OffsetAttribute offsetAtt = dpEnum.attributes().getAttribute(OffsetAttribute.class);\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      for(int posUpto=0;posUpto<freq;posUpto++) {\n        final int pos = dpEnum.nextPosition();\n        final Token token = new Token(term,\n                                      offsetAtt.startOffset(),\n                                      offsetAtt.endOffset());\n        if (tokenPositionsGuaranteedContiguous && pos != -1) {\n          // We have positions stored and a guarantee that the token position\n          // information is contiguous\n\n          // This may be fast BUT wont work if Tokenizers used which create >1\n          // token in same position or\n          // creates jumps in position numbers - this code would fail under those\n          // circumstances\n\n          // tokens stored with positions - can use this to index straight into\n          // sorted array\n          tokensInOriginalOrder[pos] = token;\n        } else {\n          // tokens NOT stored with positions or not guaranteed contiguous - must\n          // add to list and sort later\n          if (unsortedTokens == null) {\n            unsortedTokens = new ArrayList<Token>();\n          }\n          unsortedTokens.add(token);\n        }\n      }\n    }\n\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","sourceOld":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    BytesRef[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(),\n              offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(),\n              offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"31f025ae60076ae95274433f3fe8e6ace2857a87","date":1326669465,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(Terms,boolean).mjava","pathOld":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(Terms,boolean).mjava","sourceNew":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(Terms tpv,\n      boolean tokenPositionsGuaranteedContiguous) \n  throws IOException {\n    if (!tokenPositionsGuaranteedContiguous && hasPositions(tpv)) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    TermsEnum termsEnum = tpv.iterator(null);\n    int totalTokens = 0;\n    while(termsEnum.next() != null) {\n      totalTokens += (int) termsEnum.totalTermFreq();\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    termsEnum = tpv.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while ((text = termsEnum.next()) != null) {\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum, true);\n      if (dpEnum == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n\n      final String term = text.utf8ToString();\n\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      for(int posUpto=0;posUpto<freq;posUpto++) {\n        final int pos = dpEnum.nextPosition();\n        final Token token = new Token(term,\n                                      dpEnum.startOffset(),\n                                      dpEnum.endOffset());\n        if (tokenPositionsGuaranteedContiguous && pos != -1) {\n          // We have positions stored and a guarantee that the token position\n          // information is contiguous\n\n          // This may be fast BUT wont work if Tokenizers used which create >1\n          // token in same position or\n          // creates jumps in position numbers - this code would fail under those\n          // circumstances\n\n          // tokens stored with positions - can use this to index straight into\n          // sorted array\n          tokensInOriginalOrder[pos] = token;\n        } else {\n          // tokens NOT stored with positions or not guaranteed contiguous - must\n          // add to list and sort later\n          if (unsortedTokens == null) {\n            unsortedTokens = new ArrayList<Token>();\n          }\n          unsortedTokens.add(token);\n        }\n      }\n    }\n\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","sourceOld":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(Terms tpv,\n      boolean tokenPositionsGuaranteedContiguous) \n  throws IOException {\n    if (!tokenPositionsGuaranteedContiguous && hasPositions(tpv)) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    TermsEnum termsEnum = tpv.iterator(null);\n    int totalTokens = 0;\n    while(termsEnum.next() != null) {\n      totalTokens += (int) termsEnum.totalTermFreq();\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    termsEnum = tpv.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while ((text = termsEnum.next()) != null) {\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum);\n      if (dpEnum == null || (!dpEnum.attributes().hasAttribute(OffsetAttribute.class))) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n\n      final String term = text.utf8ToString();\n\n      final OffsetAttribute offsetAtt = dpEnum.attributes().getAttribute(OffsetAttribute.class);\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      for(int posUpto=0;posUpto<freq;posUpto++) {\n        final int pos = dpEnum.nextPosition();\n        final Token token = new Token(term,\n                                      offsetAtt.startOffset(),\n                                      offsetAtt.endOffset());\n        if (tokenPositionsGuaranteedContiguous && pos != -1) {\n          // We have positions stored and a guarantee that the token position\n          // information is contiguous\n\n          // This may be fast BUT wont work if Tokenizers used which create >1\n          // token in same position or\n          // creates jumps in position numbers - this code would fail under those\n          // circumstances\n\n          // tokens stored with positions - can use this to index straight into\n          // sorted array\n          tokensInOriginalOrder[pos] = token;\n        } else {\n          // tokens NOT stored with positions or not guaranteed contiguous - must\n          // add to list and sort later\n          if (unsortedTokens == null) {\n            unsortedTokens = new ArrayList<Token>();\n          }\n          unsortedTokens.add(token);\n        }\n      }\n    }\n\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(Terms,boolean).mjava","pathOld":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(Terms,boolean).mjava","sourceNew":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(Terms tpv,\n      boolean tokenPositionsGuaranteedContiguous) \n  throws IOException {\n    if (!tokenPositionsGuaranteedContiguous && hasPositions(tpv)) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    TermsEnum termsEnum = tpv.iterator(null);\n    int totalTokens = 0;\n    while(termsEnum.next() != null) {\n      totalTokens += (int) termsEnum.totalTermFreq();\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    termsEnum = tpv.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while ((text = termsEnum.next()) != null) {\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum, true);\n      if (dpEnum == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n\n      final String term = text.utf8ToString();\n\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      for(int posUpto=0;posUpto<freq;posUpto++) {\n        final int pos = dpEnum.nextPosition();\n        final Token token = new Token(term,\n                                      dpEnum.startOffset(),\n                                      dpEnum.endOffset());\n        if (tokenPositionsGuaranteedContiguous && pos != -1) {\n          // We have positions stored and a guarantee that the token position\n          // information is contiguous\n\n          // This may be fast BUT wont work if Tokenizers used which create >1\n          // token in same position or\n          // creates jumps in position numbers - this code would fail under those\n          // circumstances\n\n          // tokens stored with positions - can use this to index straight into\n          // sorted array\n          tokensInOriginalOrder[pos] = token;\n        } else {\n          // tokens NOT stored with positions or not guaranteed contiguous - must\n          // add to list and sort later\n          if (unsortedTokens == null) {\n            unsortedTokens = new ArrayList<Token>();\n          }\n          unsortedTokens.add(token);\n        }\n      }\n    }\n\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","sourceOld":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(Terms tpv,\n      boolean tokenPositionsGuaranteedContiguous) \n  throws IOException {\n    if (!tokenPositionsGuaranteedContiguous && hasPositions(tpv)) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    TermsEnum termsEnum = tpv.iterator(null);\n    int totalTokens = 0;\n    while(termsEnum.next() != null) {\n      totalTokens += (int) termsEnum.totalTermFreq();\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    termsEnum = tpv.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while ((text = termsEnum.next()) != null) {\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum, true);\n      if (dpEnum == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n\n      final String term = text.utf8ToString();\n\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      for(int posUpto=0;posUpto<freq;posUpto++) {\n        final int pos = dpEnum.nextPosition();\n        final Token token = new Token(term,\n                                      dpEnum.startOffset(),\n                                      dpEnum.endOffset());\n        if (tokenPositionsGuaranteedContiguous && pos != -1) {\n          // We have positions stored and a guarantee that the token position\n          // information is contiguous\n\n          // This may be fast BUT wont work if Tokenizers used which create >1\n          // token in same position or\n          // creates jumps in position numbers - this code would fail under those\n          // circumstances\n\n          // tokens stored with positions - can use this to index straight into\n          // sorted array\n          tokensInOriginalOrder[pos] = token;\n        } else {\n          // tokens NOT stored with positions or not guaranteed contiguous - must\n          // add to list and sort later\n          if (unsortedTokens == null) {\n            unsortedTokens = new ArrayList<Token>();\n          }\n          unsortedTokens.add(token);\n        }\n      }\n    }\n\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["31f025ae60076ae95274433f3fe8e6ace2857a87"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3cc749c053615f5871f3b95715fe292f34e70a53":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"31f025ae60076ae95274433f3fe8e6ace2857a87":["3cc749c053615f5871f3b95715fe292f34e70a53"]},"commit2Childs":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3cc749c053615f5871f3b95715fe292f34e70a53"],"3cc749c053615f5871f3b95715fe292f34e70a53":["31f025ae60076ae95274433f3fe8e6ace2857a87"],"31f025ae60076ae95274433f3fe8e6ace2857a87":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}