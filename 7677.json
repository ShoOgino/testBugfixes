{"path":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCountsSingleValue(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","commits":[{"id":"744b111b17d15d490a648eb021bfa240e7f11556","date":1487008069,"type":0,"author":"Tomas Fernandez Lobbe","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCountsSingleValue(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","pathOld":"/dev/null","sourceNew":"  private static NamedList<Integer> getCountsSingleValue(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumberType numericType = ft.getNumberType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    zeros = zeros && !ft.isPointField() && sf.indexed(); // We don't return zeros when using PointFields or when index=false\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable(true);\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n          case DATE:\n          case INTEGER:\n            // Long, Date and Integer\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError(\"Unexpected type: \" + numericType);\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed() && !sf.hasDocValues()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is neither indexed nor docValues\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"022a4de90e0479b604264ca9c2e134c996454ab3","date":1487118265,"type":0,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCountsSingleValue(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","pathOld":"/dev/null","sourceNew":"  private static NamedList<Integer> getCountsSingleValue(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumberType numericType = ft.getNumberType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    zeros = zeros && !ft.isPointField() && sf.indexed(); // We don't return zeros when using PointFields or when index=false\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable(true);\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n          case DATE:\n          case INTEGER:\n            // Long, Date and Integer\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError(\"Unexpected type: \" + numericType);\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed() && !sf.hasDocValues()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is neither indexed nor docValues\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b49c0eb9ef7bc25609a89e7986ce7e6eeb9c9d96","date":1487122334,"type":4,"author":"Noble Paul","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCountsSingleValue(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","sourceNew":null,"sourceOld":"  private static NamedList<Integer> getCountsSingleValue(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumberType numericType = ft.getNumberType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    zeros = zeros && !ft.isPointField() && sf.indexed(); // We don't return zeros when using PointFields or when index=false\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable(true);\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n          case DATE:\n          case INTEGER:\n            // Long, Date and Integer\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError(\"Unexpected type: \" + numericType);\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed() && !sf.hasDocValues()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is neither indexed nor docValues\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"897b06b1364bd1f658a8be7591e43f0851458e7f","date":1487123008,"type":0,"author":"Ishan Chattopadhyaya","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCountsSingleValue(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","pathOld":"/dev/null","sourceNew":"  private static NamedList<Integer> getCountsSingleValue(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumberType numericType = ft.getNumberType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    zeros = zeros && !ft.isPointField() && sf.indexed(); // We don't return zeros when using PointFields or when index=false\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable(true);\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n          case DATE:\n          case INTEGER:\n            // Long, Date and Integer\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError(\"Unexpected type: \" + numericType);\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed() && !sf.hasDocValues()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is neither indexed nor docValues\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["7679cc7d5b465ec8936979698cedf5fdbd71c95c"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0b21c0ef4cefc9a6c012a77388d894c9d2ceda12","date":1501539735,"type":3,"author":"Steve Rowe","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCountsSingleValue(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCountsSingleValue(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","sourceNew":"  private static NamedList<Integer> getCountsSingleValue(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumberType numericType = ft.getNumberType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    if (zeros && ft.isPointField()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n          \"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is Points-based\");\n    }\n\n    zeros = zeros && !ft.isPointField() && sf.indexed(); // We don't return zeros when using PointFields or when index=false\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable(true);\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n          case DATE:\n          case INTEGER:\n            // Long, Date and Integer\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError(\"Unexpected type: \" + numericType);\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed() && !sf.hasDocValues()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is neither indexed nor docValues\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","sourceOld":"  private static NamedList<Integer> getCountsSingleValue(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumberType numericType = ft.getNumberType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    zeros = zeros && !ft.isPointField() && sf.indexed(); // We don't return zeros when using PointFields or when index=false\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable(true);\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n          case DATE:\n          case INTEGER:\n            // Long, Date and Integer\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError(\"Unexpected type: \" + numericType);\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed() && !sf.hasDocValues()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is neither indexed nor docValues\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"29100f1cf3406ddc394298904704d022872303d5","date":1501594929,"type":3,"author":"Steve Rowe","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCountsSingleValue(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCountsSingleValue(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","sourceNew":"  private static NamedList<Integer> getCountsSingleValue(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumberType numericType = ft.getNumberType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    zeros = zeros && !ft.isPointField() && sf.indexed(); // We don't return zeros when using PointFields or when index=false\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable(true);\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n          case DATE:\n          case INTEGER:\n            // Long, Date and Integer\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError(\"Unexpected type: \" + numericType);\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed() && !sf.hasDocValues()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is neither indexed nor docValues\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","sourceOld":"  private static NamedList<Integer> getCountsSingleValue(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumberType numericType = ft.getNumberType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    if (zeros && ft.isPointField()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n          \"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is Points-based\");\n    }\n\n    zeros = zeros && !ft.isPointField() && sf.indexed(); // We don't return zeros when using PointFields or when index=false\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable(true);\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n          case DATE:\n          case INTEGER:\n            // Long, Date and Integer\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError(\"Unexpected type: \" + numericType);\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed() && !sf.hasDocValues()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is neither indexed nor docValues\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7679cc7d5b465ec8936979698cedf5fdbd71c95c","date":1566227764,"type":3,"author":"Munendra S N","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCountsSingleValue(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCountsSingleValue(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","sourceNew":"  private static NamedList<Integer> getCountsSingleValue(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumberType numericType = ft.getNumberType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    zeros = zeros && !ft.isPointField() && sf.indexed(); // We don't return zeros when using PointFields or when index=false\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable(true);\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n          case DATE:\n          case INTEGER:\n            // Long, Date and Integer\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError(\"Unexpected type: \" + numericType);\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    final NamedList<Integer> result = new NamedList<>();\n    if (limit == 0) {\n      return finalize(result, missingCount, missing);\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed() && !sf.hasDocValues()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is neither indexed nor docValues\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    return finalize(result, missingCount, missing);\n  }\n\n","sourceOld":"  private static NamedList<Integer> getCountsSingleValue(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumberType numericType = ft.getNumberType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    zeros = zeros && !ft.isPointField() && sf.indexed(); // We don't return zeros when using PointFields or when index=false\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable(true);\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n          case DATE:\n          case INTEGER:\n            // Long, Date and Integer\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError(\"Unexpected type: \" + numericType);\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed() && !sf.hasDocValues()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is neither indexed nor docValues\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","bugFix":["897b06b1364bd1f658a8be7591e43f0851458e7f"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b49c0eb9ef7bc25609a89e7986ce7e6eeb9c9d96":["022a4de90e0479b604264ca9c2e134c996454ab3"],"29100f1cf3406ddc394298904704d022872303d5":["0b21c0ef4cefc9a6c012a77388d894c9d2ceda12"],"022a4de90e0479b604264ca9c2e134c996454ab3":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","744b111b17d15d490a648eb021bfa240e7f11556"],"897b06b1364bd1f658a8be7591e43f0851458e7f":["b49c0eb9ef7bc25609a89e7986ce7e6eeb9c9d96"],"7679cc7d5b465ec8936979698cedf5fdbd71c95c":["29100f1cf3406ddc394298904704d022872303d5"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"0b21c0ef4cefc9a6c012a77388d894c9d2ceda12":["897b06b1364bd1f658a8be7591e43f0851458e7f"],"744b111b17d15d490a648eb021bfa240e7f11556":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["7679cc7d5b465ec8936979698cedf5fdbd71c95c"]},"commit2Childs":{"b49c0eb9ef7bc25609a89e7986ce7e6eeb9c9d96":["897b06b1364bd1f658a8be7591e43f0851458e7f"],"022a4de90e0479b604264ca9c2e134c996454ab3":["b49c0eb9ef7bc25609a89e7986ce7e6eeb9c9d96"],"29100f1cf3406ddc394298904704d022872303d5":["7679cc7d5b465ec8936979698cedf5fdbd71c95c"],"897b06b1364bd1f658a8be7591e43f0851458e7f":["0b21c0ef4cefc9a6c012a77388d894c9d2ceda12"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["022a4de90e0479b604264ca9c2e134c996454ab3","744b111b17d15d490a648eb021bfa240e7f11556"],"7679cc7d5b465ec8936979698cedf5fdbd71c95c":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"0b21c0ef4cefc9a6c012a77388d894c9d2ceda12":["29100f1cf3406ddc394298904704d022872303d5"],"744b111b17d15d490a648eb021bfa240e7f11556":["022a4de90e0479b604264ca9c2e134c996454ab3"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}