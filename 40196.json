{"path":"solr/core/src/java/org/apache/solr/analytics/request/AnalyticsStats#execute().mjava","commits":[{"id":"ff403346522eaa9de75403763e297e83a3ea585c","date":1384904731,"type":0,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/analytics/request/AnalyticsStats#execute().mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Calculates the analytics requested in the Parameters.\n   * \n   * @return List of results formated to mirror the input XML.\n   * @throws IOException if execution fails\n   */\n  public NamedList<?> execute() throws IOException {\n    statsCollector.startRequest();\n    NamedList<Object> res = new NamedList<Object>();\n    List<AnalyticsRequest> requests;\n    \n    requests = AnalyticsRequestFactory.parse(searcher.getSchema(), params);\n\n    if(requests == null || requests.size()==0){\n      return res;\n    }\n    statsCollector.addRequests(requests.size());\n    // Computing each Analytics Request Seperately\n    for( AnalyticsRequest areq : requests ){\n      // The Accumulator which will control the statistics generation\n      // for the entire analytics request\n      ValueAccumulator accumulator; \n      \n      // The number of total facet requests\n      int facets = areq.getFieldFacets().size()+areq.getRangeFacets().size()+areq.getQueryFacets().size();\n      try {\n        if( facets== 0 ){\n          accumulator = BasicAccumulator.create(searcher, docs, areq);\n        } else {\n          accumulator = FacetingAccumulator.create(searcher, docs, areq, req);\n        }\n      } catch (IOException e) {\n        System.err.println(e.getMessage());\n        continue;\n      }\n\n      statsCollector.addStatsCollected(((BasicAccumulator)accumulator).getNumStatsCollectors());\n      statsCollector.addStatsRequests(areq.getExpressions().size());\n      statsCollector.addFieldFacets(areq.getFieldFacets().size());\n      statsCollector.addRangeFacets(areq.getRangeFacets().size());\n      statsCollector.addQueryFacets(areq.getQueryFacets().size());\n      statsCollector.addQueries(((BasicAccumulator)accumulator).getNumQueries());\n      \n      // Loop through the documents returned by the query and add to accumulator\n      Filter filter = docs.getTopFilter();\n      List<AtomicReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      for (int leafNum = 0; leafNum < contexts.size(); leafNum++) {\n        AtomicReaderContext context = contexts.get(leafNum);\n        DocIdSet dis = filter.getDocIdSet(context, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n\n        if (disi != null) {\n          accumulator.setNextReader(context);\n          int doc = disi.nextDoc();\n          while( doc != DocIdSetIterator.NO_MORE_DOCS){\n            // Add a document to the statistics being generated\n            accumulator.collect(doc);\n            doc = disi.nextDoc();\n          }\n        }\n      }\n      \n      // do some post-processing\n      accumulator.postProcess();\n     \n      // compute the stats\n      accumulator.compute();\n      \n      res.add(areq.getName(),accumulator.export());\n    }\n\n    statsCollector.endRequest();\n    return res;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["645e9dc687d04dbe961b35c0f3a305c48e892615"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"74f45af4339b0daf7a95c820ab88c1aea74fbce0","date":1387475327,"type":0,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/analytics/request/AnalyticsStats#execute().mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Calculates the analytics requested in the Parameters.\n   * \n   * @return List of results formated to mirror the input XML.\n   * @throws IOException if execution fails\n   */\n  public NamedList<?> execute() throws IOException {\n    statsCollector.startRequest();\n    NamedList<Object> res = new NamedList<Object>();\n    List<AnalyticsRequest> requests;\n    \n    requests = AnalyticsRequestFactory.parse(searcher.getSchema(), params);\n\n    if(requests == null || requests.size()==0){\n      return res;\n    }\n    statsCollector.addRequests(requests.size());\n    // Computing each Analytics Request Seperately\n    for( AnalyticsRequest areq : requests ){\n      // The Accumulator which will control the statistics generation\n      // for the entire analytics request\n      ValueAccumulator accumulator; \n      \n      // The number of total facet requests\n      int facets = areq.getFieldFacets().size()+areq.getRangeFacets().size()+areq.getQueryFacets().size();\n      try {\n        if( facets== 0 ){\n          accumulator = BasicAccumulator.create(searcher, docs, areq);\n        } else {\n          accumulator = FacetingAccumulator.create(searcher, docs, areq, req);\n        }\n      } catch (IOException e) {\n        System.err.println(e.getMessage());\n        continue;\n      }\n\n      statsCollector.addStatsCollected(((BasicAccumulator)accumulator).getNumStatsCollectors());\n      statsCollector.addStatsRequests(areq.getExpressions().size());\n      statsCollector.addFieldFacets(areq.getFieldFacets().size());\n      statsCollector.addRangeFacets(areq.getRangeFacets().size());\n      statsCollector.addQueryFacets(areq.getQueryFacets().size());\n      statsCollector.addQueries(((BasicAccumulator)accumulator).getNumQueries());\n      \n      // Loop through the documents returned by the query and add to accumulator\n      Filter filter = docs.getTopFilter();\n      List<AtomicReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      for (int leafNum = 0; leafNum < contexts.size(); leafNum++) {\n        AtomicReaderContext context = contexts.get(leafNum);\n        DocIdSet dis = filter.getDocIdSet(context, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n\n        if (disi != null) {\n          accumulator.setNextReader(context);\n          int doc = disi.nextDoc();\n          while( doc != DocIdSetIterator.NO_MORE_DOCS){\n            // Add a document to the statistics being generated\n            accumulator.collect(doc);\n            doc = disi.nextDoc();\n          }\n        }\n      }\n      \n      // do some post-processing\n      accumulator.postProcess();\n     \n      // compute the stats\n      accumulator.compute();\n      \n      res.add(areq.getName(),accumulator.export());\n    }\n\n    statsCollector.endRequest();\n    return res;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/analytics/request/AnalyticsStats#execute().mjava","pathOld":"solr/core/src/java/org/apache/solr/analytics/request/AnalyticsStats#execute().mjava","sourceNew":"  /**\n   * Calculates the analytics requested in the Parameters.\n   * \n   * @return List of results formated to mirror the input XML.\n   * @throws IOException if execution fails\n   */\n  public NamedList<?> execute() throws IOException {\n    statsCollector.startRequest();\n    NamedList<Object> res = new NamedList<>();\n    List<AnalyticsRequest> requests;\n    \n    requests = AnalyticsRequestFactory.parse(searcher.getSchema(), params);\n\n    if(requests == null || requests.size()==0){\n      return res;\n    }\n    statsCollector.addRequests(requests.size());\n    // Computing each Analytics Request Seperately\n    for( AnalyticsRequest areq : requests ){\n      // The Accumulator which will control the statistics generation\n      // for the entire analytics request\n      ValueAccumulator accumulator; \n      \n      // The number of total facet requests\n      int facets = areq.getFieldFacets().size()+areq.getRangeFacets().size()+areq.getQueryFacets().size();\n      try {\n        if( facets== 0 ){\n          accumulator = BasicAccumulator.create(searcher, docs, areq);\n        } else {\n          accumulator = FacetingAccumulator.create(searcher, docs, areq, req);\n        }\n      } catch (IOException e) {\n        System.err.println(e.getMessage());\n        continue;\n      }\n\n      statsCollector.addStatsCollected(((BasicAccumulator)accumulator).getNumStatsCollectors());\n      statsCollector.addStatsRequests(areq.getExpressions().size());\n      statsCollector.addFieldFacets(areq.getFieldFacets().size());\n      statsCollector.addRangeFacets(areq.getRangeFacets().size());\n      statsCollector.addQueryFacets(areq.getQueryFacets().size());\n      statsCollector.addQueries(((BasicAccumulator)accumulator).getNumQueries());\n      \n      // Loop through the documents returned by the query and add to accumulator\n      Filter filter = docs.getTopFilter();\n      List<AtomicReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      for (int leafNum = 0; leafNum < contexts.size(); leafNum++) {\n        AtomicReaderContext context = contexts.get(leafNum);\n        DocIdSet dis = filter.getDocIdSet(context, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n\n        if (disi != null) {\n          accumulator.setNextReader(context);\n          int doc = disi.nextDoc();\n          while( doc != DocIdSetIterator.NO_MORE_DOCS){\n            // Add a document to the statistics being generated\n            accumulator.collect(doc);\n            doc = disi.nextDoc();\n          }\n        }\n      }\n      \n      // do some post-processing\n      accumulator.postProcess();\n     \n      // compute the stats\n      accumulator.compute();\n      \n      res.add(areq.getName(),accumulator.export());\n    }\n\n    statsCollector.endRequest();\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Calculates the analytics requested in the Parameters.\n   * \n   * @return List of results formated to mirror the input XML.\n   * @throws IOException if execution fails\n   */\n  public NamedList<?> execute() throws IOException {\n    statsCollector.startRequest();\n    NamedList<Object> res = new NamedList<Object>();\n    List<AnalyticsRequest> requests;\n    \n    requests = AnalyticsRequestFactory.parse(searcher.getSchema(), params);\n\n    if(requests == null || requests.size()==0){\n      return res;\n    }\n    statsCollector.addRequests(requests.size());\n    // Computing each Analytics Request Seperately\n    for( AnalyticsRequest areq : requests ){\n      // The Accumulator which will control the statistics generation\n      // for the entire analytics request\n      ValueAccumulator accumulator; \n      \n      // The number of total facet requests\n      int facets = areq.getFieldFacets().size()+areq.getRangeFacets().size()+areq.getQueryFacets().size();\n      try {\n        if( facets== 0 ){\n          accumulator = BasicAccumulator.create(searcher, docs, areq);\n        } else {\n          accumulator = FacetingAccumulator.create(searcher, docs, areq, req);\n        }\n      } catch (IOException e) {\n        System.err.println(e.getMessage());\n        continue;\n      }\n\n      statsCollector.addStatsCollected(((BasicAccumulator)accumulator).getNumStatsCollectors());\n      statsCollector.addStatsRequests(areq.getExpressions().size());\n      statsCollector.addFieldFacets(areq.getFieldFacets().size());\n      statsCollector.addRangeFacets(areq.getRangeFacets().size());\n      statsCollector.addQueryFacets(areq.getQueryFacets().size());\n      statsCollector.addQueries(((BasicAccumulator)accumulator).getNumQueries());\n      \n      // Loop through the documents returned by the query and add to accumulator\n      Filter filter = docs.getTopFilter();\n      List<AtomicReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      for (int leafNum = 0; leafNum < contexts.size(); leafNum++) {\n        AtomicReaderContext context = contexts.get(leafNum);\n        DocIdSet dis = filter.getDocIdSet(context, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n\n        if (disi != null) {\n          accumulator.setNextReader(context);\n          int doc = disi.nextDoc();\n          while( doc != DocIdSetIterator.NO_MORE_DOCS){\n            // Add a document to the statistics being generated\n            accumulator.collect(doc);\n            doc = disi.nextDoc();\n          }\n        }\n      }\n      \n      // do some post-processing\n      accumulator.postProcess();\n     \n      // compute the stats\n      accumulator.compute();\n      \n      res.add(areq.getName(),accumulator.export());\n    }\n\n    statsCollector.endRequest();\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"645e9dc687d04dbe961b35c0f3a305c48e892615","date":1396352607,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/analytics/request/AnalyticsStats#execute().mjava","pathOld":"solr/core/src/java/org/apache/solr/analytics/request/AnalyticsStats#execute().mjava","sourceNew":"  /**\n   * Calculates the analytics requested in the Parameters.\n   * \n   * @return List of results formated to mirror the input XML.\n   * @throws IOException if execution fails\n   */\n  public NamedList<?> execute() throws IOException {\n    statsCollector.startRequest();\n    NamedList<Object> res = new NamedList<>();\n    List<AnalyticsRequest> requests;\n    \n    requests = AnalyticsRequestFactory.parse(searcher.getSchema(), params);\n\n    if(requests == null || requests.size()==0){\n      return res;\n    }\n    statsCollector.addRequests(requests.size());\n    \n    // Get filter to all docs\n    Filter filter = docs.getTopFilter();\n    \n    // Computing each Analytics Request Seperately\n    for( AnalyticsRequest areq : requests ){\n      // The Accumulator which will control the statistics generation\n      // for the entire analytics request\n      ValueAccumulator accumulator; \n      \n      // The number of total facet requests\n      int facets = areq.getFieldFacets().size()+areq.getRangeFacets().size()+areq.getQueryFacets().size();\n      try {\n        if( facets== 0 ){\n          accumulator = BasicAccumulator.create(searcher, docs, areq);\n        } else {\n          accumulator = FacetingAccumulator.create(searcher, docs, areq, req);\n        }\n      } catch (IOException e) {\n        log.warn(\"Analytics request '\"+areq.getName()+\"' failed\", e);\n        continue;\n      }\n\n      statsCollector.addStatsCollected(((BasicAccumulator)accumulator).getNumStatsCollectors());\n      statsCollector.addStatsRequests(areq.getExpressions().size());\n      statsCollector.addFieldFacets(areq.getFieldFacets().size());\n      statsCollector.addRangeFacets(areq.getRangeFacets().size());\n      statsCollector.addQueryFacets(areq.getQueryFacets().size());\n      statsCollector.addQueries(((BasicAccumulator)accumulator).getNumQueries());\n      \n      // Loop through the documents returned by the query and add to accumulator\n      List<AtomicReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      for (int leafNum = 0; leafNum < contexts.size(); leafNum++) {\n        AtomicReaderContext context = contexts.get(leafNum);\n        DocIdSet dis = filter.getDocIdSet(context, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n\n        if (disi != null) {\n          accumulator.setNextReader(context);\n          int doc = disi.nextDoc();\n          while( doc != DocIdSetIterator.NO_MORE_DOCS){\n            // Add a document to the statistics being generated\n            accumulator.collect(doc);\n            doc = disi.nextDoc();\n          }\n        }\n      }\n      \n      // do some post-processing\n      accumulator.postProcess();\n     \n      // compute the stats\n      accumulator.compute();\n      \n      res.add(areq.getName(),accumulator.export());\n    }\n\n    statsCollector.endRequest();\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Calculates the analytics requested in the Parameters.\n   * \n   * @return List of results formated to mirror the input XML.\n   * @throws IOException if execution fails\n   */\n  public NamedList<?> execute() throws IOException {\n    statsCollector.startRequest();\n    NamedList<Object> res = new NamedList<>();\n    List<AnalyticsRequest> requests;\n    \n    requests = AnalyticsRequestFactory.parse(searcher.getSchema(), params);\n\n    if(requests == null || requests.size()==0){\n      return res;\n    }\n    statsCollector.addRequests(requests.size());\n    // Computing each Analytics Request Seperately\n    for( AnalyticsRequest areq : requests ){\n      // The Accumulator which will control the statistics generation\n      // for the entire analytics request\n      ValueAccumulator accumulator; \n      \n      // The number of total facet requests\n      int facets = areq.getFieldFacets().size()+areq.getRangeFacets().size()+areq.getQueryFacets().size();\n      try {\n        if( facets== 0 ){\n          accumulator = BasicAccumulator.create(searcher, docs, areq);\n        } else {\n          accumulator = FacetingAccumulator.create(searcher, docs, areq, req);\n        }\n      } catch (IOException e) {\n        System.err.println(e.getMessage());\n        continue;\n      }\n\n      statsCollector.addStatsCollected(((BasicAccumulator)accumulator).getNumStatsCollectors());\n      statsCollector.addStatsRequests(areq.getExpressions().size());\n      statsCollector.addFieldFacets(areq.getFieldFacets().size());\n      statsCollector.addRangeFacets(areq.getRangeFacets().size());\n      statsCollector.addQueryFacets(areq.getQueryFacets().size());\n      statsCollector.addQueries(((BasicAccumulator)accumulator).getNumQueries());\n      \n      // Loop through the documents returned by the query and add to accumulator\n      Filter filter = docs.getTopFilter();\n      List<AtomicReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      for (int leafNum = 0; leafNum < contexts.size(); leafNum++) {\n        AtomicReaderContext context = contexts.get(leafNum);\n        DocIdSet dis = filter.getDocIdSet(context, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n\n        if (disi != null) {\n          accumulator.setNextReader(context);\n          int doc = disi.nextDoc();\n          while( doc != DocIdSetIterator.NO_MORE_DOCS){\n            // Add a document to the statistics being generated\n            accumulator.collect(doc);\n            doc = disi.nextDoc();\n          }\n        }\n      }\n      \n      // do some post-processing\n      accumulator.postProcess();\n     \n      // compute the stats\n      accumulator.compute();\n      \n      res.add(areq.getName(),accumulator.export());\n    }\n\n    statsCollector.endRequest();\n    return res;\n  }\n\n","bugFix":["ff403346522eaa9de75403763e297e83a3ea585c"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5eb2511ababf862ea11e10761c70ee560cd84510","date":1396607225,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/analytics/request/AnalyticsStats#execute().mjava","pathOld":"solr/core/src/java/org/apache/solr/analytics/request/AnalyticsStats#execute().mjava","sourceNew":"  /**\n   * Calculates the analytics requested in the Parameters.\n   * \n   * @return List of results formated to mirror the input XML.\n   * @throws IOException if execution fails\n   */\n  public NamedList<?> execute() throws IOException {\n    statsCollector.startRequest();\n    NamedList<Object> res = new NamedList<>();\n    List<AnalyticsRequest> requests;\n    \n    requests = AnalyticsRequestFactory.parse(searcher.getSchema(), params);\n\n    if(requests == null || requests.size()==0){\n      return res;\n    }\n    statsCollector.addRequests(requests.size());\n    \n    // Get filter to all docs\n    Filter filter = docs.getTopFilter();\n    \n    // Computing each Analytics Request Seperately\n    for( AnalyticsRequest areq : requests ){\n      // The Accumulator which will control the statistics generation\n      // for the entire analytics request\n      ValueAccumulator accumulator; \n      \n      // The number of total facet requests\n      int facets = areq.getFieldFacets().size()+areq.getRangeFacets().size()+areq.getQueryFacets().size();\n      try {\n        if( facets== 0 ){\n          accumulator = BasicAccumulator.create(searcher, docs, areq);\n        } else {\n          accumulator = FacetingAccumulator.create(searcher, docs, areq, req);\n        }\n      } catch (IOException e) {\n        log.warn(\"Analytics request '\"+areq.getName()+\"' failed\", e);\n        continue;\n      }\n\n      statsCollector.addStatsCollected(((BasicAccumulator)accumulator).getNumStatsCollectors());\n      statsCollector.addStatsRequests(areq.getExpressions().size());\n      statsCollector.addFieldFacets(areq.getFieldFacets().size());\n      statsCollector.addRangeFacets(areq.getRangeFacets().size());\n      statsCollector.addQueryFacets(areq.getQueryFacets().size());\n      statsCollector.addQueries(((BasicAccumulator)accumulator).getNumQueries());\n      \n      // Loop through the documents returned by the query and add to accumulator\n      List<AtomicReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      for (int leafNum = 0; leafNum < contexts.size(); leafNum++) {\n        AtomicReaderContext context = contexts.get(leafNum);\n        DocIdSet dis = filter.getDocIdSet(context, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n\n        if (disi != null) {\n          accumulator.setNextReader(context);\n          int doc = disi.nextDoc();\n          while( doc != DocIdSetIterator.NO_MORE_DOCS){\n            // Add a document to the statistics being generated\n            accumulator.collect(doc);\n            doc = disi.nextDoc();\n          }\n        }\n      }\n      \n      // do some post-processing\n      accumulator.postProcess();\n     \n      // compute the stats\n      accumulator.compute();\n      \n      res.add(areq.getName(),accumulator.export());\n    }\n\n    statsCollector.endRequest();\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Calculates the analytics requested in the Parameters.\n   * \n   * @return List of results formated to mirror the input XML.\n   * @throws IOException if execution fails\n   */\n  public NamedList<?> execute() throws IOException {\n    statsCollector.startRequest();\n    NamedList<Object> res = new NamedList<>();\n    List<AnalyticsRequest> requests;\n    \n    requests = AnalyticsRequestFactory.parse(searcher.getSchema(), params);\n\n    if(requests == null || requests.size()==0){\n      return res;\n    }\n    statsCollector.addRequests(requests.size());\n    // Computing each Analytics Request Seperately\n    for( AnalyticsRequest areq : requests ){\n      // The Accumulator which will control the statistics generation\n      // for the entire analytics request\n      ValueAccumulator accumulator; \n      \n      // The number of total facet requests\n      int facets = areq.getFieldFacets().size()+areq.getRangeFacets().size()+areq.getQueryFacets().size();\n      try {\n        if( facets== 0 ){\n          accumulator = BasicAccumulator.create(searcher, docs, areq);\n        } else {\n          accumulator = FacetingAccumulator.create(searcher, docs, areq, req);\n        }\n      } catch (IOException e) {\n        System.err.println(e.getMessage());\n        continue;\n      }\n\n      statsCollector.addStatsCollected(((BasicAccumulator)accumulator).getNumStatsCollectors());\n      statsCollector.addStatsRequests(areq.getExpressions().size());\n      statsCollector.addFieldFacets(areq.getFieldFacets().size());\n      statsCollector.addRangeFacets(areq.getRangeFacets().size());\n      statsCollector.addQueryFacets(areq.getQueryFacets().size());\n      statsCollector.addQueries(((BasicAccumulator)accumulator).getNumQueries());\n      \n      // Loop through the documents returned by the query and add to accumulator\n      Filter filter = docs.getTopFilter();\n      List<AtomicReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      for (int leafNum = 0; leafNum < contexts.size(); leafNum++) {\n        AtomicReaderContext context = contexts.get(leafNum);\n        DocIdSet dis = filter.getDocIdSet(context, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n\n        if (disi != null) {\n          accumulator.setNextReader(context);\n          int doc = disi.nextDoc();\n          while( doc != DocIdSetIterator.NO_MORE_DOCS){\n            // Add a document to the statistics being generated\n            accumulator.collect(doc);\n            doc = disi.nextDoc();\n          }\n        }\n      }\n      \n      // do some post-processing\n      accumulator.postProcess();\n     \n      // compute the stats\n      accumulator.compute();\n      \n      res.add(areq.getName(),accumulator.export());\n    }\n\n    statsCollector.endRequest();\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae73da626f97850c922c42736f808d0378e165f0","date":1396625460,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/analytics/request/AnalyticsStats#execute().mjava","pathOld":"solr/core/src/java/org/apache/solr/analytics/request/AnalyticsStats#execute().mjava","sourceNew":"  /**\n   * Calculates the analytics requested in the Parameters.\n   * \n   * @return List of results formated to mirror the input XML.\n   * @throws IOException if execution fails\n   */\n  public NamedList<?> execute() throws IOException {\n    statsCollector.startRequest();\n    NamedList<Object> res = new NamedList<>();\n    List<AnalyticsRequest> requests;\n    \n    requests = AnalyticsRequestFactory.parse(searcher.getSchema(), params);\n\n    if(requests == null || requests.size()==0){\n      return res;\n    }\n    statsCollector.addRequests(requests.size());\n    \n    // Get filter to all docs\n    Filter filter = docs.getTopFilter();\n    \n    // Computing each Analytics Request Seperately\n    for( AnalyticsRequest areq : requests ){\n      // The Accumulator which will control the statistics generation\n      // for the entire analytics request\n      ValueAccumulator accumulator; \n      \n      // The number of total facet requests\n      int facets = areq.getFieldFacets().size()+areq.getRangeFacets().size()+areq.getQueryFacets().size();\n      try {\n        if( facets== 0 ){\n          accumulator = BasicAccumulator.create(searcher, docs, areq);\n        } else {\n          accumulator = FacetingAccumulator.create(searcher, docs, areq, req);\n        }\n      } catch (IOException e) {\n        log.warn(\"Analytics request '\"+areq.getName()+\"' failed\", e);\n        continue;\n      }\n\n      statsCollector.addStatsCollected(((BasicAccumulator)accumulator).getNumStatsCollectors());\n      statsCollector.addStatsRequests(areq.getExpressions().size());\n      statsCollector.addFieldFacets(areq.getFieldFacets().size());\n      statsCollector.addRangeFacets(areq.getRangeFacets().size());\n      statsCollector.addQueryFacets(areq.getQueryFacets().size());\n      statsCollector.addQueries(((BasicAccumulator)accumulator).getNumQueries());\n      \n      // Loop through the documents returned by the query and add to accumulator\n      List<AtomicReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      for (int leafNum = 0; leafNum < contexts.size(); leafNum++) {\n        AtomicReaderContext context = contexts.get(leafNum);\n        DocIdSet dis = filter.getDocIdSet(context, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n\n        if (disi != null) {\n          accumulator.getLeafCollector(context);\n          int doc = disi.nextDoc();\n          while( doc != DocIdSetIterator.NO_MORE_DOCS){\n            // Add a document to the statistics being generated\n            accumulator.collect(doc);\n            doc = disi.nextDoc();\n          }\n        }\n      }\n      \n      // do some post-processing\n      accumulator.postProcess();\n     \n      // compute the stats\n      accumulator.compute();\n      \n      res.add(areq.getName(),accumulator.export());\n    }\n\n    statsCollector.endRequest();\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Calculates the analytics requested in the Parameters.\n   * \n   * @return List of results formated to mirror the input XML.\n   * @throws IOException if execution fails\n   */\n  public NamedList<?> execute() throws IOException {\n    statsCollector.startRequest();\n    NamedList<Object> res = new NamedList<>();\n    List<AnalyticsRequest> requests;\n    \n    requests = AnalyticsRequestFactory.parse(searcher.getSchema(), params);\n\n    if(requests == null || requests.size()==0){\n      return res;\n    }\n    statsCollector.addRequests(requests.size());\n    \n    // Get filter to all docs\n    Filter filter = docs.getTopFilter();\n    \n    // Computing each Analytics Request Seperately\n    for( AnalyticsRequest areq : requests ){\n      // The Accumulator which will control the statistics generation\n      // for the entire analytics request\n      ValueAccumulator accumulator; \n      \n      // The number of total facet requests\n      int facets = areq.getFieldFacets().size()+areq.getRangeFacets().size()+areq.getQueryFacets().size();\n      try {\n        if( facets== 0 ){\n          accumulator = BasicAccumulator.create(searcher, docs, areq);\n        } else {\n          accumulator = FacetingAccumulator.create(searcher, docs, areq, req);\n        }\n      } catch (IOException e) {\n        log.warn(\"Analytics request '\"+areq.getName()+\"' failed\", e);\n        continue;\n      }\n\n      statsCollector.addStatsCollected(((BasicAccumulator)accumulator).getNumStatsCollectors());\n      statsCollector.addStatsRequests(areq.getExpressions().size());\n      statsCollector.addFieldFacets(areq.getFieldFacets().size());\n      statsCollector.addRangeFacets(areq.getRangeFacets().size());\n      statsCollector.addQueryFacets(areq.getQueryFacets().size());\n      statsCollector.addQueries(((BasicAccumulator)accumulator).getNumQueries());\n      \n      // Loop through the documents returned by the query and add to accumulator\n      List<AtomicReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      for (int leafNum = 0; leafNum < contexts.size(); leafNum++) {\n        AtomicReaderContext context = contexts.get(leafNum);\n        DocIdSet dis = filter.getDocIdSet(context, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n\n        if (disi != null) {\n          accumulator.setNextReader(context);\n          int doc = disi.nextDoc();\n          while( doc != DocIdSetIterator.NO_MORE_DOCS){\n            // Add a document to the statistics being generated\n            accumulator.collect(doc);\n            doc = disi.nextDoc();\n          }\n        }\n      }\n      \n      // do some post-processing\n      accumulator.postProcess();\n     \n      // compute the stats\n      accumulator.compute();\n      \n      res.add(areq.getName(),accumulator.export());\n    }\n\n    statsCollector.endRequest();\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"15e323346eac5e4685c0a9f2df85eb96b4239bbb","date":1396688577,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/analytics/request/AnalyticsStats#execute().mjava","pathOld":"solr/core/src/java/org/apache/solr/analytics/request/AnalyticsStats#execute().mjava","sourceNew":"  /**\n   * Calculates the analytics requested in the Parameters.\n   * \n   * @return List of results formated to mirror the input XML.\n   * @throws IOException if execution fails\n   */\n  public NamedList<?> execute() throws IOException {\n    statsCollector.startRequest();\n    NamedList<Object> res = new NamedList<>();\n    List<AnalyticsRequest> requests;\n    \n    requests = AnalyticsRequestFactory.parse(searcher.getSchema(), params);\n\n    if(requests == null || requests.size()==0){\n      return res;\n    }\n    statsCollector.addRequests(requests.size());\n    \n    // Get filter to all docs\n    Filter filter = docs.getTopFilter();\n    \n    // Computing each Analytics Request Seperately\n    for( AnalyticsRequest areq : requests ){\n      // The Accumulator which will control the statistics generation\n      // for the entire analytics request\n      ValueAccumulator accumulator; \n      \n      // The number of total facet requests\n      int facets = areq.getFieldFacets().size()+areq.getRangeFacets().size()+areq.getQueryFacets().size();\n      try {\n        if( facets== 0 ){\n          accumulator = BasicAccumulator.create(searcher, docs, areq);\n        } else {\n          accumulator = FacetingAccumulator.create(searcher, docs, areq, req);\n        }\n      } catch (IOException e) {\n        log.warn(\"Analytics request '\"+areq.getName()+\"' failed\", e);\n        continue;\n      }\n\n      statsCollector.addStatsCollected(((BasicAccumulator)accumulator).getNumStatsCollectors());\n      statsCollector.addStatsRequests(areq.getExpressions().size());\n      statsCollector.addFieldFacets(areq.getFieldFacets().size());\n      statsCollector.addRangeFacets(areq.getRangeFacets().size());\n      statsCollector.addQueryFacets(areq.getQueryFacets().size());\n      statsCollector.addQueries(((BasicAccumulator)accumulator).getNumQueries());\n      \n      // Loop through the documents returned by the query and add to accumulator\n      List<AtomicReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      for (int leafNum = 0; leafNum < contexts.size(); leafNum++) {\n        AtomicReaderContext context = contexts.get(leafNum);\n        DocIdSet dis = filter.getDocIdSet(context, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n\n        if (disi != null) {\n          accumulator.getLeafCollector(context);\n          int doc = disi.nextDoc();\n          while( doc != DocIdSetIterator.NO_MORE_DOCS){\n            // Add a document to the statistics being generated\n            accumulator.collect(doc);\n            doc = disi.nextDoc();\n          }\n        }\n      }\n      \n      // do some post-processing\n      accumulator.postProcess();\n     \n      // compute the stats\n      accumulator.compute();\n      \n      res.add(areq.getName(),accumulator.export());\n    }\n\n    statsCollector.endRequest();\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Calculates the analytics requested in the Parameters.\n   * \n   * @return List of results formated to mirror the input XML.\n   * @throws IOException if execution fails\n   */\n  public NamedList<?> execute() throws IOException {\n    statsCollector.startRequest();\n    NamedList<Object> res = new NamedList<>();\n    List<AnalyticsRequest> requests;\n    \n    requests = AnalyticsRequestFactory.parse(searcher.getSchema(), params);\n\n    if(requests == null || requests.size()==0){\n      return res;\n    }\n    statsCollector.addRequests(requests.size());\n    \n    // Get filter to all docs\n    Filter filter = docs.getTopFilter();\n    \n    // Computing each Analytics Request Seperately\n    for( AnalyticsRequest areq : requests ){\n      // The Accumulator which will control the statistics generation\n      // for the entire analytics request\n      ValueAccumulator accumulator; \n      \n      // The number of total facet requests\n      int facets = areq.getFieldFacets().size()+areq.getRangeFacets().size()+areq.getQueryFacets().size();\n      try {\n        if( facets== 0 ){\n          accumulator = BasicAccumulator.create(searcher, docs, areq);\n        } else {\n          accumulator = FacetingAccumulator.create(searcher, docs, areq, req);\n        }\n      } catch (IOException e) {\n        log.warn(\"Analytics request '\"+areq.getName()+\"' failed\", e);\n        continue;\n      }\n\n      statsCollector.addStatsCollected(((BasicAccumulator)accumulator).getNumStatsCollectors());\n      statsCollector.addStatsRequests(areq.getExpressions().size());\n      statsCollector.addFieldFacets(areq.getFieldFacets().size());\n      statsCollector.addRangeFacets(areq.getRangeFacets().size());\n      statsCollector.addQueryFacets(areq.getQueryFacets().size());\n      statsCollector.addQueries(((BasicAccumulator)accumulator).getNumQueries());\n      \n      // Loop through the documents returned by the query and add to accumulator\n      List<AtomicReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      for (int leafNum = 0; leafNum < contexts.size(); leafNum++) {\n        AtomicReaderContext context = contexts.get(leafNum);\n        DocIdSet dis = filter.getDocIdSet(context, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n\n        if (disi != null) {\n          accumulator.setNextReader(context);\n          int doc = disi.nextDoc();\n          while( doc != DocIdSetIterator.NO_MORE_DOCS){\n            // Add a document to the statistics being generated\n            accumulator.collect(doc);\n            doc = disi.nextDoc();\n          }\n        }\n      }\n      \n      // do some post-processing\n      accumulator.postProcess();\n     \n      // compute the stats\n      accumulator.compute();\n      \n      res.add(areq.getName(),accumulator.export());\n    }\n\n    statsCollector.endRequest();\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0f719faa74f7213d4a395510dbc1f1b7cb178484","date":1410881394,"type":5,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/contrib/analytics/src/java/org/apache/solr/analytics/request/AnalyticsStats#execute().mjava","pathOld":"solr/core/src/java/org/apache/solr/analytics/request/AnalyticsStats#execute().mjava","sourceNew":"  /**\n   * Calculates the analytics requested in the Parameters.\n   * \n   * @return List of results formated to mirror the input XML.\n   * @throws IOException if execution fails\n   */\n  public NamedList<?> execute() throws IOException {\n    statsCollector.startRequest();\n    NamedList<Object> res = new NamedList<>();\n    List<AnalyticsRequest> requests;\n    \n    requests = AnalyticsRequestFactory.parse(searcher.getSchema(), params);\n\n    if(requests == null || requests.size()==0){\n      return res;\n    }\n    statsCollector.addRequests(requests.size());\n    \n    // Get filter to all docs\n    Filter filter = docs.getTopFilter();\n    \n    // Computing each Analytics Request Seperately\n    for( AnalyticsRequest areq : requests ){\n      // The Accumulator which will control the statistics generation\n      // for the entire analytics request\n      ValueAccumulator accumulator; \n      \n      // The number of total facet requests\n      int facets = areq.getFieldFacets().size()+areq.getRangeFacets().size()+areq.getQueryFacets().size();\n      try {\n        if( facets== 0 ){\n          accumulator = BasicAccumulator.create(searcher, docs, areq);\n        } else {\n          accumulator = FacetingAccumulator.create(searcher, docs, areq, req);\n        }\n      } catch (IOException e) {\n        log.warn(\"Analytics request '\"+areq.getName()+\"' failed\", e);\n        continue;\n      }\n\n      statsCollector.addStatsCollected(((BasicAccumulator)accumulator).getNumStatsCollectors());\n      statsCollector.addStatsRequests(areq.getExpressions().size());\n      statsCollector.addFieldFacets(areq.getFieldFacets().size());\n      statsCollector.addRangeFacets(areq.getRangeFacets().size());\n      statsCollector.addQueryFacets(areq.getQueryFacets().size());\n      statsCollector.addQueries(((BasicAccumulator)accumulator).getNumQueries());\n      \n      // Loop through the documents returned by the query and add to accumulator\n      List<AtomicReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      for (int leafNum = 0; leafNum < contexts.size(); leafNum++) {\n        AtomicReaderContext context = contexts.get(leafNum);\n        DocIdSet dis = filter.getDocIdSet(context, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n\n        if (disi != null) {\n          accumulator.getLeafCollector(context);\n          int doc = disi.nextDoc();\n          while( doc != DocIdSetIterator.NO_MORE_DOCS){\n            // Add a document to the statistics being generated\n            accumulator.collect(doc);\n            doc = disi.nextDoc();\n          }\n        }\n      }\n      \n      // do some post-processing\n      accumulator.postProcess();\n     \n      // compute the stats\n      accumulator.compute();\n      \n      res.add(areq.getName(),accumulator.export());\n    }\n\n    statsCollector.endRequest();\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Calculates the analytics requested in the Parameters.\n   * \n   * @return List of results formated to mirror the input XML.\n   * @throws IOException if execution fails\n   */\n  public NamedList<?> execute() throws IOException {\n    statsCollector.startRequest();\n    NamedList<Object> res = new NamedList<>();\n    List<AnalyticsRequest> requests;\n    \n    requests = AnalyticsRequestFactory.parse(searcher.getSchema(), params);\n\n    if(requests == null || requests.size()==0){\n      return res;\n    }\n    statsCollector.addRequests(requests.size());\n    \n    // Get filter to all docs\n    Filter filter = docs.getTopFilter();\n    \n    // Computing each Analytics Request Seperately\n    for( AnalyticsRequest areq : requests ){\n      // The Accumulator which will control the statistics generation\n      // for the entire analytics request\n      ValueAccumulator accumulator; \n      \n      // The number of total facet requests\n      int facets = areq.getFieldFacets().size()+areq.getRangeFacets().size()+areq.getQueryFacets().size();\n      try {\n        if( facets== 0 ){\n          accumulator = BasicAccumulator.create(searcher, docs, areq);\n        } else {\n          accumulator = FacetingAccumulator.create(searcher, docs, areq, req);\n        }\n      } catch (IOException e) {\n        log.warn(\"Analytics request '\"+areq.getName()+\"' failed\", e);\n        continue;\n      }\n\n      statsCollector.addStatsCollected(((BasicAccumulator)accumulator).getNumStatsCollectors());\n      statsCollector.addStatsRequests(areq.getExpressions().size());\n      statsCollector.addFieldFacets(areq.getFieldFacets().size());\n      statsCollector.addRangeFacets(areq.getRangeFacets().size());\n      statsCollector.addQueryFacets(areq.getQueryFacets().size());\n      statsCollector.addQueries(((BasicAccumulator)accumulator).getNumQueries());\n      \n      // Loop through the documents returned by the query and add to accumulator\n      List<AtomicReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      for (int leafNum = 0; leafNum < contexts.size(); leafNum++) {\n        AtomicReaderContext context = contexts.get(leafNum);\n        DocIdSet dis = filter.getDocIdSet(context, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n\n        if (disi != null) {\n          accumulator.getLeafCollector(context);\n          int doc = disi.nextDoc();\n          while( doc != DocIdSetIterator.NO_MORE_DOCS){\n            // Add a document to the statistics being generated\n            accumulator.collect(doc);\n            doc = disi.nextDoc();\n          }\n        }\n      }\n      \n      // do some post-processing\n      accumulator.postProcess();\n     \n      // compute the stats\n      accumulator.compute();\n      \n      res.add(areq.getName(),accumulator.export());\n    }\n\n    statsCollector.endRequest();\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"5eb2511ababf862ea11e10761c70ee560cd84510":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","645e9dc687d04dbe961b35c0f3a305c48e892615"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["ff403346522eaa9de75403763e297e83a3ea585c"],"15e323346eac5e4685c0a9f2df85eb96b4239bbb":["5eb2511ababf862ea11e10761c70ee560cd84510","ae73da626f97850c922c42736f808d0378e165f0"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"ae73da626f97850c922c42736f808d0378e165f0":["645e9dc687d04dbe961b35c0f3a305c48e892615"],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","ff403346522eaa9de75403763e297e83a3ea585c"],"645e9dc687d04dbe961b35c0f3a305c48e892615":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"ff403346522eaa9de75403763e297e83a3ea585c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"0f719faa74f7213d4a395510dbc1f1b7cb178484":["ae73da626f97850c922c42736f808d0378e165f0"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["0f719faa74f7213d4a395510dbc1f1b7cb178484"]},"commit2Childs":{"5eb2511ababf862ea11e10761c70ee560cd84510":["15e323346eac5e4685c0a9f2df85eb96b4239bbb"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["5eb2511ababf862ea11e10761c70ee560cd84510","645e9dc687d04dbe961b35c0f3a305c48e892615"],"15e323346eac5e4685c0a9f2df85eb96b4239bbb":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["74f45af4339b0daf7a95c820ab88c1aea74fbce0","ff403346522eaa9de75403763e297e83a3ea585c"],"ae73da626f97850c922c42736f808d0378e165f0":["15e323346eac5e4685c0a9f2df85eb96b4239bbb","0f719faa74f7213d4a395510dbc1f1b7cb178484"],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":[],"645e9dc687d04dbe961b35c0f3a305c48e892615":["5eb2511ababf862ea11e10761c70ee560cd84510","ae73da626f97850c922c42736f808d0378e165f0"],"ff403346522eaa9de75403763e297e83a3ea585c":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","74f45af4339b0daf7a95c820ab88c1aea74fbce0"],"0f719faa74f7213d4a395510dbc1f1b7cb178484":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["15e323346eac5e4685c0a9f2df85eb96b4239bbb","74f45af4339b0daf7a95c820ab88c1aea74fbce0","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}