{"path":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptions().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptions().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptions().mjava","sourceNew":"  public void testDocumentsWriterExceptions() throws IOException {\n    Analyzer analyzer = new Analyzer(new Analyzer.PerFieldReuseStrategy()) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    for(int i=0;i<2;i++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle i=\" + i);\n      }\n      MockDirectoryWrapper dir = newDirectory();\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n\n      // don't allow a sudden merge to clean up the deleted\n      // doc below:\n      LogMergePolicy lmp = (LogMergePolicy) writer.getConfig().getMergePolicy();\n      lmp.setMergeFactor(Math.max(lmp.getMergeFactor(), 5));\n\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      writer.addDocument(doc);\n      writer.addDocument(doc);\n      doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n      doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n      try {\n        writer.addDocument(doc);\n        fail(\"did not hit expected exception\");\n      } catch (IOException ioe) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: hit expected exception\");\n          ioe.printStackTrace(System.out);\n        }\n      }\n\n      if (0 == i) {\n        doc = new Document();\n        doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n        writer.addDocument(doc);\n        writer.addDocument(doc);\n      }\n      writer.close();\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: open reader\");\n      }\n      IndexReader reader = IndexReader.open(dir);\n      if (i == 0) { \n        int expected = 5;\n        assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n        assertEquals(expected, reader.maxDoc());\n        int numDel = 0;\n        final Bits liveDocs = MultiFields.getLiveDocs(reader);\n        assertNotNull(liveDocs);\n        for(int j=0;j<reader.maxDoc();j++) {\n          if (!liveDocs.get(j))\n            numDel++;\n          else {\n            reader.document(j);\n            reader.getTermVectors(j);\n          }\n        }\n        assertEquals(1, numDel);\n      }\n      reader.close();\n\n      writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n          analyzer).setMaxBufferedDocs(10));\n      doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = IndexReader.open(dir);\n      int expected = 19+(1-i)*2;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n      assertEquals(0, numDel);\n\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testDocumentsWriterExceptions() throws IOException {\n    Analyzer analyzer = new Analyzer(new Analyzer.PerFieldReuseStrategy()) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    for(int i=0;i<2;i++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle i=\" + i);\n      }\n      MockDirectoryWrapper dir = newDirectory();\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n\n      // don't allow a sudden merge to clean up the deleted\n      // doc below:\n      LogMergePolicy lmp = (LogMergePolicy) writer.getConfig().getMergePolicy();\n      lmp.setMergeFactor(Math.max(lmp.getMergeFactor(), 5));\n\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      writer.addDocument(doc);\n      writer.addDocument(doc);\n      doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n      doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n      try {\n        writer.addDocument(doc);\n        fail(\"did not hit expected exception\");\n      } catch (IOException ioe) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: hit expected exception\");\n          ioe.printStackTrace(System.out);\n        }\n      }\n\n      if (0 == i) {\n        doc = new Document();\n        doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n        writer.addDocument(doc);\n        writer.addDocument(doc);\n      }\n      writer.close();\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: open reader\");\n      }\n      IndexReader reader = IndexReader.open(dir);\n      if (i == 0) { \n        int expected = 5;\n        assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n        assertEquals(expected, reader.maxDoc());\n        int numDel = 0;\n        final Bits liveDocs = MultiFields.getLiveDocs(reader);\n        assertNotNull(liveDocs);\n        for(int j=0;j<reader.maxDoc();j++) {\n          if (!liveDocs.get(j))\n            numDel++;\n          else {\n            reader.document(j);\n            reader.getTermVectors(j);\n          }\n        }\n        assertEquals(1, numDel);\n      }\n      reader.close();\n\n      writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n          analyzer).setMaxBufferedDocs(10));\n      doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = IndexReader.open(dir);\n      int expected = 19+(1-i)*2;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n      assertEquals(0, numDel);\n\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f","date":1338430031,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptions().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptions().mjava","sourceNew":"  public void testDocumentsWriterExceptions() throws IOException {\n    Analyzer analyzer = new Analyzer(new Analyzer.PerFieldReuseStrategy()) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    for(int i=0;i<2;i++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle i=\" + i);\n      }\n      MockDirectoryWrapper dir = newDirectory();\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n\n      // don't allow a sudden merge to clean up the deleted\n      // doc below:\n      LogMergePolicy lmp = (LogMergePolicy) writer.getConfig().getMergePolicy();\n      lmp.setMergeFactor(Math.max(lmp.getMergeFactor(), 5));\n\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      writer.addDocument(doc);\n      writer.addDocument(doc);\n      doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n      doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n      try {\n        writer.addDocument(doc);\n        fail(\"did not hit expected exception\");\n      } catch (IOException ioe) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: hit expected exception\");\n          ioe.printStackTrace(System.out);\n        }\n      }\n\n      if (0 == i) {\n        doc = new Document();\n        doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n        writer.addDocument(doc);\n        writer.addDocument(doc);\n      }\n      writer.close();\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: open reader\");\n      }\n      IndexReader reader = DirectoryReader.open(dir);\n      if (i == 0) { \n        int expected = 5;\n        assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n        assertEquals(expected, reader.maxDoc());\n        int numDel = 0;\n        final Bits liveDocs = MultiFields.getLiveDocs(reader);\n        assertNotNull(liveDocs);\n        for(int j=0;j<reader.maxDoc();j++) {\n          if (!liveDocs.get(j))\n            numDel++;\n          else {\n            reader.document(j);\n            reader.getTermVectors(j);\n          }\n        }\n        assertEquals(1, numDel);\n      }\n      reader.close();\n\n      writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n          analyzer).setMaxBufferedDocs(10));\n      doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = DirectoryReader.open(dir);\n      int expected = 19+(1-i)*2;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n      assertEquals(0, numDel);\n\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testDocumentsWriterExceptions() throws IOException {\n    Analyzer analyzer = new Analyzer(new Analyzer.PerFieldReuseStrategy()) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    for(int i=0;i<2;i++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle i=\" + i);\n      }\n      MockDirectoryWrapper dir = newDirectory();\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n\n      // don't allow a sudden merge to clean up the deleted\n      // doc below:\n      LogMergePolicy lmp = (LogMergePolicy) writer.getConfig().getMergePolicy();\n      lmp.setMergeFactor(Math.max(lmp.getMergeFactor(), 5));\n\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      writer.addDocument(doc);\n      writer.addDocument(doc);\n      doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n      doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n      try {\n        writer.addDocument(doc);\n        fail(\"did not hit expected exception\");\n      } catch (IOException ioe) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: hit expected exception\");\n          ioe.printStackTrace(System.out);\n        }\n      }\n\n      if (0 == i) {\n        doc = new Document();\n        doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n        writer.addDocument(doc);\n        writer.addDocument(doc);\n      }\n      writer.close();\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: open reader\");\n      }\n      IndexReader reader = IndexReader.open(dir);\n      if (i == 0) { \n        int expected = 5;\n        assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n        assertEquals(expected, reader.maxDoc());\n        int numDel = 0;\n        final Bits liveDocs = MultiFields.getLiveDocs(reader);\n        assertNotNull(liveDocs);\n        for(int j=0;j<reader.maxDoc();j++) {\n          if (!liveDocs.get(j))\n            numDel++;\n          else {\n            reader.document(j);\n            reader.getTermVectors(j);\n          }\n        }\n        assertEquals(1, numDel);\n      }\n      reader.close();\n\n      writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n          analyzer).setMaxBufferedDocs(10));\n      doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = IndexReader.open(dir);\n      int expected = 19+(1-i)*2;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n      assertEquals(0, numDel);\n\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d19974432be9aed28ee7dca73bdf01d139e763a9","date":1342822166,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptions().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptions().mjava","sourceNew":"  public void testDocumentsWriterExceptions() throws IOException {\n    Analyzer analyzer = new Analyzer(new Analyzer.PerFieldReuseStrategy()) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    for(int i=0;i<2;i++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle i=\" + i);\n      }\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n\n      // don't allow a sudden merge to clean up the deleted\n      // doc below:\n      LogMergePolicy lmp = (LogMergePolicy) writer.getConfig().getMergePolicy();\n      lmp.setMergeFactor(Math.max(lmp.getMergeFactor(), 5));\n\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      writer.addDocument(doc);\n      writer.addDocument(doc);\n      doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n      doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n      try {\n        writer.addDocument(doc);\n        fail(\"did not hit expected exception\");\n      } catch (IOException ioe) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: hit expected exception\");\n          ioe.printStackTrace(System.out);\n        }\n      }\n\n      if (0 == i) {\n        doc = new Document();\n        doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n        writer.addDocument(doc);\n        writer.addDocument(doc);\n      }\n      writer.close();\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: open reader\");\n      }\n      IndexReader reader = DirectoryReader.open(dir);\n      if (i == 0) { \n        int expected = 5;\n        assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n        assertEquals(expected, reader.maxDoc());\n        int numDel = 0;\n        final Bits liveDocs = MultiFields.getLiveDocs(reader);\n        assertNotNull(liveDocs);\n        for(int j=0;j<reader.maxDoc();j++) {\n          if (!liveDocs.get(j))\n            numDel++;\n          else {\n            reader.document(j);\n            reader.getTermVectors(j);\n          }\n        }\n        assertEquals(1, numDel);\n      }\n      reader.close();\n\n      writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n          analyzer).setMaxBufferedDocs(10));\n      doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = DirectoryReader.open(dir);\n      int expected = 19+(1-i)*2;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n      assertEquals(0, numDel);\n\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testDocumentsWriterExceptions() throws IOException {\n    Analyzer analyzer = new Analyzer(new Analyzer.PerFieldReuseStrategy()) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    for(int i=0;i<2;i++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle i=\" + i);\n      }\n      MockDirectoryWrapper dir = newDirectory();\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n\n      // don't allow a sudden merge to clean up the deleted\n      // doc below:\n      LogMergePolicy lmp = (LogMergePolicy) writer.getConfig().getMergePolicy();\n      lmp.setMergeFactor(Math.max(lmp.getMergeFactor(), 5));\n\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      writer.addDocument(doc);\n      writer.addDocument(doc);\n      doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n      doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n      try {\n        writer.addDocument(doc);\n        fail(\"did not hit expected exception\");\n      } catch (IOException ioe) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: hit expected exception\");\n          ioe.printStackTrace(System.out);\n        }\n      }\n\n      if (0 == i) {\n        doc = new Document();\n        doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n        writer.addDocument(doc);\n        writer.addDocument(doc);\n      }\n      writer.close();\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: open reader\");\n      }\n      IndexReader reader = DirectoryReader.open(dir);\n      if (i == 0) { \n        int expected = 5;\n        assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n        assertEquals(expected, reader.maxDoc());\n        int numDel = 0;\n        final Bits liveDocs = MultiFields.getLiveDocs(reader);\n        assertNotNull(liveDocs);\n        for(int j=0;j<reader.maxDoc();j++) {\n          if (!liveDocs.get(j))\n            numDel++;\n          else {\n            reader.document(j);\n            reader.getTermVectors(j);\n          }\n        }\n        assertEquals(1, numDel);\n      }\n      reader.close();\n\n      writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n          analyzer).setMaxBufferedDocs(10));\n      doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = DirectoryReader.open(dir);\n      int expected = 19+(1-i)*2;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n      assertEquals(0, numDel);\n\n      dir.close();\n    }\n  }\n\n","bugFix":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4b51f65902cc2d20ddeb7a5b949aaddf990f31a7","date":1343059585,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptions().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptions().mjava","sourceNew":"  public void testDocumentsWriterExceptions() throws IOException {\n    Analyzer analyzer = new Analyzer(new Analyzer.PerFieldReuseStrategy()) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    for(int i=0;i<2;i++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle i=\" + i);\n      }\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n\n      // don't allow a sudden merge to clean up the deleted\n      // doc below:\n      LogMergePolicy lmp = (LogMergePolicy) writer.getConfig().getMergePolicy();\n      lmp.setMergeFactor(Math.max(lmp.getMergeFactor(), 5));\n\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      writer.addDocument(doc);\n      writer.addDocument(doc);\n      doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n      doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n      try {\n        writer.addDocument(doc);\n        fail(\"did not hit expected exception\");\n      } catch (IOException ioe) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: hit expected exception\");\n          ioe.printStackTrace(System.out);\n        }\n      }\n\n      if (0 == i) {\n        doc = new Document();\n        doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n        writer.addDocument(doc);\n        writer.addDocument(doc);\n      }\n      writer.close();\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: open reader\");\n      }\n      IndexReader reader = DirectoryReader.open(dir);\n      if (i == 0) { \n        int expected = 5;\n        assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n        assertEquals(expected, reader.maxDoc());\n        int numDel = 0;\n        final Bits liveDocs = MultiFields.getLiveDocs(reader);\n        assertNotNull(liveDocs);\n        for(int j=0;j<reader.maxDoc();j++) {\n          if (!liveDocs.get(j))\n            numDel++;\n          else {\n            reader.document(j);\n            reader.getTermVectors(j);\n          }\n        }\n        assertEquals(1, numDel);\n      }\n      reader.close();\n\n      writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n          analyzer).setMaxBufferedDocs(10));\n      doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = DirectoryReader.open(dir);\n      int expected = 19+(1-i)*2;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n      assertEquals(0, numDel);\n\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testDocumentsWriterExceptions() throws IOException {\n    Analyzer analyzer = new Analyzer(new Analyzer.PerFieldReuseStrategy()) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    for(int i=0;i<2;i++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle i=\" + i);\n      }\n      MockDirectoryWrapper dir = newDirectory();\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n\n      // don't allow a sudden merge to clean up the deleted\n      // doc below:\n      LogMergePolicy lmp = (LogMergePolicy) writer.getConfig().getMergePolicy();\n      lmp.setMergeFactor(Math.max(lmp.getMergeFactor(), 5));\n\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      writer.addDocument(doc);\n      writer.addDocument(doc);\n      doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n      doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n      try {\n        writer.addDocument(doc);\n        fail(\"did not hit expected exception\");\n      } catch (IOException ioe) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: hit expected exception\");\n          ioe.printStackTrace(System.out);\n        }\n      }\n\n      if (0 == i) {\n        doc = new Document();\n        doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n        writer.addDocument(doc);\n        writer.addDocument(doc);\n      }\n      writer.close();\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: open reader\");\n      }\n      IndexReader reader = DirectoryReader.open(dir);\n      if (i == 0) { \n        int expected = 5;\n        assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n        assertEquals(expected, reader.maxDoc());\n        int numDel = 0;\n        final Bits liveDocs = MultiFields.getLiveDocs(reader);\n        assertNotNull(liveDocs);\n        for(int j=0;j<reader.maxDoc();j++) {\n          if (!liveDocs.get(j))\n            numDel++;\n          else {\n            reader.document(j);\n            reader.getTermVectors(j);\n          }\n        }\n        assertEquals(1, numDel);\n      }\n      reader.close();\n\n      writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n          analyzer).setMaxBufferedDocs(10));\n      doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = DirectoryReader.open(dir);\n      int expected = 19+(1-i)*2;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n      assertEquals(0, numDel);\n\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"aba371508186796cc6151d8223a5b4e16d02e26e","date":1343474871,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptions().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptions().mjava","sourceNew":"  public void testDocumentsWriterExceptions() throws IOException {\n    Analyzer analyzer = new Analyzer(new Analyzer.PerFieldReuseStrategy()) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    for(int i=0;i<2;i++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle i=\" + i);\n      }\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n\n      // don't allow a sudden merge to clean up the deleted\n      // doc below:\n      LogMergePolicy lmp = (LogMergePolicy) writer.getConfig().getMergePolicy();\n      lmp.setMergeFactor(Math.max(lmp.getMergeFactor(), 5));\n\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      writer.addDocument(doc);\n      writer.addDocument(doc);\n      doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n      doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n      try {\n        writer.addDocument(doc);\n        fail(\"did not hit expected exception\");\n      } catch (IOException ioe) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: hit expected exception\");\n          ioe.printStackTrace(System.out);\n        }\n      }\n\n      if (0 == i) {\n        doc = new Document();\n        doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n        writer.addDocument(doc);\n        writer.addDocument(doc);\n      }\n      writer.close();\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: open reader\");\n      }\n      IndexReader reader = DirectoryReader.open(dir);\n      if (i == 0) { \n        int expected = 5;\n        assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n        assertEquals(expected, reader.maxDoc());\n        int numDel = 0;\n        final Bits liveDocs = MultiFields.getLiveDocs(reader);\n        assertNotNull(liveDocs);\n        for(int j=0;j<reader.maxDoc();j++) {\n          if (!liveDocs.get(j))\n            numDel++;\n          else {\n            reader.document(j);\n            reader.getTermVectors(j);\n          }\n        }\n        assertEquals(1, numDel);\n      }\n      reader.close();\n\n      writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n          analyzer).setMaxBufferedDocs(10));\n      doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = DirectoryReader.open(dir);\n      int expected = 19+(1-i)*2;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n      assertEquals(0, numDel);\n\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testDocumentsWriterExceptions() throws IOException {\n    Analyzer analyzer = new Analyzer(new Analyzer.PerFieldReuseStrategy()) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    for(int i=0;i<2;i++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle i=\" + i);\n      }\n      MockDirectoryWrapper dir = newDirectory();\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n\n      // don't allow a sudden merge to clean up the deleted\n      // doc below:\n      LogMergePolicy lmp = (LogMergePolicy) writer.getConfig().getMergePolicy();\n      lmp.setMergeFactor(Math.max(lmp.getMergeFactor(), 5));\n\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      writer.addDocument(doc);\n      writer.addDocument(doc);\n      doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n      doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n      try {\n        writer.addDocument(doc);\n        fail(\"did not hit expected exception\");\n      } catch (IOException ioe) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: hit expected exception\");\n          ioe.printStackTrace(System.out);\n        }\n      }\n\n      if (0 == i) {\n        doc = new Document();\n        doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n        writer.addDocument(doc);\n        writer.addDocument(doc);\n      }\n      writer.close();\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: open reader\");\n      }\n      IndexReader reader = DirectoryReader.open(dir);\n      if (i == 0) { \n        int expected = 5;\n        assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n        assertEquals(expected, reader.maxDoc());\n        int numDel = 0;\n        final Bits liveDocs = MultiFields.getLiveDocs(reader);\n        assertNotNull(liveDocs);\n        for(int j=0;j<reader.maxDoc();j++) {\n          if (!liveDocs.get(j))\n            numDel++;\n          else {\n            reader.document(j);\n            reader.getTermVectors(j);\n          }\n        }\n        assertEquals(1, numDel);\n      }\n      reader.close();\n\n      writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n          analyzer).setMaxBufferedDocs(10));\n      doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = DirectoryReader.open(dir);\n      int expected = 19+(1-i)*2;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n      assertEquals(0, numDel);\n\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"38061899d760e06a12fe186bc1f09ca9ff0e64a6","date":1376491296,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptions().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptions().mjava","sourceNew":"  public void testDocumentsWriterExceptions() throws IOException {\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    for(int i=0;i<2;i++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle i=\" + i);\n      }\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n\n      // don't allow a sudden merge to clean up the deleted\n      // doc below:\n      LogMergePolicy lmp = (LogMergePolicy) writer.getConfig().getMergePolicy();\n      lmp.setMergeFactor(Math.max(lmp.getMergeFactor(), 5));\n\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      writer.addDocument(doc);\n      writer.addDocument(doc);\n      doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n      doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n      try {\n        writer.addDocument(doc);\n        fail(\"did not hit expected exception\");\n      } catch (IOException ioe) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: hit expected exception\");\n          ioe.printStackTrace(System.out);\n        }\n      }\n\n      if (0 == i) {\n        doc = new Document();\n        doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n        writer.addDocument(doc);\n        writer.addDocument(doc);\n      }\n      writer.close();\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: open reader\");\n      }\n      IndexReader reader = DirectoryReader.open(dir);\n      if (i == 0) { \n        int expected = 5;\n        assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n        assertEquals(expected, reader.maxDoc());\n        int numDel = 0;\n        final Bits liveDocs = MultiFields.getLiveDocs(reader);\n        assertNotNull(liveDocs);\n        for(int j=0;j<reader.maxDoc();j++) {\n          if (!liveDocs.get(j))\n            numDel++;\n          else {\n            reader.document(j);\n            reader.getTermVectors(j);\n          }\n        }\n        assertEquals(1, numDel);\n      }\n      reader.close();\n\n      writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n          analyzer).setMaxBufferedDocs(10));\n      doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = DirectoryReader.open(dir);\n      int expected = 19+(1-i)*2;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n      assertEquals(0, numDel);\n\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testDocumentsWriterExceptions() throws IOException {\n    Analyzer analyzer = new Analyzer(new Analyzer.PerFieldReuseStrategy()) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    for(int i=0;i<2;i++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle i=\" + i);\n      }\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n\n      // don't allow a sudden merge to clean up the deleted\n      // doc below:\n      LogMergePolicy lmp = (LogMergePolicy) writer.getConfig().getMergePolicy();\n      lmp.setMergeFactor(Math.max(lmp.getMergeFactor(), 5));\n\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      writer.addDocument(doc);\n      writer.addDocument(doc);\n      doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n      doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n      try {\n        writer.addDocument(doc);\n        fail(\"did not hit expected exception\");\n      } catch (IOException ioe) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: hit expected exception\");\n          ioe.printStackTrace(System.out);\n        }\n      }\n\n      if (0 == i) {\n        doc = new Document();\n        doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n        writer.addDocument(doc);\n        writer.addDocument(doc);\n      }\n      writer.close();\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: open reader\");\n      }\n      IndexReader reader = DirectoryReader.open(dir);\n      if (i == 0) { \n        int expected = 5;\n        assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n        assertEquals(expected, reader.maxDoc());\n        int numDel = 0;\n        final Bits liveDocs = MultiFields.getLiveDocs(reader);\n        assertNotNull(liveDocs);\n        for(int j=0;j<reader.maxDoc();j++) {\n          if (!liveDocs.get(j))\n            numDel++;\n          else {\n            reader.document(j);\n            reader.getTermVectors(j);\n          }\n        }\n        assertEquals(1, numDel);\n      }\n      reader.close();\n\n      writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n          analyzer).setMaxBufferedDocs(10));\n      doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = DirectoryReader.open(dir);\n      int expected = 19+(1-i)*2;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n      assertEquals(0, numDel);\n\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3dffec77fb8f7d0e9ca4869dddd6af94528b4576","date":1377875202,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptions().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptions().mjava","sourceNew":"  public void testDocumentsWriterExceptions() throws IOException {\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    for(int i=0;i<2;i++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle i=\" + i);\n      }\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n\n      // don't allow a sudden merge to clean up the deleted\n      // doc below:\n      LogMergePolicy lmp = (LogMergePolicy) writer.getConfig().getMergePolicy();\n      lmp.setMergeFactor(Math.max(lmp.getMergeFactor(), 5));\n\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      writer.addDocument(doc);\n      writer.addDocument(doc);\n      doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n      doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n      try {\n        writer.addDocument(doc);\n        fail(\"did not hit expected exception\");\n      } catch (IOException ioe) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: hit expected exception\");\n          ioe.printStackTrace(System.out);\n        }\n      }\n\n      if (0 == i) {\n        doc = new Document();\n        doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n        writer.addDocument(doc);\n        writer.addDocument(doc);\n      }\n      writer.close();\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: open reader\");\n      }\n      IndexReader reader = DirectoryReader.open(dir);\n      if (i == 0) { \n        int expected = 5;\n        assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n        assertEquals(expected, reader.maxDoc());\n        int numDel = 0;\n        final Bits liveDocs = MultiFields.getLiveDocs(reader);\n        assertNotNull(liveDocs);\n        for(int j=0;j<reader.maxDoc();j++) {\n          if (!liveDocs.get(j))\n            numDel++;\n          else {\n            reader.document(j);\n            reader.getTermVectors(j);\n          }\n        }\n        assertEquals(1, numDel);\n      }\n      reader.close();\n\n      writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n          analyzer).setMaxBufferedDocs(10));\n      doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = DirectoryReader.open(dir);\n      int expected = 19+(1-i)*2;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n      assertEquals(0, numDel);\n\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testDocumentsWriterExceptions() throws IOException {\n    Analyzer analyzer = new Analyzer(new Analyzer.PerFieldReuseStrategy()) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    for(int i=0;i<2;i++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle i=\" + i);\n      }\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n\n      // don't allow a sudden merge to clean up the deleted\n      // doc below:\n      LogMergePolicy lmp = (LogMergePolicy) writer.getConfig().getMergePolicy();\n      lmp.setMergeFactor(Math.max(lmp.getMergeFactor(), 5));\n\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      writer.addDocument(doc);\n      writer.addDocument(doc);\n      doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n      doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n      try {\n        writer.addDocument(doc);\n        fail(\"did not hit expected exception\");\n      } catch (IOException ioe) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: hit expected exception\");\n          ioe.printStackTrace(System.out);\n        }\n      }\n\n      if (0 == i) {\n        doc = new Document();\n        doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n        writer.addDocument(doc);\n        writer.addDocument(doc);\n      }\n      writer.close();\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: open reader\");\n      }\n      IndexReader reader = DirectoryReader.open(dir);\n      if (i == 0) { \n        int expected = 5;\n        assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n        assertEquals(expected, reader.maxDoc());\n        int numDel = 0;\n        final Bits liveDocs = MultiFields.getLiveDocs(reader);\n        assertNotNull(liveDocs);\n        for(int j=0;j<reader.maxDoc();j++) {\n          if (!liveDocs.get(j))\n            numDel++;\n          else {\n            reader.document(j);\n            reader.getTermVectors(j);\n          }\n        }\n        assertEquals(1, numDel);\n      }\n      reader.close();\n\n      writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n          analyzer).setMaxBufferedDocs(10));\n      doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = DirectoryReader.open(dir);\n      int expected = 19+(1-i)*2;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n      assertEquals(0, numDel);\n\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","date":1389274049,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptions().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptions().mjava","sourceNew":"  public void testDocumentsWriterExceptions() throws IOException {\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    for(int i=0;i<2;i++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle i=\" + i);\n      }\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n\n      // don't allow a sudden merge to clean up the deleted\n      // doc below:\n      LogMergePolicy lmp = (LogMergePolicy) writer.getConfig().getMergePolicy();\n      lmp.setMergeFactor(Math.max(lmp.getMergeFactor(), 5));\n\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      writer.addDocument(doc);\n      writer.addDocument(doc);\n      doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n      doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n      try {\n        writer.addDocument(doc);\n        fail(\"did not hit expected exception\");\n      } catch (IOException ioe) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: hit expected exception\");\n          ioe.printStackTrace(System.out);\n        }\n      }\n\n      if (0 == i) {\n        doc = new Document();\n        doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n        writer.addDocument(doc);\n        writer.addDocument(doc);\n      }\n      writer.close();\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: open reader\");\n      }\n      IndexReader reader = DirectoryReader.open(dir);\n      if (i == 0) { \n        int expected = 5;\n        assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n        assertEquals(expected, reader.maxDoc());\n        int numDel = 0;\n        final Bits liveDocs = MultiFields.getLiveDocs(reader);\n        assertNotNull(liveDocs);\n        for(int j=0;j<reader.maxDoc();j++) {\n          if (!liveDocs.get(j))\n            numDel++;\n          else {\n            reader.document(j);\n            reader.getTermVectors(j);\n          }\n        }\n        assertEquals(1, numDel);\n      }\n      reader.close();\n\n      writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n          analyzer).setMaxBufferedDocs(10));\n      doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = DirectoryReader.open(dir);\n      int expected = 19+(1-i)*2;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n      assertEquals(0, numDel);\n\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testDocumentsWriterExceptions() throws IOException {\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    for(int i=0;i<2;i++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle i=\" + i);\n      }\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n\n      // don't allow a sudden merge to clean up the deleted\n      // doc below:\n      LogMergePolicy lmp = (LogMergePolicy) writer.getConfig().getMergePolicy();\n      lmp.setMergeFactor(Math.max(lmp.getMergeFactor(), 5));\n\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      writer.addDocument(doc);\n      writer.addDocument(doc);\n      doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n      doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n      try {\n        writer.addDocument(doc);\n        fail(\"did not hit expected exception\");\n      } catch (IOException ioe) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: hit expected exception\");\n          ioe.printStackTrace(System.out);\n        }\n      }\n\n      if (0 == i) {\n        doc = new Document();\n        doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n        writer.addDocument(doc);\n        writer.addDocument(doc);\n      }\n      writer.close();\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: open reader\");\n      }\n      IndexReader reader = DirectoryReader.open(dir);\n      if (i == 0) { \n        int expected = 5;\n        assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n        assertEquals(expected, reader.maxDoc());\n        int numDel = 0;\n        final Bits liveDocs = MultiFields.getLiveDocs(reader);\n        assertNotNull(liveDocs);\n        for(int j=0;j<reader.maxDoc();j++) {\n          if (!liveDocs.get(j))\n            numDel++;\n          else {\n            reader.document(j);\n            reader.getTermVectors(j);\n          }\n        }\n        assertEquals(1, numDel);\n      }\n      reader.close();\n\n      writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n          analyzer).setMaxBufferedDocs(10));\n      doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = DirectoryReader.open(dir);\n      int expected = 19+(1-i)*2;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n      assertEquals(0, numDel);\n\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptions().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptions().mjava","sourceNew":"  public void testDocumentsWriterExceptions() throws IOException {\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    for(int i=0;i<2;i++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle i=\" + i);\n      }\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n\n      // don't allow a sudden merge to clean up the deleted\n      // doc below:\n      LogMergePolicy lmp = (LogMergePolicy) writer.getConfig().getMergePolicy();\n      lmp.setMergeFactor(Math.max(lmp.getMergeFactor(), 5));\n\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      writer.addDocument(doc);\n      writer.addDocument(doc);\n      doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n      doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n      try {\n        writer.addDocument(doc);\n        fail(\"did not hit expected exception\");\n      } catch (IOException ioe) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: hit expected exception\");\n          ioe.printStackTrace(System.out);\n        }\n      }\n\n      if (0 == i) {\n        doc = new Document();\n        doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n        writer.addDocument(doc);\n        writer.addDocument(doc);\n      }\n      writer.shutdown();\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: open reader\");\n      }\n      IndexReader reader = DirectoryReader.open(dir);\n      if (i == 0) { \n        int expected = 5;\n        assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n        assertEquals(expected, reader.maxDoc());\n        int numDel = 0;\n        final Bits liveDocs = MultiFields.getLiveDocs(reader);\n        assertNotNull(liveDocs);\n        for(int j=0;j<reader.maxDoc();j++) {\n          if (!liveDocs.get(j))\n            numDel++;\n          else {\n            reader.document(j);\n            reader.getTermVectors(j);\n          }\n        }\n        assertEquals(1, numDel);\n      }\n      reader.close();\n\n      writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n          analyzer).setMaxBufferedDocs(10));\n      doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.shutdown();\n\n      reader = DirectoryReader.open(dir);\n      int expected = 19+(1-i)*2;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n      assertEquals(0, numDel);\n\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testDocumentsWriterExceptions() throws IOException {\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    for(int i=0;i<2;i++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle i=\" + i);\n      }\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n\n      // don't allow a sudden merge to clean up the deleted\n      // doc below:\n      LogMergePolicy lmp = (LogMergePolicy) writer.getConfig().getMergePolicy();\n      lmp.setMergeFactor(Math.max(lmp.getMergeFactor(), 5));\n\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      writer.addDocument(doc);\n      writer.addDocument(doc);\n      doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n      doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n      try {\n        writer.addDocument(doc);\n        fail(\"did not hit expected exception\");\n      } catch (IOException ioe) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: hit expected exception\");\n          ioe.printStackTrace(System.out);\n        }\n      }\n\n      if (0 == i) {\n        doc = new Document();\n        doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n        writer.addDocument(doc);\n        writer.addDocument(doc);\n      }\n      writer.close();\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: open reader\");\n      }\n      IndexReader reader = DirectoryReader.open(dir);\n      if (i == 0) { \n        int expected = 5;\n        assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n        assertEquals(expected, reader.maxDoc());\n        int numDel = 0;\n        final Bits liveDocs = MultiFields.getLiveDocs(reader);\n        assertNotNull(liveDocs);\n        for(int j=0;j<reader.maxDoc();j++) {\n          if (!liveDocs.get(j))\n            numDel++;\n          else {\n            reader.document(j);\n            reader.getTermVectors(j);\n          }\n        }\n        assertEquals(1, numDel);\n      }\n      reader.close();\n\n      writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n          analyzer).setMaxBufferedDocs(10));\n      doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = DirectoryReader.open(dir);\n      int expected = 19+(1-i)*2;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n      assertEquals(0, numDel);\n\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0a16b40feb4e6e0d55c1716733bde48296bedd20","date":1400540388,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptions().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptions().mjava","sourceNew":"  public void testDocumentsWriterExceptions() throws IOException {\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    for(int i=0;i<2;i++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle i=\" + i);\n      }\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n\n      // don't allow a sudden merge to clean up the deleted\n      // doc below:\n      LogMergePolicy lmp = (LogMergePolicy) writer.getConfig().getMergePolicy();\n      lmp.setMergeFactor(Math.max(lmp.getMergeFactor(), 5));\n\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      writer.addDocument(doc);\n      writer.addDocument(doc);\n      doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n      doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n      try {\n        writer.addDocument(doc);\n        fail(\"did not hit expected exception\");\n      } catch (IOException ioe) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: hit expected exception\");\n          ioe.printStackTrace(System.out);\n        }\n      }\n\n      if (0 == i) {\n        doc = new Document();\n        doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n        writer.addDocument(doc);\n        writer.addDocument(doc);\n      }\n      writer.shutdown();\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: open reader\");\n      }\n      IndexReader reader = DirectoryReader.open(dir);\n      if (i == 0) { \n        int expected = 5;\n        assertEquals(expected-1, reader.docFreq(new Term(\"contents\", \"here\")));\n        assertEquals(expected, reader.maxDoc());\n        int numDel = 0;\n        final Bits liveDocs = MultiFields.getLiveDocs(reader);\n        assertNotNull(liveDocs);\n        for(int j=0;j<reader.maxDoc();j++) {\n          if (!liveDocs.get(j))\n            numDel++;\n          else {\n            reader.document(j);\n            reader.getTermVectors(j);\n          }\n        }\n        assertEquals(1, numDel);\n      }\n      reader.close();\n\n      writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n          analyzer).setMaxBufferedDocs(10));\n      doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.shutdown();\n\n      reader = DirectoryReader.open(dir);\n      int expected = 19+(1-i)*2;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n      assertEquals(0, numDel);\n\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testDocumentsWriterExceptions() throws IOException {\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    for(int i=0;i<2;i++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle i=\" + i);\n      }\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n\n      // don't allow a sudden merge to clean up the deleted\n      // doc below:\n      LogMergePolicy lmp = (LogMergePolicy) writer.getConfig().getMergePolicy();\n      lmp.setMergeFactor(Math.max(lmp.getMergeFactor(), 5));\n\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      writer.addDocument(doc);\n      writer.addDocument(doc);\n      doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n      doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n      try {\n        writer.addDocument(doc);\n        fail(\"did not hit expected exception\");\n      } catch (IOException ioe) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: hit expected exception\");\n          ioe.printStackTrace(System.out);\n        }\n      }\n\n      if (0 == i) {\n        doc = new Document();\n        doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n        writer.addDocument(doc);\n        writer.addDocument(doc);\n      }\n      writer.shutdown();\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: open reader\");\n      }\n      IndexReader reader = DirectoryReader.open(dir);\n      if (i == 0) { \n        int expected = 5;\n        assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n        assertEquals(expected, reader.maxDoc());\n        int numDel = 0;\n        final Bits liveDocs = MultiFields.getLiveDocs(reader);\n        assertNotNull(liveDocs);\n        for(int j=0;j<reader.maxDoc();j++) {\n          if (!liveDocs.get(j))\n            numDel++;\n          else {\n            reader.document(j);\n            reader.getTermVectors(j);\n          }\n        }\n        assertEquals(1, numDel);\n      }\n      reader.close();\n\n      writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n          analyzer).setMaxBufferedDocs(10));\n      doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.shutdown();\n\n      reader = DirectoryReader.open(dir);\n      int expected = 19+(1-i)*2;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n      assertEquals(0, numDel);\n\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"45a621dd071a902e1fd30367200d7bbbea037706","date":1400686915,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptions().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptions().mjava","sourceNew":"  public void testDocumentsWriterExceptions() throws IOException {\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    for(int i=0;i<2;i++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle i=\" + i);\n      }\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n\n      // don't allow a sudden merge to clean up the deleted\n      // doc below:\n      LogMergePolicy lmp = (LogMergePolicy) writer.getConfig().getMergePolicy();\n      lmp.setMergeFactor(Math.max(lmp.getMergeFactor(), 5));\n\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      writer.addDocument(doc);\n      writer.addDocument(doc);\n      doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n      doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n      try {\n        writer.addDocument(doc);\n        fail(\"did not hit expected exception\");\n      } catch (IOException ioe) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: hit expected exception\");\n          ioe.printStackTrace(System.out);\n        }\n      }\n\n      if (0 == i) {\n        doc = new Document();\n        doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n        writer.addDocument(doc);\n        writer.addDocument(doc);\n      }\n      writer.shutdown();\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: open reader\");\n      }\n      IndexReader reader = DirectoryReader.open(dir);\n      if (i == 0) { \n        int expected = 5;\n        assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n        assertEquals(expected, reader.maxDoc());\n        int numDel = 0;\n        final Bits liveDocs = MultiFields.getLiveDocs(reader);\n        assertNotNull(liveDocs);\n        for(int j=0;j<reader.maxDoc();j++) {\n          if (!liveDocs.get(j))\n            numDel++;\n          else {\n            reader.document(j);\n            reader.getTermVectors(j);\n          }\n        }\n        assertEquals(1, numDel);\n      }\n      reader.close();\n\n      writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n          analyzer).setMaxBufferedDocs(10));\n      doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.shutdown();\n\n      reader = DirectoryReader.open(dir);\n      int expected = 19+(1-i)*2;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n      assertEquals(0, numDel);\n\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testDocumentsWriterExceptions() throws IOException {\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    for(int i=0;i<2;i++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle i=\" + i);\n      }\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n\n      // don't allow a sudden merge to clean up the deleted\n      // doc below:\n      LogMergePolicy lmp = (LogMergePolicy) writer.getConfig().getMergePolicy();\n      lmp.setMergeFactor(Math.max(lmp.getMergeFactor(), 5));\n\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      writer.addDocument(doc);\n      writer.addDocument(doc);\n      doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n      doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n      try {\n        writer.addDocument(doc);\n        fail(\"did not hit expected exception\");\n      } catch (IOException ioe) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: hit expected exception\");\n          ioe.printStackTrace(System.out);\n        }\n      }\n\n      if (0 == i) {\n        doc = new Document();\n        doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n        writer.addDocument(doc);\n        writer.addDocument(doc);\n      }\n      writer.shutdown();\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: open reader\");\n      }\n      IndexReader reader = DirectoryReader.open(dir);\n      if (i == 0) { \n        int expected = 5;\n        assertEquals(expected-1, reader.docFreq(new Term(\"contents\", \"here\")));\n        assertEquals(expected, reader.maxDoc());\n        int numDel = 0;\n        final Bits liveDocs = MultiFields.getLiveDocs(reader);\n        assertNotNull(liveDocs);\n        for(int j=0;j<reader.maxDoc();j++) {\n          if (!liveDocs.get(j))\n            numDel++;\n          else {\n            reader.document(j);\n            reader.getTermVectors(j);\n          }\n        }\n        assertEquals(1, numDel);\n      }\n      reader.close();\n\n      writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n          analyzer).setMaxBufferedDocs(10));\n      doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.shutdown();\n\n      reader = DirectoryReader.open(dir);\n      int expected = 19+(1-i)*2;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n      assertEquals(0, numDel);\n\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptions().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptions().mjava","sourceNew":"  public void testDocumentsWriterExceptions() throws IOException {\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    for(int i=0;i<2;i++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle i=\" + i);\n      }\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                                  .setMergePolicy(newLogMergePolicy()));\n\n      // don't allow a sudden merge to clean up the deleted\n      // doc below:\n      LogMergePolicy lmp = (LogMergePolicy) writer.getConfig().getMergePolicy();\n      lmp.setMergeFactor(Math.max(lmp.getMergeFactor(), 5));\n\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      writer.addDocument(doc);\n      writer.addDocument(doc);\n      doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n      doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n      try {\n        writer.addDocument(doc);\n        fail(\"did not hit expected exception\");\n      } catch (IOException ioe) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: hit expected exception\");\n          ioe.printStackTrace(System.out);\n        }\n      }\n\n      if (0 == i) {\n        doc = new Document();\n        doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n        writer.addDocument(doc);\n        writer.addDocument(doc);\n      }\n      writer.shutdown();\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: open reader\");\n      }\n      IndexReader reader = DirectoryReader.open(dir);\n      if (i == 0) { \n        int expected = 5;\n        assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n        assertEquals(expected, reader.maxDoc());\n        int numDel = 0;\n        final Bits liveDocs = MultiFields.getLiveDocs(reader);\n        assertNotNull(liveDocs);\n        for(int j=0;j<reader.maxDoc();j++) {\n          if (!liveDocs.get(j))\n            numDel++;\n          else {\n            reader.document(j);\n            reader.getTermVectors(j);\n          }\n        }\n        assertEquals(1, numDel);\n      }\n      reader.close();\n\n      writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                      .setMaxBufferedDocs(10));\n      doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.shutdown();\n\n      reader = DirectoryReader.open(dir);\n      int expected = 19+(1-i)*2;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n      assertEquals(0, numDel);\n\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testDocumentsWriterExceptions() throws IOException {\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    for(int i=0;i<2;i++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle i=\" + i);\n      }\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n\n      // don't allow a sudden merge to clean up the deleted\n      // doc below:\n      LogMergePolicy lmp = (LogMergePolicy) writer.getConfig().getMergePolicy();\n      lmp.setMergeFactor(Math.max(lmp.getMergeFactor(), 5));\n\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      writer.addDocument(doc);\n      writer.addDocument(doc);\n      doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n      doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n      try {\n        writer.addDocument(doc);\n        fail(\"did not hit expected exception\");\n      } catch (IOException ioe) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: hit expected exception\");\n          ioe.printStackTrace(System.out);\n        }\n      }\n\n      if (0 == i) {\n        doc = new Document();\n        doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n        writer.addDocument(doc);\n        writer.addDocument(doc);\n      }\n      writer.shutdown();\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: open reader\");\n      }\n      IndexReader reader = DirectoryReader.open(dir);\n      if (i == 0) { \n        int expected = 5;\n        assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n        assertEquals(expected, reader.maxDoc());\n        int numDel = 0;\n        final Bits liveDocs = MultiFields.getLiveDocs(reader);\n        assertNotNull(liveDocs);\n        for(int j=0;j<reader.maxDoc();j++) {\n          if (!liveDocs.get(j))\n            numDel++;\n          else {\n            reader.document(j);\n            reader.getTermVectors(j);\n          }\n        }\n        assertEquals(1, numDel);\n      }\n      reader.close();\n\n      writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n          analyzer).setMaxBufferedDocs(10));\n      doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.shutdown();\n\n      reader = DirectoryReader.open(dir);\n      int expected = 19+(1-i)*2;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n      assertEquals(0, numDel);\n\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptions().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptions().mjava","sourceNew":"  public void testDocumentsWriterExceptions() throws IOException {\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    for(int i=0;i<2;i++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle i=\" + i);\n      }\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                                  .setMergePolicy(newLogMergePolicy()));\n\n      // don't allow a sudden merge to clean up the deleted\n      // doc below:\n      LogMergePolicy lmp = (LogMergePolicy) writer.getConfig().getMergePolicy();\n      lmp.setMergeFactor(Math.max(lmp.getMergeFactor(), 5));\n\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      writer.addDocument(doc);\n      writer.addDocument(doc);\n      doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n      doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n      try {\n        writer.addDocument(doc);\n        fail(\"did not hit expected exception\");\n      } catch (IOException ioe) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: hit expected exception\");\n          ioe.printStackTrace(System.out);\n        }\n      }\n\n      if (0 == i) {\n        doc = new Document();\n        doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n        writer.addDocument(doc);\n        writer.addDocument(doc);\n      }\n      writer.close();\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: open reader\");\n      }\n      IndexReader reader = DirectoryReader.open(dir);\n      if (i == 0) { \n        int expected = 5;\n        assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n        assertEquals(expected, reader.maxDoc());\n        int numDel = 0;\n        final Bits liveDocs = MultiFields.getLiveDocs(reader);\n        assertNotNull(liveDocs);\n        for(int j=0;j<reader.maxDoc();j++) {\n          if (!liveDocs.get(j))\n            numDel++;\n          else {\n            reader.document(j);\n            reader.getTermVectors(j);\n          }\n        }\n        assertEquals(1, numDel);\n      }\n      reader.close();\n\n      writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                      .setMaxBufferedDocs(10));\n      doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = DirectoryReader.open(dir);\n      int expected = 19+(1-i)*2;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n      assertEquals(0, numDel);\n\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testDocumentsWriterExceptions() throws IOException {\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    for(int i=0;i<2;i++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle i=\" + i);\n      }\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                                  .setMergePolicy(newLogMergePolicy()));\n\n      // don't allow a sudden merge to clean up the deleted\n      // doc below:\n      LogMergePolicy lmp = (LogMergePolicy) writer.getConfig().getMergePolicy();\n      lmp.setMergeFactor(Math.max(lmp.getMergeFactor(), 5));\n\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      writer.addDocument(doc);\n      writer.addDocument(doc);\n      doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n      doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n      try {\n        writer.addDocument(doc);\n        fail(\"did not hit expected exception\");\n      } catch (IOException ioe) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: hit expected exception\");\n          ioe.printStackTrace(System.out);\n        }\n      }\n\n      if (0 == i) {\n        doc = new Document();\n        doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n        writer.addDocument(doc);\n        writer.addDocument(doc);\n      }\n      writer.shutdown();\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: open reader\");\n      }\n      IndexReader reader = DirectoryReader.open(dir);\n      if (i == 0) { \n        int expected = 5;\n        assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n        assertEquals(expected, reader.maxDoc());\n        int numDel = 0;\n        final Bits liveDocs = MultiFields.getLiveDocs(reader);\n        assertNotNull(liveDocs);\n        for(int j=0;j<reader.maxDoc();j++) {\n          if (!liveDocs.get(j))\n            numDel++;\n          else {\n            reader.document(j);\n            reader.getTermVectors(j);\n          }\n        }\n        assertEquals(1, numDel);\n      }\n      reader.close();\n\n      writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                      .setMaxBufferedDocs(10));\n      doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.shutdown();\n\n      reader = DirectoryReader.open(dir);\n      int expected = 19+(1-i)*2;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n      assertEquals(0, numDel);\n\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"04e775de416dd2d8067b10db1c8af975a1d5017e","date":1539906554,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptions().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptions().mjava","sourceNew":"  public void testDocumentsWriterExceptions() throws IOException {\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    for(int i=0;i<2;i++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle i=\" + i);\n      }\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                                  .setMergePolicy(newLogMergePolicy()));\n\n      // don't allow a sudden merge to clean up the deleted\n      // doc below:\n      LogMergePolicy lmp = (LogMergePolicy) writer.getConfig().getMergePolicy();\n      lmp.setMergeFactor(Math.max(lmp.getMergeFactor(), 5));\n\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      writer.addDocument(doc);\n      writer.addDocument(doc);\n      doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n      doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n      try {\n        writer.addDocument(doc);\n        fail(\"did not hit expected exception\");\n      } catch (IOException ioe) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: hit expected exception\");\n          ioe.printStackTrace(System.out);\n        }\n      }\n\n      if (0 == i) {\n        doc = new Document();\n        doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n        writer.addDocument(doc);\n        writer.addDocument(doc);\n      }\n      writer.close();\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: open reader\");\n      }\n      IndexReader reader = DirectoryReader.open(dir);\n      if (i == 0) { \n        int expected = 5;\n        assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n        assertEquals(expected, reader.maxDoc());\n        int numDel = 0;\n        final Bits liveDocs = MultiBits.getLiveDocs(reader);\n        assertNotNull(liveDocs);\n        for(int j=0;j<reader.maxDoc();j++) {\n          if (!liveDocs.get(j))\n            numDel++;\n          else {\n            reader.document(j);\n            reader.getTermVectors(j);\n          }\n        }\n        assertEquals(1, numDel);\n      }\n      reader.close();\n\n      writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                      .setMaxBufferedDocs(10));\n      doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = DirectoryReader.open(dir);\n      int expected = 19+(1-i)*2;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      assertNull(MultiBits.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n      assertEquals(0, numDel);\n\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testDocumentsWriterExceptions() throws IOException {\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    for(int i=0;i<2;i++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle i=\" + i);\n      }\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                                  .setMergePolicy(newLogMergePolicy()));\n\n      // don't allow a sudden merge to clean up the deleted\n      // doc below:\n      LogMergePolicy lmp = (LogMergePolicy) writer.getConfig().getMergePolicy();\n      lmp.setMergeFactor(Math.max(lmp.getMergeFactor(), 5));\n\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      writer.addDocument(doc);\n      writer.addDocument(doc);\n      doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n      doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n      try {\n        writer.addDocument(doc);\n        fail(\"did not hit expected exception\");\n      } catch (IOException ioe) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: hit expected exception\");\n          ioe.printStackTrace(System.out);\n        }\n      }\n\n      if (0 == i) {\n        doc = new Document();\n        doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n        writer.addDocument(doc);\n        writer.addDocument(doc);\n      }\n      writer.close();\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: open reader\");\n      }\n      IndexReader reader = DirectoryReader.open(dir);\n      if (i == 0) { \n        int expected = 5;\n        assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n        assertEquals(expected, reader.maxDoc());\n        int numDel = 0;\n        final Bits liveDocs = MultiFields.getLiveDocs(reader);\n        assertNotNull(liveDocs);\n        for(int j=0;j<reader.maxDoc();j++) {\n          if (!liveDocs.get(j))\n            numDel++;\n          else {\n            reader.document(j);\n            reader.getTermVectors(j);\n          }\n        }\n        assertEquals(1, numDel);\n      }\n      reader.close();\n\n      writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                      .setMaxBufferedDocs(10));\n      doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = DirectoryReader.open(dir);\n      int expected = 19+(1-i)*2;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n      assertEquals(0, numDel);\n\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"0a16b40feb4e6e0d55c1716733bde48296bedd20":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"45a621dd071a902e1fd30367200d7bbbea037706":["0a16b40feb4e6e0d55c1716733bde48296bedd20"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["38061899d760e06a12fe186bc1f09ca9ff0e64a6"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":["d19974432be9aed28ee7dca73bdf01d139e763a9","38061899d760e06a12fe186bc1f09ca9ff0e64a6"],"38061899d760e06a12fe186bc1f09ca9ff0e64a6":["d19974432be9aed28ee7dca73bdf01d139e763a9"],"aba371508186796cc6151d8223a5b4e16d02e26e":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f","d19974432be9aed28ee7dca73bdf01d139e763a9"],"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"d19974432be9aed28ee7dca73bdf01d139e763a9":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"04e775de416dd2d8067b10db1c8af975a1d5017e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"],"4b51f65902cc2d20ddeb7a5b949aaddf990f31a7":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f","d19974432be9aed28ee7dca73bdf01d139e763a9"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["04e775de416dd2d8067b10db1c8af975a1d5017e"]},"commit2Childs":{"0a16b40feb4e6e0d55c1716733bde48296bedd20":["45a621dd071a902e1fd30367200d7bbbea037706"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"45a621dd071a902e1fd30367200d7bbbea037706":[],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":[],"38061899d760e06a12fe186bc1f09ca9ff0e64a6":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","3dffec77fb8f7d0e9ca4869dddd6af94528b4576"],"aba371508186796cc6151d8223a5b4e16d02e26e":[],"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["aba371508186796cc6151d8223a5b4e16d02e26e","d19974432be9aed28ee7dca73bdf01d139e763a9","4b51f65902cc2d20ddeb7a5b949aaddf990f31a7"],"d19974432be9aed28ee7dca73bdf01d139e763a9":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576","38061899d760e06a12fe186bc1f09ca9ff0e64a6","aba371508186796cc6151d8223a5b4e16d02e26e","4b51f65902cc2d20ddeb7a5b949aaddf990f31a7"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["04e775de416dd2d8067b10db1c8af975a1d5017e"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["0a16b40feb4e6e0d55c1716733bde48296bedd20","54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"04e775de416dd2d8067b10db1c8af975a1d5017e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"4b51f65902cc2d20ddeb7a5b949aaddf990f31a7":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["45a621dd071a902e1fd30367200d7bbbea037706","3dffec77fb8f7d0e9ca4869dddd6af94528b4576","aba371508186796cc6151d8223a5b4e16d02e26e","4b51f65902cc2d20ddeb7a5b949aaddf990f31a7","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}