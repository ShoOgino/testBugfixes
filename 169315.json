{"path":"lucene/core/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"lucene/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new TextField(\"preanalyzed\", stream));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getLiveDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"),\n                                                                          false);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"),\n                                                     false);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"),\n                                                     false);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n    dir.close();\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new TextField(\"preanalyzed\", stream));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getLiveDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"),\n                                                                          false);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"),\n                                                     false);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"),\n                                                     false);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f08557cdb6c60ac7b88a9342c983a20cd236e74f","date":1330954480,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new TextField(\"preanalyzed\", stream));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getLiveDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"),\n                                                                          false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"),\n                                                     false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"),\n                                                     false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n    dir.close();\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new TextField(\"preanalyzed\", stream));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getLiveDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"),\n                                                                          false);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"),\n                                                     false);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"),\n                                                     false);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","date":1331075828,"type":3,"author":"Ryan McKinley","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new TextField(\"preanalyzed\", stream));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getLiveDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"),\n                                                                          false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"),\n                                                     false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"),\n                                                     false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n    dir.close();\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new TextField(\"preanalyzed\", stream));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getLiveDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"),\n                                                                          false);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"),\n                                                     false);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"),\n                                                     false);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new TextField(\"preanalyzed\", stream));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getLiveDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"),\n                                                                          false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"),\n                                                     false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"),\n                                                     false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n    dir.close();\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new TextField(\"preanalyzed\", stream));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getLiveDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"),\n                                                                          false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"),\n                                                     false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"),\n                                                     false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4d3e8520fd031bab31fd0e4d480e55958bc45efe","date":1340901565,"type":3,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new TextField(\"preanalyzed\", stream));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getLiveDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"),\n                                                                          false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"),\n                                                     false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"),\n                                                     false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n    dir.close();\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new TextField(\"preanalyzed\", stream));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getLiveDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"),\n                                                                          false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"),\n                                                     false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"),\n                                                     false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n    dir.close();\n  }\n\n","bugFix":["74a5e7f20b4a444da9df3b2c0f331fa7a1f64223"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe33227f6805edab2036cbb80645cc4e2d1fa424","date":1342713534,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new TextField(\"preanalyzed\", stream));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getLiveDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"),\n                                                                          false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"),\n                                                     false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"),\n                                                     false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n    dir.close();\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new TextField(\"preanalyzed\", stream));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getLiveDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"),\n                                                                          false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"),\n                                                     false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"),\n                                                     false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"322360ac5185a8446d3e0b530b2068bef67cd3d5","date":1343669494,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new TextField(\"preanalyzed\", stream));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getLiveDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n    dir.close();\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new TextField(\"preanalyzed\", stream));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getLiveDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"),\n                                                                          false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"),\n                                                     false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"),\n                                                     false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","date":1343768312,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new TextField(\"preanalyzed\", stream));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getLiveDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n    dir.close();\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new TextField(\"preanalyzed\", stream));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getLiveDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"),\n                                                                          false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"),\n                                                     false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"),\n                                                     false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d6f074e73200c07d54f242d3880a8da5a35ff97b","date":1344507653,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new TextField(\"preanalyzed\", stream));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getLiveDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n    dir.close();\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new TextField(\"preanalyzed\", stream));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getLiveDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"),\n                                                                          false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"),\n                                                     false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"),\n                                                     false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new TextField(\"preanalyzed\", stream));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getLiveDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.shutdown();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n    dir.close();\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new TextField(\"preanalyzed\", stream));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getLiveDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new TextField(\"preanalyzed\", stream));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getLiveDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n    dir.close();\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new TextField(\"preanalyzed\", stream));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getLiveDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.shutdown();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"55244759f906151d96839f8451dee793acb06e75","date":1418999882,"type":3,"author":"David Wayne Smiley","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    AtomicInteger resetCount = new AtomicInteger(0);\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n\n      @Override\n      public void reset() throws IOException {\n        super.reset();\n        resetCount.incrementAndGet();\n      }\n\n      @Override\n      public boolean incrementToken() {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n\n    stream = new CachingTokenFilter(stream);\n\n    doc.add(new TextField(\"preanalyzed\", stream));\n\n    // 1) we consume all tokens twice before we add the doc to the index\n    assertFalse(((CachingTokenFilter)stream).isCached());\n    stream.reset();\n    assertFalse(((CachingTokenFilter) stream).isCached());\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    assertTrue(((CachingTokenFilter)stream).isCached());\n\n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getLiveDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n\n    assertEquals(1, resetCount.get());\n\n    dir.close();\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new TextField(\"preanalyzed\", stream));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getLiveDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"51f5280f31484820499077f41fcdfe92d527d9dc","date":1423229122,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    AtomicInteger resetCount = new AtomicInteger(0);\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n\n      @Override\n      public void reset() throws IOException {\n        super.reset();\n        resetCount.incrementAndGet();\n      }\n\n      @Override\n      public boolean incrementToken() {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n\n    stream = new CachingTokenFilter(stream);\n\n    doc.add(new TextField(\"preanalyzed\", stream));\n\n    // 1) we consume all tokens twice before we add the doc to the index\n    assertFalse(((CachingTokenFilter)stream).isCached());\n    stream.reset();\n    assertFalse(((CachingTokenFilter) stream).isCached());\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    assertTrue(((CachingTokenFilter)stream).isCached());\n\n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    PostingsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getLiveDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n\n    assertEquals(1, resetCount.get());\n\n    dir.close();\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    AtomicInteger resetCount = new AtomicInteger(0);\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n\n      @Override\n      public void reset() throws IOException {\n        super.reset();\n        resetCount.incrementAndGet();\n      }\n\n      @Override\n      public boolean incrementToken() {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n\n    stream = new CachingTokenFilter(stream);\n\n    doc.add(new TextField(\"preanalyzed\", stream));\n\n    // 1) we consume all tokens twice before we add the doc to the index\n    assertFalse(((CachingTokenFilter)stream).isCached());\n    stream.reset();\n    assertFalse(((CachingTokenFilter) stream).isCached());\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    assertTrue(((CachingTokenFilter)stream).isCached());\n\n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getLiveDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n\n    assertEquals(1, resetCount.get());\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0f4464508ee83288c8c4585b533f9faaa93aa314","date":1435240759,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    AtomicInteger resetCount = new AtomicInteger(0);\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n\n      @Override\n      public void reset() throws IOException {\n        super.reset();\n        resetCount.incrementAndGet();\n      }\n\n      @Override\n      public boolean incrementToken() {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n\n    stream = new CachingTokenFilter(stream);\n\n    doc.add(new TextField(\"preanalyzed\", stream));\n\n    // 1) we consume all tokens twice before we add the doc to the index\n    assertFalse(((CachingTokenFilter)stream).isCached());\n    stream.reset();\n    assertFalse(((CachingTokenFilter) stream).isCached());\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    assertTrue(((CachingTokenFilter)stream).isCached());\n\n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    PostingsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n\n    assertEquals(1, resetCount.get());\n\n    dir.close();\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    AtomicInteger resetCount = new AtomicInteger(0);\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n\n      @Override\n      public void reset() throws IOException {\n        super.reset();\n        resetCount.incrementAndGet();\n      }\n\n      @Override\n      public boolean incrementToken() {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n\n    stream = new CachingTokenFilter(stream);\n\n    doc.add(new TextField(\"preanalyzed\", stream));\n\n    // 1) we consume all tokens twice before we add the doc to the index\n    assertFalse(((CachingTokenFilter)stream).isCached());\n    stream.reset();\n    assertFalse(((CachingTokenFilter) stream).isCached());\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    assertTrue(((CachingTokenFilter)stream).isCached());\n\n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    PostingsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getLiveDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n\n    assertEquals(1, resetCount.get());\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"04e775de416dd2d8067b10db1c8af975a1d5017e","date":1539906554,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    AtomicInteger resetCount = new AtomicInteger(0);\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n\n      @Override\n      public void reset() throws IOException {\n        super.reset();\n        resetCount.incrementAndGet();\n      }\n\n      @Override\n      public boolean incrementToken() {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n\n    stream = new CachingTokenFilter(stream);\n\n    doc.add(new TextField(\"preanalyzed\", stream));\n\n    // 1) we consume all tokens twice before we add the doc to the index\n    assertFalse(((CachingTokenFilter)stream).isCached());\n    stream.reset();\n    assertFalse(((CachingTokenFilter) stream).isCached());\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    assertTrue(((CachingTokenFilter)stream).isCached());\n\n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    PostingsEnum termPositions = MultiTerms.getTermPostingsEnum(reader,\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiTerms.getTermPostingsEnum(reader,\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiTerms.getTermPostingsEnum(reader,\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n\n    assertEquals(1, resetCount.get());\n\n    dir.close();\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    AtomicInteger resetCount = new AtomicInteger(0);\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n\n      @Override\n      public void reset() throws IOException {\n        super.reset();\n        resetCount.incrementAndGet();\n      }\n\n      @Override\n      public boolean incrementToken() {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n\n    stream = new CachingTokenFilter(stream);\n\n    doc.add(new TextField(\"preanalyzed\", stream));\n\n    // 1) we consume all tokens twice before we add the doc to the index\n    assertFalse(((CachingTokenFilter)stream).isCached());\n    stream.reset();\n    assertFalse(((CachingTokenFilter) stream).isCached());\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    assertTrue(((CachingTokenFilter)stream).isCached());\n\n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    PostingsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n\n    assertEquals(1, resetCount.get());\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab":["3a119bbc8703c10faa329ec201c654b3a35a1e3e","f08557cdb6c60ac7b88a9342c983a20cd236e74f"],"f08557cdb6c60ac7b88a9342c983a20cd236e74f":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"51f5280f31484820499077f41fcdfe92d527d9dc":["55244759f906151d96839f8451dee793acb06e75"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["51f5280f31484820499077f41fcdfe92d527d9dc"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":["629c38c4ae4e303d0617e05fbfe508140b32f0a3","4d3e8520fd031bab31fd0e4d480e55958bc45efe"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":["fe33227f6805edab2036cbb80645cc4e2d1fa424","322360ac5185a8446d3e0b530b2068bef67cd3d5"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["4d3e8520fd031bab31fd0e4d480e55958bc45efe","322360ac5185a8446d3e0b530b2068bef67cd3d5"],"04e775de416dd2d8067b10db1c8af975a1d5017e":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["322360ac5185a8446d3e0b530b2068bef67cd3d5"],"4d3e8520fd031bab31fd0e4d480e55958bc45efe":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["f08557cdb6c60ac7b88a9342c983a20cd236e74f"],"55244759f906151d96839f8451dee793acb06e75":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"322360ac5185a8446d3e0b530b2068bef67cd3d5":["4d3e8520fd031bab31fd0e4d480e55958bc45efe"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["04e775de416dd2d8067b10db1c8af975a1d5017e"]},"commit2Childs":{"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab":[],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","f08557cdb6c60ac7b88a9342c983a20cd236e74f"],"f08557cdb6c60ac7b88a9342c983a20cd236e74f":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"51f5280f31484820499077f41fcdfe92d527d9dc":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["04e775de416dd2d8067b10db1c8af975a1d5017e"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":["b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["55244759f906151d96839f8451dee793acb06e75"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":[],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"04e775de416dd2d8067b10db1c8af975a1d5017e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["fe33227f6805edab2036cbb80645cc4e2d1fa424","4d3e8520fd031bab31fd0e4d480e55958bc45efe"],"4d3e8520fd031bab31fd0e4d480e55958bc45efe":["fe33227f6805edab2036cbb80645cc4e2d1fa424","d6f074e73200c07d54f242d3880a8da5a35ff97b","322360ac5185a8446d3e0b530b2068bef67cd3d5"],"55244759f906151d96839f8451dee793acb06e75":["51f5280f31484820499077f41fcdfe92d527d9dc"],"322360ac5185a8446d3e0b530b2068bef67cd3d5":["b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b","ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}