{"path":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#init(Document,int).mjava","commits":[{"id":"4350b17bd363cd13a95171b8df1ca62ea4c3e71c","date":1183562198,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#init(Document,int).mjava","pathOld":"/dev/null","sourceNew":"    /** Initializes shared state for this new document */\n    void init(Document doc, int docID) throws IOException {\n\n      this.docID = docID;\n      docBoost = doc.getBoost();\n      numStoredFields = 0;\n      numFieldData = 0;\n      numVectorFields = 0;\n\n      List docFields = doc.getFields();\n      final int numDocFields = docFields.size();\n      boolean docHasVectors = false;\n\n      // Absorb any new fields first seen in this document.\n      // Also absorb any changes to fields we had already\n      // seen before (eg suddenly turning on norms or\n      // vectors, etc.):\n\n      for(int i=0;i<numDocFields;i++) {\n        Fieldable field = (Fieldable) docFields.get(i);\n\n        FieldInfo fi = fieldInfos.add(field.name(), field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false);\n        numStoredFields += field.isStored() ? 1:0;\n        if (fi.isIndexed && !fi.omitNorms) {\n          // Maybe grow our buffered norms\n          if (norms.length <= fi.number) {\n            int newSize = (int) ((1+fi.number)*1.25);\n            BufferedNorms[] newNorms = new BufferedNorms[newSize];\n            System.arraycopy(norms, 0, newNorms, 0, norms.length);\n            norms = newNorms;\n          }\n          \n          if (norms[fi.number] == null)\n            norms[fi.number] = new BufferedNorms();\n\n          hasNorms = true;\n        }\n\n        // Make sure we have a FieldData allocated\n        int hashPos = fi.name.hashCode() & fieldDataHashMask;\n        FieldData fp = fieldDataHash[hashPos];\n        while(fp != null && !fp.fieldInfo.name.equals(fi.name))\n          fp = fp.next;\n\n        if (fp == null) {\n\n          fp = new FieldData(fi);\n          fp.next = fieldDataHash[hashPos];\n          fieldDataHash[hashPos] = fp;\n\n          if (numAllFieldData == allFieldDataArray.length) {\n            int newSize = (int) (allFieldDataArray.length*1.5);\n\n            FieldData newArray[] = new FieldData[newSize];\n            System.arraycopy(allFieldDataArray, 0, newArray, 0, numAllFieldData);\n            allFieldDataArray = newArray;\n\n            // Rehash\n            newSize = fieldDataHash.length*2;\n            newArray = new FieldData[newSize];\n            fieldDataHashMask = newSize-1;\n            for(int j=0;j<fieldDataHash.length;j++) {\n              FieldData fp0 = fieldDataHash[j];\n              while(fp0 != null) {\n                hashPos = fp0.fieldInfo.name.hashCode() & fieldDataHashMask;\n                FieldData nextFP0 = fp0.next;\n                fp0.next = newArray[hashPos];\n                newArray[hashPos] = fp0;\n                fp0 = nextFP0;\n              }\n            }\n            fieldDataHash = newArray;\n          }\n          allFieldDataArray[numAllFieldData++] = fp;\n        } else {\n          assert fp.fieldInfo == fi;\n        }\n\n        if (docID != fp.lastDocID) {\n\n          // First time we're seeing this field for this doc\n          fp.lastDocID = docID;\n          fp.fieldCount = 0;\n          fp.doVectors = fp.doVectorPositions = fp.doVectorOffsets = false;\n          fp.doNorms = fi.isIndexed && !fi.omitNorms;\n\n          if (numFieldData == fieldDataArray.length) {\n            int newSize = fieldDataArray.length*2;\n            FieldData newArray[] = new FieldData[newSize];\n            System.arraycopy(fieldDataArray, 0, newArray, 0, numFieldData);\n            fieldDataArray = newArray;\n\n          }\n          fieldDataArray[numFieldData++] = fp;\n        }\n\n        if (field.isTermVectorStored()) {\n          if (!fp.doVectors) {\n            if (numVectorFields++ == vectorFieldPointers.length) {\n              final int newSize = (int) (numVectorFields*1.5);\n              vectorFieldPointers = new long[newSize];\n              vectorFieldNumbers = new int[newSize];\n            }\n          }\n          fp.doVectors = true;\n          docHasVectors = true;\n\n          fp.doVectorPositions |= field.isStorePositionWithTermVector();\n          fp.doVectorOffsets |= field.isStoreOffsetWithTermVector();\n        }\n\n        if (fp.fieldCount == fp.docFields.length) {\n          Fieldable[] newArray = new Fieldable[fp.docFields.length*2];\n          System.arraycopy(fp.docFields, 0, newArray, 0, fp.docFields.length);\n          fp.docFields = newArray;\n        }\n\n        // Lazily allocate arrays for postings:\n        if (field.isIndexed() && fp.postingsHash == null)\n          fp.initPostingArrays();\n\n        fp.docFields[fp.fieldCount++] = field;\n      }\n\n      final int numFields = fieldInfos.size();\n\n      // Maybe init the local & global fieldsWriter\n      if (localFieldsWriter == null) {\n        if (fieldsWriter == null) {\n          assert docStoreSegment == null;\n          assert segment != null;\n          docStoreSegment = segment;\n          fieldsWriter = new FieldsWriter(directory, docStoreSegment, fieldInfos);\n          files = null;\n        }\n        localFieldsWriter = new FieldsWriter(null, fdtLocal, fieldInfos);\n      }\n\n      // First time we see a doc that has field(s) with\n      // stored vectors, we init our tvx writer\n      if (docHasVectors) {\n        if (tvx == null) {\n          assert docStoreSegment != null;\n          tvx = directory.createOutput(docStoreSegment + TermVectorsWriter.TVX_EXTENSION);\n          tvx.writeInt(TermVectorsWriter.FORMAT_VERSION);\n          tvd = directory.createOutput(docStoreSegment +  TermVectorsWriter.TVD_EXTENSION);\n          tvd.writeInt(TermVectorsWriter.FORMAT_VERSION);\n          tvf = directory.createOutput(docStoreSegment +  TermVectorsWriter.TVF_EXTENSION);\n          tvf.writeInt(TermVectorsWriter.FORMAT_VERSION);\n          files = null;\n\n          // We must \"catch up\" for all docIDs that had no\n          // vectors before this one\n          for(int i=0;i<docID;i++)\n            tvx.writeLong(0);\n        }\n\n        numVectorFields = 0;\n      }\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["c9a0deca56efc5191d6b3c41047fd538f3fae1d8","83bbb041887bbef07b8a98d08a0e1713ce137039","11764865fb318bf86302eab36bdf9cd00c50c110","5a251aa47d1808cbae42c0e172d698c377430e60"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"11764865fb318bf86302eab36bdf9cd00c50c110","date":1190109214,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#init(Document,int).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#init(Document,int).mjava","sourceNew":"    /** Initializes shared state for this new document */\n    void init(Document doc, int docID) throws IOException {\n\n      this.docID = docID;\n      docBoost = doc.getBoost();\n      numStoredFields = 0;\n      numFieldData = 0;\n      numVectorFields = 0;\n\n      List docFields = doc.getFields();\n      final int numDocFields = docFields.size();\n      boolean docHasVectors = false;\n\n      // Absorb any new fields first seen in this document.\n      // Also absorb any changes to fields we had already\n      // seen before (eg suddenly turning on norms or\n      // vectors, etc.):\n\n      for(int i=0;i<numDocFields;i++) {\n        Fieldable field = (Fieldable) docFields.get(i);\n\n        FieldInfo fi = fieldInfos.add(field.name(), field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false);\n        numStoredFields += field.isStored() ? 1:0;\n        if (fi.isIndexed && !fi.omitNorms) {\n          // Maybe grow our buffered norms\n          if (norms.length <= fi.number) {\n            int newSize = (int) ((1+fi.number)*1.25);\n            BufferedNorms[] newNorms = new BufferedNorms[newSize];\n            System.arraycopy(norms, 0, newNorms, 0, norms.length);\n            norms = newNorms;\n          }\n          \n          if (norms[fi.number] == null)\n            norms[fi.number] = new BufferedNorms();\n\n          hasNorms = true;\n        }\n\n        // Make sure we have a FieldData allocated\n        int hashPos = fi.name.hashCode() & fieldDataHashMask;\n        FieldData fp = fieldDataHash[hashPos];\n        while(fp != null && !fp.fieldInfo.name.equals(fi.name))\n          fp = fp.next;\n\n        if (fp == null) {\n\n          fp = new FieldData(fi);\n          fp.next = fieldDataHash[hashPos];\n          fieldDataHash[hashPos] = fp;\n\n          if (numAllFieldData == allFieldDataArray.length) {\n            int newSize = (int) (allFieldDataArray.length*1.5);\n\n            FieldData newArray[] = new FieldData[newSize];\n            System.arraycopy(allFieldDataArray, 0, newArray, 0, numAllFieldData);\n            allFieldDataArray = newArray;\n\n            // Rehash\n            newSize = fieldDataHash.length*2;\n            newArray = new FieldData[newSize];\n            fieldDataHashMask = newSize-1;\n            for(int j=0;j<fieldDataHash.length;j++) {\n              FieldData fp0 = fieldDataHash[j];\n              while(fp0 != null) {\n                hashPos = fp0.fieldInfo.name.hashCode() & fieldDataHashMask;\n                FieldData nextFP0 = fp0.next;\n                fp0.next = newArray[hashPos];\n                newArray[hashPos] = fp0;\n                fp0 = nextFP0;\n              }\n            }\n            fieldDataHash = newArray;\n          }\n          allFieldDataArray[numAllFieldData++] = fp;\n        } else {\n          assert fp.fieldInfo == fi;\n        }\n\n        if (docID != fp.lastDocID) {\n\n          // First time we're seeing this field for this doc\n          fp.lastDocID = docID;\n          fp.fieldCount = 0;\n          fp.doVectors = fp.doVectorPositions = fp.doVectorOffsets = false;\n          fp.doNorms = fi.isIndexed && !fi.omitNorms;\n\n          if (numFieldData == fieldDataArray.length) {\n            int newSize = fieldDataArray.length*2;\n            FieldData newArray[] = new FieldData[newSize];\n            System.arraycopy(fieldDataArray, 0, newArray, 0, numFieldData);\n            fieldDataArray = newArray;\n\n          }\n          fieldDataArray[numFieldData++] = fp;\n        }\n\n        if (field.isTermVectorStored()) {\n          if (!fp.doVectors) {\n            if (numVectorFields++ == vectorFieldPointers.length) {\n              final int newSize = (int) (numVectorFields*1.5);\n              vectorFieldPointers = new long[newSize];\n              vectorFieldNumbers = new int[newSize];\n            }\n          }\n          fp.doVectors = true;\n          docHasVectors = true;\n\n          fp.doVectorPositions |= field.isStorePositionWithTermVector();\n          fp.doVectorOffsets |= field.isStoreOffsetWithTermVector();\n        }\n\n        if (fp.fieldCount == fp.docFields.length) {\n          Fieldable[] newArray = new Fieldable[fp.docFields.length*2];\n          System.arraycopy(fp.docFields, 0, newArray, 0, fp.docFields.length);\n          fp.docFields = newArray;\n        }\n\n        // Lazily allocate arrays for postings:\n        if (field.isIndexed() && fp.postingsHash == null)\n          fp.initPostingArrays();\n\n        fp.docFields[fp.fieldCount++] = field;\n      }\n\n      final int numFields = fieldInfos.size();\n\n      // Maybe init the local & global fieldsWriter\n      if (localFieldsWriter == null) {\n        if (fieldsWriter == null) {\n          assert docStoreSegment == null;\n          assert segment != null;\n          docStoreSegment = segment;\n          fieldsWriter = new FieldsWriter(directory, docStoreSegment, fieldInfos);\n          files = null;\n        }\n        localFieldsWriter = new FieldsWriter(null, fdtLocal, fieldInfos);\n      }\n\n      // First time we see a doc that has field(s) with\n      // stored vectors, we init our tvx writer\n      if (docHasVectors) {\n        if (tvx == null) {\n          assert docStoreSegment != null;\n          tvx = directory.createOutput(docStoreSegment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION);\n          tvx.writeInt(TermVectorsReader.FORMAT_VERSION);\n          tvd = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);\n          tvd.writeInt(TermVectorsReader.FORMAT_VERSION);\n          tvf = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_FIELDS_EXTENSION);\n          tvf.writeInt(TermVectorsReader.FORMAT_VERSION);\n          files = null;\n\n          // We must \"catch up\" for all docIDs that had no\n          // vectors before this one\n          for(int i=0;i<docID;i++)\n            tvx.writeLong(0);\n        }\n\n        numVectorFields = 0;\n      }\n    }\n\n","sourceOld":"    /** Initializes shared state for this new document */\n    void init(Document doc, int docID) throws IOException {\n\n      this.docID = docID;\n      docBoost = doc.getBoost();\n      numStoredFields = 0;\n      numFieldData = 0;\n      numVectorFields = 0;\n\n      List docFields = doc.getFields();\n      final int numDocFields = docFields.size();\n      boolean docHasVectors = false;\n\n      // Absorb any new fields first seen in this document.\n      // Also absorb any changes to fields we had already\n      // seen before (eg suddenly turning on norms or\n      // vectors, etc.):\n\n      for(int i=0;i<numDocFields;i++) {\n        Fieldable field = (Fieldable) docFields.get(i);\n\n        FieldInfo fi = fieldInfos.add(field.name(), field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false);\n        numStoredFields += field.isStored() ? 1:0;\n        if (fi.isIndexed && !fi.omitNorms) {\n          // Maybe grow our buffered norms\n          if (norms.length <= fi.number) {\n            int newSize = (int) ((1+fi.number)*1.25);\n            BufferedNorms[] newNorms = new BufferedNorms[newSize];\n            System.arraycopy(norms, 0, newNorms, 0, norms.length);\n            norms = newNorms;\n          }\n          \n          if (norms[fi.number] == null)\n            norms[fi.number] = new BufferedNorms();\n\n          hasNorms = true;\n        }\n\n        // Make sure we have a FieldData allocated\n        int hashPos = fi.name.hashCode() & fieldDataHashMask;\n        FieldData fp = fieldDataHash[hashPos];\n        while(fp != null && !fp.fieldInfo.name.equals(fi.name))\n          fp = fp.next;\n\n        if (fp == null) {\n\n          fp = new FieldData(fi);\n          fp.next = fieldDataHash[hashPos];\n          fieldDataHash[hashPos] = fp;\n\n          if (numAllFieldData == allFieldDataArray.length) {\n            int newSize = (int) (allFieldDataArray.length*1.5);\n\n            FieldData newArray[] = new FieldData[newSize];\n            System.arraycopy(allFieldDataArray, 0, newArray, 0, numAllFieldData);\n            allFieldDataArray = newArray;\n\n            // Rehash\n            newSize = fieldDataHash.length*2;\n            newArray = new FieldData[newSize];\n            fieldDataHashMask = newSize-1;\n            for(int j=0;j<fieldDataHash.length;j++) {\n              FieldData fp0 = fieldDataHash[j];\n              while(fp0 != null) {\n                hashPos = fp0.fieldInfo.name.hashCode() & fieldDataHashMask;\n                FieldData nextFP0 = fp0.next;\n                fp0.next = newArray[hashPos];\n                newArray[hashPos] = fp0;\n                fp0 = nextFP0;\n              }\n            }\n            fieldDataHash = newArray;\n          }\n          allFieldDataArray[numAllFieldData++] = fp;\n        } else {\n          assert fp.fieldInfo == fi;\n        }\n\n        if (docID != fp.lastDocID) {\n\n          // First time we're seeing this field for this doc\n          fp.lastDocID = docID;\n          fp.fieldCount = 0;\n          fp.doVectors = fp.doVectorPositions = fp.doVectorOffsets = false;\n          fp.doNorms = fi.isIndexed && !fi.omitNorms;\n\n          if (numFieldData == fieldDataArray.length) {\n            int newSize = fieldDataArray.length*2;\n            FieldData newArray[] = new FieldData[newSize];\n            System.arraycopy(fieldDataArray, 0, newArray, 0, numFieldData);\n            fieldDataArray = newArray;\n\n          }\n          fieldDataArray[numFieldData++] = fp;\n        }\n\n        if (field.isTermVectorStored()) {\n          if (!fp.doVectors) {\n            if (numVectorFields++ == vectorFieldPointers.length) {\n              final int newSize = (int) (numVectorFields*1.5);\n              vectorFieldPointers = new long[newSize];\n              vectorFieldNumbers = new int[newSize];\n            }\n          }\n          fp.doVectors = true;\n          docHasVectors = true;\n\n          fp.doVectorPositions |= field.isStorePositionWithTermVector();\n          fp.doVectorOffsets |= field.isStoreOffsetWithTermVector();\n        }\n\n        if (fp.fieldCount == fp.docFields.length) {\n          Fieldable[] newArray = new Fieldable[fp.docFields.length*2];\n          System.arraycopy(fp.docFields, 0, newArray, 0, fp.docFields.length);\n          fp.docFields = newArray;\n        }\n\n        // Lazily allocate arrays for postings:\n        if (field.isIndexed() && fp.postingsHash == null)\n          fp.initPostingArrays();\n\n        fp.docFields[fp.fieldCount++] = field;\n      }\n\n      final int numFields = fieldInfos.size();\n\n      // Maybe init the local & global fieldsWriter\n      if (localFieldsWriter == null) {\n        if (fieldsWriter == null) {\n          assert docStoreSegment == null;\n          assert segment != null;\n          docStoreSegment = segment;\n          fieldsWriter = new FieldsWriter(directory, docStoreSegment, fieldInfos);\n          files = null;\n        }\n        localFieldsWriter = new FieldsWriter(null, fdtLocal, fieldInfos);\n      }\n\n      // First time we see a doc that has field(s) with\n      // stored vectors, we init our tvx writer\n      if (docHasVectors) {\n        if (tvx == null) {\n          assert docStoreSegment != null;\n          tvx = directory.createOutput(docStoreSegment + TermVectorsWriter.TVX_EXTENSION);\n          tvx.writeInt(TermVectorsWriter.FORMAT_VERSION);\n          tvd = directory.createOutput(docStoreSegment +  TermVectorsWriter.TVD_EXTENSION);\n          tvd.writeInt(TermVectorsWriter.FORMAT_VERSION);\n          tvf = directory.createOutput(docStoreSegment +  TermVectorsWriter.TVF_EXTENSION);\n          tvf.writeInt(TermVectorsWriter.FORMAT_VERSION);\n          files = null;\n\n          // We must \"catch up\" for all docIDs that had no\n          // vectors before this one\n          for(int i=0;i<docID;i++)\n            tvx.writeLong(0);\n        }\n\n        numVectorFields = 0;\n      }\n    }\n\n","bugFix":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"bugIntro":["83bbb041887bbef07b8a98d08a0e1713ce137039"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b6a1f29c9b1051488fd5fa7d56c98db5f4388408","date":1196281221,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#init(Document,int).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#init(Document,int).mjava","sourceNew":"    /** Initializes shared state for this new document */\n    void init(Document doc, int docID) throws IOException {\n\n      this.docID = docID;\n      docBoost = doc.getBoost();\n      numStoredFields = 0;\n      numFieldData = 0;\n      numVectorFields = 0;\n\n      List docFields = doc.getFields();\n      final int numDocFields = docFields.size();\n      boolean docHasVectors = false;\n\n      // Absorb any new fields first seen in this document.\n      // Also absorb any changes to fields we had already\n      // seen before (eg suddenly turning on norms or\n      // vectors, etc.):\n\n      for(int i=0;i<numDocFields;i++) {\n        Fieldable field = (Fieldable) docFields.get(i);\n\n        FieldInfo fi = fieldInfos.add(field.name(), field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false);\n        numStoredFields += field.isStored() ? 1:0;\n        if (fi.isIndexed && !fi.omitNorms) {\n          // Maybe grow our buffered norms\n          if (norms.length <= fi.number) {\n            int newSize = (int) ((1+fi.number)*1.25);\n            BufferedNorms[] newNorms = new BufferedNorms[newSize];\n            System.arraycopy(norms, 0, newNorms, 0, norms.length);\n            norms = newNorms;\n          }\n          \n          if (norms[fi.number] == null)\n            norms[fi.number] = new BufferedNorms();\n\n          hasNorms = true;\n        }\n\n        // Make sure we have a FieldData allocated\n        int hashPos = fi.name.hashCode() & fieldDataHashMask;\n        FieldData fp = fieldDataHash[hashPos];\n        while(fp != null && !fp.fieldInfo.name.equals(fi.name))\n          fp = fp.next;\n\n        if (fp == null) {\n\n          fp = new FieldData(fi);\n          fp.next = fieldDataHash[hashPos];\n          fieldDataHash[hashPos] = fp;\n\n          if (numAllFieldData == allFieldDataArray.length) {\n            int newSize = (int) (allFieldDataArray.length*1.5);\n\n            FieldData newArray[] = new FieldData[newSize];\n            System.arraycopy(allFieldDataArray, 0, newArray, 0, numAllFieldData);\n            allFieldDataArray = newArray;\n\n            // Rehash\n            newSize = fieldDataHash.length*2;\n            newArray = new FieldData[newSize];\n            fieldDataHashMask = newSize-1;\n            for(int j=0;j<fieldDataHash.length;j++) {\n              FieldData fp0 = fieldDataHash[j];\n              while(fp0 != null) {\n                hashPos = fp0.fieldInfo.name.hashCode() & fieldDataHashMask;\n                FieldData nextFP0 = fp0.next;\n                fp0.next = newArray[hashPos];\n                newArray[hashPos] = fp0;\n                fp0 = nextFP0;\n              }\n            }\n            fieldDataHash = newArray;\n          }\n          allFieldDataArray[numAllFieldData++] = fp;\n        } else {\n          assert fp.fieldInfo == fi;\n        }\n\n        if (docID != fp.lastDocID) {\n\n          // First time we're seeing this field for this doc\n          fp.lastDocID = docID;\n          fp.fieldCount = 0;\n          fp.doVectors = fp.doVectorPositions = fp.doVectorOffsets = false;\n          fp.doNorms = fi.isIndexed && !fi.omitNorms;\n\n          if (numFieldData == fieldDataArray.length) {\n            int newSize = fieldDataArray.length*2;\n            FieldData newArray[] = new FieldData[newSize];\n            System.arraycopy(fieldDataArray, 0, newArray, 0, numFieldData);\n            fieldDataArray = newArray;\n\n          }\n          fieldDataArray[numFieldData++] = fp;\n        }\n\n        if (field.isTermVectorStored()) {\n          if (!fp.doVectors) {\n            if (numVectorFields++ == vectorFieldPointers.length) {\n              final int newSize = (int) (numVectorFields*1.5);\n              vectorFieldPointers = new long[newSize];\n              vectorFieldNumbers = new int[newSize];\n            }\n          }\n          fp.doVectors = true;\n          docHasVectors = true;\n\n          fp.doVectorPositions |= field.isStorePositionWithTermVector();\n          fp.doVectorOffsets |= field.isStoreOffsetWithTermVector();\n        }\n\n        if (fp.fieldCount == fp.docFields.length) {\n          Fieldable[] newArray = new Fieldable[fp.docFields.length*2];\n          System.arraycopy(fp.docFields, 0, newArray, 0, fp.docFields.length);\n          fp.docFields = newArray;\n        }\n\n        // Lazily allocate arrays for postings:\n        if (field.isIndexed() && fp.postingsHash == null)\n          fp.initPostingArrays();\n\n        fp.docFields[fp.fieldCount++] = field;\n      }\n\n      // Maybe init the local & global fieldsWriter\n      if (localFieldsWriter == null) {\n        if (fieldsWriter == null) {\n          assert docStoreSegment == null;\n          assert segment != null;\n          docStoreSegment = segment;\n          fieldsWriter = new FieldsWriter(directory, docStoreSegment, fieldInfos);\n          files = null;\n        }\n        localFieldsWriter = new FieldsWriter(null, fdtLocal, fieldInfos);\n      }\n\n      // First time we see a doc that has field(s) with\n      // stored vectors, we init our tvx writer\n      if (docHasVectors) {\n        if (tvx == null) {\n          assert docStoreSegment != null;\n          tvx = directory.createOutput(docStoreSegment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION);\n          tvx.writeInt(TermVectorsReader.FORMAT_VERSION);\n          tvd = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);\n          tvd.writeInt(TermVectorsReader.FORMAT_VERSION);\n          tvf = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_FIELDS_EXTENSION);\n          tvf.writeInt(TermVectorsReader.FORMAT_VERSION);\n          files = null;\n\n          // We must \"catch up\" for all docIDs that had no\n          // vectors before this one\n          for(int i=0;i<docID;i++)\n            tvx.writeLong(0);\n        }\n\n        numVectorFields = 0;\n      }\n    }\n\n","sourceOld":"    /** Initializes shared state for this new document */\n    void init(Document doc, int docID) throws IOException {\n\n      this.docID = docID;\n      docBoost = doc.getBoost();\n      numStoredFields = 0;\n      numFieldData = 0;\n      numVectorFields = 0;\n\n      List docFields = doc.getFields();\n      final int numDocFields = docFields.size();\n      boolean docHasVectors = false;\n\n      // Absorb any new fields first seen in this document.\n      // Also absorb any changes to fields we had already\n      // seen before (eg suddenly turning on norms or\n      // vectors, etc.):\n\n      for(int i=0;i<numDocFields;i++) {\n        Fieldable field = (Fieldable) docFields.get(i);\n\n        FieldInfo fi = fieldInfos.add(field.name(), field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false);\n        numStoredFields += field.isStored() ? 1:0;\n        if (fi.isIndexed && !fi.omitNorms) {\n          // Maybe grow our buffered norms\n          if (norms.length <= fi.number) {\n            int newSize = (int) ((1+fi.number)*1.25);\n            BufferedNorms[] newNorms = new BufferedNorms[newSize];\n            System.arraycopy(norms, 0, newNorms, 0, norms.length);\n            norms = newNorms;\n          }\n          \n          if (norms[fi.number] == null)\n            norms[fi.number] = new BufferedNorms();\n\n          hasNorms = true;\n        }\n\n        // Make sure we have a FieldData allocated\n        int hashPos = fi.name.hashCode() & fieldDataHashMask;\n        FieldData fp = fieldDataHash[hashPos];\n        while(fp != null && !fp.fieldInfo.name.equals(fi.name))\n          fp = fp.next;\n\n        if (fp == null) {\n\n          fp = new FieldData(fi);\n          fp.next = fieldDataHash[hashPos];\n          fieldDataHash[hashPos] = fp;\n\n          if (numAllFieldData == allFieldDataArray.length) {\n            int newSize = (int) (allFieldDataArray.length*1.5);\n\n            FieldData newArray[] = new FieldData[newSize];\n            System.arraycopy(allFieldDataArray, 0, newArray, 0, numAllFieldData);\n            allFieldDataArray = newArray;\n\n            // Rehash\n            newSize = fieldDataHash.length*2;\n            newArray = new FieldData[newSize];\n            fieldDataHashMask = newSize-1;\n            for(int j=0;j<fieldDataHash.length;j++) {\n              FieldData fp0 = fieldDataHash[j];\n              while(fp0 != null) {\n                hashPos = fp0.fieldInfo.name.hashCode() & fieldDataHashMask;\n                FieldData nextFP0 = fp0.next;\n                fp0.next = newArray[hashPos];\n                newArray[hashPos] = fp0;\n                fp0 = nextFP0;\n              }\n            }\n            fieldDataHash = newArray;\n          }\n          allFieldDataArray[numAllFieldData++] = fp;\n        } else {\n          assert fp.fieldInfo == fi;\n        }\n\n        if (docID != fp.lastDocID) {\n\n          // First time we're seeing this field for this doc\n          fp.lastDocID = docID;\n          fp.fieldCount = 0;\n          fp.doVectors = fp.doVectorPositions = fp.doVectorOffsets = false;\n          fp.doNorms = fi.isIndexed && !fi.omitNorms;\n\n          if (numFieldData == fieldDataArray.length) {\n            int newSize = fieldDataArray.length*2;\n            FieldData newArray[] = new FieldData[newSize];\n            System.arraycopy(fieldDataArray, 0, newArray, 0, numFieldData);\n            fieldDataArray = newArray;\n\n          }\n          fieldDataArray[numFieldData++] = fp;\n        }\n\n        if (field.isTermVectorStored()) {\n          if (!fp.doVectors) {\n            if (numVectorFields++ == vectorFieldPointers.length) {\n              final int newSize = (int) (numVectorFields*1.5);\n              vectorFieldPointers = new long[newSize];\n              vectorFieldNumbers = new int[newSize];\n            }\n          }\n          fp.doVectors = true;\n          docHasVectors = true;\n\n          fp.doVectorPositions |= field.isStorePositionWithTermVector();\n          fp.doVectorOffsets |= field.isStoreOffsetWithTermVector();\n        }\n\n        if (fp.fieldCount == fp.docFields.length) {\n          Fieldable[] newArray = new Fieldable[fp.docFields.length*2];\n          System.arraycopy(fp.docFields, 0, newArray, 0, fp.docFields.length);\n          fp.docFields = newArray;\n        }\n\n        // Lazily allocate arrays for postings:\n        if (field.isIndexed() && fp.postingsHash == null)\n          fp.initPostingArrays();\n\n        fp.docFields[fp.fieldCount++] = field;\n      }\n\n      final int numFields = fieldInfos.size();\n\n      // Maybe init the local & global fieldsWriter\n      if (localFieldsWriter == null) {\n        if (fieldsWriter == null) {\n          assert docStoreSegment == null;\n          assert segment != null;\n          docStoreSegment = segment;\n          fieldsWriter = new FieldsWriter(directory, docStoreSegment, fieldInfos);\n          files = null;\n        }\n        localFieldsWriter = new FieldsWriter(null, fdtLocal, fieldInfos);\n      }\n\n      // First time we see a doc that has field(s) with\n      // stored vectors, we init our tvx writer\n      if (docHasVectors) {\n        if (tvx == null) {\n          assert docStoreSegment != null;\n          tvx = directory.createOutput(docStoreSegment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION);\n          tvx.writeInt(TermVectorsReader.FORMAT_VERSION);\n          tvd = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);\n          tvd.writeInt(TermVectorsReader.FORMAT_VERSION);\n          tvf = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_FIELDS_EXTENSION);\n          tvf.writeInt(TermVectorsReader.FORMAT_VERSION);\n          files = null;\n\n          // We must \"catch up\" for all docIDs that had no\n          // vectors before this one\n          for(int i=0;i<docID;i++)\n            tvx.writeLong(0);\n        }\n\n        numVectorFields = 0;\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"741a5cca05cabe1e7482410a29e563a08379251a","date":1196676550,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#init(Document,int).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#init(Document,int).mjava","sourceNew":"    /** Initializes shared state for this new document */\n    void init(Document doc, int docID) throws IOException {\n\n      this.docID = docID;\n      docBoost = doc.getBoost();\n      numStoredFields = 0;\n      numFieldData = 0;\n      numVectorFields = 0;\n      maxTermHit = 0;\n\n      List docFields = doc.getFields();\n      final int numDocFields = docFields.size();\n      boolean docHasVectors = false;\n\n      // Absorb any new fields first seen in this document.\n      // Also absorb any changes to fields we had already\n      // seen before (eg suddenly turning on norms or\n      // vectors, etc.):\n\n      for(int i=0;i<numDocFields;i++) {\n        Fieldable field = (Fieldable) docFields.get(i);\n\n        FieldInfo fi = fieldInfos.add(field.name(), field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false);\n        numStoredFields += field.isStored() ? 1:0;\n        if (fi.isIndexed && !fi.omitNorms) {\n          // Maybe grow our buffered norms\n          if (norms.length <= fi.number) {\n            int newSize = (int) ((1+fi.number)*1.25);\n            BufferedNorms[] newNorms = new BufferedNorms[newSize];\n            System.arraycopy(norms, 0, newNorms, 0, norms.length);\n            norms = newNorms;\n          }\n          \n          if (norms[fi.number] == null)\n            norms[fi.number] = new BufferedNorms();\n\n          hasNorms = true;\n        }\n\n        // Make sure we have a FieldData allocated\n        int hashPos = fi.name.hashCode() & fieldDataHashMask;\n        FieldData fp = fieldDataHash[hashPos];\n        while(fp != null && !fp.fieldInfo.name.equals(fi.name))\n          fp = fp.next;\n\n        if (fp == null) {\n\n          fp = new FieldData(fi);\n          fp.next = fieldDataHash[hashPos];\n          fieldDataHash[hashPos] = fp;\n\n          if (numAllFieldData == allFieldDataArray.length) {\n            int newSize = (int) (allFieldDataArray.length*1.5);\n\n            FieldData newArray[] = new FieldData[newSize];\n            System.arraycopy(allFieldDataArray, 0, newArray, 0, numAllFieldData);\n            allFieldDataArray = newArray;\n\n            // Rehash\n            newSize = fieldDataHash.length*2;\n            newArray = new FieldData[newSize];\n            fieldDataHashMask = newSize-1;\n            for(int j=0;j<fieldDataHash.length;j++) {\n              FieldData fp0 = fieldDataHash[j];\n              while(fp0 != null) {\n                hashPos = fp0.fieldInfo.name.hashCode() & fieldDataHashMask;\n                FieldData nextFP0 = fp0.next;\n                fp0.next = newArray[hashPos];\n                newArray[hashPos] = fp0;\n                fp0 = nextFP0;\n              }\n            }\n            fieldDataHash = newArray;\n          }\n          allFieldDataArray[numAllFieldData++] = fp;\n        } else {\n          assert fp.fieldInfo == fi;\n        }\n\n        if (docID != fp.lastDocID) {\n\n          // First time we're seeing this field for this doc\n          fp.lastDocID = docID;\n          fp.fieldCount = 0;\n          fp.doVectors = fp.doVectorPositions = fp.doVectorOffsets = false;\n          fp.doNorms = fi.isIndexed && !fi.omitNorms;\n\n          if (numFieldData == fieldDataArray.length) {\n            int newSize = fieldDataArray.length*2;\n            FieldData newArray[] = new FieldData[newSize];\n            System.arraycopy(fieldDataArray, 0, newArray, 0, numFieldData);\n            fieldDataArray = newArray;\n\n          }\n          fieldDataArray[numFieldData++] = fp;\n        }\n\n        if (field.isTermVectorStored()) {\n          if (!fp.doVectors) {\n            if (numVectorFields++ == vectorFieldPointers.length) {\n              final int newSize = (int) (numVectorFields*1.5);\n              vectorFieldPointers = new long[newSize];\n              vectorFieldNumbers = new int[newSize];\n            }\n          }\n          fp.doVectors = true;\n          docHasVectors = true;\n\n          fp.doVectorPositions |= field.isStorePositionWithTermVector();\n          fp.doVectorOffsets |= field.isStoreOffsetWithTermVector();\n        }\n\n        if (fp.fieldCount == fp.docFields.length) {\n          Fieldable[] newArray = new Fieldable[fp.docFields.length*2];\n          System.arraycopy(fp.docFields, 0, newArray, 0, fp.docFields.length);\n          fp.docFields = newArray;\n        }\n\n        // Lazily allocate arrays for postings:\n        if (field.isIndexed() && fp.postingsHash == null)\n          fp.initPostingArrays();\n\n        fp.docFields[fp.fieldCount++] = field;\n      }\n\n      // Maybe init the local & global fieldsWriter\n      if (localFieldsWriter == null) {\n        if (fieldsWriter == null) {\n          assert docStoreSegment == null;\n          assert segment != null;\n          docStoreSegment = segment;\n          fieldsWriter = new FieldsWriter(directory, docStoreSegment, fieldInfos);\n          files = null;\n        }\n        localFieldsWriter = new FieldsWriter(null, fdtLocal, fieldInfos);\n      }\n\n      // First time we see a doc that has field(s) with\n      // stored vectors, we init our tvx writer\n      if (docHasVectors) {\n        if (tvx == null) {\n          assert docStoreSegment != null;\n          tvx = directory.createOutput(docStoreSegment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION);\n          tvx.writeInt(TermVectorsReader.FORMAT_VERSION);\n          tvd = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);\n          tvd.writeInt(TermVectorsReader.FORMAT_VERSION);\n          tvf = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_FIELDS_EXTENSION);\n          tvf.writeInt(TermVectorsReader.FORMAT_VERSION);\n          files = null;\n\n          // We must \"catch up\" for all docIDs that had no\n          // vectors before this one\n          for(int i=0;i<docID;i++)\n            tvx.writeLong(0);\n        }\n\n        numVectorFields = 0;\n      }\n    }\n\n","sourceOld":"    /** Initializes shared state for this new document */\n    void init(Document doc, int docID) throws IOException {\n\n      this.docID = docID;\n      docBoost = doc.getBoost();\n      numStoredFields = 0;\n      numFieldData = 0;\n      numVectorFields = 0;\n\n      List docFields = doc.getFields();\n      final int numDocFields = docFields.size();\n      boolean docHasVectors = false;\n\n      // Absorb any new fields first seen in this document.\n      // Also absorb any changes to fields we had already\n      // seen before (eg suddenly turning on norms or\n      // vectors, etc.):\n\n      for(int i=0;i<numDocFields;i++) {\n        Fieldable field = (Fieldable) docFields.get(i);\n\n        FieldInfo fi = fieldInfos.add(field.name(), field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false);\n        numStoredFields += field.isStored() ? 1:0;\n        if (fi.isIndexed && !fi.omitNorms) {\n          // Maybe grow our buffered norms\n          if (norms.length <= fi.number) {\n            int newSize = (int) ((1+fi.number)*1.25);\n            BufferedNorms[] newNorms = new BufferedNorms[newSize];\n            System.arraycopy(norms, 0, newNorms, 0, norms.length);\n            norms = newNorms;\n          }\n          \n          if (norms[fi.number] == null)\n            norms[fi.number] = new BufferedNorms();\n\n          hasNorms = true;\n        }\n\n        // Make sure we have a FieldData allocated\n        int hashPos = fi.name.hashCode() & fieldDataHashMask;\n        FieldData fp = fieldDataHash[hashPos];\n        while(fp != null && !fp.fieldInfo.name.equals(fi.name))\n          fp = fp.next;\n\n        if (fp == null) {\n\n          fp = new FieldData(fi);\n          fp.next = fieldDataHash[hashPos];\n          fieldDataHash[hashPos] = fp;\n\n          if (numAllFieldData == allFieldDataArray.length) {\n            int newSize = (int) (allFieldDataArray.length*1.5);\n\n            FieldData newArray[] = new FieldData[newSize];\n            System.arraycopy(allFieldDataArray, 0, newArray, 0, numAllFieldData);\n            allFieldDataArray = newArray;\n\n            // Rehash\n            newSize = fieldDataHash.length*2;\n            newArray = new FieldData[newSize];\n            fieldDataHashMask = newSize-1;\n            for(int j=0;j<fieldDataHash.length;j++) {\n              FieldData fp0 = fieldDataHash[j];\n              while(fp0 != null) {\n                hashPos = fp0.fieldInfo.name.hashCode() & fieldDataHashMask;\n                FieldData nextFP0 = fp0.next;\n                fp0.next = newArray[hashPos];\n                newArray[hashPos] = fp0;\n                fp0 = nextFP0;\n              }\n            }\n            fieldDataHash = newArray;\n          }\n          allFieldDataArray[numAllFieldData++] = fp;\n        } else {\n          assert fp.fieldInfo == fi;\n        }\n\n        if (docID != fp.lastDocID) {\n\n          // First time we're seeing this field for this doc\n          fp.lastDocID = docID;\n          fp.fieldCount = 0;\n          fp.doVectors = fp.doVectorPositions = fp.doVectorOffsets = false;\n          fp.doNorms = fi.isIndexed && !fi.omitNorms;\n\n          if (numFieldData == fieldDataArray.length) {\n            int newSize = fieldDataArray.length*2;\n            FieldData newArray[] = new FieldData[newSize];\n            System.arraycopy(fieldDataArray, 0, newArray, 0, numFieldData);\n            fieldDataArray = newArray;\n\n          }\n          fieldDataArray[numFieldData++] = fp;\n        }\n\n        if (field.isTermVectorStored()) {\n          if (!fp.doVectors) {\n            if (numVectorFields++ == vectorFieldPointers.length) {\n              final int newSize = (int) (numVectorFields*1.5);\n              vectorFieldPointers = new long[newSize];\n              vectorFieldNumbers = new int[newSize];\n            }\n          }\n          fp.doVectors = true;\n          docHasVectors = true;\n\n          fp.doVectorPositions |= field.isStorePositionWithTermVector();\n          fp.doVectorOffsets |= field.isStoreOffsetWithTermVector();\n        }\n\n        if (fp.fieldCount == fp.docFields.length) {\n          Fieldable[] newArray = new Fieldable[fp.docFields.length*2];\n          System.arraycopy(fp.docFields, 0, newArray, 0, fp.docFields.length);\n          fp.docFields = newArray;\n        }\n\n        // Lazily allocate arrays for postings:\n        if (field.isIndexed() && fp.postingsHash == null)\n          fp.initPostingArrays();\n\n        fp.docFields[fp.fieldCount++] = field;\n      }\n\n      // Maybe init the local & global fieldsWriter\n      if (localFieldsWriter == null) {\n        if (fieldsWriter == null) {\n          assert docStoreSegment == null;\n          assert segment != null;\n          docStoreSegment = segment;\n          fieldsWriter = new FieldsWriter(directory, docStoreSegment, fieldInfos);\n          files = null;\n        }\n        localFieldsWriter = new FieldsWriter(null, fdtLocal, fieldInfos);\n      }\n\n      // First time we see a doc that has field(s) with\n      // stored vectors, we init our tvx writer\n      if (docHasVectors) {\n        if (tvx == null) {\n          assert docStoreSegment != null;\n          tvx = directory.createOutput(docStoreSegment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION);\n          tvx.writeInt(TermVectorsReader.FORMAT_VERSION);\n          tvd = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);\n          tvd.writeInt(TermVectorsReader.FORMAT_VERSION);\n          tvf = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_FIELDS_EXTENSION);\n          tvf.writeInt(TermVectorsReader.FORMAT_VERSION);\n          files = null;\n\n          // We must \"catch up\" for all docIDs that had no\n          // vectors before this one\n          for(int i=0;i<docID;i++)\n            tvx.writeLong(0);\n        }\n\n        numVectorFields = 0;\n      }\n    }\n\n","bugFix":null,"bugIntro":["5a251aa47d1808cbae42c0e172d698c377430e60"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8560794cda5bcd510c60e38ed553e9c5a6204983","date":1196807382,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#init(Document,int).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#init(Document,int).mjava","sourceNew":"    /** Initializes shared state for this new document */\n    void init(Document doc, int docID) throws IOException {\n\n      abortOnExc = false;\n      this.docID = docID;\n      docBoost = doc.getBoost();\n      numStoredFields = 0;\n      numFieldData = 0;\n      numVectorFields = 0;\n      maxTermHit = 0;\n\n      List docFields = doc.getFields();\n      final int numDocFields = docFields.size();\n      boolean docHasVectors = false;\n\n      // Absorb any new fields first seen in this document.\n      // Also absorb any changes to fields we had already\n      // seen before (eg suddenly turning on norms or\n      // vectors, etc.):\n\n      for(int i=0;i<numDocFields;i++) {\n        Fieldable field = (Fieldable) docFields.get(i);\n\n        FieldInfo fi = fieldInfos.add(field.name(), field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false);\n        numStoredFields += field.isStored() ? 1:0;\n        if (fi.isIndexed && !fi.omitNorms) {\n          // Maybe grow our buffered norms\n          if (norms.length <= fi.number) {\n            int newSize = (int) ((1+fi.number)*1.25);\n            BufferedNorms[] newNorms = new BufferedNorms[newSize];\n            System.arraycopy(norms, 0, newNorms, 0, norms.length);\n            norms = newNorms;\n          }\n          \n          if (norms[fi.number] == null)\n            norms[fi.number] = new BufferedNorms();\n\n          hasNorms = true;\n        }\n\n        // Make sure we have a FieldData allocated\n        int hashPos = fi.name.hashCode() & fieldDataHashMask;\n        FieldData fp = fieldDataHash[hashPos];\n        while(fp != null && !fp.fieldInfo.name.equals(fi.name))\n          fp = fp.next;\n\n        if (fp == null) {\n\n          fp = new FieldData(fi);\n          fp.next = fieldDataHash[hashPos];\n          fieldDataHash[hashPos] = fp;\n\n          if (numAllFieldData == allFieldDataArray.length) {\n            int newSize = (int) (allFieldDataArray.length*1.5);\n\n            FieldData newArray[] = new FieldData[newSize];\n            System.arraycopy(allFieldDataArray, 0, newArray, 0, numAllFieldData);\n            allFieldDataArray = newArray;\n\n            // Rehash\n            newSize = fieldDataHash.length*2;\n            newArray = new FieldData[newSize];\n            fieldDataHashMask = newSize-1;\n            for(int j=0;j<fieldDataHash.length;j++) {\n              FieldData fp0 = fieldDataHash[j];\n              while(fp0 != null) {\n                hashPos = fp0.fieldInfo.name.hashCode() & fieldDataHashMask;\n                FieldData nextFP0 = fp0.next;\n                fp0.next = newArray[hashPos];\n                newArray[hashPos] = fp0;\n                fp0 = nextFP0;\n              }\n            }\n            fieldDataHash = newArray;\n          }\n          allFieldDataArray[numAllFieldData++] = fp;\n        } else {\n          assert fp.fieldInfo == fi;\n        }\n\n        if (docID != fp.lastDocID) {\n\n          // First time we're seeing this field for this doc\n          fp.lastDocID = docID;\n          fp.fieldCount = 0;\n          fp.doVectors = fp.doVectorPositions = fp.doVectorOffsets = false;\n          fp.doNorms = fi.isIndexed && !fi.omitNorms;\n\n          if (numFieldData == fieldDataArray.length) {\n            int newSize = fieldDataArray.length*2;\n            FieldData newArray[] = new FieldData[newSize];\n            System.arraycopy(fieldDataArray, 0, newArray, 0, numFieldData);\n            fieldDataArray = newArray;\n\n          }\n          fieldDataArray[numFieldData++] = fp;\n        }\n\n        if (field.isTermVectorStored()) {\n          if (!fp.doVectors) {\n            if (numVectorFields++ == vectorFieldPointers.length) {\n              final int newSize = (int) (numVectorFields*1.5);\n              vectorFieldPointers = new long[newSize];\n              vectorFieldNumbers = new int[newSize];\n            }\n          }\n          fp.doVectors = true;\n          docHasVectors = true;\n\n          fp.doVectorPositions |= field.isStorePositionWithTermVector();\n          fp.doVectorOffsets |= field.isStoreOffsetWithTermVector();\n        }\n\n        if (fp.fieldCount == fp.docFields.length) {\n          Fieldable[] newArray = new Fieldable[fp.docFields.length*2];\n          System.arraycopy(fp.docFields, 0, newArray, 0, fp.docFields.length);\n          fp.docFields = newArray;\n        }\n\n        // Lazily allocate arrays for postings:\n        if (field.isIndexed() && fp.postingsHash == null)\n          fp.initPostingArrays();\n\n        fp.docFields[fp.fieldCount++] = field;\n      }\n\n      // Maybe init the local & global fieldsWriter\n      if (localFieldsWriter == null) {\n        if (fieldsWriter == null) {\n          assert docStoreSegment == null;\n          assert segment != null;\n          docStoreSegment = segment;\n          fieldsWriter = new FieldsWriter(directory, docStoreSegment, fieldInfos);\n          files = null;\n        }\n        localFieldsWriter = new FieldsWriter(null, fdtLocal, fieldInfos);\n      }\n\n      // First time we see a doc that has field(s) with\n      // stored vectors, we init our tvx writer\n      if (docHasVectors) {\n        if (tvx == null) {\n          assert docStoreSegment != null;\n          tvx = directory.createOutput(docStoreSegment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION);\n          tvx.writeInt(TermVectorsReader.FORMAT_VERSION);\n          tvd = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);\n          tvd.writeInt(TermVectorsReader.FORMAT_VERSION);\n          tvf = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_FIELDS_EXTENSION);\n          tvf.writeInt(TermVectorsReader.FORMAT_VERSION);\n          files = null;\n\n          // We must \"catch up\" for all docIDs that had no\n          // vectors before this one\n          for(int i=0;i<docID;i++)\n            tvx.writeLong(0);\n        }\n\n        numVectorFields = 0;\n      }\n    }\n\n","sourceOld":"    /** Initializes shared state for this new document */\n    void init(Document doc, int docID) throws IOException {\n\n      this.docID = docID;\n      docBoost = doc.getBoost();\n      numStoredFields = 0;\n      numFieldData = 0;\n      numVectorFields = 0;\n      maxTermHit = 0;\n\n      List docFields = doc.getFields();\n      final int numDocFields = docFields.size();\n      boolean docHasVectors = false;\n\n      // Absorb any new fields first seen in this document.\n      // Also absorb any changes to fields we had already\n      // seen before (eg suddenly turning on norms or\n      // vectors, etc.):\n\n      for(int i=0;i<numDocFields;i++) {\n        Fieldable field = (Fieldable) docFields.get(i);\n\n        FieldInfo fi = fieldInfos.add(field.name(), field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false);\n        numStoredFields += field.isStored() ? 1:0;\n        if (fi.isIndexed && !fi.omitNorms) {\n          // Maybe grow our buffered norms\n          if (norms.length <= fi.number) {\n            int newSize = (int) ((1+fi.number)*1.25);\n            BufferedNorms[] newNorms = new BufferedNorms[newSize];\n            System.arraycopy(norms, 0, newNorms, 0, norms.length);\n            norms = newNorms;\n          }\n          \n          if (norms[fi.number] == null)\n            norms[fi.number] = new BufferedNorms();\n\n          hasNorms = true;\n        }\n\n        // Make sure we have a FieldData allocated\n        int hashPos = fi.name.hashCode() & fieldDataHashMask;\n        FieldData fp = fieldDataHash[hashPos];\n        while(fp != null && !fp.fieldInfo.name.equals(fi.name))\n          fp = fp.next;\n\n        if (fp == null) {\n\n          fp = new FieldData(fi);\n          fp.next = fieldDataHash[hashPos];\n          fieldDataHash[hashPos] = fp;\n\n          if (numAllFieldData == allFieldDataArray.length) {\n            int newSize = (int) (allFieldDataArray.length*1.5);\n\n            FieldData newArray[] = new FieldData[newSize];\n            System.arraycopy(allFieldDataArray, 0, newArray, 0, numAllFieldData);\n            allFieldDataArray = newArray;\n\n            // Rehash\n            newSize = fieldDataHash.length*2;\n            newArray = new FieldData[newSize];\n            fieldDataHashMask = newSize-1;\n            for(int j=0;j<fieldDataHash.length;j++) {\n              FieldData fp0 = fieldDataHash[j];\n              while(fp0 != null) {\n                hashPos = fp0.fieldInfo.name.hashCode() & fieldDataHashMask;\n                FieldData nextFP0 = fp0.next;\n                fp0.next = newArray[hashPos];\n                newArray[hashPos] = fp0;\n                fp0 = nextFP0;\n              }\n            }\n            fieldDataHash = newArray;\n          }\n          allFieldDataArray[numAllFieldData++] = fp;\n        } else {\n          assert fp.fieldInfo == fi;\n        }\n\n        if (docID != fp.lastDocID) {\n\n          // First time we're seeing this field for this doc\n          fp.lastDocID = docID;\n          fp.fieldCount = 0;\n          fp.doVectors = fp.doVectorPositions = fp.doVectorOffsets = false;\n          fp.doNorms = fi.isIndexed && !fi.omitNorms;\n\n          if (numFieldData == fieldDataArray.length) {\n            int newSize = fieldDataArray.length*2;\n            FieldData newArray[] = new FieldData[newSize];\n            System.arraycopy(fieldDataArray, 0, newArray, 0, numFieldData);\n            fieldDataArray = newArray;\n\n          }\n          fieldDataArray[numFieldData++] = fp;\n        }\n\n        if (field.isTermVectorStored()) {\n          if (!fp.doVectors) {\n            if (numVectorFields++ == vectorFieldPointers.length) {\n              final int newSize = (int) (numVectorFields*1.5);\n              vectorFieldPointers = new long[newSize];\n              vectorFieldNumbers = new int[newSize];\n            }\n          }\n          fp.doVectors = true;\n          docHasVectors = true;\n\n          fp.doVectorPositions |= field.isStorePositionWithTermVector();\n          fp.doVectorOffsets |= field.isStoreOffsetWithTermVector();\n        }\n\n        if (fp.fieldCount == fp.docFields.length) {\n          Fieldable[] newArray = new Fieldable[fp.docFields.length*2];\n          System.arraycopy(fp.docFields, 0, newArray, 0, fp.docFields.length);\n          fp.docFields = newArray;\n        }\n\n        // Lazily allocate arrays for postings:\n        if (field.isIndexed() && fp.postingsHash == null)\n          fp.initPostingArrays();\n\n        fp.docFields[fp.fieldCount++] = field;\n      }\n\n      // Maybe init the local & global fieldsWriter\n      if (localFieldsWriter == null) {\n        if (fieldsWriter == null) {\n          assert docStoreSegment == null;\n          assert segment != null;\n          docStoreSegment = segment;\n          fieldsWriter = new FieldsWriter(directory, docStoreSegment, fieldInfos);\n          files = null;\n        }\n        localFieldsWriter = new FieldsWriter(null, fdtLocal, fieldInfos);\n      }\n\n      // First time we see a doc that has field(s) with\n      // stored vectors, we init our tvx writer\n      if (docHasVectors) {\n        if (tvx == null) {\n          assert docStoreSegment != null;\n          tvx = directory.createOutput(docStoreSegment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION);\n          tvx.writeInt(TermVectorsReader.FORMAT_VERSION);\n          tvd = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);\n          tvd.writeInt(TermVectorsReader.FORMAT_VERSION);\n          tvf = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_FIELDS_EXTENSION);\n          tvf.writeInt(TermVectorsReader.FORMAT_VERSION);\n          files = null;\n\n          // We must \"catch up\" for all docIDs that had no\n          // vectors before this one\n          for(int i=0;i<docID;i++)\n            tvx.writeLong(0);\n        }\n\n        numVectorFields = 0;\n      }\n    }\n\n","bugFix":null,"bugIntro":["83bbb041887bbef07b8a98d08a0e1713ce137039"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c9a0deca56efc5191d6b3c41047fd538f3fae1d8","date":1198156049,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#init(Document,int).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#init(Document,int).mjava","sourceNew":"    /** Initializes shared state for this new document */\n    void init(Document doc, int docID) throws IOException {\n\n      abortOnExc = false;\n      this.docID = docID;\n      docBoost = doc.getBoost();\n      numStoredFields = 0;\n      numFieldData = 0;\n      numVectorFields = 0;\n      maxTermHit = 0;\n\n      assert 0 == fdtLocal.length();\n      assert 0 == tvfLocal.length();\n\n      List docFields = doc.getFields();\n      final int numDocFields = docFields.size();\n      boolean docHasVectors = false;\n\n      // Absorb any new fields first seen in this document.\n      // Also absorb any changes to fields we had already\n      // seen before (eg suddenly turning on norms or\n      // vectors, etc.):\n\n      for(int i=0;i<numDocFields;i++) {\n        Fieldable field = (Fieldable) docFields.get(i);\n\n        FieldInfo fi = fieldInfos.add(field.name(), field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false);\n        if (fi.isIndexed && !fi.omitNorms) {\n          // Maybe grow our buffered norms\n          if (norms.length <= fi.number) {\n            int newSize = (int) ((1+fi.number)*1.25);\n            BufferedNorms[] newNorms = new BufferedNorms[newSize];\n            System.arraycopy(norms, 0, newNorms, 0, norms.length);\n            norms = newNorms;\n          }\n          \n          if (norms[fi.number] == null)\n            norms[fi.number] = new BufferedNorms();\n\n          hasNorms = true;\n        }\n\n        // Make sure we have a FieldData allocated\n        int hashPos = fi.name.hashCode() & fieldDataHashMask;\n        FieldData fp = fieldDataHash[hashPos];\n        while(fp != null && !fp.fieldInfo.name.equals(fi.name))\n          fp = fp.next;\n\n        if (fp == null) {\n\n          fp = new FieldData(fi);\n          fp.next = fieldDataHash[hashPos];\n          fieldDataHash[hashPos] = fp;\n\n          if (numAllFieldData == allFieldDataArray.length) {\n            int newSize = (int) (allFieldDataArray.length*1.5);\n\n            FieldData newArray[] = new FieldData[newSize];\n            System.arraycopy(allFieldDataArray, 0, newArray, 0, numAllFieldData);\n            allFieldDataArray = newArray;\n\n            // Rehash\n            newSize = fieldDataHash.length*2;\n            newArray = new FieldData[newSize];\n            fieldDataHashMask = newSize-1;\n            for(int j=0;j<fieldDataHash.length;j++) {\n              FieldData fp0 = fieldDataHash[j];\n              while(fp0 != null) {\n                hashPos = fp0.fieldInfo.name.hashCode() & fieldDataHashMask;\n                FieldData nextFP0 = fp0.next;\n                fp0.next = newArray[hashPos];\n                newArray[hashPos] = fp0;\n                fp0 = nextFP0;\n              }\n            }\n            fieldDataHash = newArray;\n          }\n          allFieldDataArray[numAllFieldData++] = fp;\n        } else {\n          assert fp.fieldInfo == fi;\n        }\n\n        if (docID != fp.lastDocID) {\n\n          // First time we're seeing this field for this doc\n          fp.lastDocID = docID;\n          fp.fieldCount = 0;\n          fp.doVectors = fp.doVectorPositions = fp.doVectorOffsets = false;\n          fp.doNorms = fi.isIndexed && !fi.omitNorms;\n\n          if (numFieldData == fieldDataArray.length) {\n            int newSize = fieldDataArray.length*2;\n            FieldData newArray[] = new FieldData[newSize];\n            System.arraycopy(fieldDataArray, 0, newArray, 0, numFieldData);\n            fieldDataArray = newArray;\n\n          }\n          fieldDataArray[numFieldData++] = fp;\n        }\n\n        if (field.isTermVectorStored()) {\n          if (!fp.doVectors) {\n            if (numVectorFields++ == vectorFieldPointers.length) {\n              final int newSize = (int) (numVectorFields*1.5);\n              vectorFieldPointers = new long[newSize];\n              vectorFieldNumbers = new int[newSize];\n            }\n          }\n          fp.doVectors = true;\n          docHasVectors = true;\n\n          fp.doVectorPositions |= field.isStorePositionWithTermVector();\n          fp.doVectorOffsets |= field.isStoreOffsetWithTermVector();\n        }\n\n        if (fp.fieldCount == fp.docFields.length) {\n          Fieldable[] newArray = new Fieldable[fp.docFields.length*2];\n          System.arraycopy(fp.docFields, 0, newArray, 0, fp.docFields.length);\n          fp.docFields = newArray;\n        }\n\n        // Lazily allocate arrays for postings:\n        if (field.isIndexed() && fp.postingsHash == null)\n          fp.initPostingArrays();\n\n        fp.docFields[fp.fieldCount++] = field;\n      }\n\n      // Maybe init the local & global fieldsWriter\n      if (localFieldsWriter == null) {\n        if (fieldsWriter == null) {\n          assert docStoreSegment == null;\n          assert segment != null;\n          docStoreSegment = segment;\n          fieldsWriter = new FieldsWriter(directory, docStoreSegment, fieldInfos);\n          files = null;\n        }\n        localFieldsWriter = new FieldsWriter(null, fdtLocal, fieldInfos);\n      }\n\n      // First time we see a doc that has field(s) with\n      // stored vectors, we init our tvx writer\n      if (docHasVectors) {\n        if (tvx == null) {\n          assert docStoreSegment != null;\n          tvx = directory.createOutput(docStoreSegment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION);\n          tvx.writeInt(TermVectorsReader.FORMAT_VERSION);\n          tvd = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);\n          tvd.writeInt(TermVectorsReader.FORMAT_VERSION);\n          tvf = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_FIELDS_EXTENSION);\n          tvf.writeInt(TermVectorsReader.FORMAT_VERSION);\n          files = null;\n\n          // We must \"catch up\" for all docIDs that had no\n          // vectors before this one\n          for(int i=0;i<docID;i++)\n            tvx.writeLong(0);\n        }\n\n        numVectorFields = 0;\n      }\n    }\n\n","sourceOld":"    /** Initializes shared state for this new document */\n    void init(Document doc, int docID) throws IOException {\n\n      abortOnExc = false;\n      this.docID = docID;\n      docBoost = doc.getBoost();\n      numStoredFields = 0;\n      numFieldData = 0;\n      numVectorFields = 0;\n      maxTermHit = 0;\n\n      List docFields = doc.getFields();\n      final int numDocFields = docFields.size();\n      boolean docHasVectors = false;\n\n      // Absorb any new fields first seen in this document.\n      // Also absorb any changes to fields we had already\n      // seen before (eg suddenly turning on norms or\n      // vectors, etc.):\n\n      for(int i=0;i<numDocFields;i++) {\n        Fieldable field = (Fieldable) docFields.get(i);\n\n        FieldInfo fi = fieldInfos.add(field.name(), field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false);\n        numStoredFields += field.isStored() ? 1:0;\n        if (fi.isIndexed && !fi.omitNorms) {\n          // Maybe grow our buffered norms\n          if (norms.length <= fi.number) {\n            int newSize = (int) ((1+fi.number)*1.25);\n            BufferedNorms[] newNorms = new BufferedNorms[newSize];\n            System.arraycopy(norms, 0, newNorms, 0, norms.length);\n            norms = newNorms;\n          }\n          \n          if (norms[fi.number] == null)\n            norms[fi.number] = new BufferedNorms();\n\n          hasNorms = true;\n        }\n\n        // Make sure we have a FieldData allocated\n        int hashPos = fi.name.hashCode() & fieldDataHashMask;\n        FieldData fp = fieldDataHash[hashPos];\n        while(fp != null && !fp.fieldInfo.name.equals(fi.name))\n          fp = fp.next;\n\n        if (fp == null) {\n\n          fp = new FieldData(fi);\n          fp.next = fieldDataHash[hashPos];\n          fieldDataHash[hashPos] = fp;\n\n          if (numAllFieldData == allFieldDataArray.length) {\n            int newSize = (int) (allFieldDataArray.length*1.5);\n\n            FieldData newArray[] = new FieldData[newSize];\n            System.arraycopy(allFieldDataArray, 0, newArray, 0, numAllFieldData);\n            allFieldDataArray = newArray;\n\n            // Rehash\n            newSize = fieldDataHash.length*2;\n            newArray = new FieldData[newSize];\n            fieldDataHashMask = newSize-1;\n            for(int j=0;j<fieldDataHash.length;j++) {\n              FieldData fp0 = fieldDataHash[j];\n              while(fp0 != null) {\n                hashPos = fp0.fieldInfo.name.hashCode() & fieldDataHashMask;\n                FieldData nextFP0 = fp0.next;\n                fp0.next = newArray[hashPos];\n                newArray[hashPos] = fp0;\n                fp0 = nextFP0;\n              }\n            }\n            fieldDataHash = newArray;\n          }\n          allFieldDataArray[numAllFieldData++] = fp;\n        } else {\n          assert fp.fieldInfo == fi;\n        }\n\n        if (docID != fp.lastDocID) {\n\n          // First time we're seeing this field for this doc\n          fp.lastDocID = docID;\n          fp.fieldCount = 0;\n          fp.doVectors = fp.doVectorPositions = fp.doVectorOffsets = false;\n          fp.doNorms = fi.isIndexed && !fi.omitNorms;\n\n          if (numFieldData == fieldDataArray.length) {\n            int newSize = fieldDataArray.length*2;\n            FieldData newArray[] = new FieldData[newSize];\n            System.arraycopy(fieldDataArray, 0, newArray, 0, numFieldData);\n            fieldDataArray = newArray;\n\n          }\n          fieldDataArray[numFieldData++] = fp;\n        }\n\n        if (field.isTermVectorStored()) {\n          if (!fp.doVectors) {\n            if (numVectorFields++ == vectorFieldPointers.length) {\n              final int newSize = (int) (numVectorFields*1.5);\n              vectorFieldPointers = new long[newSize];\n              vectorFieldNumbers = new int[newSize];\n            }\n          }\n          fp.doVectors = true;\n          docHasVectors = true;\n\n          fp.doVectorPositions |= field.isStorePositionWithTermVector();\n          fp.doVectorOffsets |= field.isStoreOffsetWithTermVector();\n        }\n\n        if (fp.fieldCount == fp.docFields.length) {\n          Fieldable[] newArray = new Fieldable[fp.docFields.length*2];\n          System.arraycopy(fp.docFields, 0, newArray, 0, fp.docFields.length);\n          fp.docFields = newArray;\n        }\n\n        // Lazily allocate arrays for postings:\n        if (field.isIndexed() && fp.postingsHash == null)\n          fp.initPostingArrays();\n\n        fp.docFields[fp.fieldCount++] = field;\n      }\n\n      // Maybe init the local & global fieldsWriter\n      if (localFieldsWriter == null) {\n        if (fieldsWriter == null) {\n          assert docStoreSegment == null;\n          assert segment != null;\n          docStoreSegment = segment;\n          fieldsWriter = new FieldsWriter(directory, docStoreSegment, fieldInfos);\n          files = null;\n        }\n        localFieldsWriter = new FieldsWriter(null, fdtLocal, fieldInfos);\n      }\n\n      // First time we see a doc that has field(s) with\n      // stored vectors, we init our tvx writer\n      if (docHasVectors) {\n        if (tvx == null) {\n          assert docStoreSegment != null;\n          tvx = directory.createOutput(docStoreSegment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION);\n          tvx.writeInt(TermVectorsReader.FORMAT_VERSION);\n          tvd = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);\n          tvd.writeInt(TermVectorsReader.FORMAT_VERSION);\n          tvf = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_FIELDS_EXTENSION);\n          tvf.writeInt(TermVectorsReader.FORMAT_VERSION);\n          files = null;\n\n          // We must \"catch up\" for all docIDs that had no\n          // vectors before this one\n          for(int i=0;i<docID;i++)\n            tvx.writeLong(0);\n        }\n\n        numVectorFields = 0;\n      }\n    }\n\n","bugFix":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5a251aa47d1808cbae42c0e172d698c377430e60","date":1199375390,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#init(Document,int).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#init(Document,int).mjava","sourceNew":"    /** Initializes shared state for this new document */\n    void init(Document doc, int docID) throws IOException {\n\n      abortOnExc = false;\n      this.docID = docID;\n      docBoost = doc.getBoost();\n      numStoredFields = 0;\n      numFieldData = 0;\n      numVectorFields = 0;\n      maxTermPrefix = null;\n\n      assert 0 == fdtLocal.length();\n      assert 0 == tvfLocal.length();\n\n      List docFields = doc.getFields();\n      final int numDocFields = docFields.size();\n      boolean docHasVectors = false;\n\n      // Absorb any new fields first seen in this document.\n      // Also absorb any changes to fields we had already\n      // seen before (eg suddenly turning on norms or\n      // vectors, etc.):\n\n      for(int i=0;i<numDocFields;i++) {\n        Fieldable field = (Fieldable) docFields.get(i);\n\n        FieldInfo fi = fieldInfos.add(field.name(), field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false);\n        if (fi.isIndexed && !fi.omitNorms) {\n          // Maybe grow our buffered norms\n          if (norms.length <= fi.number) {\n            int newSize = (int) ((1+fi.number)*1.25);\n            BufferedNorms[] newNorms = new BufferedNorms[newSize];\n            System.arraycopy(norms, 0, newNorms, 0, norms.length);\n            norms = newNorms;\n          }\n          \n          if (norms[fi.number] == null)\n            norms[fi.number] = new BufferedNorms();\n\n          hasNorms = true;\n        }\n\n        // Make sure we have a FieldData allocated\n        int hashPos = fi.name.hashCode() & fieldDataHashMask;\n        FieldData fp = fieldDataHash[hashPos];\n        while(fp != null && !fp.fieldInfo.name.equals(fi.name))\n          fp = fp.next;\n\n        if (fp == null) {\n\n          fp = new FieldData(fi);\n          fp.next = fieldDataHash[hashPos];\n          fieldDataHash[hashPos] = fp;\n\n          if (numAllFieldData == allFieldDataArray.length) {\n            int newSize = (int) (allFieldDataArray.length*1.5);\n\n            FieldData newArray[] = new FieldData[newSize];\n            System.arraycopy(allFieldDataArray, 0, newArray, 0, numAllFieldData);\n            allFieldDataArray = newArray;\n\n            // Rehash\n            newSize = fieldDataHash.length*2;\n            newArray = new FieldData[newSize];\n            fieldDataHashMask = newSize-1;\n            for(int j=0;j<fieldDataHash.length;j++) {\n              FieldData fp0 = fieldDataHash[j];\n              while(fp0 != null) {\n                hashPos = fp0.fieldInfo.name.hashCode() & fieldDataHashMask;\n                FieldData nextFP0 = fp0.next;\n                fp0.next = newArray[hashPos];\n                newArray[hashPos] = fp0;\n                fp0 = nextFP0;\n              }\n            }\n            fieldDataHash = newArray;\n          }\n          allFieldDataArray[numAllFieldData++] = fp;\n        } else {\n          assert fp.fieldInfo == fi;\n        }\n\n        if (docID != fp.lastDocID) {\n\n          // First time we're seeing this field for this doc\n          fp.lastDocID = docID;\n          fp.fieldCount = 0;\n          fp.doVectors = fp.doVectorPositions = fp.doVectorOffsets = false;\n          fp.doNorms = fi.isIndexed && !fi.omitNorms;\n\n          if (numFieldData == fieldDataArray.length) {\n            int newSize = fieldDataArray.length*2;\n            FieldData newArray[] = new FieldData[newSize];\n            System.arraycopy(fieldDataArray, 0, newArray, 0, numFieldData);\n            fieldDataArray = newArray;\n\n          }\n          fieldDataArray[numFieldData++] = fp;\n        }\n\n        if (field.isTermVectorStored()) {\n          if (!fp.doVectors && numVectorFields++ == vectorFieldPointers.length) {\n            final int newSize = (int) (numVectorFields*1.5);\n            vectorFieldPointers = new long[newSize];\n            vectorFieldNumbers = new int[newSize];\n          }\n          fp.doVectors = true;\n          docHasVectors = true;\n\n          fp.doVectorPositions |= field.isStorePositionWithTermVector();\n          fp.doVectorOffsets |= field.isStoreOffsetWithTermVector();\n        }\n\n        if (fp.fieldCount == fp.docFields.length) {\n          Fieldable[] newArray = new Fieldable[fp.docFields.length*2];\n          System.arraycopy(fp.docFields, 0, newArray, 0, fp.docFields.length);\n          fp.docFields = newArray;\n        }\n\n        // Lazily allocate arrays for postings:\n        if (field.isIndexed() && fp.postingsHash == null)\n          fp.initPostingArrays();\n\n        fp.docFields[fp.fieldCount++] = field;\n      }\n\n      // Maybe init the local & global fieldsWriter\n      if (localFieldsWriter == null) {\n        if (fieldsWriter == null) {\n          assert docStoreSegment == null;\n          assert segment != null;\n          docStoreSegment = segment;\n          fieldsWriter = new FieldsWriter(directory, docStoreSegment, fieldInfos);\n          files = null;\n        }\n        localFieldsWriter = new FieldsWriter(null, fdtLocal, fieldInfos);\n      }\n\n      // First time we see a doc that has field(s) with\n      // stored vectors, we init our tvx writer\n      if (docHasVectors) {\n        if (tvx == null) {\n          assert docStoreSegment != null;\n          tvx = directory.createOutput(docStoreSegment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION);\n          tvx.writeInt(TermVectorsReader.FORMAT_VERSION);\n          tvd = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);\n          tvd.writeInt(TermVectorsReader.FORMAT_VERSION);\n          tvf = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_FIELDS_EXTENSION);\n          tvf.writeInt(TermVectorsReader.FORMAT_VERSION);\n          files = null;\n\n          // We must \"catch up\" for all docIDs that had no\n          // vectors before this one\n          for(int i=0;i<docID;i++)\n            tvx.writeLong(0);\n        }\n\n        numVectorFields = 0;\n      }\n    }\n\n","sourceOld":"    /** Initializes shared state for this new document */\n    void init(Document doc, int docID) throws IOException {\n\n      abortOnExc = false;\n      this.docID = docID;\n      docBoost = doc.getBoost();\n      numStoredFields = 0;\n      numFieldData = 0;\n      numVectorFields = 0;\n      maxTermHit = 0;\n\n      assert 0 == fdtLocal.length();\n      assert 0 == tvfLocal.length();\n\n      List docFields = doc.getFields();\n      final int numDocFields = docFields.size();\n      boolean docHasVectors = false;\n\n      // Absorb any new fields first seen in this document.\n      // Also absorb any changes to fields we had already\n      // seen before (eg suddenly turning on norms or\n      // vectors, etc.):\n\n      for(int i=0;i<numDocFields;i++) {\n        Fieldable field = (Fieldable) docFields.get(i);\n\n        FieldInfo fi = fieldInfos.add(field.name(), field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false);\n        if (fi.isIndexed && !fi.omitNorms) {\n          // Maybe grow our buffered norms\n          if (norms.length <= fi.number) {\n            int newSize = (int) ((1+fi.number)*1.25);\n            BufferedNorms[] newNorms = new BufferedNorms[newSize];\n            System.arraycopy(norms, 0, newNorms, 0, norms.length);\n            norms = newNorms;\n          }\n          \n          if (norms[fi.number] == null)\n            norms[fi.number] = new BufferedNorms();\n\n          hasNorms = true;\n        }\n\n        // Make sure we have a FieldData allocated\n        int hashPos = fi.name.hashCode() & fieldDataHashMask;\n        FieldData fp = fieldDataHash[hashPos];\n        while(fp != null && !fp.fieldInfo.name.equals(fi.name))\n          fp = fp.next;\n\n        if (fp == null) {\n\n          fp = new FieldData(fi);\n          fp.next = fieldDataHash[hashPos];\n          fieldDataHash[hashPos] = fp;\n\n          if (numAllFieldData == allFieldDataArray.length) {\n            int newSize = (int) (allFieldDataArray.length*1.5);\n\n            FieldData newArray[] = new FieldData[newSize];\n            System.arraycopy(allFieldDataArray, 0, newArray, 0, numAllFieldData);\n            allFieldDataArray = newArray;\n\n            // Rehash\n            newSize = fieldDataHash.length*2;\n            newArray = new FieldData[newSize];\n            fieldDataHashMask = newSize-1;\n            for(int j=0;j<fieldDataHash.length;j++) {\n              FieldData fp0 = fieldDataHash[j];\n              while(fp0 != null) {\n                hashPos = fp0.fieldInfo.name.hashCode() & fieldDataHashMask;\n                FieldData nextFP0 = fp0.next;\n                fp0.next = newArray[hashPos];\n                newArray[hashPos] = fp0;\n                fp0 = nextFP0;\n              }\n            }\n            fieldDataHash = newArray;\n          }\n          allFieldDataArray[numAllFieldData++] = fp;\n        } else {\n          assert fp.fieldInfo == fi;\n        }\n\n        if (docID != fp.lastDocID) {\n\n          // First time we're seeing this field for this doc\n          fp.lastDocID = docID;\n          fp.fieldCount = 0;\n          fp.doVectors = fp.doVectorPositions = fp.doVectorOffsets = false;\n          fp.doNorms = fi.isIndexed && !fi.omitNorms;\n\n          if (numFieldData == fieldDataArray.length) {\n            int newSize = fieldDataArray.length*2;\n            FieldData newArray[] = new FieldData[newSize];\n            System.arraycopy(fieldDataArray, 0, newArray, 0, numFieldData);\n            fieldDataArray = newArray;\n\n          }\n          fieldDataArray[numFieldData++] = fp;\n        }\n\n        if (field.isTermVectorStored()) {\n          if (!fp.doVectors) {\n            if (numVectorFields++ == vectorFieldPointers.length) {\n              final int newSize = (int) (numVectorFields*1.5);\n              vectorFieldPointers = new long[newSize];\n              vectorFieldNumbers = new int[newSize];\n            }\n          }\n          fp.doVectors = true;\n          docHasVectors = true;\n\n          fp.doVectorPositions |= field.isStorePositionWithTermVector();\n          fp.doVectorOffsets |= field.isStoreOffsetWithTermVector();\n        }\n\n        if (fp.fieldCount == fp.docFields.length) {\n          Fieldable[] newArray = new Fieldable[fp.docFields.length*2];\n          System.arraycopy(fp.docFields, 0, newArray, 0, fp.docFields.length);\n          fp.docFields = newArray;\n        }\n\n        // Lazily allocate arrays for postings:\n        if (field.isIndexed() && fp.postingsHash == null)\n          fp.initPostingArrays();\n\n        fp.docFields[fp.fieldCount++] = field;\n      }\n\n      // Maybe init the local & global fieldsWriter\n      if (localFieldsWriter == null) {\n        if (fieldsWriter == null) {\n          assert docStoreSegment == null;\n          assert segment != null;\n          docStoreSegment = segment;\n          fieldsWriter = new FieldsWriter(directory, docStoreSegment, fieldInfos);\n          files = null;\n        }\n        localFieldsWriter = new FieldsWriter(null, fdtLocal, fieldInfos);\n      }\n\n      // First time we see a doc that has field(s) with\n      // stored vectors, we init our tvx writer\n      if (docHasVectors) {\n        if (tvx == null) {\n          assert docStoreSegment != null;\n          tvx = directory.createOutput(docStoreSegment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION);\n          tvx.writeInt(TermVectorsReader.FORMAT_VERSION);\n          tvd = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);\n          tvd.writeInt(TermVectorsReader.FORMAT_VERSION);\n          tvf = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_FIELDS_EXTENSION);\n          tvf.writeInt(TermVectorsReader.FORMAT_VERSION);\n          files = null;\n\n          // We must \"catch up\" for all docIDs that had no\n          // vectors before this one\n          for(int i=0;i<docID;i++)\n            tvx.writeLong(0);\n        }\n\n        numVectorFields = 0;\n      }\n    }\n\n","bugFix":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c","741a5cca05cabe1e7482410a29e563a08379251a"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"83bbb041887bbef07b8a98d08a0e1713ce137039","date":1200330381,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#init(Document,int).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#init(Document,int).mjava","sourceNew":"    /** Initializes shared state for this new document */\n    void init(Document doc, int docID) throws IOException, AbortException {\n\n      assert !isIdle;\n\n      this.docID = docID;\n      docBoost = doc.getBoost();\n      numStoredFields = 0;\n      numFieldData = 0;\n      numVectorFields = 0;\n      maxTermPrefix = null;\n\n      assert 0 == fdtLocal.length();\n      assert 0 == fdtLocal.getFilePointer();\n      assert 0 == tvfLocal.length();\n      assert 0 == tvfLocal.getFilePointer();\n      final int thisFieldGen = fieldGen++;\n\n      List docFields = doc.getFields();\n      final int numDocFields = docFields.size();\n      boolean docHasVectors = false;\n\n      // Absorb any new fields first seen in this document.\n      // Also absorb any changes to fields we had already\n      // seen before (eg suddenly turning on norms or\n      // vectors, etc.):\n\n      for(int i=0;i<numDocFields;i++) {\n        Fieldable field = (Fieldable) docFields.get(i);\n\n        FieldInfo fi = fieldInfos.add(field.name(), field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false);\n        if (fi.isIndexed && !fi.omitNorms) {\n          // Maybe grow our buffered norms\n          if (norms.length <= fi.number) {\n            int newSize = (int) ((1+fi.number)*1.25);\n            BufferedNorms[] newNorms = new BufferedNorms[newSize];\n            System.arraycopy(norms, 0, newNorms, 0, norms.length);\n            norms = newNorms;\n          }\n          \n          if (norms[fi.number] == null)\n            norms[fi.number] = new BufferedNorms();\n\n          hasNorms = true;\n        }\n\n        // Make sure we have a FieldData allocated\n        int hashPos = fi.name.hashCode() & fieldDataHashMask;\n        FieldData fp = fieldDataHash[hashPos];\n        while(fp != null && !fp.fieldInfo.name.equals(fi.name))\n          fp = fp.next;\n\n        if (fp == null) {\n\n          fp = new FieldData(fi);\n          fp.next = fieldDataHash[hashPos];\n          fieldDataHash[hashPos] = fp;\n\n          if (numAllFieldData == allFieldDataArray.length) {\n            int newSize = (int) (allFieldDataArray.length*1.5);\n            int newHashSize = fieldDataHash.length*2;\n\n            FieldData newArray[] = new FieldData[newSize];\n            FieldData newHashArray[] = new FieldData[newHashSize];\n            System.arraycopy(allFieldDataArray, 0, newArray, 0, numAllFieldData);\n\n            // Rehash\n            fieldDataHashMask = newSize-1;\n            for(int j=0;j<fieldDataHash.length;j++) {\n              FieldData fp0 = fieldDataHash[j];\n              while(fp0 != null) {\n                hashPos = fp0.fieldInfo.name.hashCode() & fieldDataHashMask;\n                FieldData nextFP0 = fp0.next;\n                fp0.next = newHashArray[hashPos];\n                newHashArray[hashPos] = fp0;\n                fp0 = nextFP0;\n              }\n            }\n\n            allFieldDataArray = newArray;\n            fieldDataHash = newHashArray;\n          }\n          allFieldDataArray[numAllFieldData++] = fp;\n        } else {\n          assert fp.fieldInfo == fi;\n        }\n\n        if (thisFieldGen != fp.lastGen) {\n\n          // First time we're seeing this field for this doc\n          fp.lastGen = thisFieldGen;\n          fp.fieldCount = 0;\n          fp.doVectors = fp.doVectorPositions = fp.doVectorOffsets = false;\n          fp.doNorms = fi.isIndexed && !fi.omitNorms;\n\n          if (numFieldData == fieldDataArray.length) {\n            int newSize = fieldDataArray.length*2;\n            FieldData newArray[] = new FieldData[newSize];\n            System.arraycopy(fieldDataArray, 0, newArray, 0, numFieldData);\n            fieldDataArray = newArray;\n\n          }\n          fieldDataArray[numFieldData++] = fp;\n        }\n\n        if (field.isTermVectorStored()) {\n          if (!fp.doVectors && numVectorFields++ == vectorFieldPointers.length) {\n            final int newSize = (int) (numVectorFields*1.5);\n            vectorFieldPointers = new long[newSize];\n            vectorFieldNumbers = new int[newSize];\n          }\n          fp.doVectors = true;\n          docHasVectors = true;\n\n          fp.doVectorPositions |= field.isStorePositionWithTermVector();\n          fp.doVectorOffsets |= field.isStoreOffsetWithTermVector();\n        }\n\n        if (fp.fieldCount == fp.docFields.length) {\n          Fieldable[] newArray = new Fieldable[fp.docFields.length*2];\n          System.arraycopy(fp.docFields, 0, newArray, 0, fp.docFields.length);\n          fp.docFields = newArray;\n        }\n\n        // Lazily allocate arrays for postings:\n        if (field.isIndexed() && fp.postingsHash == null)\n          fp.initPostingArrays();\n\n        fp.docFields[fp.fieldCount++] = field;\n      }\n\n      // Maybe init the local & global fieldsWriter\n      if (localFieldsWriter == null) {\n        if (fieldsWriter == null) {\n          assert docStoreSegment == null;\n          assert segment != null;\n          docStoreSegment = segment;\n          // If we hit an exception while init'ing the\n          // fieldsWriter, we must abort this segment\n          // because those files will be in an unknown\n          // state:\n          try {\n            fieldsWriter = new FieldsWriter(directory, docStoreSegment, fieldInfos);\n          } catch (Throwable t) {\n            throw new AbortException(t, DocumentsWriter.this);\n          }\n          files = null;\n        }\n        localFieldsWriter = new FieldsWriter(null, fdtLocal, fieldInfos);\n      }\n\n      // First time we see a doc that has field(s) with\n      // stored vectors, we init our tvx writer\n      if (docHasVectors) {\n        if (tvx == null) {\n          assert docStoreSegment != null;\n          // If we hit an exception while init'ing the term\n          // vector output files, we must abort this segment\n          // because those files will be in an unknown\n          // state:\n          try {\n            tvx = directory.createOutput(docStoreSegment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION);\n            tvx.writeInt(TermVectorsReader.FORMAT_VERSION);\n            tvd = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);\n            tvd.writeInt(TermVectorsReader.FORMAT_VERSION);\n            tvf = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_FIELDS_EXTENSION);\n            tvf.writeInt(TermVectorsReader.FORMAT_VERSION);\n\n            // We must \"catch up\" for all docIDs that had no\n            // vectors before this one\n            for(int i=0;i<docID;i++)\n              tvx.writeLong(0);\n          } catch (Throwable t) {\n            throw new AbortException(t, DocumentsWriter.this);\n          }\n          files = null;\n        }\n\n        numVectorFields = 0;\n      }\n    }\n\n","sourceOld":"    /** Initializes shared state for this new document */\n    void init(Document doc, int docID) throws IOException {\n\n      abortOnExc = false;\n      this.docID = docID;\n      docBoost = doc.getBoost();\n      numStoredFields = 0;\n      numFieldData = 0;\n      numVectorFields = 0;\n      maxTermPrefix = null;\n\n      assert 0 == fdtLocal.length();\n      assert 0 == tvfLocal.length();\n\n      List docFields = doc.getFields();\n      final int numDocFields = docFields.size();\n      boolean docHasVectors = false;\n\n      // Absorb any new fields first seen in this document.\n      // Also absorb any changes to fields we had already\n      // seen before (eg suddenly turning on norms or\n      // vectors, etc.):\n\n      for(int i=0;i<numDocFields;i++) {\n        Fieldable field = (Fieldable) docFields.get(i);\n\n        FieldInfo fi = fieldInfos.add(field.name(), field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false);\n        if (fi.isIndexed && !fi.omitNorms) {\n          // Maybe grow our buffered norms\n          if (norms.length <= fi.number) {\n            int newSize = (int) ((1+fi.number)*1.25);\n            BufferedNorms[] newNorms = new BufferedNorms[newSize];\n            System.arraycopy(norms, 0, newNorms, 0, norms.length);\n            norms = newNorms;\n          }\n          \n          if (norms[fi.number] == null)\n            norms[fi.number] = new BufferedNorms();\n\n          hasNorms = true;\n        }\n\n        // Make sure we have a FieldData allocated\n        int hashPos = fi.name.hashCode() & fieldDataHashMask;\n        FieldData fp = fieldDataHash[hashPos];\n        while(fp != null && !fp.fieldInfo.name.equals(fi.name))\n          fp = fp.next;\n\n        if (fp == null) {\n\n          fp = new FieldData(fi);\n          fp.next = fieldDataHash[hashPos];\n          fieldDataHash[hashPos] = fp;\n\n          if (numAllFieldData == allFieldDataArray.length) {\n            int newSize = (int) (allFieldDataArray.length*1.5);\n\n            FieldData newArray[] = new FieldData[newSize];\n            System.arraycopy(allFieldDataArray, 0, newArray, 0, numAllFieldData);\n            allFieldDataArray = newArray;\n\n            // Rehash\n            newSize = fieldDataHash.length*2;\n            newArray = new FieldData[newSize];\n            fieldDataHashMask = newSize-1;\n            for(int j=0;j<fieldDataHash.length;j++) {\n              FieldData fp0 = fieldDataHash[j];\n              while(fp0 != null) {\n                hashPos = fp0.fieldInfo.name.hashCode() & fieldDataHashMask;\n                FieldData nextFP0 = fp0.next;\n                fp0.next = newArray[hashPos];\n                newArray[hashPos] = fp0;\n                fp0 = nextFP0;\n              }\n            }\n            fieldDataHash = newArray;\n          }\n          allFieldDataArray[numAllFieldData++] = fp;\n        } else {\n          assert fp.fieldInfo == fi;\n        }\n\n        if (docID != fp.lastDocID) {\n\n          // First time we're seeing this field for this doc\n          fp.lastDocID = docID;\n          fp.fieldCount = 0;\n          fp.doVectors = fp.doVectorPositions = fp.doVectorOffsets = false;\n          fp.doNorms = fi.isIndexed && !fi.omitNorms;\n\n          if (numFieldData == fieldDataArray.length) {\n            int newSize = fieldDataArray.length*2;\n            FieldData newArray[] = new FieldData[newSize];\n            System.arraycopy(fieldDataArray, 0, newArray, 0, numFieldData);\n            fieldDataArray = newArray;\n\n          }\n          fieldDataArray[numFieldData++] = fp;\n        }\n\n        if (field.isTermVectorStored()) {\n          if (!fp.doVectors && numVectorFields++ == vectorFieldPointers.length) {\n            final int newSize = (int) (numVectorFields*1.5);\n            vectorFieldPointers = new long[newSize];\n            vectorFieldNumbers = new int[newSize];\n          }\n          fp.doVectors = true;\n          docHasVectors = true;\n\n          fp.doVectorPositions |= field.isStorePositionWithTermVector();\n          fp.doVectorOffsets |= field.isStoreOffsetWithTermVector();\n        }\n\n        if (fp.fieldCount == fp.docFields.length) {\n          Fieldable[] newArray = new Fieldable[fp.docFields.length*2];\n          System.arraycopy(fp.docFields, 0, newArray, 0, fp.docFields.length);\n          fp.docFields = newArray;\n        }\n\n        // Lazily allocate arrays for postings:\n        if (field.isIndexed() && fp.postingsHash == null)\n          fp.initPostingArrays();\n\n        fp.docFields[fp.fieldCount++] = field;\n      }\n\n      // Maybe init the local & global fieldsWriter\n      if (localFieldsWriter == null) {\n        if (fieldsWriter == null) {\n          assert docStoreSegment == null;\n          assert segment != null;\n          docStoreSegment = segment;\n          fieldsWriter = new FieldsWriter(directory, docStoreSegment, fieldInfos);\n          files = null;\n        }\n        localFieldsWriter = new FieldsWriter(null, fdtLocal, fieldInfos);\n      }\n\n      // First time we see a doc that has field(s) with\n      // stored vectors, we init our tvx writer\n      if (docHasVectors) {\n        if (tvx == null) {\n          assert docStoreSegment != null;\n          tvx = directory.createOutput(docStoreSegment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION);\n          tvx.writeInt(TermVectorsReader.FORMAT_VERSION);\n          tvd = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);\n          tvd.writeInt(TermVectorsReader.FORMAT_VERSION);\n          tvf = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_FIELDS_EXTENSION);\n          tvf.writeInt(TermVectorsReader.FORMAT_VERSION);\n          files = null;\n\n          // We must \"catch up\" for all docIDs that had no\n          // vectors before this one\n          for(int i=0;i<docID;i++)\n            tvx.writeLong(0);\n        }\n\n        numVectorFields = 0;\n      }\n    }\n\n","bugFix":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c","11764865fb318bf86302eab36bdf9cd00c50c110","8560794cda5bcd510c60e38ed553e9c5a6204983"],"bugIntro":["7439af33438ca32496b7ce4a341617a480b7137a"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b3d08461c77d39c25ea6ff0cd05b32f948fa2a33","date":1201260752,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#init(Document,int).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#init(Document,int).mjava","sourceNew":"    /** Initializes shared state for this new document */\n    void init(Document doc, int docID) throws IOException, AbortException {\n\n      assert !isIdle;\n\n      this.docID = docID;\n      docBoost = doc.getBoost();\n      numStoredFields = 0;\n      numFieldData = 0;\n      numVectorFields = 0;\n      maxTermPrefix = null;\n\n      assert 0 == fdtLocal.length();\n      assert 0 == fdtLocal.getFilePointer();\n      assert 0 == tvfLocal.length();\n      assert 0 == tvfLocal.getFilePointer();\n      final int thisFieldGen = fieldGen++;\n\n      List docFields = doc.getFields();\n      final int numDocFields = docFields.size();\n      boolean docHasVectors = false;\n\n      // Absorb any new fields first seen in this document.\n      // Also absorb any changes to fields we had already\n      // seen before (eg suddenly turning on norms or\n      // vectors, etc.):\n\n      for(int i=0;i<numDocFields;i++) {\n        Fieldable field = (Fieldable) docFields.get(i);\n\n        FieldInfo fi = fieldInfos.add(field.name(), field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false);\n        if (fi.isIndexed && !fi.omitNorms) {\n          // Maybe grow our buffered norms\n          if (norms.length <= fi.number) {\n            int newSize = (int) ((1+fi.number)*1.25);\n            BufferedNorms[] newNorms = new BufferedNorms[newSize];\n            System.arraycopy(norms, 0, newNorms, 0, norms.length);\n            norms = newNorms;\n          }\n          \n          if (norms[fi.number] == null)\n            norms[fi.number] = new BufferedNorms();\n\n          hasNorms = true;\n        }\n\n        // Make sure we have a FieldData allocated\n        int hashPos = fi.name.hashCode() & fieldDataHashMask;\n        FieldData fp = fieldDataHash[hashPos];\n        while(fp != null && !fp.fieldInfo.name.equals(fi.name))\n          fp = fp.next;\n\n        if (fp == null) {\n\n          fp = new FieldData(fi);\n          fp.next = fieldDataHash[hashPos];\n          fieldDataHash[hashPos] = fp;\n\n          if (numAllFieldData == allFieldDataArray.length) {\n            int newSize = (int) (allFieldDataArray.length*1.5);\n            int newHashSize = fieldDataHash.length*2;\n\n            FieldData newArray[] = new FieldData[newSize];\n            FieldData newHashArray[] = new FieldData[newHashSize];\n            System.arraycopy(allFieldDataArray, 0, newArray, 0, numAllFieldData);\n\n            // Rehash\n            fieldDataHashMask = newSize-1;\n            for(int j=0;j<fieldDataHash.length;j++) {\n              FieldData fp0 = fieldDataHash[j];\n              while(fp0 != null) {\n                hashPos = fp0.fieldInfo.name.hashCode() & fieldDataHashMask;\n                FieldData nextFP0 = fp0.next;\n                fp0.next = newHashArray[hashPos];\n                newHashArray[hashPos] = fp0;\n                fp0 = nextFP0;\n              }\n            }\n\n            allFieldDataArray = newArray;\n            fieldDataHash = newHashArray;\n          }\n          allFieldDataArray[numAllFieldData++] = fp;\n        } else {\n          assert fp.fieldInfo == fi;\n        }\n\n        if (thisFieldGen != fp.lastGen) {\n\n          // First time we're seeing this field for this doc\n          fp.lastGen = thisFieldGen;\n          fp.fieldCount = 0;\n          fp.doVectors = fp.doVectorPositions = fp.doVectorOffsets = false;\n          fp.doNorms = fi.isIndexed && !fi.omitNorms;\n\n          if (numFieldData == fieldDataArray.length) {\n            int newSize = fieldDataArray.length*2;\n            FieldData newArray[] = new FieldData[newSize];\n            System.arraycopy(fieldDataArray, 0, newArray, 0, numFieldData);\n            fieldDataArray = newArray;\n\n          }\n          fieldDataArray[numFieldData++] = fp;\n        }\n\n        if (field.isTermVectorStored()) {\n          if (!fp.doVectors && numVectorFields++ == vectorFieldPointers.length) {\n            final int newSize = (int) (numVectorFields*1.5);\n            vectorFieldPointers = new long[newSize];\n            vectorFieldNumbers = new int[newSize];\n          }\n          fp.doVectors = true;\n          docHasVectors = true;\n\n          fp.doVectorPositions |= field.isStorePositionWithTermVector();\n          fp.doVectorOffsets |= field.isStoreOffsetWithTermVector();\n        }\n\n        if (fp.fieldCount == fp.docFields.length) {\n          Fieldable[] newArray = new Fieldable[fp.docFields.length*2];\n          System.arraycopy(fp.docFields, 0, newArray, 0, fp.docFields.length);\n          fp.docFields = newArray;\n        }\n\n        // Lazily allocate arrays for postings:\n        if (field.isIndexed() && fp.postingsHash == null)\n          fp.initPostingArrays();\n\n        fp.docFields[fp.fieldCount++] = field;\n      }\n\n      // Maybe init the local & global fieldsWriter\n      if (localFieldsWriter == null) {\n        if (fieldsWriter == null) {\n          assert docStoreSegment == null;\n          assert segment != null;\n          docStoreSegment = segment;\n          // If we hit an exception while init'ing the\n          // fieldsWriter, we must abort this segment\n          // because those files will be in an unknown\n          // state:\n          try {\n            fieldsWriter = new FieldsWriter(directory, docStoreSegment, fieldInfos);\n          } catch (Throwable t) {\n            throw new AbortException(t, DocumentsWriter.this);\n          }\n          files = null;\n        }\n        localFieldsWriter = new FieldsWriter(null, fdtLocal, fieldInfos);\n      }\n\n      // First time we see a doc that has field(s) with\n      // stored vectors, we init our tvx writer\n      if (docHasVectors) {\n        if (tvx == null) {\n          assert docStoreSegment != null;\n          // If we hit an exception while init'ing the term\n          // vector output files, we must abort this segment\n          // because those files will be in an unknown\n          // state:\n          try {\n            tvx = directory.createOutput(docStoreSegment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION);\n            tvx.writeInt(TermVectorsReader.FORMAT_VERSION2);\n            tvd = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);\n            tvd.writeInt(TermVectorsReader.FORMAT_VERSION2);\n            tvf = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_FIELDS_EXTENSION);\n            tvf.writeInt(TermVectorsReader.FORMAT_VERSION2);\n\n            // We must \"catch up\" for all docIDs that had no\n            // vectors before this one\n            for(int i=0;i<docID;i++) {\n              tvx.writeLong(0);\n              tvx.writeLong(0);\n            }\n          } catch (Throwable t) {\n            throw new AbortException(t, DocumentsWriter.this);\n          }\n          files = null;\n        }\n        numVectorFields = 0;\n      }\n    }\n\n","sourceOld":"    /** Initializes shared state for this new document */\n    void init(Document doc, int docID) throws IOException, AbortException {\n\n      assert !isIdle;\n\n      this.docID = docID;\n      docBoost = doc.getBoost();\n      numStoredFields = 0;\n      numFieldData = 0;\n      numVectorFields = 0;\n      maxTermPrefix = null;\n\n      assert 0 == fdtLocal.length();\n      assert 0 == fdtLocal.getFilePointer();\n      assert 0 == tvfLocal.length();\n      assert 0 == tvfLocal.getFilePointer();\n      final int thisFieldGen = fieldGen++;\n\n      List docFields = doc.getFields();\n      final int numDocFields = docFields.size();\n      boolean docHasVectors = false;\n\n      // Absorb any new fields first seen in this document.\n      // Also absorb any changes to fields we had already\n      // seen before (eg suddenly turning on norms or\n      // vectors, etc.):\n\n      for(int i=0;i<numDocFields;i++) {\n        Fieldable field = (Fieldable) docFields.get(i);\n\n        FieldInfo fi = fieldInfos.add(field.name(), field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false);\n        if (fi.isIndexed && !fi.omitNorms) {\n          // Maybe grow our buffered norms\n          if (norms.length <= fi.number) {\n            int newSize = (int) ((1+fi.number)*1.25);\n            BufferedNorms[] newNorms = new BufferedNorms[newSize];\n            System.arraycopy(norms, 0, newNorms, 0, norms.length);\n            norms = newNorms;\n          }\n          \n          if (norms[fi.number] == null)\n            norms[fi.number] = new BufferedNorms();\n\n          hasNorms = true;\n        }\n\n        // Make sure we have a FieldData allocated\n        int hashPos = fi.name.hashCode() & fieldDataHashMask;\n        FieldData fp = fieldDataHash[hashPos];\n        while(fp != null && !fp.fieldInfo.name.equals(fi.name))\n          fp = fp.next;\n\n        if (fp == null) {\n\n          fp = new FieldData(fi);\n          fp.next = fieldDataHash[hashPos];\n          fieldDataHash[hashPos] = fp;\n\n          if (numAllFieldData == allFieldDataArray.length) {\n            int newSize = (int) (allFieldDataArray.length*1.5);\n            int newHashSize = fieldDataHash.length*2;\n\n            FieldData newArray[] = new FieldData[newSize];\n            FieldData newHashArray[] = new FieldData[newHashSize];\n            System.arraycopy(allFieldDataArray, 0, newArray, 0, numAllFieldData);\n\n            // Rehash\n            fieldDataHashMask = newSize-1;\n            for(int j=0;j<fieldDataHash.length;j++) {\n              FieldData fp0 = fieldDataHash[j];\n              while(fp0 != null) {\n                hashPos = fp0.fieldInfo.name.hashCode() & fieldDataHashMask;\n                FieldData nextFP0 = fp0.next;\n                fp0.next = newHashArray[hashPos];\n                newHashArray[hashPos] = fp0;\n                fp0 = nextFP0;\n              }\n            }\n\n            allFieldDataArray = newArray;\n            fieldDataHash = newHashArray;\n          }\n          allFieldDataArray[numAllFieldData++] = fp;\n        } else {\n          assert fp.fieldInfo == fi;\n        }\n\n        if (thisFieldGen != fp.lastGen) {\n\n          // First time we're seeing this field for this doc\n          fp.lastGen = thisFieldGen;\n          fp.fieldCount = 0;\n          fp.doVectors = fp.doVectorPositions = fp.doVectorOffsets = false;\n          fp.doNorms = fi.isIndexed && !fi.omitNorms;\n\n          if (numFieldData == fieldDataArray.length) {\n            int newSize = fieldDataArray.length*2;\n            FieldData newArray[] = new FieldData[newSize];\n            System.arraycopy(fieldDataArray, 0, newArray, 0, numFieldData);\n            fieldDataArray = newArray;\n\n          }\n          fieldDataArray[numFieldData++] = fp;\n        }\n\n        if (field.isTermVectorStored()) {\n          if (!fp.doVectors && numVectorFields++ == vectorFieldPointers.length) {\n            final int newSize = (int) (numVectorFields*1.5);\n            vectorFieldPointers = new long[newSize];\n            vectorFieldNumbers = new int[newSize];\n          }\n          fp.doVectors = true;\n          docHasVectors = true;\n\n          fp.doVectorPositions |= field.isStorePositionWithTermVector();\n          fp.doVectorOffsets |= field.isStoreOffsetWithTermVector();\n        }\n\n        if (fp.fieldCount == fp.docFields.length) {\n          Fieldable[] newArray = new Fieldable[fp.docFields.length*2];\n          System.arraycopy(fp.docFields, 0, newArray, 0, fp.docFields.length);\n          fp.docFields = newArray;\n        }\n\n        // Lazily allocate arrays for postings:\n        if (field.isIndexed() && fp.postingsHash == null)\n          fp.initPostingArrays();\n\n        fp.docFields[fp.fieldCount++] = field;\n      }\n\n      // Maybe init the local & global fieldsWriter\n      if (localFieldsWriter == null) {\n        if (fieldsWriter == null) {\n          assert docStoreSegment == null;\n          assert segment != null;\n          docStoreSegment = segment;\n          // If we hit an exception while init'ing the\n          // fieldsWriter, we must abort this segment\n          // because those files will be in an unknown\n          // state:\n          try {\n            fieldsWriter = new FieldsWriter(directory, docStoreSegment, fieldInfos);\n          } catch (Throwable t) {\n            throw new AbortException(t, DocumentsWriter.this);\n          }\n          files = null;\n        }\n        localFieldsWriter = new FieldsWriter(null, fdtLocal, fieldInfos);\n      }\n\n      // First time we see a doc that has field(s) with\n      // stored vectors, we init our tvx writer\n      if (docHasVectors) {\n        if (tvx == null) {\n          assert docStoreSegment != null;\n          // If we hit an exception while init'ing the term\n          // vector output files, we must abort this segment\n          // because those files will be in an unknown\n          // state:\n          try {\n            tvx = directory.createOutput(docStoreSegment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION);\n            tvx.writeInt(TermVectorsReader.FORMAT_VERSION);\n            tvd = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);\n            tvd.writeInt(TermVectorsReader.FORMAT_VERSION);\n            tvf = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_FIELDS_EXTENSION);\n            tvf.writeInt(TermVectorsReader.FORMAT_VERSION);\n\n            // We must \"catch up\" for all docIDs that had no\n            // vectors before this one\n            for(int i=0;i<docID;i++)\n              tvx.writeLong(0);\n          } catch (Throwable t) {\n            throw new AbortException(t, DocumentsWriter.this);\n          }\n          files = null;\n        }\n\n        numVectorFields = 0;\n      }\n    }\n\n","bugFix":null,"bugIntro":["7439af33438ca32496b7ce4a341617a480b7137a"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7439af33438ca32496b7ce4a341617a480b7137a","date":1202418816,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#init(Document,int).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#init(Document,int).mjava","sourceNew":"    /** Initializes shared state for this new document */\n    void init(Document doc, int docID) throws IOException, AbortException {\n\n      assert !isIdle;\n\n      this.docID = docID;\n      docBoost = doc.getBoost();\n      numStoredFields = 0;\n      numFieldData = 0;\n      numVectorFields = 0;\n      maxTermPrefix = null;\n\n      assert 0 == fdtLocal.length();\n      assert 0 == fdtLocal.getFilePointer();\n      assert 0 == tvfLocal.length();\n      assert 0 == tvfLocal.getFilePointer();\n      final int thisFieldGen = fieldGen++;\n\n      List docFields = doc.getFields();\n      final int numDocFields = docFields.size();\n      boolean docHasVectors = false;\n\n      // Absorb any new fields first seen in this document.\n      // Also absorb any changes to fields we had already\n      // seen before (eg suddenly turning on norms or\n      // vectors, etc.):\n\n      for(int i=0;i<numDocFields;i++) {\n        Fieldable field = (Fieldable) docFields.get(i);\n\n        FieldInfo fi = fieldInfos.add(field.name(), field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false);\n        if (fi.isIndexed && !fi.omitNorms) {\n          // Maybe grow our buffered norms\n          if (norms.length <= fi.number) {\n            int newSize = (int) ((1+fi.number)*1.25);\n            BufferedNorms[] newNorms = new BufferedNorms[newSize];\n            System.arraycopy(norms, 0, newNorms, 0, norms.length);\n            norms = newNorms;\n          }\n          \n          if (norms[fi.number] == null)\n            norms[fi.number] = new BufferedNorms();\n\n          hasNorms = true;\n        }\n\n        // Make sure we have a FieldData allocated\n        int hashPos = fi.name.hashCode() & fieldDataHashMask;\n        FieldData fp = fieldDataHash[hashPos];\n        while(fp != null && !fp.fieldInfo.name.equals(fi.name))\n          fp = fp.next;\n\n        if (fp == null) {\n\n          fp = new FieldData(fi);\n          fp.next = fieldDataHash[hashPos];\n          fieldDataHash[hashPos] = fp;\n\n          if (numAllFieldData == allFieldDataArray.length) {\n            int newSize = (int) (allFieldDataArray.length*1.5);\n            int newHashSize = fieldDataHash.length*2;\n\n            FieldData newArray[] = new FieldData[newSize];\n            FieldData newHashArray[] = new FieldData[newHashSize];\n            System.arraycopy(allFieldDataArray, 0, newArray, 0, numAllFieldData);\n\n            // Rehash\n            fieldDataHashMask = newSize-1;\n            for(int j=0;j<fieldDataHash.length;j++) {\n              FieldData fp0 = fieldDataHash[j];\n              while(fp0 != null) {\n                hashPos = fp0.fieldInfo.name.hashCode() & fieldDataHashMask;\n                FieldData nextFP0 = fp0.next;\n                fp0.next = newHashArray[hashPos];\n                newHashArray[hashPos] = fp0;\n                fp0 = nextFP0;\n              }\n            }\n\n            allFieldDataArray = newArray;\n            fieldDataHash = newHashArray;\n          }\n          allFieldDataArray[numAllFieldData++] = fp;\n        } else {\n          assert fp.fieldInfo == fi;\n        }\n\n        if (thisFieldGen != fp.lastGen) {\n\n          // First time we're seeing this field for this doc\n          fp.lastGen = thisFieldGen;\n          fp.fieldCount = 0;\n          fp.doVectors = fp.doVectorPositions = fp.doVectorOffsets = false;\n          fp.doNorms = fi.isIndexed && !fi.omitNorms;\n\n          if (numFieldData == fieldDataArray.length) {\n            int newSize = fieldDataArray.length*2;\n            FieldData newArray[] = new FieldData[newSize];\n            System.arraycopy(fieldDataArray, 0, newArray, 0, numFieldData);\n            fieldDataArray = newArray;\n\n          }\n          fieldDataArray[numFieldData++] = fp;\n        }\n\n        if (field.isTermVectorStored()) {\n          if (!fp.doVectors && numVectorFields++ == vectorFieldPointers.length) {\n            final int newSize = (int) (numVectorFields*1.5);\n            vectorFieldPointers = new long[newSize];\n            vectorFieldNumbers = new int[newSize];\n          }\n          fp.doVectors = true;\n          docHasVectors = true;\n\n          fp.doVectorPositions |= field.isStorePositionWithTermVector();\n          fp.doVectorOffsets |= field.isStoreOffsetWithTermVector();\n        }\n\n        if (fp.fieldCount == fp.docFields.length) {\n          Fieldable[] newArray = new Fieldable[fp.docFields.length*2];\n          System.arraycopy(fp.docFields, 0, newArray, 0, fp.docFields.length);\n          fp.docFields = newArray;\n        }\n\n        // Lazily allocate arrays for postings:\n        if (field.isIndexed() && fp.postingsHash == null)\n          fp.initPostingArrays();\n\n        fp.docFields[fp.fieldCount++] = field;\n      }\n\n      // Maybe init the local & global fieldsWriter\n      if (localFieldsWriter == null) {\n        if (fieldsWriter == null) {\n          assert docStoreSegment == null;\n          assert segment != null;\n          docStoreSegment = segment;\n          // If we hit an exception while init'ing the\n          // fieldsWriter, we must abort this segment\n          // because those files will be in an unknown\n          // state:\n          try {\n            fieldsWriter = new FieldsWriter(directory, docStoreSegment, fieldInfos);\n          } catch (Throwable t) {\n            throw new AbortException(t, DocumentsWriter.this);\n          }\n          files = null;\n        }\n        localFieldsWriter = new FieldsWriter(null, fdtLocal, fieldInfos);\n      }\n\n      // First time we see a doc that has field(s) with\n      // stored vectors, we init our tvx writer\n      if (docHasVectors) {\n        if (tvx == null) {\n          assert docStoreSegment != null;\n          // If we hit an exception while init'ing the term\n          // vector output files, we must abort this segment\n          // because those files will be in an unknown\n          // state:\n          try {\n            tvx = directory.createOutput(docStoreSegment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION);\n            tvx.writeInt(TermVectorsReader.FORMAT_VERSION2);\n            tvd = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);\n            tvd.writeInt(TermVectorsReader.FORMAT_VERSION2);\n            tvf = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_FIELDS_EXTENSION);\n            tvf.writeInt(TermVectorsReader.FORMAT_VERSION2);\n\n            // We must \"catch up\" for all docs before us\n            // that had no vectors:\n            final long tvdPos = tvd.getFilePointer();\n            tvd.writeVInt(0);\n            for(int i=0;i<numDocsInStore-1;i++) {\n              tvx.writeLong(tvdPos);\n              tvx.writeLong(0);\n            }\n          } catch (Throwable t) {\n            throw new AbortException(t, DocumentsWriter.this);\n          }\n          files = null;\n        }\n        numVectorFields = 0;\n      }\n    }\n\n","sourceOld":"    /** Initializes shared state for this new document */\n    void init(Document doc, int docID) throws IOException, AbortException {\n\n      assert !isIdle;\n\n      this.docID = docID;\n      docBoost = doc.getBoost();\n      numStoredFields = 0;\n      numFieldData = 0;\n      numVectorFields = 0;\n      maxTermPrefix = null;\n\n      assert 0 == fdtLocal.length();\n      assert 0 == fdtLocal.getFilePointer();\n      assert 0 == tvfLocal.length();\n      assert 0 == tvfLocal.getFilePointer();\n      final int thisFieldGen = fieldGen++;\n\n      List docFields = doc.getFields();\n      final int numDocFields = docFields.size();\n      boolean docHasVectors = false;\n\n      // Absorb any new fields first seen in this document.\n      // Also absorb any changes to fields we had already\n      // seen before (eg suddenly turning on norms or\n      // vectors, etc.):\n\n      for(int i=0;i<numDocFields;i++) {\n        Fieldable field = (Fieldable) docFields.get(i);\n\n        FieldInfo fi = fieldInfos.add(field.name(), field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false);\n        if (fi.isIndexed && !fi.omitNorms) {\n          // Maybe grow our buffered norms\n          if (norms.length <= fi.number) {\n            int newSize = (int) ((1+fi.number)*1.25);\n            BufferedNorms[] newNorms = new BufferedNorms[newSize];\n            System.arraycopy(norms, 0, newNorms, 0, norms.length);\n            norms = newNorms;\n          }\n          \n          if (norms[fi.number] == null)\n            norms[fi.number] = new BufferedNorms();\n\n          hasNorms = true;\n        }\n\n        // Make sure we have a FieldData allocated\n        int hashPos = fi.name.hashCode() & fieldDataHashMask;\n        FieldData fp = fieldDataHash[hashPos];\n        while(fp != null && !fp.fieldInfo.name.equals(fi.name))\n          fp = fp.next;\n\n        if (fp == null) {\n\n          fp = new FieldData(fi);\n          fp.next = fieldDataHash[hashPos];\n          fieldDataHash[hashPos] = fp;\n\n          if (numAllFieldData == allFieldDataArray.length) {\n            int newSize = (int) (allFieldDataArray.length*1.5);\n            int newHashSize = fieldDataHash.length*2;\n\n            FieldData newArray[] = new FieldData[newSize];\n            FieldData newHashArray[] = new FieldData[newHashSize];\n            System.arraycopy(allFieldDataArray, 0, newArray, 0, numAllFieldData);\n\n            // Rehash\n            fieldDataHashMask = newSize-1;\n            for(int j=0;j<fieldDataHash.length;j++) {\n              FieldData fp0 = fieldDataHash[j];\n              while(fp0 != null) {\n                hashPos = fp0.fieldInfo.name.hashCode() & fieldDataHashMask;\n                FieldData nextFP0 = fp0.next;\n                fp0.next = newHashArray[hashPos];\n                newHashArray[hashPos] = fp0;\n                fp0 = nextFP0;\n              }\n            }\n\n            allFieldDataArray = newArray;\n            fieldDataHash = newHashArray;\n          }\n          allFieldDataArray[numAllFieldData++] = fp;\n        } else {\n          assert fp.fieldInfo == fi;\n        }\n\n        if (thisFieldGen != fp.lastGen) {\n\n          // First time we're seeing this field for this doc\n          fp.lastGen = thisFieldGen;\n          fp.fieldCount = 0;\n          fp.doVectors = fp.doVectorPositions = fp.doVectorOffsets = false;\n          fp.doNorms = fi.isIndexed && !fi.omitNorms;\n\n          if (numFieldData == fieldDataArray.length) {\n            int newSize = fieldDataArray.length*2;\n            FieldData newArray[] = new FieldData[newSize];\n            System.arraycopy(fieldDataArray, 0, newArray, 0, numFieldData);\n            fieldDataArray = newArray;\n\n          }\n          fieldDataArray[numFieldData++] = fp;\n        }\n\n        if (field.isTermVectorStored()) {\n          if (!fp.doVectors && numVectorFields++ == vectorFieldPointers.length) {\n            final int newSize = (int) (numVectorFields*1.5);\n            vectorFieldPointers = new long[newSize];\n            vectorFieldNumbers = new int[newSize];\n          }\n          fp.doVectors = true;\n          docHasVectors = true;\n\n          fp.doVectorPositions |= field.isStorePositionWithTermVector();\n          fp.doVectorOffsets |= field.isStoreOffsetWithTermVector();\n        }\n\n        if (fp.fieldCount == fp.docFields.length) {\n          Fieldable[] newArray = new Fieldable[fp.docFields.length*2];\n          System.arraycopy(fp.docFields, 0, newArray, 0, fp.docFields.length);\n          fp.docFields = newArray;\n        }\n\n        // Lazily allocate arrays for postings:\n        if (field.isIndexed() && fp.postingsHash == null)\n          fp.initPostingArrays();\n\n        fp.docFields[fp.fieldCount++] = field;\n      }\n\n      // Maybe init the local & global fieldsWriter\n      if (localFieldsWriter == null) {\n        if (fieldsWriter == null) {\n          assert docStoreSegment == null;\n          assert segment != null;\n          docStoreSegment = segment;\n          // If we hit an exception while init'ing the\n          // fieldsWriter, we must abort this segment\n          // because those files will be in an unknown\n          // state:\n          try {\n            fieldsWriter = new FieldsWriter(directory, docStoreSegment, fieldInfos);\n          } catch (Throwable t) {\n            throw new AbortException(t, DocumentsWriter.this);\n          }\n          files = null;\n        }\n        localFieldsWriter = new FieldsWriter(null, fdtLocal, fieldInfos);\n      }\n\n      // First time we see a doc that has field(s) with\n      // stored vectors, we init our tvx writer\n      if (docHasVectors) {\n        if (tvx == null) {\n          assert docStoreSegment != null;\n          // If we hit an exception while init'ing the term\n          // vector output files, we must abort this segment\n          // because those files will be in an unknown\n          // state:\n          try {\n            tvx = directory.createOutput(docStoreSegment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION);\n            tvx.writeInt(TermVectorsReader.FORMAT_VERSION2);\n            tvd = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);\n            tvd.writeInt(TermVectorsReader.FORMAT_VERSION2);\n            tvf = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_FIELDS_EXTENSION);\n            tvf.writeInt(TermVectorsReader.FORMAT_VERSION2);\n\n            // We must \"catch up\" for all docIDs that had no\n            // vectors before this one\n            for(int i=0;i<docID;i++) {\n              tvx.writeLong(0);\n              tvx.writeLong(0);\n            }\n          } catch (Throwable t) {\n            throw new AbortException(t, DocumentsWriter.this);\n          }\n          files = null;\n        }\n        numVectorFields = 0;\n      }\n    }\n\n","bugFix":["83bbb041887bbef07b8a98d08a0e1713ce137039","b3d08461c77d39c25ea6ff0cd05b32f948fa2a33"],"bugIntro":["49adbad5232116eb2448ea8166464e6a68bca007"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"49adbad5232116eb2448ea8166464e6a68bca007","date":1202851885,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#init(Document,int).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#init(Document,int).mjava","sourceNew":"    /** Initializes shared state for this new document */\n    void init(Document doc, int docID) throws IOException, AbortException {\n\n      assert !isIdle;\n\n      this.docID = docID;\n      docBoost = doc.getBoost();\n      numStoredFields = 0;\n      numFieldData = 0;\n      numVectorFields = 0;\n      maxTermPrefix = null;\n\n      assert 0 == fdtLocal.length();\n      assert 0 == fdtLocal.getFilePointer();\n      assert 0 == tvfLocal.length();\n      assert 0 == tvfLocal.getFilePointer();\n      final int thisFieldGen = fieldGen++;\n\n      List docFields = doc.getFields();\n      final int numDocFields = docFields.size();\n      boolean docHasVectors = false;\n\n      // Absorb any new fields first seen in this document.\n      // Also absorb any changes to fields we had already\n      // seen before (eg suddenly turning on norms or\n      // vectors, etc.):\n\n      for(int i=0;i<numDocFields;i++) {\n        Fieldable field = (Fieldable) docFields.get(i);\n\n        FieldInfo fi = fieldInfos.add(field.name(), field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false);\n        if (fi.isIndexed && !fi.omitNorms) {\n          // Maybe grow our buffered norms\n          if (norms.length <= fi.number) {\n            int newSize = (int) ((1+fi.number)*1.25);\n            BufferedNorms[] newNorms = new BufferedNorms[newSize];\n            System.arraycopy(norms, 0, newNorms, 0, norms.length);\n            norms = newNorms;\n          }\n          \n          if (norms[fi.number] == null)\n            norms[fi.number] = new BufferedNorms();\n\n          hasNorms = true;\n        }\n\n        // Make sure we have a FieldData allocated\n        int hashPos = fi.name.hashCode() & fieldDataHashMask;\n        FieldData fp = fieldDataHash[hashPos];\n        while(fp != null && !fp.fieldInfo.name.equals(fi.name))\n          fp = fp.next;\n\n        if (fp == null) {\n\n          fp = new FieldData(fi);\n          fp.next = fieldDataHash[hashPos];\n          fieldDataHash[hashPos] = fp;\n\n          if (numAllFieldData == allFieldDataArray.length) {\n            int newSize = (int) (allFieldDataArray.length*1.5);\n            int newHashSize = fieldDataHash.length*2;\n\n            FieldData newArray[] = new FieldData[newSize];\n            FieldData newHashArray[] = new FieldData[newHashSize];\n            System.arraycopy(allFieldDataArray, 0, newArray, 0, numAllFieldData);\n\n            // Rehash\n            fieldDataHashMask = newSize-1;\n            for(int j=0;j<fieldDataHash.length;j++) {\n              FieldData fp0 = fieldDataHash[j];\n              while(fp0 != null) {\n                hashPos = fp0.fieldInfo.name.hashCode() & fieldDataHashMask;\n                FieldData nextFP0 = fp0.next;\n                fp0.next = newHashArray[hashPos];\n                newHashArray[hashPos] = fp0;\n                fp0 = nextFP0;\n              }\n            }\n\n            allFieldDataArray = newArray;\n            fieldDataHash = newHashArray;\n          }\n          allFieldDataArray[numAllFieldData++] = fp;\n        } else {\n          assert fp.fieldInfo == fi;\n        }\n\n        if (thisFieldGen != fp.lastGen) {\n\n          // First time we're seeing this field for this doc\n          fp.lastGen = thisFieldGen;\n          fp.fieldCount = 0;\n          fp.doVectors = fp.doVectorPositions = fp.doVectorOffsets = false;\n          fp.doNorms = fi.isIndexed && !fi.omitNorms;\n\n          if (numFieldData == fieldDataArray.length) {\n            int newSize = fieldDataArray.length*2;\n            FieldData newArray[] = new FieldData[newSize];\n            System.arraycopy(fieldDataArray, 0, newArray, 0, numFieldData);\n            fieldDataArray = newArray;\n\n          }\n          fieldDataArray[numFieldData++] = fp;\n        }\n\n        if (field.isTermVectorStored()) {\n          if (!fp.doVectors && numVectorFields++ == vectorFieldPointers.length) {\n            final int newSize = (int) (numVectorFields*1.5);\n            vectorFieldPointers = new long[newSize];\n            vectorFieldNumbers = new int[newSize];\n          }\n          fp.doVectors = true;\n          docHasVectors = true;\n\n          fp.doVectorPositions |= field.isStorePositionWithTermVector();\n          fp.doVectorOffsets |= field.isStoreOffsetWithTermVector();\n        }\n\n        if (fp.fieldCount == fp.docFields.length) {\n          Fieldable[] newArray = new Fieldable[fp.docFields.length*2];\n          System.arraycopy(fp.docFields, 0, newArray, 0, fp.docFields.length);\n          fp.docFields = newArray;\n        }\n\n        // Lazily allocate arrays for postings:\n        if (field.isIndexed() && fp.postingsHash == null)\n          fp.initPostingArrays();\n\n        fp.docFields[fp.fieldCount++] = field;\n      }\n\n      // Maybe init the local & global fieldsWriter\n      if (localFieldsWriter == null) {\n        if (fieldsWriter == null) {\n          assert docStoreSegment == null;\n          assert segment != null;\n          docStoreSegment = segment;\n          // If we hit an exception while init'ing the\n          // fieldsWriter, we must abort this segment\n          // because those files will be in an unknown\n          // state:\n          try {\n            fieldsWriter = new FieldsWriter(directory, docStoreSegment, fieldInfos);\n          } catch (Throwable t) {\n            throw new AbortException(t, DocumentsWriter.this);\n          }\n          files = null;\n        }\n        localFieldsWriter = new FieldsWriter(null, fdtLocal, fieldInfos);\n      }\n\n      // First time we see a doc that has field(s) with\n      // stored vectors, we init our tvx writer\n      if (docHasVectors) {\n        if (tvx == null) {\n          assert docStoreSegment != null;\n          // If we hit an exception while init'ing the term\n          // vector output files, we must abort this segment\n          // because those files will be in an unknown\n          // state:\n          try {\n            tvx = directory.createOutput(docStoreSegment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION);\n            tvx.writeInt(TermVectorsReader.FORMAT_VERSION2);\n            tvd = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);\n            tvd.writeInt(TermVectorsReader.FORMAT_VERSION2);\n            tvf = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_FIELDS_EXTENSION);\n            tvf.writeInt(TermVectorsReader.FORMAT_VERSION2);\n\n            // We must \"catch up\" for all docs before us\n            // that had no vectors:\n            for(int i=0;i<numDocsInStore;i++) {\n              tvx.writeLong(tvd.getFilePointer());\n              tvd.writeVInt(0);\n              tvx.writeLong(0);\n            }\n          } catch (Throwable t) {\n            throw new AbortException(t, DocumentsWriter.this);\n          }\n          files = null;\n        }\n        numVectorFields = 0;\n      }\n    }\n\n","sourceOld":"    /** Initializes shared state for this new document */\n    void init(Document doc, int docID) throws IOException, AbortException {\n\n      assert !isIdle;\n\n      this.docID = docID;\n      docBoost = doc.getBoost();\n      numStoredFields = 0;\n      numFieldData = 0;\n      numVectorFields = 0;\n      maxTermPrefix = null;\n\n      assert 0 == fdtLocal.length();\n      assert 0 == fdtLocal.getFilePointer();\n      assert 0 == tvfLocal.length();\n      assert 0 == tvfLocal.getFilePointer();\n      final int thisFieldGen = fieldGen++;\n\n      List docFields = doc.getFields();\n      final int numDocFields = docFields.size();\n      boolean docHasVectors = false;\n\n      // Absorb any new fields first seen in this document.\n      // Also absorb any changes to fields we had already\n      // seen before (eg suddenly turning on norms or\n      // vectors, etc.):\n\n      for(int i=0;i<numDocFields;i++) {\n        Fieldable field = (Fieldable) docFields.get(i);\n\n        FieldInfo fi = fieldInfos.add(field.name(), field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false);\n        if (fi.isIndexed && !fi.omitNorms) {\n          // Maybe grow our buffered norms\n          if (norms.length <= fi.number) {\n            int newSize = (int) ((1+fi.number)*1.25);\n            BufferedNorms[] newNorms = new BufferedNorms[newSize];\n            System.arraycopy(norms, 0, newNorms, 0, norms.length);\n            norms = newNorms;\n          }\n          \n          if (norms[fi.number] == null)\n            norms[fi.number] = new BufferedNorms();\n\n          hasNorms = true;\n        }\n\n        // Make sure we have a FieldData allocated\n        int hashPos = fi.name.hashCode() & fieldDataHashMask;\n        FieldData fp = fieldDataHash[hashPos];\n        while(fp != null && !fp.fieldInfo.name.equals(fi.name))\n          fp = fp.next;\n\n        if (fp == null) {\n\n          fp = new FieldData(fi);\n          fp.next = fieldDataHash[hashPos];\n          fieldDataHash[hashPos] = fp;\n\n          if (numAllFieldData == allFieldDataArray.length) {\n            int newSize = (int) (allFieldDataArray.length*1.5);\n            int newHashSize = fieldDataHash.length*2;\n\n            FieldData newArray[] = new FieldData[newSize];\n            FieldData newHashArray[] = new FieldData[newHashSize];\n            System.arraycopy(allFieldDataArray, 0, newArray, 0, numAllFieldData);\n\n            // Rehash\n            fieldDataHashMask = newSize-1;\n            for(int j=0;j<fieldDataHash.length;j++) {\n              FieldData fp0 = fieldDataHash[j];\n              while(fp0 != null) {\n                hashPos = fp0.fieldInfo.name.hashCode() & fieldDataHashMask;\n                FieldData nextFP0 = fp0.next;\n                fp0.next = newHashArray[hashPos];\n                newHashArray[hashPos] = fp0;\n                fp0 = nextFP0;\n              }\n            }\n\n            allFieldDataArray = newArray;\n            fieldDataHash = newHashArray;\n          }\n          allFieldDataArray[numAllFieldData++] = fp;\n        } else {\n          assert fp.fieldInfo == fi;\n        }\n\n        if (thisFieldGen != fp.lastGen) {\n\n          // First time we're seeing this field for this doc\n          fp.lastGen = thisFieldGen;\n          fp.fieldCount = 0;\n          fp.doVectors = fp.doVectorPositions = fp.doVectorOffsets = false;\n          fp.doNorms = fi.isIndexed && !fi.omitNorms;\n\n          if (numFieldData == fieldDataArray.length) {\n            int newSize = fieldDataArray.length*2;\n            FieldData newArray[] = new FieldData[newSize];\n            System.arraycopy(fieldDataArray, 0, newArray, 0, numFieldData);\n            fieldDataArray = newArray;\n\n          }\n          fieldDataArray[numFieldData++] = fp;\n        }\n\n        if (field.isTermVectorStored()) {\n          if (!fp.doVectors && numVectorFields++ == vectorFieldPointers.length) {\n            final int newSize = (int) (numVectorFields*1.5);\n            vectorFieldPointers = new long[newSize];\n            vectorFieldNumbers = new int[newSize];\n          }\n          fp.doVectors = true;\n          docHasVectors = true;\n\n          fp.doVectorPositions |= field.isStorePositionWithTermVector();\n          fp.doVectorOffsets |= field.isStoreOffsetWithTermVector();\n        }\n\n        if (fp.fieldCount == fp.docFields.length) {\n          Fieldable[] newArray = new Fieldable[fp.docFields.length*2];\n          System.arraycopy(fp.docFields, 0, newArray, 0, fp.docFields.length);\n          fp.docFields = newArray;\n        }\n\n        // Lazily allocate arrays for postings:\n        if (field.isIndexed() && fp.postingsHash == null)\n          fp.initPostingArrays();\n\n        fp.docFields[fp.fieldCount++] = field;\n      }\n\n      // Maybe init the local & global fieldsWriter\n      if (localFieldsWriter == null) {\n        if (fieldsWriter == null) {\n          assert docStoreSegment == null;\n          assert segment != null;\n          docStoreSegment = segment;\n          // If we hit an exception while init'ing the\n          // fieldsWriter, we must abort this segment\n          // because those files will be in an unknown\n          // state:\n          try {\n            fieldsWriter = new FieldsWriter(directory, docStoreSegment, fieldInfos);\n          } catch (Throwable t) {\n            throw new AbortException(t, DocumentsWriter.this);\n          }\n          files = null;\n        }\n        localFieldsWriter = new FieldsWriter(null, fdtLocal, fieldInfos);\n      }\n\n      // First time we see a doc that has field(s) with\n      // stored vectors, we init our tvx writer\n      if (docHasVectors) {\n        if (tvx == null) {\n          assert docStoreSegment != null;\n          // If we hit an exception while init'ing the term\n          // vector output files, we must abort this segment\n          // because those files will be in an unknown\n          // state:\n          try {\n            tvx = directory.createOutput(docStoreSegment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION);\n            tvx.writeInt(TermVectorsReader.FORMAT_VERSION2);\n            tvd = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);\n            tvd.writeInt(TermVectorsReader.FORMAT_VERSION2);\n            tvf = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_FIELDS_EXTENSION);\n            tvf.writeInt(TermVectorsReader.FORMAT_VERSION2);\n\n            // We must \"catch up\" for all docs before us\n            // that had no vectors:\n            final long tvdPos = tvd.getFilePointer();\n            tvd.writeVInt(0);\n            for(int i=0;i<numDocsInStore-1;i++) {\n              tvx.writeLong(tvdPos);\n              tvx.writeLong(0);\n            }\n          } catch (Throwable t) {\n            throw new AbortException(t, DocumentsWriter.this);\n          }\n          files = null;\n        }\n        numVectorFields = 0;\n      }\n    }\n\n","bugFix":["7439af33438ca32496b7ce4a341617a480b7137a"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ac1242ce6d99813874fddfe4ca5f57779beddb22","date":1204500247,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#init(Document,int).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#init(Document,int).mjava","sourceNew":"    /** Initializes shared state for this new document */\n    void init(Document doc, int docID) throws IOException, AbortException {\n\n      assert !isIdle;\n      assert writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n      this.docID = docID;\n      docBoost = doc.getBoost();\n      numStoredFields = 0;\n      numFieldData = 0;\n      numVectorFields = 0;\n      maxTermPrefix = null;\n\n      assert 0 == fdtLocal.length();\n      assert 0 == fdtLocal.getFilePointer();\n      assert 0 == tvfLocal.length();\n      assert 0 == tvfLocal.getFilePointer();\n      final int thisFieldGen = fieldGen++;\n\n      List docFields = doc.getFields();\n      final int numDocFields = docFields.size();\n      boolean docHasVectors = false;\n\n      // Absorb any new fields first seen in this document.\n      // Also absorb any changes to fields we had already\n      // seen before (eg suddenly turning on norms or\n      // vectors, etc.):\n\n      for(int i=0;i<numDocFields;i++) {\n        Fieldable field = (Fieldable) docFields.get(i);\n\n        FieldInfo fi = fieldInfos.add(field.name(), field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false);\n        if (fi.isIndexed && !fi.omitNorms) {\n          // Maybe grow our buffered norms\n          if (norms.length <= fi.number) {\n            int newSize = (int) ((1+fi.number)*1.25);\n            BufferedNorms[] newNorms = new BufferedNorms[newSize];\n            System.arraycopy(norms, 0, newNorms, 0, norms.length);\n            norms = newNorms;\n          }\n          \n          if (norms[fi.number] == null)\n            norms[fi.number] = new BufferedNorms();\n\n          hasNorms = true;\n        }\n\n        // Make sure we have a FieldData allocated\n        int hashPos = fi.name.hashCode() & fieldDataHashMask;\n        FieldData fp = fieldDataHash[hashPos];\n        while(fp != null && !fp.fieldInfo.name.equals(fi.name))\n          fp = fp.next;\n\n        if (fp == null) {\n\n          fp = new FieldData(fi);\n          fp.next = fieldDataHash[hashPos];\n          fieldDataHash[hashPos] = fp;\n\n          if (numAllFieldData == allFieldDataArray.length) {\n            int newSize = (int) (allFieldDataArray.length*1.5);\n            int newHashSize = fieldDataHash.length*2;\n\n            FieldData newArray[] = new FieldData[newSize];\n            FieldData newHashArray[] = new FieldData[newHashSize];\n            System.arraycopy(allFieldDataArray, 0, newArray, 0, numAllFieldData);\n\n            // Rehash\n            fieldDataHashMask = newSize-1;\n            for(int j=0;j<fieldDataHash.length;j++) {\n              FieldData fp0 = fieldDataHash[j];\n              while(fp0 != null) {\n                hashPos = fp0.fieldInfo.name.hashCode() & fieldDataHashMask;\n                FieldData nextFP0 = fp0.next;\n                fp0.next = newHashArray[hashPos];\n                newHashArray[hashPos] = fp0;\n                fp0 = nextFP0;\n              }\n            }\n\n            allFieldDataArray = newArray;\n            fieldDataHash = newHashArray;\n          }\n          allFieldDataArray[numAllFieldData++] = fp;\n        } else {\n          assert fp.fieldInfo == fi;\n        }\n\n        if (thisFieldGen != fp.lastGen) {\n\n          // First time we're seeing this field for this doc\n          fp.lastGen = thisFieldGen;\n          fp.fieldCount = 0;\n          fp.doVectors = fp.doVectorPositions = fp.doVectorOffsets = false;\n          fp.doNorms = fi.isIndexed && !fi.omitNorms;\n\n          if (numFieldData == fieldDataArray.length) {\n            int newSize = fieldDataArray.length*2;\n            FieldData newArray[] = new FieldData[newSize];\n            System.arraycopy(fieldDataArray, 0, newArray, 0, numFieldData);\n            fieldDataArray = newArray;\n\n          }\n          fieldDataArray[numFieldData++] = fp;\n        }\n\n        if (field.isTermVectorStored()) {\n          if (!fp.doVectors && numVectorFields++ == vectorFieldPointers.length) {\n            final int newSize = (int) (numVectorFields*1.5);\n            vectorFieldPointers = new long[newSize];\n            vectorFieldNumbers = new int[newSize];\n          }\n          fp.doVectors = true;\n          docHasVectors = true;\n\n          fp.doVectorPositions |= field.isStorePositionWithTermVector();\n          fp.doVectorOffsets |= field.isStoreOffsetWithTermVector();\n        }\n\n        if (fp.fieldCount == fp.docFields.length) {\n          Fieldable[] newArray = new Fieldable[fp.docFields.length*2];\n          System.arraycopy(fp.docFields, 0, newArray, 0, fp.docFields.length);\n          fp.docFields = newArray;\n        }\n\n        // Lazily allocate arrays for postings:\n        if (field.isIndexed() && fp.postingsHash == null)\n          fp.initPostingArrays();\n\n        fp.docFields[fp.fieldCount++] = field;\n      }\n\n      // Maybe init the local & global fieldsWriter\n      if (localFieldsWriter == null) {\n        if (fieldsWriter == null) {\n          assert docStoreSegment == null;\n          assert segment != null;\n          docStoreSegment = segment;\n          // If we hit an exception while init'ing the\n          // fieldsWriter, we must abort this segment\n          // because those files will be in an unknown\n          // state:\n          try {\n            fieldsWriter = new FieldsWriter(directory, docStoreSegment, fieldInfos);\n          } catch (Throwable t) {\n            throw new AbortException(t, DocumentsWriter.this);\n          }\n          files = null;\n        }\n        localFieldsWriter = new FieldsWriter(null, fdtLocal, fieldInfos);\n      }\n\n      // First time we see a doc that has field(s) with\n      // stored vectors, we init our tvx writer\n      if (docHasVectors) {\n        if (tvx == null) {\n          assert docStoreSegment != null;\n          // If we hit an exception while init'ing the term\n          // vector output files, we must abort this segment\n          // because those files will be in an unknown\n          // state:\n          try {\n            tvx = directory.createOutput(docStoreSegment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION);\n            tvx.writeInt(TermVectorsReader.FORMAT_VERSION2);\n            tvd = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);\n            tvd.writeInt(TermVectorsReader.FORMAT_VERSION2);\n            tvf = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_FIELDS_EXTENSION);\n            tvf.writeInt(TermVectorsReader.FORMAT_VERSION2);\n\n            // We must \"catch up\" for all docs before us\n            // that had no vectors:\n            for(int i=0;i<numDocsInStore;i++) {\n              tvx.writeLong(tvd.getFilePointer());\n              tvd.writeVInt(0);\n              tvx.writeLong(0);\n            }\n          } catch (Throwable t) {\n            throw new AbortException(t, DocumentsWriter.this);\n          }\n          files = null;\n        }\n        numVectorFields = 0;\n      }\n    }\n\n","sourceOld":"    /** Initializes shared state for this new document */\n    void init(Document doc, int docID) throws IOException, AbortException {\n\n      assert !isIdle;\n\n      this.docID = docID;\n      docBoost = doc.getBoost();\n      numStoredFields = 0;\n      numFieldData = 0;\n      numVectorFields = 0;\n      maxTermPrefix = null;\n\n      assert 0 == fdtLocal.length();\n      assert 0 == fdtLocal.getFilePointer();\n      assert 0 == tvfLocal.length();\n      assert 0 == tvfLocal.getFilePointer();\n      final int thisFieldGen = fieldGen++;\n\n      List docFields = doc.getFields();\n      final int numDocFields = docFields.size();\n      boolean docHasVectors = false;\n\n      // Absorb any new fields first seen in this document.\n      // Also absorb any changes to fields we had already\n      // seen before (eg suddenly turning on norms or\n      // vectors, etc.):\n\n      for(int i=0;i<numDocFields;i++) {\n        Fieldable field = (Fieldable) docFields.get(i);\n\n        FieldInfo fi = fieldInfos.add(field.name(), field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false);\n        if (fi.isIndexed && !fi.omitNorms) {\n          // Maybe grow our buffered norms\n          if (norms.length <= fi.number) {\n            int newSize = (int) ((1+fi.number)*1.25);\n            BufferedNorms[] newNorms = new BufferedNorms[newSize];\n            System.arraycopy(norms, 0, newNorms, 0, norms.length);\n            norms = newNorms;\n          }\n          \n          if (norms[fi.number] == null)\n            norms[fi.number] = new BufferedNorms();\n\n          hasNorms = true;\n        }\n\n        // Make sure we have a FieldData allocated\n        int hashPos = fi.name.hashCode() & fieldDataHashMask;\n        FieldData fp = fieldDataHash[hashPos];\n        while(fp != null && !fp.fieldInfo.name.equals(fi.name))\n          fp = fp.next;\n\n        if (fp == null) {\n\n          fp = new FieldData(fi);\n          fp.next = fieldDataHash[hashPos];\n          fieldDataHash[hashPos] = fp;\n\n          if (numAllFieldData == allFieldDataArray.length) {\n            int newSize = (int) (allFieldDataArray.length*1.5);\n            int newHashSize = fieldDataHash.length*2;\n\n            FieldData newArray[] = new FieldData[newSize];\n            FieldData newHashArray[] = new FieldData[newHashSize];\n            System.arraycopy(allFieldDataArray, 0, newArray, 0, numAllFieldData);\n\n            // Rehash\n            fieldDataHashMask = newSize-1;\n            for(int j=0;j<fieldDataHash.length;j++) {\n              FieldData fp0 = fieldDataHash[j];\n              while(fp0 != null) {\n                hashPos = fp0.fieldInfo.name.hashCode() & fieldDataHashMask;\n                FieldData nextFP0 = fp0.next;\n                fp0.next = newHashArray[hashPos];\n                newHashArray[hashPos] = fp0;\n                fp0 = nextFP0;\n              }\n            }\n\n            allFieldDataArray = newArray;\n            fieldDataHash = newHashArray;\n          }\n          allFieldDataArray[numAllFieldData++] = fp;\n        } else {\n          assert fp.fieldInfo == fi;\n        }\n\n        if (thisFieldGen != fp.lastGen) {\n\n          // First time we're seeing this field for this doc\n          fp.lastGen = thisFieldGen;\n          fp.fieldCount = 0;\n          fp.doVectors = fp.doVectorPositions = fp.doVectorOffsets = false;\n          fp.doNorms = fi.isIndexed && !fi.omitNorms;\n\n          if (numFieldData == fieldDataArray.length) {\n            int newSize = fieldDataArray.length*2;\n            FieldData newArray[] = new FieldData[newSize];\n            System.arraycopy(fieldDataArray, 0, newArray, 0, numFieldData);\n            fieldDataArray = newArray;\n\n          }\n          fieldDataArray[numFieldData++] = fp;\n        }\n\n        if (field.isTermVectorStored()) {\n          if (!fp.doVectors && numVectorFields++ == vectorFieldPointers.length) {\n            final int newSize = (int) (numVectorFields*1.5);\n            vectorFieldPointers = new long[newSize];\n            vectorFieldNumbers = new int[newSize];\n          }\n          fp.doVectors = true;\n          docHasVectors = true;\n\n          fp.doVectorPositions |= field.isStorePositionWithTermVector();\n          fp.doVectorOffsets |= field.isStoreOffsetWithTermVector();\n        }\n\n        if (fp.fieldCount == fp.docFields.length) {\n          Fieldable[] newArray = new Fieldable[fp.docFields.length*2];\n          System.arraycopy(fp.docFields, 0, newArray, 0, fp.docFields.length);\n          fp.docFields = newArray;\n        }\n\n        // Lazily allocate arrays for postings:\n        if (field.isIndexed() && fp.postingsHash == null)\n          fp.initPostingArrays();\n\n        fp.docFields[fp.fieldCount++] = field;\n      }\n\n      // Maybe init the local & global fieldsWriter\n      if (localFieldsWriter == null) {\n        if (fieldsWriter == null) {\n          assert docStoreSegment == null;\n          assert segment != null;\n          docStoreSegment = segment;\n          // If we hit an exception while init'ing the\n          // fieldsWriter, we must abort this segment\n          // because those files will be in an unknown\n          // state:\n          try {\n            fieldsWriter = new FieldsWriter(directory, docStoreSegment, fieldInfos);\n          } catch (Throwable t) {\n            throw new AbortException(t, DocumentsWriter.this);\n          }\n          files = null;\n        }\n        localFieldsWriter = new FieldsWriter(null, fdtLocal, fieldInfos);\n      }\n\n      // First time we see a doc that has field(s) with\n      // stored vectors, we init our tvx writer\n      if (docHasVectors) {\n        if (tvx == null) {\n          assert docStoreSegment != null;\n          // If we hit an exception while init'ing the term\n          // vector output files, we must abort this segment\n          // because those files will be in an unknown\n          // state:\n          try {\n            tvx = directory.createOutput(docStoreSegment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION);\n            tvx.writeInt(TermVectorsReader.FORMAT_VERSION2);\n            tvd = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);\n            tvd.writeInt(TermVectorsReader.FORMAT_VERSION2);\n            tvf = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_FIELDS_EXTENSION);\n            tvf.writeInt(TermVectorsReader.FORMAT_VERSION2);\n\n            // We must \"catch up\" for all docs before us\n            // that had no vectors:\n            for(int i=0;i<numDocsInStore;i++) {\n              tvx.writeLong(tvd.getFilePointer());\n              tvd.writeVInt(0);\n              tvx.writeLong(0);\n            }\n          } catch (Throwable t) {\n            throw new AbortException(t, DocumentsWriter.this);\n          }\n          files = null;\n        }\n        numVectorFields = 0;\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5a0af3a442be522899177e5e11384a45a6784a3f","date":1205348952,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#init(Document,int).mjava","sourceNew":null,"sourceOld":"    /** Initializes shared state for this new document */\n    void init(Document doc, int docID) throws IOException, AbortException {\n\n      assert !isIdle;\n      assert writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n      this.docID = docID;\n      docBoost = doc.getBoost();\n      numStoredFields = 0;\n      numFieldData = 0;\n      numVectorFields = 0;\n      maxTermPrefix = null;\n\n      assert 0 == fdtLocal.length();\n      assert 0 == fdtLocal.getFilePointer();\n      assert 0 == tvfLocal.length();\n      assert 0 == tvfLocal.getFilePointer();\n      final int thisFieldGen = fieldGen++;\n\n      List docFields = doc.getFields();\n      final int numDocFields = docFields.size();\n      boolean docHasVectors = false;\n\n      // Absorb any new fields first seen in this document.\n      // Also absorb any changes to fields we had already\n      // seen before (eg suddenly turning on norms or\n      // vectors, etc.):\n\n      for(int i=0;i<numDocFields;i++) {\n        Fieldable field = (Fieldable) docFields.get(i);\n\n        FieldInfo fi = fieldInfos.add(field.name(), field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false);\n        if (fi.isIndexed && !fi.omitNorms) {\n          // Maybe grow our buffered norms\n          if (norms.length <= fi.number) {\n            int newSize = (int) ((1+fi.number)*1.25);\n            BufferedNorms[] newNorms = new BufferedNorms[newSize];\n            System.arraycopy(norms, 0, newNorms, 0, norms.length);\n            norms = newNorms;\n          }\n          \n          if (norms[fi.number] == null)\n            norms[fi.number] = new BufferedNorms();\n\n          hasNorms = true;\n        }\n\n        // Make sure we have a FieldData allocated\n        int hashPos = fi.name.hashCode() & fieldDataHashMask;\n        FieldData fp = fieldDataHash[hashPos];\n        while(fp != null && !fp.fieldInfo.name.equals(fi.name))\n          fp = fp.next;\n\n        if (fp == null) {\n\n          fp = new FieldData(fi);\n          fp.next = fieldDataHash[hashPos];\n          fieldDataHash[hashPos] = fp;\n\n          if (numAllFieldData == allFieldDataArray.length) {\n            int newSize = (int) (allFieldDataArray.length*1.5);\n            int newHashSize = fieldDataHash.length*2;\n\n            FieldData newArray[] = new FieldData[newSize];\n            FieldData newHashArray[] = new FieldData[newHashSize];\n            System.arraycopy(allFieldDataArray, 0, newArray, 0, numAllFieldData);\n\n            // Rehash\n            fieldDataHashMask = newSize-1;\n            for(int j=0;j<fieldDataHash.length;j++) {\n              FieldData fp0 = fieldDataHash[j];\n              while(fp0 != null) {\n                hashPos = fp0.fieldInfo.name.hashCode() & fieldDataHashMask;\n                FieldData nextFP0 = fp0.next;\n                fp0.next = newHashArray[hashPos];\n                newHashArray[hashPos] = fp0;\n                fp0 = nextFP0;\n              }\n            }\n\n            allFieldDataArray = newArray;\n            fieldDataHash = newHashArray;\n          }\n          allFieldDataArray[numAllFieldData++] = fp;\n        } else {\n          assert fp.fieldInfo == fi;\n        }\n\n        if (thisFieldGen != fp.lastGen) {\n\n          // First time we're seeing this field for this doc\n          fp.lastGen = thisFieldGen;\n          fp.fieldCount = 0;\n          fp.doVectors = fp.doVectorPositions = fp.doVectorOffsets = false;\n          fp.doNorms = fi.isIndexed && !fi.omitNorms;\n\n          if (numFieldData == fieldDataArray.length) {\n            int newSize = fieldDataArray.length*2;\n            FieldData newArray[] = new FieldData[newSize];\n            System.arraycopy(fieldDataArray, 0, newArray, 0, numFieldData);\n            fieldDataArray = newArray;\n\n          }\n          fieldDataArray[numFieldData++] = fp;\n        }\n\n        if (field.isTermVectorStored()) {\n          if (!fp.doVectors && numVectorFields++ == vectorFieldPointers.length) {\n            final int newSize = (int) (numVectorFields*1.5);\n            vectorFieldPointers = new long[newSize];\n            vectorFieldNumbers = new int[newSize];\n          }\n          fp.doVectors = true;\n          docHasVectors = true;\n\n          fp.doVectorPositions |= field.isStorePositionWithTermVector();\n          fp.doVectorOffsets |= field.isStoreOffsetWithTermVector();\n        }\n\n        if (fp.fieldCount == fp.docFields.length) {\n          Fieldable[] newArray = new Fieldable[fp.docFields.length*2];\n          System.arraycopy(fp.docFields, 0, newArray, 0, fp.docFields.length);\n          fp.docFields = newArray;\n        }\n\n        // Lazily allocate arrays for postings:\n        if (field.isIndexed() && fp.postingsHash == null)\n          fp.initPostingArrays();\n\n        fp.docFields[fp.fieldCount++] = field;\n      }\n\n      // Maybe init the local & global fieldsWriter\n      if (localFieldsWriter == null) {\n        if (fieldsWriter == null) {\n          assert docStoreSegment == null;\n          assert segment != null;\n          docStoreSegment = segment;\n          // If we hit an exception while init'ing the\n          // fieldsWriter, we must abort this segment\n          // because those files will be in an unknown\n          // state:\n          try {\n            fieldsWriter = new FieldsWriter(directory, docStoreSegment, fieldInfos);\n          } catch (Throwable t) {\n            throw new AbortException(t, DocumentsWriter.this);\n          }\n          files = null;\n        }\n        localFieldsWriter = new FieldsWriter(null, fdtLocal, fieldInfos);\n      }\n\n      // First time we see a doc that has field(s) with\n      // stored vectors, we init our tvx writer\n      if (docHasVectors) {\n        if (tvx == null) {\n          assert docStoreSegment != null;\n          // If we hit an exception while init'ing the term\n          // vector output files, we must abort this segment\n          // because those files will be in an unknown\n          // state:\n          try {\n            tvx = directory.createOutput(docStoreSegment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION);\n            tvx.writeInt(TermVectorsReader.FORMAT_VERSION2);\n            tvd = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);\n            tvd.writeInt(TermVectorsReader.FORMAT_VERSION2);\n            tvf = directory.createOutput(docStoreSegment +  \".\" + IndexFileNames.VECTORS_FIELDS_EXTENSION);\n            tvf.writeInt(TermVectorsReader.FORMAT_VERSION2);\n\n            // We must \"catch up\" for all docs before us\n            // that had no vectors:\n            for(int i=0;i<numDocsInStore;i++) {\n              tvx.writeLong(tvd.getFilePointer());\n              tvd.writeVInt(0);\n              tvx.writeLong(0);\n            }\n          } catch (Throwable t) {\n            throw new AbortException(t, DocumentsWriter.this);\n          }\n          files = null;\n        }\n        numVectorFields = 0;\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"83bbb041887bbef07b8a98d08a0e1713ce137039":["5a251aa47d1808cbae42c0e172d698c377430e60"],"b3d08461c77d39c25ea6ff0cd05b32f948fa2a33":["83bbb041887bbef07b8a98d08a0e1713ce137039"],"11764865fb318bf86302eab36bdf9cd00c50c110":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"741a5cca05cabe1e7482410a29e563a08379251a":["b6a1f29c9b1051488fd5fa7d56c98db5f4388408"],"b6a1f29c9b1051488fd5fa7d56c98db5f4388408":["11764865fb318bf86302eab36bdf9cd00c50c110"],"4350b17bd363cd13a95171b8df1ca62ea4c3e71c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"49adbad5232116eb2448ea8166464e6a68bca007":["7439af33438ca32496b7ce4a341617a480b7137a"],"7439af33438ca32496b7ce4a341617a480b7137a":["b3d08461c77d39c25ea6ff0cd05b32f948fa2a33"],"c9a0deca56efc5191d6b3c41047fd538f3fae1d8":["8560794cda5bcd510c60e38ed553e9c5a6204983"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"5a251aa47d1808cbae42c0e172d698c377430e60":["c9a0deca56efc5191d6b3c41047fd538f3fae1d8"],"8560794cda5bcd510c60e38ed553e9c5a6204983":["741a5cca05cabe1e7482410a29e563a08379251a"],"5a0af3a442be522899177e5e11384a45a6784a3f":["ac1242ce6d99813874fddfe4ca5f57779beddb22"],"ac1242ce6d99813874fddfe4ca5f57779beddb22":["49adbad5232116eb2448ea8166464e6a68bca007"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["5a0af3a442be522899177e5e11384a45a6784a3f"]},"commit2Childs":{"83bbb041887bbef07b8a98d08a0e1713ce137039":["b3d08461c77d39c25ea6ff0cd05b32f948fa2a33"],"b3d08461c77d39c25ea6ff0cd05b32f948fa2a33":["7439af33438ca32496b7ce4a341617a480b7137a"],"11764865fb318bf86302eab36bdf9cd00c50c110":["b6a1f29c9b1051488fd5fa7d56c98db5f4388408"],"4350b17bd363cd13a95171b8df1ca62ea4c3e71c":["11764865fb318bf86302eab36bdf9cd00c50c110"],"741a5cca05cabe1e7482410a29e563a08379251a":["8560794cda5bcd510c60e38ed553e9c5a6204983"],"b6a1f29c9b1051488fd5fa7d56c98db5f4388408":["741a5cca05cabe1e7482410a29e563a08379251a"],"49adbad5232116eb2448ea8166464e6a68bca007":["ac1242ce6d99813874fddfe4ca5f57779beddb22"],"7439af33438ca32496b7ce4a341617a480b7137a":["49adbad5232116eb2448ea8166464e6a68bca007"],"c9a0deca56efc5191d6b3c41047fd538f3fae1d8":["5a251aa47d1808cbae42c0e172d698c377430e60"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"5a251aa47d1808cbae42c0e172d698c377430e60":["83bbb041887bbef07b8a98d08a0e1713ce137039"],"8560794cda5bcd510c60e38ed553e9c5a6204983":["c9a0deca56efc5191d6b3c41047fd538f3fae1d8"],"5a0af3a442be522899177e5e11384a45a6784a3f":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"ac1242ce6d99813874fddfe4ca5f57779beddb22":["5a0af3a442be522899177e5e11384a45a6784a3f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}