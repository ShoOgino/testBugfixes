{"path":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testManyReopensAndFields().mjava","commits":[{"id":"06805da26538ed636bd89b10c2699cc3834032ae","date":1395132972,"type":0,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testManyReopensAndFields().mjava","pathOld":"/dev/null","sourceNew":"  public void testManyReopensAndFields() throws Exception {\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    LogMergePolicy lmp = newLogMergePolicy();\n    lmp.setMergeFactor(3); // merge often\n    conf.setMergePolicy(lmp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    final boolean isNRT = random.nextBoolean();\n    DirectoryReader reader;\n    if (isNRT) {\n      reader = DirectoryReader.open(writer, true);\n    } else {\n      writer.commit();\n      reader = DirectoryReader.open(dir);\n    }\n    \n    final int numFields = random.nextInt(4) + 3; // 3-7\n    final long[] fieldValues = new long[numFields];\n    final boolean[] fieldHasValue = new boolean[numFields];\n    Arrays.fill(fieldHasValue, true);\n    for (int i = 0; i < fieldValues.length; i++) {\n      fieldValues[i] = 1;\n    }\n    \n    int numRounds = atLeast(15);\n    int docID = 0;\n    for (int i = 0; i < numRounds; i++) {\n      int numDocs = atLeast(5);\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: round=\" + i + \", numDocs=\" + numDocs);\n      for (int j = 0; j < numDocs; j++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"doc-\" + docID, Store.NO));\n        doc.add(new StringField(\"key\", \"all\", Store.NO)); // update key\n        // add all fields with their current value\n        for (int f = 0; f < fieldValues.length; f++) {\n          doc.add(new BinaryDocValuesField(\"f\" + f, toBytes(fieldValues[f])));\n        }\n        writer.addDocument(doc);\n        ++docID;\n      }\n      \n      // if field's value was unset before, unset it from all new added documents too\n      for (int field = 0; field < fieldHasValue.length; field++) {\n        if (!fieldHasValue[field]) {\n          writer.updateBinaryDocValue(new Term(\"key\", \"all\"), \"f\" + field, null);\n        }\n      }\n      \n      int fieldIdx = random.nextInt(fieldValues.length);\n      String updateField = \"f\" + fieldIdx;\n      if (random.nextBoolean()) {\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"]: unset field '\" + updateField + \"'\");\n        fieldHasValue[fieldIdx] = false;\n        writer.updateBinaryDocValue(new Term(\"key\", \"all\"), updateField, null);\n      } else {\n        fieldHasValue[fieldIdx] = true;\n        writer.updateBinaryDocValue(new Term(\"key\", \"all\"), updateField, toBytes(++fieldValues[fieldIdx]));\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"]: updated field '\" + updateField + \"' to value \" + fieldValues[fieldIdx]);\n      }\n      \n      if (random.nextDouble() < 0.2) {\n        int deleteDoc = random.nextInt(docID); // might also delete an already deleted document, ok!\n        writer.deleteDocuments(new Term(\"id\", \"doc-\" + deleteDoc));\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"]: deleted document: doc-\" + deleteDoc);\n      }\n      \n      // verify reader\n      if (!isNRT) {\n        writer.commit();\n      }\n      \n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopen reader: \" + reader);\n      DirectoryReader newReader = DirectoryReader.openIfChanged(reader);\n      assertNotNull(newReader);\n      reader.close();\n      reader = newReader;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopened reader: \" + reader);\n      assertTrue(reader.numDocs() > 0); // we delete at most one document per round\n      BytesRef scratch = new BytesRef();\n      for (AtomicReaderContext context : reader.leaves()) {\n        AtomicReader r = context.reader();\n//        System.out.println(((SegmentReader) r).getSegmentName());\n        Bits liveDocs = r.getLiveDocs();\n        for (int field = 0; field < fieldValues.length; field++) {\n          String f = \"f\" + field;\n          BinaryDocValues bdv = r.getBinaryDocValues(f);\n          Bits docsWithField = r.getDocsWithField(f);\n          assertNotNull(bdv);\n          int maxDoc = r.maxDoc();\n          for (int doc = 0; doc < maxDoc; doc++) {\n            if (liveDocs == null || liveDocs.get(doc)) {\n//              System.out.println(\"doc=\" + (doc + context.docBase) + \" f='\" + f + \"' vslue=\" + getValue(bdv, doc, scratch));\n              if (fieldHasValue[field]) {\n                assertTrue(docsWithField.get(doc));\n                assertEquals(\"invalid value for doc=\" + doc + \", field=\" + f + \", reader=\" + r, fieldValues[field], getValue(bdv, doc, scratch));\n              } else {\n                assertFalse(docsWithField.get(doc));\n              }\n            }\n          }\n        }\n      }\n//      System.out.println();\n    }\n    \n    IOUtils.close(writer, reader, dir);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["7e4c214a1f904dde76f5611b56d4081533055b3b"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testManyReopensAndFields().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testManyReopensAndFields().mjava","sourceNew":"  public void testManyReopensAndFields() throws Exception {\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    LogMergePolicy lmp = newLogMergePolicy();\n    lmp.setMergeFactor(3); // merge often\n    conf.setMergePolicy(lmp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    final boolean isNRT = random.nextBoolean();\n    DirectoryReader reader;\n    if (isNRT) {\n      reader = DirectoryReader.open(writer, true);\n    } else {\n      writer.commit();\n      reader = DirectoryReader.open(dir);\n    }\n    \n    final int numFields = random.nextInt(4) + 3; // 3-7\n    final long[] fieldValues = new long[numFields];\n    final boolean[] fieldHasValue = new boolean[numFields];\n    Arrays.fill(fieldHasValue, true);\n    for (int i = 0; i < fieldValues.length; i++) {\n      fieldValues[i] = 1;\n    }\n    \n    int numRounds = atLeast(15);\n    int docID = 0;\n    for (int i = 0; i < numRounds; i++) {\n      int numDocs = atLeast(5);\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: round=\" + i + \", numDocs=\" + numDocs);\n      for (int j = 0; j < numDocs; j++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"doc-\" + docID, Store.NO));\n        doc.add(new StringField(\"key\", \"all\", Store.NO)); // update key\n        // add all fields with their current value\n        for (int f = 0; f < fieldValues.length; f++) {\n          doc.add(new BinaryDocValuesField(\"f\" + f, toBytes(fieldValues[f])));\n        }\n        writer.addDocument(doc);\n        ++docID;\n      }\n      \n      // if field's value was unset before, unset it from all new added documents too\n      for (int field = 0; field < fieldHasValue.length; field++) {\n        if (!fieldHasValue[field]) {\n          writer.updateBinaryDocValue(new Term(\"key\", \"all\"), \"f\" + field, null);\n        }\n      }\n      \n      int fieldIdx = random.nextInt(fieldValues.length);\n      String updateField = \"f\" + fieldIdx;\n      if (random.nextBoolean()) {\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"]: unset field '\" + updateField + \"'\");\n        fieldHasValue[fieldIdx] = false;\n        writer.updateBinaryDocValue(new Term(\"key\", \"all\"), updateField, null);\n      } else {\n        fieldHasValue[fieldIdx] = true;\n        writer.updateBinaryDocValue(new Term(\"key\", \"all\"), updateField, toBytes(++fieldValues[fieldIdx]));\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"]: updated field '\" + updateField + \"' to value \" + fieldValues[fieldIdx]);\n      }\n      \n      if (random.nextDouble() < 0.2) {\n        int deleteDoc = random.nextInt(docID); // might also delete an already deleted document, ok!\n        writer.deleteDocuments(new Term(\"id\", \"doc-\" + deleteDoc));\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"]: deleted document: doc-\" + deleteDoc);\n      }\n      \n      // verify reader\n      if (!isNRT) {\n        writer.commit();\n      }\n      \n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopen reader: \" + reader);\n      DirectoryReader newReader = DirectoryReader.openIfChanged(reader);\n      assertNotNull(newReader);\n      reader.close();\n      reader = newReader;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopened reader: \" + reader);\n      assertTrue(reader.numDocs() > 0); // we delete at most one document per round\n      BytesRef scratch = new BytesRef();\n      for (AtomicReaderContext context : reader.leaves()) {\n        AtomicReader r = context.reader();\n//        System.out.println(((SegmentReader) r).getSegmentName());\n        Bits liveDocs = r.getLiveDocs();\n        for (int field = 0; field < fieldValues.length; field++) {\n          String f = \"f\" + field;\n          BinaryDocValues bdv = r.getBinaryDocValues(f);\n          Bits docsWithField = r.getDocsWithField(f);\n          assertNotNull(bdv);\n          int maxDoc = r.maxDoc();\n          for (int doc = 0; doc < maxDoc; doc++) {\n            if (liveDocs == null || liveDocs.get(doc)) {\n//              System.out.println(\"doc=\" + (doc + context.docBase) + \" f='\" + f + \"' vslue=\" + getValue(bdv, doc, scratch));\n              if (fieldHasValue[field]) {\n                assertTrue(docsWithField.get(doc));\n                assertEquals(\"invalid value for doc=\" + doc + \", field=\" + f + \", reader=\" + r, fieldValues[field], getValue(bdv, doc, scratch));\n              } else {\n                assertFalse(docsWithField.get(doc));\n              }\n            }\n          }\n        }\n      }\n//      System.out.println();\n    }\n\n    writer.shutdown();\n    IOUtils.close(reader, dir);\n  }\n\n","sourceOld":"  public void testManyReopensAndFields() throws Exception {\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    LogMergePolicy lmp = newLogMergePolicy();\n    lmp.setMergeFactor(3); // merge often\n    conf.setMergePolicy(lmp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    final boolean isNRT = random.nextBoolean();\n    DirectoryReader reader;\n    if (isNRT) {\n      reader = DirectoryReader.open(writer, true);\n    } else {\n      writer.commit();\n      reader = DirectoryReader.open(dir);\n    }\n    \n    final int numFields = random.nextInt(4) + 3; // 3-7\n    final long[] fieldValues = new long[numFields];\n    final boolean[] fieldHasValue = new boolean[numFields];\n    Arrays.fill(fieldHasValue, true);\n    for (int i = 0; i < fieldValues.length; i++) {\n      fieldValues[i] = 1;\n    }\n    \n    int numRounds = atLeast(15);\n    int docID = 0;\n    for (int i = 0; i < numRounds; i++) {\n      int numDocs = atLeast(5);\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: round=\" + i + \", numDocs=\" + numDocs);\n      for (int j = 0; j < numDocs; j++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"doc-\" + docID, Store.NO));\n        doc.add(new StringField(\"key\", \"all\", Store.NO)); // update key\n        // add all fields with their current value\n        for (int f = 0; f < fieldValues.length; f++) {\n          doc.add(new BinaryDocValuesField(\"f\" + f, toBytes(fieldValues[f])));\n        }\n        writer.addDocument(doc);\n        ++docID;\n      }\n      \n      // if field's value was unset before, unset it from all new added documents too\n      for (int field = 0; field < fieldHasValue.length; field++) {\n        if (!fieldHasValue[field]) {\n          writer.updateBinaryDocValue(new Term(\"key\", \"all\"), \"f\" + field, null);\n        }\n      }\n      \n      int fieldIdx = random.nextInt(fieldValues.length);\n      String updateField = \"f\" + fieldIdx;\n      if (random.nextBoolean()) {\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"]: unset field '\" + updateField + \"'\");\n        fieldHasValue[fieldIdx] = false;\n        writer.updateBinaryDocValue(new Term(\"key\", \"all\"), updateField, null);\n      } else {\n        fieldHasValue[fieldIdx] = true;\n        writer.updateBinaryDocValue(new Term(\"key\", \"all\"), updateField, toBytes(++fieldValues[fieldIdx]));\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"]: updated field '\" + updateField + \"' to value \" + fieldValues[fieldIdx]);\n      }\n      \n      if (random.nextDouble() < 0.2) {\n        int deleteDoc = random.nextInt(docID); // might also delete an already deleted document, ok!\n        writer.deleteDocuments(new Term(\"id\", \"doc-\" + deleteDoc));\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"]: deleted document: doc-\" + deleteDoc);\n      }\n      \n      // verify reader\n      if (!isNRT) {\n        writer.commit();\n      }\n      \n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopen reader: \" + reader);\n      DirectoryReader newReader = DirectoryReader.openIfChanged(reader);\n      assertNotNull(newReader);\n      reader.close();\n      reader = newReader;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopened reader: \" + reader);\n      assertTrue(reader.numDocs() > 0); // we delete at most one document per round\n      BytesRef scratch = new BytesRef();\n      for (AtomicReaderContext context : reader.leaves()) {\n        AtomicReader r = context.reader();\n//        System.out.println(((SegmentReader) r).getSegmentName());\n        Bits liveDocs = r.getLiveDocs();\n        for (int field = 0; field < fieldValues.length; field++) {\n          String f = \"f\" + field;\n          BinaryDocValues bdv = r.getBinaryDocValues(f);\n          Bits docsWithField = r.getDocsWithField(f);\n          assertNotNull(bdv);\n          int maxDoc = r.maxDoc();\n          for (int doc = 0; doc < maxDoc; doc++) {\n            if (liveDocs == null || liveDocs.get(doc)) {\n//              System.out.println(\"doc=\" + (doc + context.docBase) + \" f='\" + f + \"' vslue=\" + getValue(bdv, doc, scratch));\n              if (fieldHasValue[field]) {\n                assertTrue(docsWithField.get(doc));\n                assertEquals(\"invalid value for doc=\" + doc + \", field=\" + f + \", reader=\" + r, fieldValues[field], getValue(bdv, doc, scratch));\n              } else {\n                assertFalse(docsWithField.get(doc));\n              }\n            }\n          }\n        }\n      }\n//      System.out.println();\n    }\n    \n    IOUtils.close(writer, reader, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"30d3ec601cbd11cf056b7336f0e03f688ebcd9f7","date":1401116050,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testManyReopensAndFields().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testManyReopensAndFields().mjava","sourceNew":"  public void testManyReopensAndFields() throws Exception {\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    LogMergePolicy lmp = newLogMergePolicy();\n    lmp.setMergeFactor(3); // merge often\n    conf.setMergePolicy(lmp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    final boolean isNRT = random.nextBoolean();\n    DirectoryReader reader;\n    if (isNRT) {\n      reader = DirectoryReader.open(writer, true);\n    } else {\n      writer.commit();\n      reader = DirectoryReader.open(dir);\n    }\n    \n    final int numFields = random.nextInt(4) + 3; // 3-7\n    final long[] fieldValues = new long[numFields];\n    for (int i = 0; i < fieldValues.length; i++) {\n      fieldValues[i] = 1;\n    }\n    \n    int numRounds = atLeast(15);\n    int docID = 0;\n    for (int i = 0; i < numRounds; i++) {\n      int numDocs = atLeast(5);\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: round=\" + i + \", numDocs=\" + numDocs);\n      for (int j = 0; j < numDocs; j++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"doc-\" + docID, Store.NO));\n        doc.add(new StringField(\"key\", \"all\", Store.NO)); // update key\n        // add all fields with their current value\n        for (int f = 0; f < fieldValues.length; f++) {\n          doc.add(new BinaryDocValuesField(\"f\" + f, toBytes(fieldValues[f])));\n        }\n        writer.addDocument(doc);\n        ++docID;\n      }\n      \n      int fieldIdx = random.nextInt(fieldValues.length);\n      String updateField = \"f\" + fieldIdx;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: updated field '\" + updateField + \"' to value \" + fieldValues[fieldIdx]);\n      writer.updateBinaryDocValue(new Term(\"key\", \"all\"), updateField, toBytes(++fieldValues[fieldIdx]));\n      \n      if (random.nextDouble() < 0.2) {\n        int deleteDoc = random.nextInt(docID); // might also delete an already deleted document, ok!\n        writer.deleteDocuments(new Term(\"id\", \"doc-\" + deleteDoc));\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"]: deleted document: doc-\" + deleteDoc);\n      }\n      \n      // verify reader\n      if (!isNRT) {\n        writer.commit();\n      }\n      \n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopen reader: \" + reader);\n      DirectoryReader newReader = DirectoryReader.openIfChanged(reader);\n      assertNotNull(newReader);\n      reader.close();\n      reader = newReader;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopened reader: \" + reader);\n      assertTrue(reader.numDocs() > 0); // we delete at most one document per round\n      BytesRef scratch = new BytesRef();\n      for (AtomicReaderContext context : reader.leaves()) {\n        AtomicReader r = context.reader();\n//        System.out.println(((SegmentReader) r).getSegmentName());\n        Bits liveDocs = r.getLiveDocs();\n        for (int field = 0; field < fieldValues.length; field++) {\n          String f = \"f\" + field;\n          BinaryDocValues bdv = r.getBinaryDocValues(f);\n          Bits docsWithField = r.getDocsWithField(f);\n          assertNotNull(bdv);\n          int maxDoc = r.maxDoc();\n          for (int doc = 0; doc < maxDoc; doc++) {\n            if (liveDocs == null || liveDocs.get(doc)) {\n//              System.out.println(\"doc=\" + (doc + context.docBase) + \" f='\" + f + \"' vslue=\" + getValue(bdv, doc, scratch));\n              assertTrue(docsWithField.get(doc));\n              assertEquals(\"invalid value for doc=\" + doc + \", field=\" + f + \", reader=\" + r, fieldValues[field], getValue(bdv, doc, scratch));\n            }\n          }\n        }\n      }\n//      System.out.println();\n    }\n\n    writer.shutdown();\n    IOUtils.close(reader, dir);\n  }\n\n","sourceOld":"  public void testManyReopensAndFields() throws Exception {\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    LogMergePolicy lmp = newLogMergePolicy();\n    lmp.setMergeFactor(3); // merge often\n    conf.setMergePolicy(lmp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    final boolean isNRT = random.nextBoolean();\n    DirectoryReader reader;\n    if (isNRT) {\n      reader = DirectoryReader.open(writer, true);\n    } else {\n      writer.commit();\n      reader = DirectoryReader.open(dir);\n    }\n    \n    final int numFields = random.nextInt(4) + 3; // 3-7\n    final long[] fieldValues = new long[numFields];\n    final boolean[] fieldHasValue = new boolean[numFields];\n    Arrays.fill(fieldHasValue, true);\n    for (int i = 0; i < fieldValues.length; i++) {\n      fieldValues[i] = 1;\n    }\n    \n    int numRounds = atLeast(15);\n    int docID = 0;\n    for (int i = 0; i < numRounds; i++) {\n      int numDocs = atLeast(5);\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: round=\" + i + \", numDocs=\" + numDocs);\n      for (int j = 0; j < numDocs; j++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"doc-\" + docID, Store.NO));\n        doc.add(new StringField(\"key\", \"all\", Store.NO)); // update key\n        // add all fields with their current value\n        for (int f = 0; f < fieldValues.length; f++) {\n          doc.add(new BinaryDocValuesField(\"f\" + f, toBytes(fieldValues[f])));\n        }\n        writer.addDocument(doc);\n        ++docID;\n      }\n      \n      // if field's value was unset before, unset it from all new added documents too\n      for (int field = 0; field < fieldHasValue.length; field++) {\n        if (!fieldHasValue[field]) {\n          writer.updateBinaryDocValue(new Term(\"key\", \"all\"), \"f\" + field, null);\n        }\n      }\n      \n      int fieldIdx = random.nextInt(fieldValues.length);\n      String updateField = \"f\" + fieldIdx;\n      if (random.nextBoolean()) {\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"]: unset field '\" + updateField + \"'\");\n        fieldHasValue[fieldIdx] = false;\n        writer.updateBinaryDocValue(new Term(\"key\", \"all\"), updateField, null);\n      } else {\n        fieldHasValue[fieldIdx] = true;\n        writer.updateBinaryDocValue(new Term(\"key\", \"all\"), updateField, toBytes(++fieldValues[fieldIdx]));\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"]: updated field '\" + updateField + \"' to value \" + fieldValues[fieldIdx]);\n      }\n      \n      if (random.nextDouble() < 0.2) {\n        int deleteDoc = random.nextInt(docID); // might also delete an already deleted document, ok!\n        writer.deleteDocuments(new Term(\"id\", \"doc-\" + deleteDoc));\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"]: deleted document: doc-\" + deleteDoc);\n      }\n      \n      // verify reader\n      if (!isNRT) {\n        writer.commit();\n      }\n      \n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopen reader: \" + reader);\n      DirectoryReader newReader = DirectoryReader.openIfChanged(reader);\n      assertNotNull(newReader);\n      reader.close();\n      reader = newReader;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopened reader: \" + reader);\n      assertTrue(reader.numDocs() > 0); // we delete at most one document per round\n      BytesRef scratch = new BytesRef();\n      for (AtomicReaderContext context : reader.leaves()) {\n        AtomicReader r = context.reader();\n//        System.out.println(((SegmentReader) r).getSegmentName());\n        Bits liveDocs = r.getLiveDocs();\n        for (int field = 0; field < fieldValues.length; field++) {\n          String f = \"f\" + field;\n          BinaryDocValues bdv = r.getBinaryDocValues(f);\n          Bits docsWithField = r.getDocsWithField(f);\n          assertNotNull(bdv);\n          int maxDoc = r.maxDoc();\n          for (int doc = 0; doc < maxDoc; doc++) {\n            if (liveDocs == null || liveDocs.get(doc)) {\n//              System.out.println(\"doc=\" + (doc + context.docBase) + \" f='\" + f + \"' vslue=\" + getValue(bdv, doc, scratch));\n              if (fieldHasValue[field]) {\n                assertTrue(docsWithField.get(doc));\n                assertEquals(\"invalid value for doc=\" + doc + \", field=\" + f + \", reader=\" + r, fieldValues[field], getValue(bdv, doc, scratch));\n              } else {\n                assertFalse(docsWithField.get(doc));\n              }\n            }\n          }\n        }\n      }\n//      System.out.println();\n    }\n\n    writer.shutdown();\n    IOUtils.close(reader, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf","date":1401983689,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testManyReopensAndFields().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testManyReopensAndFields().mjava","sourceNew":"  public void testManyReopensAndFields() throws Exception {\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    LogMergePolicy lmp = newLogMergePolicy();\n    lmp.setMergeFactor(3); // merge often\n    conf.setMergePolicy(lmp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    final boolean isNRT = random.nextBoolean();\n    DirectoryReader reader;\n    if (isNRT) {\n      reader = DirectoryReader.open(writer, true);\n    } else {\n      writer.commit();\n      reader = DirectoryReader.open(dir);\n    }\n    \n    final int numFields = random.nextInt(4) + 3; // 3-7\n    final long[] fieldValues = new long[numFields];\n    for (int i = 0; i < fieldValues.length; i++) {\n      fieldValues[i] = 1;\n    }\n    \n    int numRounds = atLeast(15);\n    int docID = 0;\n    for (int i = 0; i < numRounds; i++) {\n      int numDocs = atLeast(5);\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: round=\" + i + \", numDocs=\" + numDocs);\n      for (int j = 0; j < numDocs; j++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"doc-\" + docID, Store.NO));\n        doc.add(new StringField(\"key\", \"all\", Store.NO)); // update key\n        // add all fields with their current value\n        for (int f = 0; f < fieldValues.length; f++) {\n          doc.add(new BinaryDocValuesField(\"f\" + f, toBytes(fieldValues[f])));\n        }\n        writer.addDocument(doc);\n        ++docID;\n      }\n      \n      int fieldIdx = random.nextInt(fieldValues.length);\n      String updateField = \"f\" + fieldIdx;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: updated field '\" + updateField + \"' to value \" + fieldValues[fieldIdx]);\n      writer.updateBinaryDocValue(new Term(\"key\", \"all\"), updateField, toBytes(++fieldValues[fieldIdx]));\n      \n      if (random.nextDouble() < 0.2) {\n        int deleteDoc = random.nextInt(docID); // might also delete an already deleted document, ok!\n        writer.deleteDocuments(new Term(\"id\", \"doc-\" + deleteDoc));\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"]: deleted document: doc-\" + deleteDoc);\n      }\n      \n      // verify reader\n      if (!isNRT) {\n        writer.commit();\n      }\n      \n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopen reader: \" + reader);\n      DirectoryReader newReader = DirectoryReader.openIfChanged(reader);\n      assertNotNull(newReader);\n      reader.close();\n      reader = newReader;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopened reader: \" + reader);\n      assertTrue(reader.numDocs() > 0); // we delete at most one document per round\n      for (AtomicReaderContext context : reader.leaves()) {\n        AtomicReader r = context.reader();\n//        System.out.println(((SegmentReader) r).getSegmentName());\n        Bits liveDocs = r.getLiveDocs();\n        for (int field = 0; field < fieldValues.length; field++) {\n          String f = \"f\" + field;\n          BinaryDocValues bdv = r.getBinaryDocValues(f);\n          Bits docsWithField = r.getDocsWithField(f);\n          assertNotNull(bdv);\n          int maxDoc = r.maxDoc();\n          for (int doc = 0; doc < maxDoc; doc++) {\n            if (liveDocs == null || liveDocs.get(doc)) {\n//              System.out.println(\"doc=\" + (doc + context.docBase) + \" f='\" + f + \"' vslue=\" + getValue(bdv, doc, scratch));\n              assertTrue(docsWithField.get(doc));\n              assertEquals(\"invalid value for doc=\" + doc + \", field=\" + f + \", reader=\" + r, fieldValues[field], getValue(bdv, doc));\n            }\n          }\n        }\n      }\n//      System.out.println();\n    }\n\n    writer.shutdown();\n    IOUtils.close(reader, dir);\n  }\n\n","sourceOld":"  public void testManyReopensAndFields() throws Exception {\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    LogMergePolicy lmp = newLogMergePolicy();\n    lmp.setMergeFactor(3); // merge often\n    conf.setMergePolicy(lmp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    final boolean isNRT = random.nextBoolean();\n    DirectoryReader reader;\n    if (isNRT) {\n      reader = DirectoryReader.open(writer, true);\n    } else {\n      writer.commit();\n      reader = DirectoryReader.open(dir);\n    }\n    \n    final int numFields = random.nextInt(4) + 3; // 3-7\n    final long[] fieldValues = new long[numFields];\n    for (int i = 0; i < fieldValues.length; i++) {\n      fieldValues[i] = 1;\n    }\n    \n    int numRounds = atLeast(15);\n    int docID = 0;\n    for (int i = 0; i < numRounds; i++) {\n      int numDocs = atLeast(5);\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: round=\" + i + \", numDocs=\" + numDocs);\n      for (int j = 0; j < numDocs; j++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"doc-\" + docID, Store.NO));\n        doc.add(new StringField(\"key\", \"all\", Store.NO)); // update key\n        // add all fields with their current value\n        for (int f = 0; f < fieldValues.length; f++) {\n          doc.add(new BinaryDocValuesField(\"f\" + f, toBytes(fieldValues[f])));\n        }\n        writer.addDocument(doc);\n        ++docID;\n      }\n      \n      int fieldIdx = random.nextInt(fieldValues.length);\n      String updateField = \"f\" + fieldIdx;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: updated field '\" + updateField + \"' to value \" + fieldValues[fieldIdx]);\n      writer.updateBinaryDocValue(new Term(\"key\", \"all\"), updateField, toBytes(++fieldValues[fieldIdx]));\n      \n      if (random.nextDouble() < 0.2) {\n        int deleteDoc = random.nextInt(docID); // might also delete an already deleted document, ok!\n        writer.deleteDocuments(new Term(\"id\", \"doc-\" + deleteDoc));\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"]: deleted document: doc-\" + deleteDoc);\n      }\n      \n      // verify reader\n      if (!isNRT) {\n        writer.commit();\n      }\n      \n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopen reader: \" + reader);\n      DirectoryReader newReader = DirectoryReader.openIfChanged(reader);\n      assertNotNull(newReader);\n      reader.close();\n      reader = newReader;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopened reader: \" + reader);\n      assertTrue(reader.numDocs() > 0); // we delete at most one document per round\n      BytesRef scratch = new BytesRef();\n      for (AtomicReaderContext context : reader.leaves()) {\n        AtomicReader r = context.reader();\n//        System.out.println(((SegmentReader) r).getSegmentName());\n        Bits liveDocs = r.getLiveDocs();\n        for (int field = 0; field < fieldValues.length; field++) {\n          String f = \"f\" + field;\n          BinaryDocValues bdv = r.getBinaryDocValues(f);\n          Bits docsWithField = r.getDocsWithField(f);\n          assertNotNull(bdv);\n          int maxDoc = r.maxDoc();\n          for (int doc = 0; doc < maxDoc; doc++) {\n            if (liveDocs == null || liveDocs.get(doc)) {\n//              System.out.println(\"doc=\" + (doc + context.docBase) + \" f='\" + f + \"' vslue=\" + getValue(bdv, doc, scratch));\n              assertTrue(docsWithField.get(doc));\n              assertEquals(\"invalid value for doc=\" + doc + \", field=\" + f + \", reader=\" + r, fieldValues[field], getValue(bdv, doc, scratch));\n            }\n          }\n        }\n      }\n//      System.out.println();\n    }\n\n    writer.shutdown();\n    IOUtils.close(reader, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testManyReopensAndFields().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testManyReopensAndFields().mjava","sourceNew":"  public void testManyReopensAndFields() throws Exception {\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    LogMergePolicy lmp = newLogMergePolicy();\n    lmp.setMergeFactor(3); // merge often\n    conf.setMergePolicy(lmp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    final boolean isNRT = random.nextBoolean();\n    DirectoryReader reader;\n    if (isNRT) {\n      reader = DirectoryReader.open(writer, true);\n    } else {\n      writer.commit();\n      reader = DirectoryReader.open(dir);\n    }\n    \n    final int numFields = random.nextInt(4) + 3; // 3-7\n    final long[] fieldValues = new long[numFields];\n    for (int i = 0; i < fieldValues.length; i++) {\n      fieldValues[i] = 1;\n    }\n    \n    int numRounds = atLeast(15);\n    int docID = 0;\n    for (int i = 0; i < numRounds; i++) {\n      int numDocs = atLeast(5);\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: round=\" + i + \", numDocs=\" + numDocs);\n      for (int j = 0; j < numDocs; j++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"doc-\" + docID, Store.NO));\n        doc.add(new StringField(\"key\", \"all\", Store.NO)); // update key\n        // add all fields with their current value\n        for (int f = 0; f < fieldValues.length; f++) {\n          doc.add(new BinaryDocValuesField(\"f\" + f, toBytes(fieldValues[f])));\n        }\n        writer.addDocument(doc);\n        ++docID;\n      }\n      \n      int fieldIdx = random.nextInt(fieldValues.length);\n      String updateField = \"f\" + fieldIdx;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: updated field '\" + updateField + \"' to value \" + fieldValues[fieldIdx]);\n      writer.updateBinaryDocValue(new Term(\"key\", \"all\"), updateField, toBytes(++fieldValues[fieldIdx]));\n      \n      if (random.nextDouble() < 0.2) {\n        int deleteDoc = random.nextInt(docID); // might also delete an already deleted document, ok!\n        writer.deleteDocuments(new Term(\"id\", \"doc-\" + deleteDoc));\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"]: deleted document: doc-\" + deleteDoc);\n      }\n      \n      // verify reader\n      if (!isNRT) {\n        writer.commit();\n      }\n      \n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopen reader: \" + reader);\n      DirectoryReader newReader = DirectoryReader.openIfChanged(reader);\n      assertNotNull(newReader);\n      reader.close();\n      reader = newReader;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopened reader: \" + reader);\n      assertTrue(reader.numDocs() > 0); // we delete at most one document per round\n      for (AtomicReaderContext context : reader.leaves()) {\n        AtomicReader r = context.reader();\n//        System.out.println(((SegmentReader) r).getSegmentName());\n        Bits liveDocs = r.getLiveDocs();\n        for (int field = 0; field < fieldValues.length; field++) {\n          String f = \"f\" + field;\n          BinaryDocValues bdv = r.getBinaryDocValues(f);\n          Bits docsWithField = r.getDocsWithField(f);\n          assertNotNull(bdv);\n          int maxDoc = r.maxDoc();\n          for (int doc = 0; doc < maxDoc; doc++) {\n            if (liveDocs == null || liveDocs.get(doc)) {\n//              System.out.println(\"doc=\" + (doc + context.docBase) + \" f='\" + f + \"' vslue=\" + getValue(bdv, doc, scratch));\n              assertTrue(docsWithField.get(doc));\n              assertEquals(\"invalid value for doc=\" + doc + \", field=\" + f + \", reader=\" + r, fieldValues[field], getValue(bdv, doc));\n            }\n          }\n        }\n      }\n//      System.out.println();\n    }\n\n    writer.shutdown();\n    IOUtils.close(reader, dir);\n  }\n\n","sourceOld":"  public void testManyReopensAndFields() throws Exception {\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    LogMergePolicy lmp = newLogMergePolicy();\n    lmp.setMergeFactor(3); // merge often\n    conf.setMergePolicy(lmp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    final boolean isNRT = random.nextBoolean();\n    DirectoryReader reader;\n    if (isNRT) {\n      reader = DirectoryReader.open(writer, true);\n    } else {\n      writer.commit();\n      reader = DirectoryReader.open(dir);\n    }\n    \n    final int numFields = random.nextInt(4) + 3; // 3-7\n    final long[] fieldValues = new long[numFields];\n    for (int i = 0; i < fieldValues.length; i++) {\n      fieldValues[i] = 1;\n    }\n    \n    int numRounds = atLeast(15);\n    int docID = 0;\n    for (int i = 0; i < numRounds; i++) {\n      int numDocs = atLeast(5);\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: round=\" + i + \", numDocs=\" + numDocs);\n      for (int j = 0; j < numDocs; j++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"doc-\" + docID, Store.NO));\n        doc.add(new StringField(\"key\", \"all\", Store.NO)); // update key\n        // add all fields with their current value\n        for (int f = 0; f < fieldValues.length; f++) {\n          doc.add(new BinaryDocValuesField(\"f\" + f, toBytes(fieldValues[f])));\n        }\n        writer.addDocument(doc);\n        ++docID;\n      }\n      \n      int fieldIdx = random.nextInt(fieldValues.length);\n      String updateField = \"f\" + fieldIdx;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: updated field '\" + updateField + \"' to value \" + fieldValues[fieldIdx]);\n      writer.updateBinaryDocValue(new Term(\"key\", \"all\"), updateField, toBytes(++fieldValues[fieldIdx]));\n      \n      if (random.nextDouble() < 0.2) {\n        int deleteDoc = random.nextInt(docID); // might also delete an already deleted document, ok!\n        writer.deleteDocuments(new Term(\"id\", \"doc-\" + deleteDoc));\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"]: deleted document: doc-\" + deleteDoc);\n      }\n      \n      // verify reader\n      if (!isNRT) {\n        writer.commit();\n      }\n      \n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopen reader: \" + reader);\n      DirectoryReader newReader = DirectoryReader.openIfChanged(reader);\n      assertNotNull(newReader);\n      reader.close();\n      reader = newReader;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopened reader: \" + reader);\n      assertTrue(reader.numDocs() > 0); // we delete at most one document per round\n      for (AtomicReaderContext context : reader.leaves()) {\n        AtomicReader r = context.reader();\n//        System.out.println(((SegmentReader) r).getSegmentName());\n        Bits liveDocs = r.getLiveDocs();\n        for (int field = 0; field < fieldValues.length; field++) {\n          String f = \"f\" + field;\n          BinaryDocValues bdv = r.getBinaryDocValues(f);\n          Bits docsWithField = r.getDocsWithField(f);\n          assertNotNull(bdv);\n          int maxDoc = r.maxDoc();\n          for (int doc = 0; doc < maxDoc; doc++) {\n            if (liveDocs == null || liveDocs.get(doc)) {\n//              System.out.println(\"doc=\" + (doc + context.docBase) + \" f='\" + f + \"' vslue=\" + getValue(bdv, doc, scratch));\n              assertTrue(docsWithField.get(doc));\n              assertEquals(\"invalid value for doc=\" + doc + \", field=\" + f + \", reader=\" + r, fieldValues[field], getValue(bdv, doc));\n            }\n          }\n        }\n      }\n//      System.out.println();\n    }\n\n    writer.shutdown();\n    IOUtils.close(reader, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testManyReopensAndFields().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testManyReopensAndFields().mjava","sourceNew":"  public void testManyReopensAndFields() throws Exception {\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    LogMergePolicy lmp = newLogMergePolicy();\n    lmp.setMergeFactor(3); // merge often\n    conf.setMergePolicy(lmp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    final boolean isNRT = random.nextBoolean();\n    DirectoryReader reader;\n    if (isNRT) {\n      reader = DirectoryReader.open(writer, true);\n    } else {\n      writer.commit();\n      reader = DirectoryReader.open(dir);\n    }\n    \n    final int numFields = random.nextInt(4) + 3; // 3-7\n    final long[] fieldValues = new long[numFields];\n    for (int i = 0; i < fieldValues.length; i++) {\n      fieldValues[i] = 1;\n    }\n    \n    int numRounds = atLeast(15);\n    int docID = 0;\n    for (int i = 0; i < numRounds; i++) {\n      int numDocs = atLeast(5);\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: round=\" + i + \", numDocs=\" + numDocs);\n      for (int j = 0; j < numDocs; j++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"doc-\" + docID, Store.NO));\n        doc.add(new StringField(\"key\", \"all\", Store.NO)); // update key\n        // add all fields with their current value\n        for (int f = 0; f < fieldValues.length; f++) {\n          doc.add(new BinaryDocValuesField(\"f\" + f, toBytes(fieldValues[f])));\n        }\n        writer.addDocument(doc);\n        ++docID;\n      }\n      \n      int fieldIdx = random.nextInt(fieldValues.length);\n      String updateField = \"f\" + fieldIdx;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: updated field '\" + updateField + \"' to value \" + fieldValues[fieldIdx]);\n      writer.updateBinaryDocValue(new Term(\"key\", \"all\"), updateField, toBytes(++fieldValues[fieldIdx]));\n      \n      if (random.nextDouble() < 0.2) {\n        int deleteDoc = random.nextInt(docID); // might also delete an already deleted document, ok!\n        writer.deleteDocuments(new Term(\"id\", \"doc-\" + deleteDoc));\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"]: deleted document: doc-\" + deleteDoc);\n      }\n      \n      // verify reader\n      if (!isNRT) {\n        writer.commit();\n      }\n      \n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopen reader: \" + reader);\n      DirectoryReader newReader = DirectoryReader.openIfChanged(reader);\n      assertNotNull(newReader);\n      reader.close();\n      reader = newReader;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopened reader: \" + reader);\n      assertTrue(reader.numDocs() > 0); // we delete at most one document per round\n      for (AtomicReaderContext context : reader.leaves()) {\n        AtomicReader r = context.reader();\n//        System.out.println(((SegmentReader) r).getSegmentName());\n        Bits liveDocs = r.getLiveDocs();\n        for (int field = 0; field < fieldValues.length; field++) {\n          String f = \"f\" + field;\n          BinaryDocValues bdv = r.getBinaryDocValues(f);\n          Bits docsWithField = r.getDocsWithField(f);\n          assertNotNull(bdv);\n          int maxDoc = r.maxDoc();\n          for (int doc = 0; doc < maxDoc; doc++) {\n            if (liveDocs == null || liveDocs.get(doc)) {\n//              System.out.println(\"doc=\" + (doc + context.docBase) + \" f='\" + f + \"' vslue=\" + getValue(bdv, doc, scratch));\n              assertTrue(docsWithField.get(doc));\n              assertEquals(\"invalid value for doc=\" + doc + \", field=\" + f + \", reader=\" + r, fieldValues[field], getValue(bdv, doc));\n            }\n          }\n        }\n      }\n//      System.out.println();\n    }\n\n    writer.close();\n    IOUtils.close(reader, dir);\n  }\n\n","sourceOld":"  public void testManyReopensAndFields() throws Exception {\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    LogMergePolicy lmp = newLogMergePolicy();\n    lmp.setMergeFactor(3); // merge often\n    conf.setMergePolicy(lmp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    final boolean isNRT = random.nextBoolean();\n    DirectoryReader reader;\n    if (isNRT) {\n      reader = DirectoryReader.open(writer, true);\n    } else {\n      writer.commit();\n      reader = DirectoryReader.open(dir);\n    }\n    \n    final int numFields = random.nextInt(4) + 3; // 3-7\n    final long[] fieldValues = new long[numFields];\n    for (int i = 0; i < fieldValues.length; i++) {\n      fieldValues[i] = 1;\n    }\n    \n    int numRounds = atLeast(15);\n    int docID = 0;\n    for (int i = 0; i < numRounds; i++) {\n      int numDocs = atLeast(5);\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: round=\" + i + \", numDocs=\" + numDocs);\n      for (int j = 0; j < numDocs; j++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"doc-\" + docID, Store.NO));\n        doc.add(new StringField(\"key\", \"all\", Store.NO)); // update key\n        // add all fields with their current value\n        for (int f = 0; f < fieldValues.length; f++) {\n          doc.add(new BinaryDocValuesField(\"f\" + f, toBytes(fieldValues[f])));\n        }\n        writer.addDocument(doc);\n        ++docID;\n      }\n      \n      int fieldIdx = random.nextInt(fieldValues.length);\n      String updateField = \"f\" + fieldIdx;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: updated field '\" + updateField + \"' to value \" + fieldValues[fieldIdx]);\n      writer.updateBinaryDocValue(new Term(\"key\", \"all\"), updateField, toBytes(++fieldValues[fieldIdx]));\n      \n      if (random.nextDouble() < 0.2) {\n        int deleteDoc = random.nextInt(docID); // might also delete an already deleted document, ok!\n        writer.deleteDocuments(new Term(\"id\", \"doc-\" + deleteDoc));\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"]: deleted document: doc-\" + deleteDoc);\n      }\n      \n      // verify reader\n      if (!isNRT) {\n        writer.commit();\n      }\n      \n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopen reader: \" + reader);\n      DirectoryReader newReader = DirectoryReader.openIfChanged(reader);\n      assertNotNull(newReader);\n      reader.close();\n      reader = newReader;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopened reader: \" + reader);\n      assertTrue(reader.numDocs() > 0); // we delete at most one document per round\n      for (AtomicReaderContext context : reader.leaves()) {\n        AtomicReader r = context.reader();\n//        System.out.println(((SegmentReader) r).getSegmentName());\n        Bits liveDocs = r.getLiveDocs();\n        for (int field = 0; field < fieldValues.length; field++) {\n          String f = \"f\" + field;\n          BinaryDocValues bdv = r.getBinaryDocValues(f);\n          Bits docsWithField = r.getDocsWithField(f);\n          assertNotNull(bdv);\n          int maxDoc = r.maxDoc();\n          for (int doc = 0; doc < maxDoc; doc++) {\n            if (liveDocs == null || liveDocs.get(doc)) {\n//              System.out.println(\"doc=\" + (doc + context.docBase) + \" f='\" + f + \"' vslue=\" + getValue(bdv, doc, scratch));\n              assertTrue(docsWithField.get(doc));\n              assertEquals(\"invalid value for doc=\" + doc + \", field=\" + f + \", reader=\" + r, fieldValues[field], getValue(bdv, doc));\n            }\n          }\n        }\n      }\n//      System.out.println();\n    }\n\n    writer.shutdown();\n    IOUtils.close(reader, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testManyReopensAndFields().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testManyReopensAndFields().mjava","sourceNew":"  public void testManyReopensAndFields() throws Exception {\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    LogMergePolicy lmp = newLogMergePolicy();\n    lmp.setMergeFactor(3); // merge often\n    conf.setMergePolicy(lmp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    final boolean isNRT = random.nextBoolean();\n    DirectoryReader reader;\n    if (isNRT) {\n      reader = DirectoryReader.open(writer, true);\n    } else {\n      writer.commit();\n      reader = DirectoryReader.open(dir);\n    }\n    \n    final int numFields = random.nextInt(4) + 3; // 3-7\n    final long[] fieldValues = new long[numFields];\n    for (int i = 0; i < fieldValues.length; i++) {\n      fieldValues[i] = 1;\n    }\n    \n    int numRounds = atLeast(15);\n    int docID = 0;\n    for (int i = 0; i < numRounds; i++) {\n      int numDocs = atLeast(5);\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: round=\" + i + \", numDocs=\" + numDocs);\n      for (int j = 0; j < numDocs; j++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"doc-\" + docID, Store.NO));\n        doc.add(new StringField(\"key\", \"all\", Store.NO)); // update key\n        // add all fields with their current value\n        for (int f = 0; f < fieldValues.length; f++) {\n          doc.add(new BinaryDocValuesField(\"f\" + f, toBytes(fieldValues[f])));\n        }\n        writer.addDocument(doc);\n        ++docID;\n      }\n      \n      int fieldIdx = random.nextInt(fieldValues.length);\n      String updateField = \"f\" + fieldIdx;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: updated field '\" + updateField + \"' to value \" + fieldValues[fieldIdx]);\n      writer.updateBinaryDocValue(new Term(\"key\", \"all\"), updateField, toBytes(++fieldValues[fieldIdx]));\n      \n      if (random.nextDouble() < 0.2) {\n        int deleteDoc = random.nextInt(docID); // might also delete an already deleted document, ok!\n        writer.deleteDocuments(new Term(\"id\", \"doc-\" + deleteDoc));\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"]: deleted document: doc-\" + deleteDoc);\n      }\n      \n      // verify reader\n      if (!isNRT) {\n        writer.commit();\n      }\n      \n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopen reader: \" + reader);\n      DirectoryReader newReader = DirectoryReader.openIfChanged(reader);\n      assertNotNull(newReader);\n      reader.close();\n      reader = newReader;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopened reader: \" + reader);\n      assertTrue(reader.numDocs() > 0); // we delete at most one document per round\n      for (LeafReaderContext context : reader.leaves()) {\n        LeafReader r = context.reader();\n//        System.out.println(((SegmentReader) r).getSegmentName());\n        Bits liveDocs = r.getLiveDocs();\n        for (int field = 0; field < fieldValues.length; field++) {\n          String f = \"f\" + field;\n          BinaryDocValues bdv = r.getBinaryDocValues(f);\n          Bits docsWithField = r.getDocsWithField(f);\n          assertNotNull(bdv);\n          int maxDoc = r.maxDoc();\n          for (int doc = 0; doc < maxDoc; doc++) {\n            if (liveDocs == null || liveDocs.get(doc)) {\n//              System.out.println(\"doc=\" + (doc + context.docBase) + \" f='\" + f + \"' vslue=\" + getValue(bdv, doc, scratch));\n              assertTrue(docsWithField.get(doc));\n              assertEquals(\"invalid value for doc=\" + doc + \", field=\" + f + \", reader=\" + r, fieldValues[field], getValue(bdv, doc));\n            }\n          }\n        }\n      }\n//      System.out.println();\n    }\n\n    writer.close();\n    IOUtils.close(reader, dir);\n  }\n\n","sourceOld":"  public void testManyReopensAndFields() throws Exception {\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    LogMergePolicy lmp = newLogMergePolicy();\n    lmp.setMergeFactor(3); // merge often\n    conf.setMergePolicy(lmp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    final boolean isNRT = random.nextBoolean();\n    DirectoryReader reader;\n    if (isNRT) {\n      reader = DirectoryReader.open(writer, true);\n    } else {\n      writer.commit();\n      reader = DirectoryReader.open(dir);\n    }\n    \n    final int numFields = random.nextInt(4) + 3; // 3-7\n    final long[] fieldValues = new long[numFields];\n    for (int i = 0; i < fieldValues.length; i++) {\n      fieldValues[i] = 1;\n    }\n    \n    int numRounds = atLeast(15);\n    int docID = 0;\n    for (int i = 0; i < numRounds; i++) {\n      int numDocs = atLeast(5);\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: round=\" + i + \", numDocs=\" + numDocs);\n      for (int j = 0; j < numDocs; j++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"doc-\" + docID, Store.NO));\n        doc.add(new StringField(\"key\", \"all\", Store.NO)); // update key\n        // add all fields with their current value\n        for (int f = 0; f < fieldValues.length; f++) {\n          doc.add(new BinaryDocValuesField(\"f\" + f, toBytes(fieldValues[f])));\n        }\n        writer.addDocument(doc);\n        ++docID;\n      }\n      \n      int fieldIdx = random.nextInt(fieldValues.length);\n      String updateField = \"f\" + fieldIdx;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: updated field '\" + updateField + \"' to value \" + fieldValues[fieldIdx]);\n      writer.updateBinaryDocValue(new Term(\"key\", \"all\"), updateField, toBytes(++fieldValues[fieldIdx]));\n      \n      if (random.nextDouble() < 0.2) {\n        int deleteDoc = random.nextInt(docID); // might also delete an already deleted document, ok!\n        writer.deleteDocuments(new Term(\"id\", \"doc-\" + deleteDoc));\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"]: deleted document: doc-\" + deleteDoc);\n      }\n      \n      // verify reader\n      if (!isNRT) {\n        writer.commit();\n      }\n      \n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopen reader: \" + reader);\n      DirectoryReader newReader = DirectoryReader.openIfChanged(reader);\n      assertNotNull(newReader);\n      reader.close();\n      reader = newReader;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopened reader: \" + reader);\n      assertTrue(reader.numDocs() > 0); // we delete at most one document per round\n      for (AtomicReaderContext context : reader.leaves()) {\n        AtomicReader r = context.reader();\n//        System.out.println(((SegmentReader) r).getSegmentName());\n        Bits liveDocs = r.getLiveDocs();\n        for (int field = 0; field < fieldValues.length; field++) {\n          String f = \"f\" + field;\n          BinaryDocValues bdv = r.getBinaryDocValues(f);\n          Bits docsWithField = r.getDocsWithField(f);\n          assertNotNull(bdv);\n          int maxDoc = r.maxDoc();\n          for (int doc = 0; doc < maxDoc; doc++) {\n            if (liveDocs == null || liveDocs.get(doc)) {\n//              System.out.println(\"doc=\" + (doc + context.docBase) + \" f='\" + f + \"' vslue=\" + getValue(bdv, doc, scratch));\n              assertTrue(docsWithField.get(doc));\n              assertEquals(\"invalid value for doc=\" + doc + \", field=\" + f + \", reader=\" + r, fieldValues[field], getValue(bdv, doc));\n            }\n          }\n        }\n      }\n//      System.out.println();\n    }\n\n    writer.close();\n    IOUtils.close(reader, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7e4c214a1f904dde76f5611b56d4081533055b3b","date":1421938451,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testManyReopensAndFields().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testManyReopensAndFields().mjava","sourceNew":"  public void testManyReopensAndFields() throws Exception {\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    LogMergePolicy lmp = newLogMergePolicy();\n    lmp.setMergeFactor(3); // merge often\n    conf.setMergePolicy(lmp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    final boolean isNRT = random.nextBoolean();\n    DirectoryReader reader;\n    if (isNRT) {\n      reader = DirectoryReader.open(writer, true);\n    } else {\n      writer.commit();\n      reader = DirectoryReader.open(dir);\n    }\n    //System.out.println(\"TEST: isNRT=\" + isNRT);\n    \n    final int numFields = random.nextInt(4) + 3; // 3-7\n    final long[] fieldValues = new long[numFields];\n    for (int i = 0; i < fieldValues.length; i++) {\n      fieldValues[i] = 1;\n    }\n    \n    int numRounds = atLeast(15);\n    int docID = 0;\n    for (int i = 0; i < numRounds; i++) {\n      int numDocs = atLeast(5);\n      //System.out.println(\"[\" + Thread.currentThread().getName() + \"]: round=\" + i + \", numDocs=\" + numDocs);\n      for (int j = 0; j < numDocs; j++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"doc-\" + docID, Store.NO));\n        doc.add(new StringField(\"key\", \"all\", Store.NO)); // update key\n        // add all fields with their current value\n        for (int f = 0; f < fieldValues.length; f++) {\n          doc.add(new BinaryDocValuesField(\"f\" + f, toBytes(fieldValues[f])));\n        }\n        writer.addDocument(doc);\n        ++docID;\n      }\n      \n      int fieldIdx = random.nextInt(fieldValues.length);\n      String updateField = \"f\" + fieldIdx;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: updated field '\" + updateField + \"' to value \" + fieldValues[fieldIdx]);\n      writer.updateBinaryDocValue(new Term(\"key\", \"all\"), updateField, toBytes(++fieldValues[fieldIdx]));\n      \n      if (random.nextDouble() < 0.2) {\n        int deleteDoc = random.nextInt(docID); // might also delete an already deleted document, ok!\n        writer.deleteDocuments(new Term(\"id\", \"doc-\" + deleteDoc));\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"]: deleted document: doc-\" + deleteDoc);\n      }\n      \n      // verify reader\n      if (!isNRT) {\n        writer.commit();\n      }\n      \n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopen reader: \" + reader);\n      DirectoryReader newReader = DirectoryReader.openIfChanged(reader);\n      assertNotNull(newReader);\n      reader.close();\n      reader = newReader;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopened reader: \" + reader);\n      assertTrue(reader.numDocs() > 0); // we delete at most one document per round\n      for (LeafReaderContext context : reader.leaves()) {\n        LeafReader r = context.reader();\n//        System.out.println(((SegmentReader) r).getSegmentName());\n        Bits liveDocs = r.getLiveDocs();\n        for (int field = 0; field < fieldValues.length; field++) {\n          String f = \"f\" + field;\n          BinaryDocValues bdv = r.getBinaryDocValues(f);\n          Bits docsWithField = r.getDocsWithField(f);\n          assertNotNull(bdv);\n          int maxDoc = r.maxDoc();\n          for (int doc = 0; doc < maxDoc; doc++) {\n            if (liveDocs == null || liveDocs.get(doc)) {\n//              System.out.println(\"doc=\" + (doc + context.docBase) + \" f='\" + f + \"' vslue=\" + getValue(bdv, doc, scratch));\n              assertTrue(docsWithField.get(doc));\n              assertEquals(\"invalid value for doc=\" + doc + \", field=\" + f + \", reader=\" + r, fieldValues[field], getValue(bdv, doc));\n            }\n          }\n        }\n      }\n//      System.out.println();\n    }\n\n    writer.close();\n    IOUtils.close(reader, dir);\n  }\n\n","sourceOld":"  public void testManyReopensAndFields() throws Exception {\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    LogMergePolicy lmp = newLogMergePolicy();\n    lmp.setMergeFactor(3); // merge often\n    conf.setMergePolicy(lmp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    final boolean isNRT = random.nextBoolean();\n    DirectoryReader reader;\n    if (isNRT) {\n      reader = DirectoryReader.open(writer, true);\n    } else {\n      writer.commit();\n      reader = DirectoryReader.open(dir);\n    }\n    \n    final int numFields = random.nextInt(4) + 3; // 3-7\n    final long[] fieldValues = new long[numFields];\n    for (int i = 0; i < fieldValues.length; i++) {\n      fieldValues[i] = 1;\n    }\n    \n    int numRounds = atLeast(15);\n    int docID = 0;\n    for (int i = 0; i < numRounds; i++) {\n      int numDocs = atLeast(5);\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: round=\" + i + \", numDocs=\" + numDocs);\n      for (int j = 0; j < numDocs; j++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"doc-\" + docID, Store.NO));\n        doc.add(new StringField(\"key\", \"all\", Store.NO)); // update key\n        // add all fields with their current value\n        for (int f = 0; f < fieldValues.length; f++) {\n          doc.add(new BinaryDocValuesField(\"f\" + f, toBytes(fieldValues[f])));\n        }\n        writer.addDocument(doc);\n        ++docID;\n      }\n      \n      int fieldIdx = random.nextInt(fieldValues.length);\n      String updateField = \"f\" + fieldIdx;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: updated field '\" + updateField + \"' to value \" + fieldValues[fieldIdx]);\n      writer.updateBinaryDocValue(new Term(\"key\", \"all\"), updateField, toBytes(++fieldValues[fieldIdx]));\n      \n      if (random.nextDouble() < 0.2) {\n        int deleteDoc = random.nextInt(docID); // might also delete an already deleted document, ok!\n        writer.deleteDocuments(new Term(\"id\", \"doc-\" + deleteDoc));\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"]: deleted document: doc-\" + deleteDoc);\n      }\n      \n      // verify reader\n      if (!isNRT) {\n        writer.commit();\n      }\n      \n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopen reader: \" + reader);\n      DirectoryReader newReader = DirectoryReader.openIfChanged(reader);\n      assertNotNull(newReader);\n      reader.close();\n      reader = newReader;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopened reader: \" + reader);\n      assertTrue(reader.numDocs() > 0); // we delete at most one document per round\n      for (LeafReaderContext context : reader.leaves()) {\n        LeafReader r = context.reader();\n//        System.out.println(((SegmentReader) r).getSegmentName());\n        Bits liveDocs = r.getLiveDocs();\n        for (int field = 0; field < fieldValues.length; field++) {\n          String f = \"f\" + field;\n          BinaryDocValues bdv = r.getBinaryDocValues(f);\n          Bits docsWithField = r.getDocsWithField(f);\n          assertNotNull(bdv);\n          int maxDoc = r.maxDoc();\n          for (int doc = 0; doc < maxDoc; doc++) {\n            if (liveDocs == null || liveDocs.get(doc)) {\n//              System.out.println(\"doc=\" + (doc + context.docBase) + \" f='\" + f + \"' vslue=\" + getValue(bdv, doc, scratch));\n              assertTrue(docsWithField.get(doc));\n              assertEquals(\"invalid value for doc=\" + doc + \", field=\" + f + \", reader=\" + r, fieldValues[field], getValue(bdv, doc));\n            }\n          }\n        }\n      }\n//      System.out.println();\n    }\n\n    writer.close();\n    IOUtils.close(reader, dir);\n  }\n\n","bugFix":["06805da26538ed636bd89b10c2699cc3834032ae"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2a1862266772deb28cdcb7d996b64d2177022687","date":1453077824,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testManyReopensAndFields().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testManyReopensAndFields().mjava","sourceNew":"  public void testManyReopensAndFields() throws Exception {\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    LogMergePolicy lmp = newLogMergePolicy();\n    lmp.setMergeFactor(3); // merge often\n    conf.setMergePolicy(lmp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    final boolean isNRT = random.nextBoolean();\n    DirectoryReader reader;\n    if (isNRT) {\n      reader = DirectoryReader.open(writer);\n    } else {\n      writer.commit();\n      reader = DirectoryReader.open(dir);\n    }\n    //System.out.println(\"TEST: isNRT=\" + isNRT);\n    \n    final int numFields = random.nextInt(4) + 3; // 3-7\n    final long[] fieldValues = new long[numFields];\n    for (int i = 0; i < fieldValues.length; i++) {\n      fieldValues[i] = 1;\n    }\n    \n    int numRounds = atLeast(15);\n    int docID = 0;\n    for (int i = 0; i < numRounds; i++) {\n      int numDocs = atLeast(5);\n      //System.out.println(\"[\" + Thread.currentThread().getName() + \"]: round=\" + i + \", numDocs=\" + numDocs);\n      for (int j = 0; j < numDocs; j++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"doc-\" + docID, Store.NO));\n        doc.add(new StringField(\"key\", \"all\", Store.NO)); // update key\n        // add all fields with their current value\n        for (int f = 0; f < fieldValues.length; f++) {\n          doc.add(new BinaryDocValuesField(\"f\" + f, toBytes(fieldValues[f])));\n        }\n        writer.addDocument(doc);\n        ++docID;\n      }\n      \n      int fieldIdx = random.nextInt(fieldValues.length);\n      String updateField = \"f\" + fieldIdx;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: updated field '\" + updateField + \"' to value \" + fieldValues[fieldIdx]);\n      writer.updateBinaryDocValue(new Term(\"key\", \"all\"), updateField, toBytes(++fieldValues[fieldIdx]));\n      \n      if (random.nextDouble() < 0.2) {\n        int deleteDoc = random.nextInt(docID); // might also delete an already deleted document, ok!\n        writer.deleteDocuments(new Term(\"id\", \"doc-\" + deleteDoc));\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"]: deleted document: doc-\" + deleteDoc);\n      }\n      \n      // verify reader\n      if (!isNRT) {\n        writer.commit();\n      }\n      \n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopen reader: \" + reader);\n      DirectoryReader newReader = DirectoryReader.openIfChanged(reader);\n      assertNotNull(newReader);\n      reader.close();\n      reader = newReader;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopened reader: \" + reader);\n      assertTrue(reader.numDocs() > 0); // we delete at most one document per round\n      for (LeafReaderContext context : reader.leaves()) {\n        LeafReader r = context.reader();\n//        System.out.println(((SegmentReader) r).getSegmentName());\n        Bits liveDocs = r.getLiveDocs();\n        for (int field = 0; field < fieldValues.length; field++) {\n          String f = \"f\" + field;\n          BinaryDocValues bdv = r.getBinaryDocValues(f);\n          Bits docsWithField = r.getDocsWithField(f);\n          assertNotNull(bdv);\n          int maxDoc = r.maxDoc();\n          for (int doc = 0; doc < maxDoc; doc++) {\n            if (liveDocs == null || liveDocs.get(doc)) {\n//              System.out.println(\"doc=\" + (doc + context.docBase) + \" f='\" + f + \"' vslue=\" + getValue(bdv, doc, scratch));\n              assertTrue(docsWithField.get(doc));\n              assertEquals(\"invalid value for doc=\" + doc + \", field=\" + f + \", reader=\" + r, fieldValues[field], getValue(bdv, doc));\n            }\n          }\n        }\n      }\n//      System.out.println();\n    }\n\n    writer.close();\n    IOUtils.close(reader, dir);\n  }\n\n","sourceOld":"  public void testManyReopensAndFields() throws Exception {\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    LogMergePolicy lmp = newLogMergePolicy();\n    lmp.setMergeFactor(3); // merge often\n    conf.setMergePolicy(lmp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    final boolean isNRT = random.nextBoolean();\n    DirectoryReader reader;\n    if (isNRT) {\n      reader = DirectoryReader.open(writer, true);\n    } else {\n      writer.commit();\n      reader = DirectoryReader.open(dir);\n    }\n    //System.out.println(\"TEST: isNRT=\" + isNRT);\n    \n    final int numFields = random.nextInt(4) + 3; // 3-7\n    final long[] fieldValues = new long[numFields];\n    for (int i = 0; i < fieldValues.length; i++) {\n      fieldValues[i] = 1;\n    }\n    \n    int numRounds = atLeast(15);\n    int docID = 0;\n    for (int i = 0; i < numRounds; i++) {\n      int numDocs = atLeast(5);\n      //System.out.println(\"[\" + Thread.currentThread().getName() + \"]: round=\" + i + \", numDocs=\" + numDocs);\n      for (int j = 0; j < numDocs; j++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"doc-\" + docID, Store.NO));\n        doc.add(new StringField(\"key\", \"all\", Store.NO)); // update key\n        // add all fields with their current value\n        for (int f = 0; f < fieldValues.length; f++) {\n          doc.add(new BinaryDocValuesField(\"f\" + f, toBytes(fieldValues[f])));\n        }\n        writer.addDocument(doc);\n        ++docID;\n      }\n      \n      int fieldIdx = random.nextInt(fieldValues.length);\n      String updateField = \"f\" + fieldIdx;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: updated field '\" + updateField + \"' to value \" + fieldValues[fieldIdx]);\n      writer.updateBinaryDocValue(new Term(\"key\", \"all\"), updateField, toBytes(++fieldValues[fieldIdx]));\n      \n      if (random.nextDouble() < 0.2) {\n        int deleteDoc = random.nextInt(docID); // might also delete an already deleted document, ok!\n        writer.deleteDocuments(new Term(\"id\", \"doc-\" + deleteDoc));\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"]: deleted document: doc-\" + deleteDoc);\n      }\n      \n      // verify reader\n      if (!isNRT) {\n        writer.commit();\n      }\n      \n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopen reader: \" + reader);\n      DirectoryReader newReader = DirectoryReader.openIfChanged(reader);\n      assertNotNull(newReader);\n      reader.close();\n      reader = newReader;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopened reader: \" + reader);\n      assertTrue(reader.numDocs() > 0); // we delete at most one document per round\n      for (LeafReaderContext context : reader.leaves()) {\n        LeafReader r = context.reader();\n//        System.out.println(((SegmentReader) r).getSegmentName());\n        Bits liveDocs = r.getLiveDocs();\n        for (int field = 0; field < fieldValues.length; field++) {\n          String f = \"f\" + field;\n          BinaryDocValues bdv = r.getBinaryDocValues(f);\n          Bits docsWithField = r.getDocsWithField(f);\n          assertNotNull(bdv);\n          int maxDoc = r.maxDoc();\n          for (int doc = 0; doc < maxDoc; doc++) {\n            if (liveDocs == null || liveDocs.get(doc)) {\n//              System.out.println(\"doc=\" + (doc + context.docBase) + \" f='\" + f + \"' vslue=\" + getValue(bdv, doc, scratch));\n              assertTrue(docsWithField.get(doc));\n              assertEquals(\"invalid value for doc=\" + doc + \", field=\" + f + \", reader=\" + r, fieldValues[field], getValue(bdv, doc));\n            }\n          }\n        }\n      }\n//      System.out.println();\n    }\n\n    writer.close();\n    IOUtils.close(reader, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6652c74b2358a0b13223817a6a793bf1c9d0749d","date":1474465301,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testManyReopensAndFields().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testManyReopensAndFields().mjava","sourceNew":"  public void testManyReopensAndFields() throws Exception {\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    LogMergePolicy lmp = newLogMergePolicy();\n    lmp.setMergeFactor(3); // merge often\n    conf.setMergePolicy(lmp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    final boolean isNRT = random.nextBoolean();\n    DirectoryReader reader;\n    if (isNRT) {\n      reader = DirectoryReader.open(writer);\n    } else {\n      writer.commit();\n      reader = DirectoryReader.open(dir);\n    }\n    //System.out.println(\"TEST: isNRT=\" + isNRT);\n    \n    final int numFields = random.nextInt(4) + 3; // 3-7\n    final long[] fieldValues = new long[numFields];\n    for (int i = 0; i < fieldValues.length; i++) {\n      fieldValues[i] = 1;\n    }\n    \n    int numRounds = atLeast(15);\n    int docID = 0;\n    for (int i = 0; i < numRounds; i++) {\n      int numDocs = atLeast(5);\n      //System.out.println(\"[\" + Thread.currentThread().getName() + \"]: round=\" + i + \", numDocs=\" + numDocs);\n      for (int j = 0; j < numDocs; j++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"doc-\" + docID, Store.NO));\n        doc.add(new StringField(\"key\", \"all\", Store.NO)); // update key\n        // add all fields with their current value\n        for (int f = 0; f < fieldValues.length; f++) {\n          doc.add(new BinaryDocValuesField(\"f\" + f, toBytes(fieldValues[f])));\n        }\n        writer.addDocument(doc);\n        ++docID;\n      }\n      \n      int fieldIdx = random.nextInt(fieldValues.length);\n      String updateField = \"f\" + fieldIdx;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: updated field '\" + updateField + \"' to value \" + fieldValues[fieldIdx]);\n      writer.updateBinaryDocValue(new Term(\"key\", \"all\"), updateField, toBytes(++fieldValues[fieldIdx]));\n      \n      if (random.nextDouble() < 0.2) {\n        int deleteDoc = random.nextInt(docID); // might also delete an already deleted document, ok!\n        writer.deleteDocuments(new Term(\"id\", \"doc-\" + deleteDoc));\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"]: deleted document: doc-\" + deleteDoc);\n      }\n      \n      // verify reader\n      if (!isNRT) {\n        writer.commit();\n      }\n      \n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopen reader: \" + reader);\n      DirectoryReader newReader = DirectoryReader.openIfChanged(reader);\n      assertNotNull(newReader);\n      reader.close();\n      reader = newReader;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopened reader: \" + reader);\n      assertTrue(reader.numDocs() > 0); // we delete at most one document per round\n      for (LeafReaderContext context : reader.leaves()) {\n        LeafReader r = context.reader();\n//        System.out.println(((SegmentReader) r).getSegmentName());\n        Bits liveDocs = r.getLiveDocs();\n        for (int field = 0; field < fieldValues.length; field++) {\n          String f = \"f\" + field;\n          BinaryDocValues bdv = r.getBinaryDocValues(f);\n          assertNotNull(bdv);\n          int maxDoc = r.maxDoc();\n          for (int doc = 0; doc < maxDoc; doc++) {\n            if (liveDocs == null || liveDocs.get(doc)) {\n              assertEquals(doc, bdv.advance(doc));\n              assertEquals(\"invalid value for doc=\" + doc + \", field=\" + f + \", reader=\" + r, fieldValues[field], getValue(bdv));\n            }\n          }\n        }\n      }\n//      System.out.println();\n    }\n\n    writer.close();\n    IOUtils.close(reader, dir);\n  }\n\n","sourceOld":"  public void testManyReopensAndFields() throws Exception {\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    LogMergePolicy lmp = newLogMergePolicy();\n    lmp.setMergeFactor(3); // merge often\n    conf.setMergePolicy(lmp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    final boolean isNRT = random.nextBoolean();\n    DirectoryReader reader;\n    if (isNRT) {\n      reader = DirectoryReader.open(writer);\n    } else {\n      writer.commit();\n      reader = DirectoryReader.open(dir);\n    }\n    //System.out.println(\"TEST: isNRT=\" + isNRT);\n    \n    final int numFields = random.nextInt(4) + 3; // 3-7\n    final long[] fieldValues = new long[numFields];\n    for (int i = 0; i < fieldValues.length; i++) {\n      fieldValues[i] = 1;\n    }\n    \n    int numRounds = atLeast(15);\n    int docID = 0;\n    for (int i = 0; i < numRounds; i++) {\n      int numDocs = atLeast(5);\n      //System.out.println(\"[\" + Thread.currentThread().getName() + \"]: round=\" + i + \", numDocs=\" + numDocs);\n      for (int j = 0; j < numDocs; j++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"doc-\" + docID, Store.NO));\n        doc.add(new StringField(\"key\", \"all\", Store.NO)); // update key\n        // add all fields with their current value\n        for (int f = 0; f < fieldValues.length; f++) {\n          doc.add(new BinaryDocValuesField(\"f\" + f, toBytes(fieldValues[f])));\n        }\n        writer.addDocument(doc);\n        ++docID;\n      }\n      \n      int fieldIdx = random.nextInt(fieldValues.length);\n      String updateField = \"f\" + fieldIdx;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: updated field '\" + updateField + \"' to value \" + fieldValues[fieldIdx]);\n      writer.updateBinaryDocValue(new Term(\"key\", \"all\"), updateField, toBytes(++fieldValues[fieldIdx]));\n      \n      if (random.nextDouble() < 0.2) {\n        int deleteDoc = random.nextInt(docID); // might also delete an already deleted document, ok!\n        writer.deleteDocuments(new Term(\"id\", \"doc-\" + deleteDoc));\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"]: deleted document: doc-\" + deleteDoc);\n      }\n      \n      // verify reader\n      if (!isNRT) {\n        writer.commit();\n      }\n      \n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopen reader: \" + reader);\n      DirectoryReader newReader = DirectoryReader.openIfChanged(reader);\n      assertNotNull(newReader);\n      reader.close();\n      reader = newReader;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopened reader: \" + reader);\n      assertTrue(reader.numDocs() > 0); // we delete at most one document per round\n      for (LeafReaderContext context : reader.leaves()) {\n        LeafReader r = context.reader();\n//        System.out.println(((SegmentReader) r).getSegmentName());\n        Bits liveDocs = r.getLiveDocs();\n        for (int field = 0; field < fieldValues.length; field++) {\n          String f = \"f\" + field;\n          BinaryDocValues bdv = r.getBinaryDocValues(f);\n          Bits docsWithField = r.getDocsWithField(f);\n          assertNotNull(bdv);\n          int maxDoc = r.maxDoc();\n          for (int doc = 0; doc < maxDoc; doc++) {\n            if (liveDocs == null || liveDocs.get(doc)) {\n//              System.out.println(\"doc=\" + (doc + context.docBase) + \" f='\" + f + \"' vslue=\" + getValue(bdv, doc, scratch));\n              assertTrue(docsWithField.get(doc));\n              assertEquals(\"invalid value for doc=\" + doc + \", field=\" + f + \", reader=\" + r, fieldValues[field], getValue(bdv, doc));\n            }\n          }\n        }\n      }\n//      System.out.println();\n    }\n\n    writer.close();\n    IOUtils.close(reader, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"17e5da53e4e5bd659e22add9bba1cfa222e7e30d","date":1475435902,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testManyReopensAndFields().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testManyReopensAndFields().mjava","sourceNew":"  public void testManyReopensAndFields() throws Exception {\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    LogMergePolicy lmp = newLogMergePolicy();\n    lmp.setMergeFactor(3); // merge often\n    conf.setMergePolicy(lmp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    final boolean isNRT = random.nextBoolean();\n    DirectoryReader reader;\n    if (isNRT) {\n      reader = DirectoryReader.open(writer);\n    } else {\n      writer.commit();\n      reader = DirectoryReader.open(dir);\n    }\n    //System.out.println(\"TEST: isNRT=\" + isNRT);\n    \n    final int numFields = random.nextInt(4) + 3; // 3-7\n    final long[] fieldValues = new long[numFields];\n    for (int i = 0; i < fieldValues.length; i++) {\n      fieldValues[i] = 1;\n    }\n    \n    int numRounds = atLeast(15);\n    int docID = 0;\n    for (int i = 0; i < numRounds; i++) {\n      int numDocs = atLeast(5);\n      //System.out.println(\"[\" + Thread.currentThread().getName() + \"]: round=\" + i + \", numDocs=\" + numDocs);\n      for (int j = 0; j < numDocs; j++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"doc-\" + docID, Store.NO));\n        doc.add(new StringField(\"key\", \"all\", Store.NO)); // update key\n        // add all fields with their current value\n        for (int f = 0; f < fieldValues.length; f++) {\n          doc.add(new BinaryDocValuesField(\"f\" + f, toBytes(fieldValues[f])));\n        }\n        writer.addDocument(doc);\n        ++docID;\n      }\n      \n      int fieldIdx = random.nextInt(fieldValues.length);\n      String updateField = \"f\" + fieldIdx;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: updated field '\" + updateField + \"' to value \" + fieldValues[fieldIdx]);\n      writer.updateBinaryDocValue(new Term(\"key\", \"all\"), updateField, toBytes(++fieldValues[fieldIdx]));\n      \n      if (random.nextDouble() < 0.2) {\n        int deleteDoc = random.nextInt(docID); // might also delete an already deleted document, ok!\n        writer.deleteDocuments(new Term(\"id\", \"doc-\" + deleteDoc));\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"]: deleted document: doc-\" + deleteDoc);\n      }\n      \n      // verify reader\n      if (!isNRT) {\n        writer.commit();\n      }\n      \n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopen reader: \" + reader);\n      DirectoryReader newReader = DirectoryReader.openIfChanged(reader);\n      assertNotNull(newReader);\n      reader.close();\n      reader = newReader;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopened reader: \" + reader);\n      assertTrue(reader.numDocs() > 0); // we delete at most one document per round\n      for (LeafReaderContext context : reader.leaves()) {\n        LeafReader r = context.reader();\n//        System.out.println(((SegmentReader) r).getSegmentName());\n        Bits liveDocs = r.getLiveDocs();\n        for (int field = 0; field < fieldValues.length; field++) {\n          String f = \"f\" + field;\n          BinaryDocValues bdv = r.getBinaryDocValues(f);\n          assertNotNull(bdv);\n          int maxDoc = r.maxDoc();\n          for (int doc = 0; doc < maxDoc; doc++) {\n            if (liveDocs == null || liveDocs.get(doc)) {\n              assertEquals(doc, bdv.advance(doc));\n              assertEquals(\"invalid value for doc=\" + doc + \", field=\" + f + \", reader=\" + r, fieldValues[field], getValue(bdv));\n            }\n          }\n        }\n      }\n//      System.out.println();\n    }\n\n    writer.close();\n    IOUtils.close(reader, dir);\n  }\n\n","sourceOld":"  public void testManyReopensAndFields() throws Exception {\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    LogMergePolicy lmp = newLogMergePolicy();\n    lmp.setMergeFactor(3); // merge often\n    conf.setMergePolicy(lmp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    final boolean isNRT = random.nextBoolean();\n    DirectoryReader reader;\n    if (isNRT) {\n      reader = DirectoryReader.open(writer);\n    } else {\n      writer.commit();\n      reader = DirectoryReader.open(dir);\n    }\n    //System.out.println(\"TEST: isNRT=\" + isNRT);\n    \n    final int numFields = random.nextInt(4) + 3; // 3-7\n    final long[] fieldValues = new long[numFields];\n    for (int i = 0; i < fieldValues.length; i++) {\n      fieldValues[i] = 1;\n    }\n    \n    int numRounds = atLeast(15);\n    int docID = 0;\n    for (int i = 0; i < numRounds; i++) {\n      int numDocs = atLeast(5);\n      //System.out.println(\"[\" + Thread.currentThread().getName() + \"]: round=\" + i + \", numDocs=\" + numDocs);\n      for (int j = 0; j < numDocs; j++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"doc-\" + docID, Store.NO));\n        doc.add(new StringField(\"key\", \"all\", Store.NO)); // update key\n        // add all fields with their current value\n        for (int f = 0; f < fieldValues.length; f++) {\n          doc.add(new BinaryDocValuesField(\"f\" + f, toBytes(fieldValues[f])));\n        }\n        writer.addDocument(doc);\n        ++docID;\n      }\n      \n      int fieldIdx = random.nextInt(fieldValues.length);\n      String updateField = \"f\" + fieldIdx;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: updated field '\" + updateField + \"' to value \" + fieldValues[fieldIdx]);\n      writer.updateBinaryDocValue(new Term(\"key\", \"all\"), updateField, toBytes(++fieldValues[fieldIdx]));\n      \n      if (random.nextDouble() < 0.2) {\n        int deleteDoc = random.nextInt(docID); // might also delete an already deleted document, ok!\n        writer.deleteDocuments(new Term(\"id\", \"doc-\" + deleteDoc));\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"]: deleted document: doc-\" + deleteDoc);\n      }\n      \n      // verify reader\n      if (!isNRT) {\n        writer.commit();\n      }\n      \n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopen reader: \" + reader);\n      DirectoryReader newReader = DirectoryReader.openIfChanged(reader);\n      assertNotNull(newReader);\n      reader.close();\n      reader = newReader;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopened reader: \" + reader);\n      assertTrue(reader.numDocs() > 0); // we delete at most one document per round\n      for (LeafReaderContext context : reader.leaves()) {\n        LeafReader r = context.reader();\n//        System.out.println(((SegmentReader) r).getSegmentName());\n        Bits liveDocs = r.getLiveDocs();\n        for (int field = 0; field < fieldValues.length; field++) {\n          String f = \"f\" + field;\n          BinaryDocValues bdv = r.getBinaryDocValues(f);\n          Bits docsWithField = r.getDocsWithField(f);\n          assertNotNull(bdv);\n          int maxDoc = r.maxDoc();\n          for (int doc = 0; doc < maxDoc; doc++) {\n            if (liveDocs == null || liveDocs.get(doc)) {\n//              System.out.println(\"doc=\" + (doc + context.docBase) + \" f='\" + f + \"' vslue=\" + getValue(bdv, doc, scratch));\n              assertTrue(docsWithField.get(doc));\n              assertEquals(\"invalid value for doc=\" + doc + \", field=\" + f + \", reader=\" + r, fieldValues[field], getValue(bdv, doc));\n            }\n          }\n        }\n      }\n//      System.out.println();\n    }\n\n    writer.close();\n    IOUtils.close(reader, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testManyReopensAndFields().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testManyReopensAndFields().mjava","sourceNew":"  public void testManyReopensAndFields() throws Exception {\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    LogMergePolicy lmp = newLogMergePolicy();\n    lmp.setMergeFactor(3); // merge often\n    conf.setMergePolicy(lmp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    final boolean isNRT = random.nextBoolean();\n    DirectoryReader reader;\n    if (isNRT) {\n      reader = DirectoryReader.open(writer);\n    } else {\n      writer.commit();\n      reader = DirectoryReader.open(dir);\n    }\n    //System.out.println(\"TEST: isNRT=\" + isNRT);\n    \n    final int numFields = random.nextInt(4) + 3; // 3-7\n    final long[] fieldValues = new long[numFields];\n    for (int i = 0; i < fieldValues.length; i++) {\n      fieldValues[i] = 1;\n    }\n    \n    int numRounds = atLeast(15);\n    int docID = 0;\n    for (int i = 0; i < numRounds; i++) {\n      int numDocs = atLeast(5);\n      //System.out.println(\"[\" + Thread.currentThread().getName() + \"]: round=\" + i + \", numDocs=\" + numDocs);\n      for (int j = 0; j < numDocs; j++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"doc-\" + docID, Store.NO));\n        doc.add(new StringField(\"key\", \"all\", Store.NO)); // update key\n        // add all fields with their current value\n        for (int f = 0; f < fieldValues.length; f++) {\n          doc.add(new BinaryDocValuesField(\"f\" + f, toBytes(fieldValues[f])));\n        }\n        writer.addDocument(doc);\n        ++docID;\n      }\n      \n      int fieldIdx = random.nextInt(fieldValues.length);\n      String updateField = \"f\" + fieldIdx;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: updated field '\" + updateField + \"' to value \" + fieldValues[fieldIdx]);\n      writer.updateBinaryDocValue(new Term(\"key\", \"all\"), updateField, toBytes(++fieldValues[fieldIdx]));\n      \n      if (random.nextDouble() < 0.2) {\n        int deleteDoc = random.nextInt(docID); // might also delete an already deleted document, ok!\n        writer.deleteDocuments(new Term(\"id\", \"doc-\" + deleteDoc));\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"]: deleted document: doc-\" + deleteDoc);\n      }\n      \n      // verify reader\n      if (!isNRT) {\n        writer.commit();\n      }\n      \n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopen reader: \" + reader);\n      DirectoryReader newReader = DirectoryReader.openIfChanged(reader);\n      assertNotNull(newReader);\n      reader.close();\n      reader = newReader;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopened reader: \" + reader);\n      assertTrue(reader.numDocs() > 0); // we delete at most one document per round\n      for (LeafReaderContext context : reader.leaves()) {\n        LeafReader r = context.reader();\n//        System.out.println(((SegmentReader) r).getSegmentName());\n        Bits liveDocs = r.getLiveDocs();\n        for (int field = 0; field < fieldValues.length; field++) {\n          String f = \"f\" + field;\n          BinaryDocValues bdv = r.getBinaryDocValues(f);\n          assertNotNull(bdv);\n          int maxDoc = r.maxDoc();\n          for (int doc = 0; doc < maxDoc; doc++) {\n            if (liveDocs == null || liveDocs.get(doc)) {\n              assertEquals(doc, bdv.advance(doc));\n              assertEquals(\"invalid value for doc=\" + doc + \", field=\" + f + \", reader=\" + r, fieldValues[field], getValue(bdv));\n            }\n          }\n        }\n      }\n//      System.out.println();\n    }\n\n    writer.close();\n    IOUtils.close(reader, dir);\n  }\n\n","sourceOld":"  public void testManyReopensAndFields() throws Exception {\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    LogMergePolicy lmp = newLogMergePolicy();\n    lmp.setMergeFactor(3); // merge often\n    conf.setMergePolicy(lmp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    final boolean isNRT = random.nextBoolean();\n    DirectoryReader reader;\n    if (isNRT) {\n      reader = DirectoryReader.open(writer);\n    } else {\n      writer.commit();\n      reader = DirectoryReader.open(dir);\n    }\n    //System.out.println(\"TEST: isNRT=\" + isNRT);\n    \n    final int numFields = random.nextInt(4) + 3; // 3-7\n    final long[] fieldValues = new long[numFields];\n    for (int i = 0; i < fieldValues.length; i++) {\n      fieldValues[i] = 1;\n    }\n    \n    int numRounds = atLeast(15);\n    int docID = 0;\n    for (int i = 0; i < numRounds; i++) {\n      int numDocs = atLeast(5);\n      //System.out.println(\"[\" + Thread.currentThread().getName() + \"]: round=\" + i + \", numDocs=\" + numDocs);\n      for (int j = 0; j < numDocs; j++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"doc-\" + docID, Store.NO));\n        doc.add(new StringField(\"key\", \"all\", Store.NO)); // update key\n        // add all fields with their current value\n        for (int f = 0; f < fieldValues.length; f++) {\n          doc.add(new BinaryDocValuesField(\"f\" + f, toBytes(fieldValues[f])));\n        }\n        writer.addDocument(doc);\n        ++docID;\n      }\n      \n      int fieldIdx = random.nextInt(fieldValues.length);\n      String updateField = \"f\" + fieldIdx;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: updated field '\" + updateField + \"' to value \" + fieldValues[fieldIdx]);\n      writer.updateBinaryDocValue(new Term(\"key\", \"all\"), updateField, toBytes(++fieldValues[fieldIdx]));\n      \n      if (random.nextDouble() < 0.2) {\n        int deleteDoc = random.nextInt(docID); // might also delete an already deleted document, ok!\n        writer.deleteDocuments(new Term(\"id\", \"doc-\" + deleteDoc));\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"]: deleted document: doc-\" + deleteDoc);\n      }\n      \n      // verify reader\n      if (!isNRT) {\n        writer.commit();\n      }\n      \n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopen reader: \" + reader);\n      DirectoryReader newReader = DirectoryReader.openIfChanged(reader);\n      assertNotNull(newReader);\n      reader.close();\n      reader = newReader;\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"]: reopened reader: \" + reader);\n      assertTrue(reader.numDocs() > 0); // we delete at most one document per round\n      for (LeafReaderContext context : reader.leaves()) {\n        LeafReader r = context.reader();\n//        System.out.println(((SegmentReader) r).getSegmentName());\n        Bits liveDocs = r.getLiveDocs();\n        for (int field = 0; field < fieldValues.length; field++) {\n          String f = \"f\" + field;\n          BinaryDocValues bdv = r.getBinaryDocValues(f);\n          Bits docsWithField = r.getDocsWithField(f);\n          assertNotNull(bdv);\n          int maxDoc = r.maxDoc();\n          for (int doc = 0; doc < maxDoc; doc++) {\n            if (liveDocs == null || liveDocs.get(doc)) {\n//              System.out.println(\"doc=\" + (doc + context.docBase) + \" f='\" + f + \"' vslue=\" + getValue(bdv, doc, scratch));\n              assertTrue(docsWithField.get(doc));\n              assertEquals(\"invalid value for doc=\" + doc + \", field=\" + f + \", reader=\" + r, fieldValues[field], getValue(bdv, doc));\n            }\n          }\n        }\n      }\n//      System.out.println();\n    }\n\n    writer.close();\n    IOUtils.close(reader, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf"],"53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf":["30d3ec601cbd11cf056b7336f0e03f688ebcd9f7"],"06805da26538ed636bd89b10c2699cc3834032ae":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["2a1862266772deb28cdcb7d996b64d2177022687","17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"2a1862266772deb28cdcb7d996b64d2177022687":["7e4c214a1f904dde76f5611b56d4081533055b3b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["2a1862266772deb28cdcb7d996b64d2177022687","6652c74b2358a0b13223817a6a793bf1c9d0749d"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["2a1862266772deb28cdcb7d996b64d2177022687"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"7e4c214a1f904dde76f5611b56d4081533055b3b":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"30d3ec601cbd11cf056b7336f0e03f688ebcd9f7":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["06805da26538ed636bd89b10c2699cc3834032ae"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"]},"commit2Childs":{"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"06805da26538ed636bd89b10c2699cc3834032ae":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["7e4c214a1f904dde76f5611b56d4081533055b3b"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"2a1862266772deb28cdcb7d996b64d2177022687":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","17e5da53e4e5bd659e22add9bba1cfa222e7e30d","6652c74b2358a0b13223817a6a793bf1c9d0749d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["06805da26538ed636bd89b10c2699cc3834032ae"],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"7e4c214a1f904dde76f5611b56d4081533055b3b":["2a1862266772deb28cdcb7d996b64d2177022687"],"30d3ec601cbd11cf056b7336f0e03f688ebcd9f7":["53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["30d3ec601cbd11cf056b7336f0e03f688ebcd9f7"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}