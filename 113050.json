{"path":"lucene/src/java/org/apache/lucene/search/ExactPhraseScorer#ExactPhraseScorer(Weight,PhraseQuery.PostingsAndFreq[],Similarity,byte[]).mjava","commits":[{"id":"953de31d76c9d58f1e3f4e41ff8a48a1529226de","date":1277371072,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/search/ExactPhraseScorer#ExactPhraseScorer(Weight,PhraseQuery.PostingsAndFreq[],Similarity,byte[]).mjava","pathOld":"/dev/null","sourceNew":"  ExactPhraseScorer(Weight weight, PhraseQuery.PostingsAndFreq[] postings,\n                    Similarity similarity, byte[] norms) throws IOException {\n    super(similarity);\n    this.weight = weight;\n    this.norms = norms;\n    this.value = weight.getValue();\n\n    chunkStates = new ChunkState[postings.length];\n\n    endMinus1 = postings.length-1;\n\n    for(int i=0;i<postings.length;i++) {\n\n      // Coarse optimization: advance(target) is fairly\n      // costly, so, if the relative freq of the 2nd\n      // rarest term is not that much (> 1/5th) rarer than\n      // the first term, then we just use .nextDoc() when\n      // ANDing.  This buys ~15% gain for phrases where\n      // freq of rarest 2 terms is close:\n      final boolean useAdvance = postings[i].docFreq > 5*postings[0].docFreq;\n      chunkStates[i] = new ChunkState(postings[i].postings, -postings[i].position, useAdvance);\n      if (i > 0 && postings[i].postings.nextDoc() == DocsEnum.NO_MORE_DOCS) {\n        noDocs = true;\n        return;\n      }\n    }\n\n    for (int i = 0; i < SCORE_CACHE_SIZE; i++) {\n      scoreCache[i] = getSimilarity().tf((float) i) * value;\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5f4e87790277826a2aea119328600dfb07761f32","date":1279827275,"type":0,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/search/ExactPhraseScorer#ExactPhraseScorer(Weight,PhraseQuery.PostingsAndFreq[],Similarity,byte[]).mjava","pathOld":"/dev/null","sourceNew":"  ExactPhraseScorer(Weight weight, PhraseQuery.PostingsAndFreq[] postings,\n                    Similarity similarity, byte[] norms) throws IOException {\n    super(similarity);\n    this.weight = weight;\n    this.norms = norms;\n    this.value = weight.getValue();\n\n    chunkStates = new ChunkState[postings.length];\n\n    endMinus1 = postings.length-1;\n\n    for(int i=0;i<postings.length;i++) {\n\n      // Coarse optimization: advance(target) is fairly\n      // costly, so, if the relative freq of the 2nd\n      // rarest term is not that much (> 1/5th) rarer than\n      // the first term, then we just use .nextDoc() when\n      // ANDing.  This buys ~15% gain for phrases where\n      // freq of rarest 2 terms is close:\n      final boolean useAdvance = postings[i].docFreq > 5*postings[0].docFreq;\n      chunkStates[i] = new ChunkState(postings[i].postings, -postings[i].position, useAdvance);\n      if (i > 0 && postings[i].postings.nextDoc() == DocsEnum.NO_MORE_DOCS) {\n        noDocs = true;\n        return;\n      }\n    }\n\n    for (int i = 0; i < SCORE_CACHE_SIZE; i++) {\n      scoreCache[i] = getSimilarity().tf((float) i) * value;\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e4946500259641951dbd7f8a61956bf14bef3f60","date":1283284212,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/search/ExactPhraseScorer#ExactPhraseScorer(Weight,PhraseQuery.PostingsAndFreq[],Similarity,byte[]).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/ExactPhraseScorer#ExactPhraseScorer(Weight,PhraseQuery.PostingsAndFreq[],Similarity,byte[]).mjava","sourceNew":"  ExactPhraseScorer(Weight weight, PhraseQuery.PostingsAndFreq[] postings,\n                    Similarity similarity, byte[] norms) throws IOException {\n    super(similarity, weight);\n    this.norms = norms;\n    this.value = weight.getValue();\n\n    chunkStates = new ChunkState[postings.length];\n\n    endMinus1 = postings.length-1;\n\n    for(int i=0;i<postings.length;i++) {\n\n      // Coarse optimization: advance(target) is fairly\n      // costly, so, if the relative freq of the 2nd\n      // rarest term is not that much (> 1/5th) rarer than\n      // the first term, then we just use .nextDoc() when\n      // ANDing.  This buys ~15% gain for phrases where\n      // freq of rarest 2 terms is close:\n      final boolean useAdvance = postings[i].docFreq > 5*postings[0].docFreq;\n      chunkStates[i] = new ChunkState(postings[i].postings, -postings[i].position, useAdvance);\n      if (i > 0 && postings[i].postings.nextDoc() == DocsEnum.NO_MORE_DOCS) {\n        noDocs = true;\n        return;\n      }\n    }\n\n    for (int i = 0; i < SCORE_CACHE_SIZE; i++) {\n      scoreCache[i] = getSimilarity().tf((float) i) * value;\n    }\n  }\n\n","sourceOld":"  ExactPhraseScorer(Weight weight, PhraseQuery.PostingsAndFreq[] postings,\n                    Similarity similarity, byte[] norms) throws IOException {\n    super(similarity);\n    this.weight = weight;\n    this.norms = norms;\n    this.value = weight.getValue();\n\n    chunkStates = new ChunkState[postings.length];\n\n    endMinus1 = postings.length-1;\n\n    for(int i=0;i<postings.length;i++) {\n\n      // Coarse optimization: advance(target) is fairly\n      // costly, so, if the relative freq of the 2nd\n      // rarest term is not that much (> 1/5th) rarer than\n      // the first term, then we just use .nextDoc() when\n      // ANDing.  This buys ~15% gain for phrases where\n      // freq of rarest 2 terms is close:\n      final boolean useAdvance = postings[i].docFreq > 5*postings[0].docFreq;\n      chunkStates[i] = new ChunkState(postings[i].postings, -postings[i].position, useAdvance);\n      if (i > 0 && postings[i].postings.nextDoc() == DocsEnum.NO_MORE_DOCS) {\n        noDocs = true;\n        return;\n      }\n    }\n\n    for (int i = 0; i < SCORE_CACHE_SIZE; i++) {\n      scoreCache[i] = getSimilarity().tf((float) i) * value;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/search/ExactPhraseScorer#ExactPhraseScorer(Weight,PhraseQuery.PostingsAndFreq[],Similarity,byte[]).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/ExactPhraseScorer#ExactPhraseScorer(Weight,PhraseQuery.PostingsAndFreq[],Similarity,byte[]).mjava","sourceNew":"  ExactPhraseScorer(Weight weight, PhraseQuery.PostingsAndFreq[] postings,\n                    Similarity similarity, byte[] norms) throws IOException {\n    super(similarity, weight);\n    this.norms = norms;\n    this.value = weight.getValue();\n\n    chunkStates = new ChunkState[postings.length];\n\n    endMinus1 = postings.length-1;\n\n    for(int i=0;i<postings.length;i++) {\n\n      // Coarse optimization: advance(target) is fairly\n      // costly, so, if the relative freq of the 2nd\n      // rarest term is not that much (> 1/5th) rarer than\n      // the first term, then we just use .nextDoc() when\n      // ANDing.  This buys ~15% gain for phrases where\n      // freq of rarest 2 terms is close:\n      final boolean useAdvance = postings[i].docFreq > 5*postings[0].docFreq;\n      chunkStates[i] = new ChunkState(postings[i].postings, -postings[i].position, useAdvance);\n      if (i > 0 && postings[i].postings.nextDoc() == DocsEnum.NO_MORE_DOCS) {\n        noDocs = true;\n        return;\n      }\n    }\n\n    for (int i = 0; i < SCORE_CACHE_SIZE; i++) {\n      scoreCache[i] = getSimilarity().tf((float) i) * value;\n    }\n  }\n\n","sourceOld":"  ExactPhraseScorer(Weight weight, PhraseQuery.PostingsAndFreq[] postings,\n                    Similarity similarity, byte[] norms) throws IOException {\n    super(similarity);\n    this.weight = weight;\n    this.norms = norms;\n    this.value = weight.getValue();\n\n    chunkStates = new ChunkState[postings.length];\n\n    endMinus1 = postings.length-1;\n\n    for(int i=0;i<postings.length;i++) {\n\n      // Coarse optimization: advance(target) is fairly\n      // costly, so, if the relative freq of the 2nd\n      // rarest term is not that much (> 1/5th) rarer than\n      // the first term, then we just use .nextDoc() when\n      // ANDing.  This buys ~15% gain for phrases where\n      // freq of rarest 2 terms is close:\n      final boolean useAdvance = postings[i].docFreq > 5*postings[0].docFreq;\n      chunkStates[i] = new ChunkState(postings[i].postings, -postings[i].position, useAdvance);\n      if (i > 0 && postings[i].postings.nextDoc() == DocsEnum.NO_MORE_DOCS) {\n        noDocs = true;\n        return;\n      }\n    }\n\n    for (int i = 0; i < SCORE_CACHE_SIZE; i++) {\n      scoreCache[i] = getSimilarity().tf((float) i) * value;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e5e679b9c5f68f1f331de920ae8366af75b44060","date":1295555804,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/search/ExactPhraseScorer#ExactPhraseScorer(Weight,PhraseQuery.PostingsAndFreq[],Similarity,byte[]).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/ExactPhraseScorer#ExactPhraseScorer(Weight,PhraseQuery.PostingsAndFreq[],Similarity,byte[]).mjava","sourceNew":"  ExactPhraseScorer(Weight weight, PhraseQuery.PostingsAndFreq[] postings,\n                    Similarity similarity, byte[] norms) throws IOException {\n    super(weight);\n    this.similarity = similarity;\n    this.norms = norms;\n    this.value = weight.getValue();\n\n    chunkStates = new ChunkState[postings.length];\n\n    endMinus1 = postings.length-1;\n\n    for(int i=0;i<postings.length;i++) {\n\n      // Coarse optimization: advance(target) is fairly\n      // costly, so, if the relative freq of the 2nd\n      // rarest term is not that much (> 1/5th) rarer than\n      // the first term, then we just use .nextDoc() when\n      // ANDing.  This buys ~15% gain for phrases where\n      // freq of rarest 2 terms is close:\n      final boolean useAdvance = postings[i].docFreq > 5*postings[0].docFreq;\n      chunkStates[i] = new ChunkState(postings[i].postings, -postings[i].position, useAdvance);\n      if (i > 0 && postings[i].postings.nextDoc() == DocsEnum.NO_MORE_DOCS) {\n        noDocs = true;\n        return;\n      }\n    }\n\n    for (int i = 0; i < SCORE_CACHE_SIZE; i++) {\n      scoreCache[i] = similarity.tf((float) i) * value;\n    }\n  }\n\n","sourceOld":"  ExactPhraseScorer(Weight weight, PhraseQuery.PostingsAndFreq[] postings,\n                    Similarity similarity, byte[] norms) throws IOException {\n    super(similarity, weight);\n    this.norms = norms;\n    this.value = weight.getValue();\n\n    chunkStates = new ChunkState[postings.length];\n\n    endMinus1 = postings.length-1;\n\n    for(int i=0;i<postings.length;i++) {\n\n      // Coarse optimization: advance(target) is fairly\n      // costly, so, if the relative freq of the 2nd\n      // rarest term is not that much (> 1/5th) rarer than\n      // the first term, then we just use .nextDoc() when\n      // ANDing.  This buys ~15% gain for phrases where\n      // freq of rarest 2 terms is close:\n      final boolean useAdvance = postings[i].docFreq > 5*postings[0].docFreq;\n      chunkStates[i] = new ChunkState(postings[i].postings, -postings[i].position, useAdvance);\n      if (i > 0 && postings[i].postings.nextDoc() == DocsEnum.NO_MORE_DOCS) {\n        noDocs = true;\n        return;\n      }\n    }\n\n    for (int i = 0; i < SCORE_CACHE_SIZE; i++) {\n      scoreCache[i] = getSimilarity().tf((float) i) * value;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e79a6d080bdd5b2a8f56342cf571b5476de04180","date":1295638686,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/search/ExactPhraseScorer#ExactPhraseScorer(Weight,PhraseQuery.PostingsAndFreq[],Similarity,byte[]).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/ExactPhraseScorer#ExactPhraseScorer(Weight,PhraseQuery.PostingsAndFreq[],Similarity,byte[]).mjava","sourceNew":"  ExactPhraseScorer(Weight weight, PhraseQuery.PostingsAndFreq[] postings,\n                    Similarity similarity, byte[] norms) throws IOException {\n    super(weight);\n    this.similarity = similarity;\n    this.norms = norms;\n    this.value = weight.getValue();\n\n    chunkStates = new ChunkState[postings.length];\n\n    endMinus1 = postings.length-1;\n\n    for(int i=0;i<postings.length;i++) {\n\n      // Coarse optimization: advance(target) is fairly\n      // costly, so, if the relative freq of the 2nd\n      // rarest term is not that much (> 1/5th) rarer than\n      // the first term, then we just use .nextDoc() when\n      // ANDing.  This buys ~15% gain for phrases where\n      // freq of rarest 2 terms is close:\n      final boolean useAdvance = postings[i].docFreq > 5*postings[0].docFreq;\n      chunkStates[i] = new ChunkState(postings[i].postings, -postings[i].position, useAdvance);\n      if (i > 0 && postings[i].postings.nextDoc() == DocsEnum.NO_MORE_DOCS) {\n        noDocs = true;\n        return;\n      }\n    }\n\n    for (int i = 0; i < SCORE_CACHE_SIZE; i++) {\n      scoreCache[i] = similarity.tf((float) i) * value;\n    }\n  }\n\n","sourceOld":"  ExactPhraseScorer(Weight weight, PhraseQuery.PostingsAndFreq[] postings,\n                    Similarity similarity, byte[] norms) throws IOException {\n    super(similarity, weight);\n    this.norms = norms;\n    this.value = weight.getValue();\n\n    chunkStates = new ChunkState[postings.length];\n\n    endMinus1 = postings.length-1;\n\n    for(int i=0;i<postings.length;i++) {\n\n      // Coarse optimization: advance(target) is fairly\n      // costly, so, if the relative freq of the 2nd\n      // rarest term is not that much (> 1/5th) rarer than\n      // the first term, then we just use .nextDoc() when\n      // ANDing.  This buys ~15% gain for phrases where\n      // freq of rarest 2 terms is close:\n      final boolean useAdvance = postings[i].docFreq > 5*postings[0].docFreq;\n      chunkStates[i] = new ChunkState(postings[i].postings, -postings[i].position, useAdvance);\n      if (i > 0 && postings[i].postings.nextDoc() == DocsEnum.NO_MORE_DOCS) {\n        noDocs = true;\n        return;\n      }\n    }\n\n    for (int i = 0; i < SCORE_CACHE_SIZE; i++) {\n      scoreCache[i] = getSimilarity().tf((float) i) * value;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/search/ExactPhraseScorer#ExactPhraseScorer(Weight,PhraseQuery.PostingsAndFreq[],Similarity,byte[]).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/ExactPhraseScorer#ExactPhraseScorer(Weight,PhraseQuery.PostingsAndFreq[],Similarity,byte[]).mjava","sourceNew":"  ExactPhraseScorer(Weight weight, PhraseQuery.PostingsAndFreq[] postings,\n                    Similarity similarity, byte[] norms) throws IOException {\n    super(weight);\n    this.similarity = similarity;\n    this.norms = norms;\n    this.value = weight.getValue();\n\n    chunkStates = new ChunkState[postings.length];\n\n    endMinus1 = postings.length-1;\n\n    for(int i=0;i<postings.length;i++) {\n\n      // Coarse optimization: advance(target) is fairly\n      // costly, so, if the relative freq of the 2nd\n      // rarest term is not that much (> 1/5th) rarer than\n      // the first term, then we just use .nextDoc() when\n      // ANDing.  This buys ~15% gain for phrases where\n      // freq of rarest 2 terms is close:\n      final boolean useAdvance = postings[i].docFreq > 5*postings[0].docFreq;\n      chunkStates[i] = new ChunkState(postings[i].postings, -postings[i].position, useAdvance);\n      if (i > 0 && postings[i].postings.nextDoc() == DocsEnum.NO_MORE_DOCS) {\n        noDocs = true;\n        return;\n      }\n    }\n\n    for (int i = 0; i < SCORE_CACHE_SIZE; i++) {\n      scoreCache[i] = similarity.tf((float) i) * value;\n    }\n  }\n\n","sourceOld":"  ExactPhraseScorer(Weight weight, PhraseQuery.PostingsAndFreq[] postings,\n                    Similarity similarity, byte[] norms) throws IOException {\n    super(similarity, weight);\n    this.norms = norms;\n    this.value = weight.getValue();\n\n    chunkStates = new ChunkState[postings.length];\n\n    endMinus1 = postings.length-1;\n\n    for(int i=0;i<postings.length;i++) {\n\n      // Coarse optimization: advance(target) is fairly\n      // costly, so, if the relative freq of the 2nd\n      // rarest term is not that much (> 1/5th) rarer than\n      // the first term, then we just use .nextDoc() when\n      // ANDing.  This buys ~15% gain for phrases where\n      // freq of rarest 2 terms is close:\n      final boolean useAdvance = postings[i].docFreq > 5*postings[0].docFreq;\n      chunkStates[i] = new ChunkState(postings[i].postings, -postings[i].position, useAdvance);\n      if (i > 0 && postings[i].postings.nextDoc() == DocsEnum.NO_MORE_DOCS) {\n        noDocs = true;\n        return;\n      }\n    }\n\n    for (int i = 0; i < SCORE_CACHE_SIZE; i++) {\n      scoreCache[i] = getSimilarity().tf((float) i) * value;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0f3cee3d20b0c786e6fca20539454262e29edcab","date":1310101685,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/search/ExactPhraseScorer#ExactPhraseScorer(Weight,PhraseQuery.PostingsAndFreq[],Similarity.ExactDocScorer).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/ExactPhraseScorer#ExactPhraseScorer(Weight,PhraseQuery.PostingsAndFreq[],Similarity,byte[]).mjava","sourceNew":"  ExactPhraseScorer(Weight weight, PhraseQuery.PostingsAndFreq[] postings,\n                    Similarity.ExactDocScorer docScorer) throws IOException {\n    super(weight);\n    this.docScorer = docScorer;\n\n    chunkStates = new ChunkState[postings.length];\n\n    endMinus1 = postings.length-1;\n\n    for(int i=0;i<postings.length;i++) {\n\n      // Coarse optimization: advance(target) is fairly\n      // costly, so, if the relative freq of the 2nd\n      // rarest term is not that much (> 1/5th) rarer than\n      // the first term, then we just use .nextDoc() when\n      // ANDing.  This buys ~15% gain for phrases where\n      // freq of rarest 2 terms is close:\n      final boolean useAdvance = postings[i].docFreq > 5*postings[0].docFreq;\n      chunkStates[i] = new ChunkState(postings[i].postings, -postings[i].position, useAdvance);\n      if (i > 0 && postings[i].postings.nextDoc() == DocsEnum.NO_MORE_DOCS) {\n        noDocs = true;\n        return;\n      }\n    }\n  }\n\n","sourceOld":"  ExactPhraseScorer(Weight weight, PhraseQuery.PostingsAndFreq[] postings,\n                    Similarity similarity, byte[] norms) throws IOException {\n    super(weight);\n    this.similarity = similarity;\n    this.norms = norms;\n    this.value = weight.getValue();\n\n    chunkStates = new ChunkState[postings.length];\n\n    endMinus1 = postings.length-1;\n\n    for(int i=0;i<postings.length;i++) {\n\n      // Coarse optimization: advance(target) is fairly\n      // costly, so, if the relative freq of the 2nd\n      // rarest term is not that much (> 1/5th) rarer than\n      // the first term, then we just use .nextDoc() when\n      // ANDing.  This buys ~15% gain for phrases where\n      // freq of rarest 2 terms is close:\n      final boolean useAdvance = postings[i].docFreq > 5*postings[0].docFreq;\n      chunkStates[i] = new ChunkState(postings[i].postings, -postings[i].position, useAdvance);\n      if (i > 0 && postings[i].postings.nextDoc() == DocsEnum.NO_MORE_DOCS) {\n        noDocs = true;\n        return;\n      }\n    }\n\n    for (int i = 0; i < SCORE_CACHE_SIZE; i++) {\n      scoreCache[i] = similarity.tf((float) i) * value;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f0b9507caf22f292ac0e5e59f62db4275adf4511","date":1310107283,"type":5,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/search/ExactPhraseScorer#ExactPhraseScorer(Weight,PhraseQuery.PostingsAndFreq[],Similarity.ExactDocScorer).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/ExactPhraseScorer#ExactPhraseScorer(Weight,PhraseQuery.PostingsAndFreq[],Similarity,byte[]).mjava","sourceNew":"  ExactPhraseScorer(Weight weight, PhraseQuery.PostingsAndFreq[] postings,\n                    Similarity.ExactDocScorer docScorer) throws IOException {\n    super(weight);\n    this.docScorer = docScorer;\n\n    chunkStates = new ChunkState[postings.length];\n\n    endMinus1 = postings.length-1;\n\n    for(int i=0;i<postings.length;i++) {\n\n      // Coarse optimization: advance(target) is fairly\n      // costly, so, if the relative freq of the 2nd\n      // rarest term is not that much (> 1/5th) rarer than\n      // the first term, then we just use .nextDoc() when\n      // ANDing.  This buys ~15% gain for phrases where\n      // freq of rarest 2 terms is close:\n      final boolean useAdvance = postings[i].docFreq > 5*postings[0].docFreq;\n      chunkStates[i] = new ChunkState(postings[i].postings, -postings[i].position, useAdvance);\n      if (i > 0 && postings[i].postings.nextDoc() == DocsEnum.NO_MORE_DOCS) {\n        noDocs = true;\n        return;\n      }\n    }\n  }\n\n","sourceOld":"  ExactPhraseScorer(Weight weight, PhraseQuery.PostingsAndFreq[] postings,\n                    Similarity similarity, byte[] norms) throws IOException {\n    super(weight);\n    this.similarity = similarity;\n    this.norms = norms;\n    this.value = weight.getValue();\n\n    chunkStates = new ChunkState[postings.length];\n\n    endMinus1 = postings.length-1;\n\n    for(int i=0;i<postings.length;i++) {\n\n      // Coarse optimization: advance(target) is fairly\n      // costly, so, if the relative freq of the 2nd\n      // rarest term is not that much (> 1/5th) rarer than\n      // the first term, then we just use .nextDoc() when\n      // ANDing.  This buys ~15% gain for phrases where\n      // freq of rarest 2 terms is close:\n      final boolean useAdvance = postings[i].docFreq > 5*postings[0].docFreq;\n      chunkStates[i] = new ChunkState(postings[i].postings, -postings[i].position, useAdvance);\n      if (i > 0 && postings[i].postings.nextDoc() == DocsEnum.NO_MORE_DOCS) {\n        noDocs = true;\n        return;\n      }\n    }\n\n    for (int i = 0; i < SCORE_CACHE_SIZE; i++) {\n      scoreCache[i] = similarity.tf((float) i) * value;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1291e4568eb7d9463d751627596ef14baf4c1603","date":1310112572,"type":5,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/search/ExactPhraseScorer#ExactPhraseScorer(Weight,PhraseQuery.PostingsAndFreq[],Similarity.ExactDocScorer).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/ExactPhraseScorer#ExactPhraseScorer(Weight,PhraseQuery.PostingsAndFreq[],Similarity,byte[]).mjava","sourceNew":"  ExactPhraseScorer(Weight weight, PhraseQuery.PostingsAndFreq[] postings,\n                    Similarity.ExactDocScorer docScorer) throws IOException {\n    super(weight);\n    this.docScorer = docScorer;\n\n    chunkStates = new ChunkState[postings.length];\n\n    endMinus1 = postings.length-1;\n\n    for(int i=0;i<postings.length;i++) {\n\n      // Coarse optimization: advance(target) is fairly\n      // costly, so, if the relative freq of the 2nd\n      // rarest term is not that much (> 1/5th) rarer than\n      // the first term, then we just use .nextDoc() when\n      // ANDing.  This buys ~15% gain for phrases where\n      // freq of rarest 2 terms is close:\n      final boolean useAdvance = postings[i].docFreq > 5*postings[0].docFreq;\n      chunkStates[i] = new ChunkState(postings[i].postings, -postings[i].position, useAdvance);\n      if (i > 0 && postings[i].postings.nextDoc() == DocsEnum.NO_MORE_DOCS) {\n        noDocs = true;\n        return;\n      }\n    }\n  }\n\n","sourceOld":"  ExactPhraseScorer(Weight weight, PhraseQuery.PostingsAndFreq[] postings,\n                    Similarity similarity, byte[] norms) throws IOException {\n    super(weight);\n    this.similarity = similarity;\n    this.norms = norms;\n    this.value = weight.getValue();\n\n    chunkStates = new ChunkState[postings.length];\n\n    endMinus1 = postings.length-1;\n\n    for(int i=0;i<postings.length;i++) {\n\n      // Coarse optimization: advance(target) is fairly\n      // costly, so, if the relative freq of the 2nd\n      // rarest term is not that much (> 1/5th) rarer than\n      // the first term, then we just use .nextDoc() when\n      // ANDing.  This buys ~15% gain for phrases where\n      // freq of rarest 2 terms is close:\n      final boolean useAdvance = postings[i].docFreq > 5*postings[0].docFreq;\n      chunkStates[i] = new ChunkState(postings[i].postings, -postings[i].position, useAdvance);\n      if (i > 0 && postings[i].postings.nextDoc() == DocsEnum.NO_MORE_DOCS) {\n        noDocs = true;\n        return;\n      }\n    }\n\n    for (int i = 0; i < SCORE_CACHE_SIZE; i++) {\n      scoreCache[i] = similarity.tf((float) i) * value;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"1291e4568eb7d9463d751627596ef14baf4c1603":["e5e679b9c5f68f1f331de920ae8366af75b44060","0f3cee3d20b0c786e6fca20539454262e29edcab"],"0f3cee3d20b0c786e6fca20539454262e29edcab":["e5e679b9c5f68f1f331de920ae8366af75b44060"],"953de31d76c9d58f1e3f4e41ff8a48a1529226de":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"f0b9507caf22f292ac0e5e59f62db4275adf4511":["e5e679b9c5f68f1f331de920ae8366af75b44060","0f3cee3d20b0c786e6fca20539454262e29edcab"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["e4946500259641951dbd7f8a61956bf14bef3f60","e5e679b9c5f68f1f331de920ae8366af75b44060"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"5f4e87790277826a2aea119328600dfb07761f32":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","953de31d76c9d58f1e3f4e41ff8a48a1529226de"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["5f4e87790277826a2aea119328600dfb07761f32","e4946500259641951dbd7f8a61956bf14bef3f60"],"e5e679b9c5f68f1f331de920ae8366af75b44060":["e4946500259641951dbd7f8a61956bf14bef3f60"],"e79a6d080bdd5b2a8f56342cf571b5476de04180":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","e5e679b9c5f68f1f331de920ae8366af75b44060"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["0f3cee3d20b0c786e6fca20539454262e29edcab"],"e4946500259641951dbd7f8a61956bf14bef3f60":["953de31d76c9d58f1e3f4e41ff8a48a1529226de"]},"commit2Childs":{"1291e4568eb7d9463d751627596ef14baf4c1603":[],"0f3cee3d20b0c786e6fca20539454262e29edcab":["1291e4568eb7d9463d751627596ef14baf4c1603","f0b9507caf22f292ac0e5e59f62db4275adf4511","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"953de31d76c9d58f1e3f4e41ff8a48a1529226de":["5f4e87790277826a2aea119328600dfb07761f32","e4946500259641951dbd7f8a61956bf14bef3f60"],"f0b9507caf22f292ac0e5e59f62db4275adf4511":[],"29ef99d61cda9641b6250bf9567329a6e65f901d":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["953de31d76c9d58f1e3f4e41ff8a48a1529226de","5f4e87790277826a2aea119328600dfb07761f32"],"5f4e87790277826a2aea119328600dfb07761f32":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["e79a6d080bdd5b2a8f56342cf571b5476de04180"],"e5e679b9c5f68f1f331de920ae8366af75b44060":["1291e4568eb7d9463d751627596ef14baf4c1603","0f3cee3d20b0c786e6fca20539454262e29edcab","f0b9507caf22f292ac0e5e59f62db4275adf4511","29ef99d61cda9641b6250bf9567329a6e65f901d","e79a6d080bdd5b2a8f56342cf571b5476de04180"],"e79a6d080bdd5b2a8f56342cf571b5476de04180":[],"e4946500259641951dbd7f8a61956bf14bef3f60":["29ef99d61cda9641b6250bf9567329a6e65f901d","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","e5e679b9c5f68f1f331de920ae8366af75b44060"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["1291e4568eb7d9463d751627596ef14baf4c1603","f0b9507caf22f292ac0e5e59f62db4275adf4511","29ef99d61cda9641b6250bf9567329a6e65f901d","e79a6d080bdd5b2a8f56342cf571b5476de04180","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}