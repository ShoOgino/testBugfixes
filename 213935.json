{"path":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getLoggingAnalyzer(Analyzer,PrintStream,String).mjava","commits":[{"id":"ea82415927cafd7c8b8bceca08f31a63db1cbdde","date":1133588579,"type":0,"author":"Wolfgang Hoschek","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getLoggingAnalyzer(Analyzer,PrintStream,String).mjava","pathOld":"/dev/null","sourceNew":"\t/**\n\t * Returns a simple analyzer wrapper that logs all tokens produced by the\n\t * underlying child analyzer to the given log stream (typically System.err);\n\t * Otherwise behaves exactly like the child analyzer, delivering the very\n\t * same tokens; useful for debugging purposes on custom indexing and/or\n\t * querying.\n\t * \n\t * @param child\n\t *            the underlying child analyzer\n\t * @param log\n\t *            the print stream to log to (typically System.err)\n\t * @param logName\n\t *            a name for this logger (typically \"log\" or similar)\n\t * @return a logging analyzer\n\t */\n\tpublic static Analyzer getLoggingAnalyzer(final Analyzer child, \n\t\t\tfinal PrintStream log, final String logName) {\n\t\t\n\t\tif (child == null) \n\t\t\tthrow new IllegalArgumentException(\"child analyzer must not be null\");\n\t\tif (log == null) \n\t\t\tthrow new IllegalArgumentException(\"logStream must not be null\");\n\n\t\treturn new Analyzer() {\n\t\t\tpublic TokenStream tokenStream(final String fieldName, Reader reader) {\n\t\t\t\treturn new TokenFilter(child.tokenStream(fieldName, reader)) {\n\t\t\t\t\tprivate int position = -1;\n\t\t\t\t\t\n\t\t\t\t\tpublic Token next() throws IOException {\n\t\t\t\t\t\tToken token = input.next(); // from filter super class\n\t\t\t\t\t\tlog.println(toString(token));\n\t\t\t\t\t\treturn token;\n\t\t\t\t\t}\n\t\t\t\t\t\n\t\t\t\t\tprivate String toString(Token token) {\n\t\t\t\t\t\tif (token == null) return \"[\" + logName + \":EOS:\" + fieldName + \"]\\n\";\n\t\t\t\t\t\t\n\t\t\t\t\t\tposition += token.getPositionIncrement();\n\t\t\t\t\t\treturn \"[\" + logName + \":\" + position + \":\" + fieldName + \":\"\n\t\t\t\t\t\t\t\t+ token.termText() + \":\" + token.startOffset()\n\t\t\t\t\t\t\t\t+ \"-\" + token.endOffset() + \":\" + token.type()\n\t\t\t\t\t\t\t\t+ \"]\";\n\t\t\t\t\t}\t\t\t\t\t\n\t\t\t\t};\n\t\t\t}\n\t\t};\n\t}\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7f68e24227d5556d33ee6d586fd9010cd9ff8bec","date":1150091176,"type":3,"author":"Otis Gospodnetic","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getLoggingAnalyzer(Analyzer,PrintStream,String).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getLoggingAnalyzer(Analyzer,PrintStream,String).mjava","sourceNew":"  /**\n   * Returns a simple analyzer wrapper that logs all tokens produced by the\n   * underlying child analyzer to the given log stream (typically System.err);\n   * Otherwise behaves exactly like the child analyzer, delivering the very\n   * same tokens; useful for debugging purposes on custom indexing and/or\n   * querying.\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @param log\n   *            the print stream to log to (typically System.err)\n   * @param logName\n   *            a name for this logger (typically \"log\" or similar)\n   * @return a logging analyzer\n   */\n  public static Analyzer getLoggingAnalyzer(final Analyzer child, \n      final PrintStream log, final String logName) {\n    \n    if (child == null) \n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n    if (log == null) \n      throw new IllegalArgumentException(\"logStream must not be null\");\n\n    return new Analyzer() {\n      public TokenStream tokenStream(final String fieldName, Reader reader) {\n        return new TokenFilter(child.tokenStream(fieldName, reader)) {\n          private int position = -1;\n          \n          public Token next() throws IOException {\n            Token token = input.next(); // from filter super class\n            log.println(toString(token));\n            return token;\n          }\n          \n          private String toString(Token token) {\n            if (token == null) return \"[\" + logName + \":EOS:\" + fieldName + \"]\\n\";\n            \n            position += token.getPositionIncrement();\n            return \"[\" + logName + \":\" + position + \":\" + fieldName + \":\"\n                + token.termText() + \":\" + token.startOffset()\n                + \"-\" + token.endOffset() + \":\" + token.type()\n                + \"]\";\n          }         \n        };\n      }\n    };\n  }\n\n","sourceOld":"\t/**\n\t * Returns a simple analyzer wrapper that logs all tokens produced by the\n\t * underlying child analyzer to the given log stream (typically System.err);\n\t * Otherwise behaves exactly like the child analyzer, delivering the very\n\t * same tokens; useful for debugging purposes on custom indexing and/or\n\t * querying.\n\t * \n\t * @param child\n\t *            the underlying child analyzer\n\t * @param log\n\t *            the print stream to log to (typically System.err)\n\t * @param logName\n\t *            a name for this logger (typically \"log\" or similar)\n\t * @return a logging analyzer\n\t */\n\tpublic static Analyzer getLoggingAnalyzer(final Analyzer child, \n\t\t\tfinal PrintStream log, final String logName) {\n\t\t\n\t\tif (child == null) \n\t\t\tthrow new IllegalArgumentException(\"child analyzer must not be null\");\n\t\tif (log == null) \n\t\t\tthrow new IllegalArgumentException(\"logStream must not be null\");\n\n\t\treturn new Analyzer() {\n\t\t\tpublic TokenStream tokenStream(final String fieldName, Reader reader) {\n\t\t\t\treturn new TokenFilter(child.tokenStream(fieldName, reader)) {\n\t\t\t\t\tprivate int position = -1;\n\t\t\t\t\t\n\t\t\t\t\tpublic Token next() throws IOException {\n\t\t\t\t\t\tToken token = input.next(); // from filter super class\n\t\t\t\t\t\tlog.println(toString(token));\n\t\t\t\t\t\treturn token;\n\t\t\t\t\t}\n\t\t\t\t\t\n\t\t\t\t\tprivate String toString(Token token) {\n\t\t\t\t\t\tif (token == null) return \"[\" + logName + \":EOS:\" + fieldName + \"]\\n\";\n\t\t\t\t\t\t\n\t\t\t\t\t\tposition += token.getPositionIncrement();\n\t\t\t\t\t\treturn \"[\" + logName + \":\" + position + \":\" + fieldName + \":\"\n\t\t\t\t\t\t\t\t+ token.termText() + \":\" + token.startOffset()\n\t\t\t\t\t\t\t\t+ \"-\" + token.endOffset() + \":\" + token.type()\n\t\t\t\t\t\t\t\t+ \"]\";\n\t\t\t\t\t}\t\t\t\t\t\n\t\t\t\t};\n\t\t\t}\n\t\t};\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7e2cb543b41c145f33390f460ee743d6693c9c6c","date":1219243087,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getLoggingAnalyzer(Analyzer,PrintStream,String).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getLoggingAnalyzer(Analyzer,PrintStream,String).mjava","sourceNew":"  /**\n   * Returns a simple analyzer wrapper that logs all tokens produced by the\n   * underlying child analyzer to the given log stream (typically System.err);\n   * Otherwise behaves exactly like the child analyzer, delivering the very\n   * same tokens; useful for debugging purposes on custom indexing and/or\n   * querying.\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @param log\n   *            the print stream to log to (typically System.err)\n   * @param logName\n   *            a name for this logger (typically \"log\" or similar)\n   * @return a logging analyzer\n   */\n  public static Analyzer getLoggingAnalyzer(final Analyzer child, \n      final PrintStream log, final String logName) {\n    \n    if (child == null) \n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n    if (log == null) \n      throw new IllegalArgumentException(\"logStream must not be null\");\n\n    return new Analyzer() {\n      public TokenStream tokenStream(final String fieldName, Reader reader) {\n        return new TokenFilter(child.tokenStream(fieldName, reader)) {\n          private int position = -1;\n          \n          public Token next(final Token reusableToken) throws IOException {\n            assert reusableToken != null;\n            Token nextToken = input.next(reusableToken); // from filter super class\n            log.println(toString(nextToken));\n            return nextToken;\n          }\n          \n          private String toString(Token token) {\n            if (token == null) return \"[\" + logName + \":EOS:\" + fieldName + \"]\\n\";\n            \n            position += token.getPositionIncrement();\n            return \"[\" + logName + \":\" + position + \":\" + fieldName + \":\"\n                + token.term() + \":\" + token.startOffset()\n                + \"-\" + token.endOffset() + \":\" + token.type()\n                + \"]\";\n          }         \n        };\n      }\n    };\n  }\n\n","sourceOld":"  /**\n   * Returns a simple analyzer wrapper that logs all tokens produced by the\n   * underlying child analyzer to the given log stream (typically System.err);\n   * Otherwise behaves exactly like the child analyzer, delivering the very\n   * same tokens; useful for debugging purposes on custom indexing and/or\n   * querying.\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @param log\n   *            the print stream to log to (typically System.err)\n   * @param logName\n   *            a name for this logger (typically \"log\" or similar)\n   * @return a logging analyzer\n   */\n  public static Analyzer getLoggingAnalyzer(final Analyzer child, \n      final PrintStream log, final String logName) {\n    \n    if (child == null) \n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n    if (log == null) \n      throw new IllegalArgumentException(\"logStream must not be null\");\n\n    return new Analyzer() {\n      public TokenStream tokenStream(final String fieldName, Reader reader) {\n        return new TokenFilter(child.tokenStream(fieldName, reader)) {\n          private int position = -1;\n          \n          public Token next() throws IOException {\n            Token token = input.next(); // from filter super class\n            log.println(toString(token));\n            return token;\n          }\n          \n          private String toString(Token token) {\n            if (token == null) return \"[\" + logName + \":EOS:\" + fieldName + \"]\\n\";\n            \n            position += token.getPositionIncrement();\n            return \"[\" + logName + \":\" + position + \":\" + fieldName + \":\"\n                + token.termText() + \":\" + token.startOffset()\n                + \"-\" + token.endOffset() + \":\" + token.type()\n                + \"]\";\n          }         \n        };\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9b5756469957918cac40a831acec9cf01c8c2bb3","date":1249167152,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getLoggingAnalyzer(Analyzer,PrintStream,String).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getLoggingAnalyzer(Analyzer,PrintStream,String).mjava","sourceNew":"  /**\n   * Returns a simple analyzer wrapper that logs all tokens produced by the\n   * underlying child analyzer to the given log stream (typically System.err);\n   * Otherwise behaves exactly like the child analyzer, delivering the very\n   * same tokens; useful for debugging purposes on custom indexing and/or\n   * querying.\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @param log\n   *            the print stream to log to (typically System.err)\n   * @param logName\n   *            a name for this logger (typically \"log\" or similar)\n   * @return a logging analyzer\n   */\n  public static Analyzer getLoggingAnalyzer(final Analyzer child, \n      final PrintStream log, final String logName) {\n    \n    if (child == null) \n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n    if (log == null) \n      throw new IllegalArgumentException(\"logStream must not be null\");\n\n    return new Analyzer() {\n      public TokenStream tokenStream(final String fieldName, Reader reader) {\n        return new TokenFilter(child.tokenStream(fieldName, reader)) {\n          private int position = -1;\n          private TermAttribute termAtt = (TermAttribute) addAttribute(TermAttribute.class);\n          private PositionIncrementAttribute posIncrAtt = (PositionIncrementAttribute) addAttribute(PositionIncrementAttribute.class);\n          private OffsetAttribute offsetAtt = (OffsetAttribute) addAttribute(OffsetAttribute.class);\n          private TypeAttribute typeAtt = (TypeAttribute) addAttribute(TypeAttribute.class);\n         \n          public boolean incrementToken() throws IOException {\n            boolean hasNext = input.incrementToken();\n            log.println(toString(hasNext));\n            return hasNext;\n          }\n          \n          private String toString(boolean hasNext) {\n            if (!hasNext) return \"[\" + logName + \":EOS:\" + fieldName + \"]\\n\";\n            \n            position += posIncrAtt.getPositionIncrement();\n            return \"[\" + logName + \":\" + position + \":\" + fieldName + \":\"\n                + termAtt.term() + \":\" + offsetAtt.startOffset()\n                + \"-\" + offsetAtt.endOffset() + \":\" + typeAtt.type()\n                + \"]\";\n          }         \n        };\n      }\n    };\n  }\n\n","sourceOld":"  /**\n   * Returns a simple analyzer wrapper that logs all tokens produced by the\n   * underlying child analyzer to the given log stream (typically System.err);\n   * Otherwise behaves exactly like the child analyzer, delivering the very\n   * same tokens; useful for debugging purposes on custom indexing and/or\n   * querying.\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @param log\n   *            the print stream to log to (typically System.err)\n   * @param logName\n   *            a name for this logger (typically \"log\" or similar)\n   * @return a logging analyzer\n   */\n  public static Analyzer getLoggingAnalyzer(final Analyzer child, \n      final PrintStream log, final String logName) {\n    \n    if (child == null) \n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n    if (log == null) \n      throw new IllegalArgumentException(\"logStream must not be null\");\n\n    return new Analyzer() {\n      public TokenStream tokenStream(final String fieldName, Reader reader) {\n        return new TokenFilter(child.tokenStream(fieldName, reader)) {\n          private int position = -1;\n          \n          public Token next(final Token reusableToken) throws IOException {\n            assert reusableToken != null;\n            Token nextToken = input.next(reusableToken); // from filter super class\n            log.println(toString(nextToken));\n            return nextToken;\n          }\n          \n          private String toString(Token token) {\n            if (token == null) return \"[\" + logName + \":EOS:\" + fieldName + \"]\\n\";\n            \n            position += token.getPositionIncrement();\n            return \"[\" + logName + \":\" + position + \":\" + fieldName + \":\"\n                + token.term() + \":\" + token.startOffset()\n                + \"-\" + token.endOffset() + \":\" + token.type()\n                + \"]\";\n          }         \n        };\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8d78f014fded44fbde905f4f84cdc21907b371e8","date":1254383623,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getLoggingAnalyzer(Analyzer,PrintStream,String).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getLoggingAnalyzer(Analyzer,PrintStream,String).mjava","sourceNew":"  /**\n   * Returns a simple analyzer wrapper that logs all tokens produced by the\n   * underlying child analyzer to the given log stream (typically System.err);\n   * Otherwise behaves exactly like the child analyzer, delivering the very\n   * same tokens; useful for debugging purposes on custom indexing and/or\n   * querying.\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @param log\n   *            the print stream to log to (typically System.err)\n   * @param logName\n   *            a name for this logger (typically \"log\" or similar)\n   * @return a logging analyzer\n   */\n  public static Analyzer getLoggingAnalyzer(final Analyzer child, \n      final PrintStream log, final String logName) {\n    \n    if (child == null) \n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n    if (log == null) \n      throw new IllegalArgumentException(\"logStream must not be null\");\n\n    return new Analyzer() {\n      public TokenStream tokenStream(final String fieldName, Reader reader) {\n        return new TokenFilter(child.tokenStream(fieldName, reader)) {\n          private int position = -1;\n          private TermAttribute termAtt = addAttribute(TermAttribute.class);\n          private PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n          private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n          private TypeAttribute typeAtt = addAttribute(TypeAttribute.class);\n         \n          public boolean incrementToken() throws IOException {\n            boolean hasNext = input.incrementToken();\n            log.println(toString(hasNext));\n            return hasNext;\n          }\n          \n          private String toString(boolean hasNext) {\n            if (!hasNext) return \"[\" + logName + \":EOS:\" + fieldName + \"]\\n\";\n            \n            position += posIncrAtt.getPositionIncrement();\n            return \"[\" + logName + \":\" + position + \":\" + fieldName + \":\"\n                + termAtt.term() + \":\" + offsetAtt.startOffset()\n                + \"-\" + offsetAtt.endOffset() + \":\" + typeAtt.type()\n                + \"]\";\n          }         \n        };\n      }\n    };\n  }\n\n","sourceOld":"  /**\n   * Returns a simple analyzer wrapper that logs all tokens produced by the\n   * underlying child analyzer to the given log stream (typically System.err);\n   * Otherwise behaves exactly like the child analyzer, delivering the very\n   * same tokens; useful for debugging purposes on custom indexing and/or\n   * querying.\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @param log\n   *            the print stream to log to (typically System.err)\n   * @param logName\n   *            a name for this logger (typically \"log\" or similar)\n   * @return a logging analyzer\n   */\n  public static Analyzer getLoggingAnalyzer(final Analyzer child, \n      final PrintStream log, final String logName) {\n    \n    if (child == null) \n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n    if (log == null) \n      throw new IllegalArgumentException(\"logStream must not be null\");\n\n    return new Analyzer() {\n      public TokenStream tokenStream(final String fieldName, Reader reader) {\n        return new TokenFilter(child.tokenStream(fieldName, reader)) {\n          private int position = -1;\n          private TermAttribute termAtt = (TermAttribute) addAttribute(TermAttribute.class);\n          private PositionIncrementAttribute posIncrAtt = (PositionIncrementAttribute) addAttribute(PositionIncrementAttribute.class);\n          private OffsetAttribute offsetAtt = (OffsetAttribute) addAttribute(OffsetAttribute.class);\n          private TypeAttribute typeAtt = (TypeAttribute) addAttribute(TypeAttribute.class);\n         \n          public boolean incrementToken() throws IOException {\n            boolean hasNext = input.incrementToken();\n            log.println(toString(hasNext));\n            return hasNext;\n          }\n          \n          private String toString(boolean hasNext) {\n            if (!hasNext) return \"[\" + logName + \":EOS:\" + fieldName + \"]\\n\";\n            \n            position += posIncrAtt.getPositionIncrement();\n            return \"[\" + logName + \":\" + position + \":\" + fieldName + \":\"\n                + termAtt.term() + \":\" + offsetAtt.startOffset()\n                + \"-\" + offsetAtt.endOffset() + \":\" + typeAtt.type()\n                + \"]\";\n          }         \n        };\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d68e5c46e6a5ebdf4dafec4a123344092b915cc0","date":1256752193,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"contrib/wordnet/src/java/org/apache/lucene/wordnet/AnalyzerUtil#getLoggingAnalyzer(Analyzer,PrintStream,String).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getLoggingAnalyzer(Analyzer,PrintStream,String).mjava","sourceNew":"  /**\n   * Returns a simple analyzer wrapper that logs all tokens produced by the\n   * underlying child analyzer to the given log stream (typically System.err);\n   * Otherwise behaves exactly like the child analyzer, delivering the very\n   * same tokens; useful for debugging purposes on custom indexing and/or\n   * querying.\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @param log\n   *            the print stream to log to (typically System.err)\n   * @param logName\n   *            a name for this logger (typically \"log\" or similar)\n   * @return a logging analyzer\n   */\n  public static Analyzer getLoggingAnalyzer(final Analyzer child, \n      final PrintStream log, final String logName) {\n    \n    if (child == null) \n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n    if (log == null) \n      throw new IllegalArgumentException(\"logStream must not be null\");\n\n    return new Analyzer() {\n      public TokenStream tokenStream(final String fieldName, Reader reader) {\n        return new TokenFilter(child.tokenStream(fieldName, reader)) {\n          private int position = -1;\n          private TermAttribute termAtt = addAttribute(TermAttribute.class);\n          private PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n          private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n          private TypeAttribute typeAtt = addAttribute(TypeAttribute.class);\n         \n          public boolean incrementToken() throws IOException {\n            boolean hasNext = input.incrementToken();\n            log.println(toString(hasNext));\n            return hasNext;\n          }\n          \n          private String toString(boolean hasNext) {\n            if (!hasNext) return \"[\" + logName + \":EOS:\" + fieldName + \"]\\n\";\n            \n            position += posIncrAtt.getPositionIncrement();\n            return \"[\" + logName + \":\" + position + \":\" + fieldName + \":\"\n                + termAtt.term() + \":\" + offsetAtt.startOffset()\n                + \"-\" + offsetAtt.endOffset() + \":\" + typeAtt.type()\n                + \"]\";\n          }         \n        };\n      }\n    };\n  }\n\n","sourceOld":"  /**\n   * Returns a simple analyzer wrapper that logs all tokens produced by the\n   * underlying child analyzer to the given log stream (typically System.err);\n   * Otherwise behaves exactly like the child analyzer, delivering the very\n   * same tokens; useful for debugging purposes on custom indexing and/or\n   * querying.\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @param log\n   *            the print stream to log to (typically System.err)\n   * @param logName\n   *            a name for this logger (typically \"log\" or similar)\n   * @return a logging analyzer\n   */\n  public static Analyzer getLoggingAnalyzer(final Analyzer child, \n      final PrintStream log, final String logName) {\n    \n    if (child == null) \n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n    if (log == null) \n      throw new IllegalArgumentException(\"logStream must not be null\");\n\n    return new Analyzer() {\n      public TokenStream tokenStream(final String fieldName, Reader reader) {\n        return new TokenFilter(child.tokenStream(fieldName, reader)) {\n          private int position = -1;\n          private TermAttribute termAtt = addAttribute(TermAttribute.class);\n          private PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n          private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n          private TypeAttribute typeAtt = addAttribute(TypeAttribute.class);\n         \n          public boolean incrementToken() throws IOException {\n            boolean hasNext = input.incrementToken();\n            log.println(toString(hasNext));\n            return hasNext;\n          }\n          \n          private String toString(boolean hasNext) {\n            if (!hasNext) return \"[\" + logName + \":EOS:\" + fieldName + \"]\\n\";\n            \n            position += posIncrAtt.getPositionIncrement();\n            return \"[\" + logName + \":\" + position + \":\" + fieldName + \":\"\n                + termAtt.term() + \":\" + offsetAtt.startOffset()\n                + \"-\" + offsetAtt.endOffset() + \":\" + typeAtt.type()\n                + \"]\";\n          }         \n        };\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"7e2cb543b41c145f33390f460ee743d6693c9c6c":["7f68e24227d5556d33ee6d586fd9010cd9ff8bec"],"ea82415927cafd7c8b8bceca08f31a63db1cbdde":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"9b5756469957918cac40a831acec9cf01c8c2bb3":["7e2cb543b41c145f33390f460ee743d6693c9c6c"],"7f68e24227d5556d33ee6d586fd9010cd9ff8bec":["ea82415927cafd7c8b8bceca08f31a63db1cbdde"],"8d78f014fded44fbde905f4f84cdc21907b371e8":["9b5756469957918cac40a831acec9cf01c8c2bb3"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d68e5c46e6a5ebdf4dafec4a123344092b915cc0"],"d68e5c46e6a5ebdf4dafec4a123344092b915cc0":["8d78f014fded44fbde905f4f84cdc21907b371e8"]},"commit2Childs":{"7e2cb543b41c145f33390f460ee743d6693c9c6c":["9b5756469957918cac40a831acec9cf01c8c2bb3"],"ea82415927cafd7c8b8bceca08f31a63db1cbdde":["7f68e24227d5556d33ee6d586fd9010cd9ff8bec"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["ea82415927cafd7c8b8bceca08f31a63db1cbdde"],"7f68e24227d5556d33ee6d586fd9010cd9ff8bec":["7e2cb543b41c145f33390f460ee743d6693c9c6c"],"9b5756469957918cac40a831acec9cf01c8c2bb3":["8d78f014fded44fbde905f4f84cdc21907b371e8"],"8d78f014fded44fbde905f4f84cdc21907b371e8":["d68e5c46e6a5ebdf4dafec4a123344092b915cc0"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"d68e5c46e6a5ebdf4dafec4a123344092b915cc0":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}