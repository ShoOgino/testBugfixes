{"path":"modules/analysis/kuromoji/src/java/org/apache/lucene/analysis/kuromoji/KuromojiTokenizer#backtrace(Position,int).mjava","commits":[{"id":"0984ad47974c2d5d354519ddb2aa8358973a6271","date":1330868053,"type":0,"author":"Christian Moen","isMerge":false,"pathNew":"modules/analysis/kuromoji/src/java/org/apache/lucene/analysis/kuromoji/KuromojiTokenizer#backtrace(Position,int).mjava","pathOld":"/dev/null","sourceNew":"  // Backtrace from the provided position, back to the last\n  // time we back-traced, accumulating the resulting tokens to\n  // the pending list.  The pending list is then in-reverse\n  // (last token should be returned first).\n  private void backtrace(final Position endPosData, final int fromIDX) throws IOException {\n    if (VERBOSE) {\n      System.out.println(\"\\n  backtrace: pos=\" + pos + \"; \" + (pos - lastBackTracePos) + \" characters; last=\" + lastBackTracePos + \" cost=\" + endPosData.costs[fromIDX]);\n    }\n    final int endPos = endPosData.pos;\n\n    final char[] fragment = buffer.get(lastBackTracePos, endPos-lastBackTracePos);\n\n    if (dotOut != null) {\n      dotOut.onBacktrace(this, positions, lastBackTracePos, endPosData, fromIDX, fragment, end);\n    }\n\n    int pos = endPos;\n    int bestIDX = fromIDX;\n    Token altToken = null;\n\n    // We trace backwards, so this will be the leftWordID of\n    // the token after the one we are now on:\n    int lastLeftWordID = -1;\n\n    int backCount = 0;\n\n    // TODO: sort of silly to make Token instances here; the\n    // back trace has all info needed to generate the\n    // token.  So, we could just directly set the attrs,\n    // from the backtrace, in incrementToken w/o ever\n    // creating Token; we'd have to defer calling freeBefore\n    // until after the bactrace was fully \"consumed\" by\n    // incrementToken.\n\n    while (pos > lastBackTracePos) {\n      //System.out.println(\"BT: back pos=\" + pos + \" bestIDX=\" + bestIDX);\n      final Position posData = positions.get(pos);\n      assert bestIDX < posData.count;\n\n      int backPos = posData.backPos[bestIDX];\n      assert backPos >= lastBackTracePos: \"backPos=\" + backPos + \" vs lastBackTracePos=\" + lastBackTracePos;\n      int length = pos - backPos;\n      Type backType = posData.backType[bestIDX];\n      int backID = posData.backID[bestIDX];\n      int nextBestIDX = posData.backIndex[bestIDX];\n\n      if (outputCompounds && searchMode && altToken == null && backType != Type.USER) {\n        \n        // In searchMode, if best path had picked a too-long\n        // token, we use the \"penalty\" to compute the allowed\n        // max cost of an alternate back-trace.  If we find an\n        // alternate back trace with cost below that\n        // threshold, we pursue it instead (but also output\n        // the long token).\n        //System.out.println(\"    2nd best backPos=\" + backPos + \" pos=\" + pos);\n\n        final int penalty = computeSecondBestThreshold(backPos, pos-backPos);\n        \n        if (penalty > 0) {\n          if (VERBOSE) {\n            System.out.println(\"  compound=\" + new String(buffer.get(backPos, pos-backPos)) + \" backPos=\" + backPos + \" pos=\" + pos + \" penalty=\" + penalty + \" cost=\" + posData.costs[bestIDX] + \" bestIDX=\" + bestIDX + \" lastLeftID=\" + lastLeftWordID);\n          }\n\n          // Use the penalty to set maxCost on the 2nd best\n          // segmentation:\n          int maxCost = posData.costs[bestIDX] + penalty;\n          if (lastLeftWordID != -1) {\n            maxCost += costs.get(getDict(backType).getRightId(backID), lastLeftWordID);\n          }\n\n          // Now, prune all too-long tokens from the graph:\n          pruneAndRescore(backPos, pos,\n                          posData.backIndex[bestIDX]);\n\n          // Finally, find 2nd best back-trace and resume\n          // backtrace there:\n          int leastCost = Integer.MAX_VALUE;\n          int leastIDX = -1;\n          for(int idx=0;idx<posData.count;idx++) {\n            int cost = posData.costs[idx];\n            //System.out.println(\"    idx=\" + idx + \" prevCost=\" + cost);\n            \n            if (lastLeftWordID != -1) {\n              cost += costs.get(getDict(posData.backType[idx]).getRightId(posData.backID[idx]),\n                                lastLeftWordID);\n              //System.out.println(\"      += bgCost=\" + costs.get(getDict(posData.backType[idx]).getRightId(posData.backID[idx]),\n              //lastLeftWordID) + \" -> \" + cost);\n            }\n            //System.out.println(\"penalty \" + posData.backPos[idx] + \" to \" + pos);\n            //cost += computePenalty(posData.backPos[idx], pos - posData.backPos[idx]);\n            if (cost < leastCost) {\n              //System.out.println(\"      ** \");\n              leastCost = cost;\n              leastIDX = idx;\n            }\n          }\n          //System.out.println(\"  leastIDX=\" + leastIDX);\n\n          if (VERBOSE) {\n            System.out.println(\"  afterPrune: \" + posData.count + \" arcs arriving; leastCost=\" + leastCost + \" vs threshold=\" + maxCost + \" lastLeftWordID=\" + lastLeftWordID);\n          }\n\n          if (leastIDX != -1 && leastCost <= maxCost && posData.backPos[leastIDX] != backPos) {\n            // We should have pruned the altToken from the graph:\n            assert posData.backPos[leastIDX] != backPos;\n\n            // Save the current compound token, to output when\n            // this alternate path joins back:\n            altToken = new Token(backID,\n                                 fragment,\n                                 backPos - lastBackTracePos,\n                                 length,\n                                 backType,\n                                 backPos,\n                                 getDict(backType));\n\n            // Redirect our backtrace to 2nd best:\n            bestIDX = leastIDX;\n            nextBestIDX = posData.backIndex[bestIDX];\n\n            backPos = posData.backPos[bestIDX];\n            length = pos - backPos;\n            backType = posData.backType[bestIDX];\n            backID = posData.backID[bestIDX];\n            backCount = 0;\n            //System.out.println(\"  do alt token!\");\n            \n          } else {\n            // I think in theory it's possible there is no\n            // 2nd best path, which is fine; in this case we\n            // only output the compound token:\n            //System.out.println(\"  no alt token! bestIDX=\" + bestIDX);\n          }\n        }\n      }\n\n      final int offset = backPos - lastBackTracePos;\n      assert offset >= 0;\n\n      if (altToken != null && altToken.getPosition() >= backPos) {\n\n        // We've backtraced to the position where the\n        // compound token starts; add it now:\n\n        // The pruning we did when we created the altToken\n        // ensures that the back trace will align back with\n        // the start of the altToken:\n        // cannot assert...\n        //assert altToken.getPosition() == backPos: altToken.getPosition() + \" vs \" + backPos;\n\n        if (VERBOSE) {\n          System.out.println(\"    add altToken=\" + altToken);\n        }\n        if (backCount > 0) {\n          backCount++;\n          altToken.setPositionLength(backCount);\n          pending.add(altToken);\n        } else {\n          // This means alt token was all punct tokens:\n          assert discardPunctuation;\n        }\n        altToken = null;\n      }\n\n      final Dictionary dict = getDict(backType);\n\n      if (backType == Type.USER) {\n\n        // Expand the phraseID we recorded into the actual\n        // segmentation:\n        final int[] wordIDAndLength = userDictionary.lookupSegmentation(backID);\n        int wordID = wordIDAndLength[0];\n        int current = 0;\n        for(int j=1; j < wordIDAndLength.length; j++) {\n          final int len = wordIDAndLength[j];\n          //System.out.println(\"    add user: len=\" + len);\n          pending.add(new Token(wordID+j-1,\n                                fragment,\n                                current + offset,\n                                len,\n                                Type.USER,\n                                current + backPos,\n                                dict));\n          if (VERBOSE) {\n            System.out.println(\"    add USER token=\" + pending.get(pending.size()-1));\n          }\n          current += len;\n        }\n\n        // Reverse the tokens we just added, because when we\n        // serve them up from incrementToken we serve in\n        // reverse:\n        Collections.reverse(pending.subList(pending.size() - (wordIDAndLength.length - 1),\n                                            pending.size()));\n\n        backCount += wordIDAndLength.length-1;\n      } else {\n\n        if (extendedMode && backType == Type.UNKNOWN) {\n          // In EXTENDED mode we convert unknown word into\n          // unigrams:\n          int unigramTokenCount = 0;\n          for(int i=length-1;i>=0;i--) {\n            int charLen = 1;\n            if (i > 0 && Character.isLowSurrogate(fragment[offset+i])) {\n              i--;\n              charLen = 2;\n            }\n            //System.out.println(\"    extended tok offset=\"\n            //+ (offset + i));\n            if (!discardPunctuation || !isPunctuation(fragment[offset+i])) {\n              pending.add(new Token(CharacterDefinition.NGRAM,\n                                    fragment,\n                                    offset + i,\n                                    charLen,\n                                    Type.UNKNOWN,\n                                    backPos + i,\n                                    unkDictionary));\n              unigramTokenCount++;\n            }\n          }\n          backCount += unigramTokenCount;\n          \n        } else if (!discardPunctuation || length == 0 || !isPunctuation(fragment[offset])) {\n          pending.add(new Token(backID,\n                                fragment,\n                                offset,\n                                length,\n                                backType,\n                                backPos,\n                                dict));\n          if (VERBOSE) {\n            System.out.println(\"    add token=\" + pending.get(pending.size()-1));\n          }\n          backCount++;\n        } else {\n          if (VERBOSE) {\n            System.out.println(\"    skip punctuation token=\" + new String(fragment, offset, length));\n          }\n        }\n      }\n\n      lastLeftWordID = dict.getLeftId(backID);\n      pos = backPos;\n      bestIDX = nextBestIDX;\n    }\n\n    lastBackTracePos = endPos;\n\n    if (VERBOSE) {\n      System.out.println(\"  freeBefore pos=\" + endPos);\n    }\n    // Notify the circular buffers that we are done with\n    // these positions:\n    buffer.freeBefore(endPos);\n    positions.freeBefore(endPos);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["1fe9452de26a70442324c5bdc5a5a333e55f07db","790bdefa98dd40c18848b1915db6ba17dabe4b61"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","date":1331075828,"type":0,"author":"Ryan McKinley","isMerge":true,"pathNew":"modules/analysis/kuromoji/src/java/org/apache/lucene/analysis/kuromoji/KuromojiTokenizer#backtrace(Position,int).mjava","pathOld":"/dev/null","sourceNew":"  // Backtrace from the provided position, back to the last\n  // time we back-traced, accumulating the resulting tokens to\n  // the pending list.  The pending list is then in-reverse\n  // (last token should be returned first).\n  private void backtrace(final Position endPosData, final int fromIDX) throws IOException {\n    if (VERBOSE) {\n      System.out.println(\"\\n  backtrace: pos=\" + pos + \"; \" + (pos - lastBackTracePos) + \" characters; last=\" + lastBackTracePos + \" cost=\" + endPosData.costs[fromIDX]);\n    }\n    final int endPos = endPosData.pos;\n\n    final char[] fragment = buffer.get(lastBackTracePos, endPos-lastBackTracePos);\n\n    if (dotOut != null) {\n      dotOut.onBacktrace(this, positions, lastBackTracePos, endPosData, fromIDX, fragment, end);\n    }\n\n    int pos = endPos;\n    int bestIDX = fromIDX;\n    Token altToken = null;\n\n    // We trace backwards, so this will be the leftWordID of\n    // the token after the one we are now on:\n    int lastLeftWordID = -1;\n\n    int backCount = 0;\n\n    // TODO: sort of silly to make Token instances here; the\n    // back trace has all info needed to generate the\n    // token.  So, we could just directly set the attrs,\n    // from the backtrace, in incrementToken w/o ever\n    // creating Token; we'd have to defer calling freeBefore\n    // until after the bactrace was fully \"consumed\" by\n    // incrementToken.\n\n    while (pos > lastBackTracePos) {\n      //System.out.println(\"BT: back pos=\" + pos + \" bestIDX=\" + bestIDX);\n      final Position posData = positions.get(pos);\n      assert bestIDX < posData.count;\n\n      int backPos = posData.backPos[bestIDX];\n      assert backPos >= lastBackTracePos: \"backPos=\" + backPos + \" vs lastBackTracePos=\" + lastBackTracePos;\n      int length = pos - backPos;\n      Type backType = posData.backType[bestIDX];\n      int backID = posData.backID[bestIDX];\n      int nextBestIDX = posData.backIndex[bestIDX];\n\n      if (outputCompounds && searchMode && altToken == null && backType != Type.USER) {\n        \n        // In searchMode, if best path had picked a too-long\n        // token, we use the \"penalty\" to compute the allowed\n        // max cost of an alternate back-trace.  If we find an\n        // alternate back trace with cost below that\n        // threshold, we pursue it instead (but also output\n        // the long token).\n        //System.out.println(\"    2nd best backPos=\" + backPos + \" pos=\" + pos);\n\n        final int penalty = computeSecondBestThreshold(backPos, pos-backPos);\n        \n        if (penalty > 0) {\n          if (VERBOSE) {\n            System.out.println(\"  compound=\" + new String(buffer.get(backPos, pos-backPos)) + \" backPos=\" + backPos + \" pos=\" + pos + \" penalty=\" + penalty + \" cost=\" + posData.costs[bestIDX] + \" bestIDX=\" + bestIDX + \" lastLeftID=\" + lastLeftWordID);\n          }\n\n          // Use the penalty to set maxCost on the 2nd best\n          // segmentation:\n          int maxCost = posData.costs[bestIDX] + penalty;\n          if (lastLeftWordID != -1) {\n            maxCost += costs.get(getDict(backType).getRightId(backID), lastLeftWordID);\n          }\n\n          // Now, prune all too-long tokens from the graph:\n          pruneAndRescore(backPos, pos,\n                          posData.backIndex[bestIDX]);\n\n          // Finally, find 2nd best back-trace and resume\n          // backtrace there:\n          int leastCost = Integer.MAX_VALUE;\n          int leastIDX = -1;\n          for(int idx=0;idx<posData.count;idx++) {\n            int cost = posData.costs[idx];\n            //System.out.println(\"    idx=\" + idx + \" prevCost=\" + cost);\n            \n            if (lastLeftWordID != -1) {\n              cost += costs.get(getDict(posData.backType[idx]).getRightId(posData.backID[idx]),\n                                lastLeftWordID);\n              //System.out.println(\"      += bgCost=\" + costs.get(getDict(posData.backType[idx]).getRightId(posData.backID[idx]),\n              //lastLeftWordID) + \" -> \" + cost);\n            }\n            //System.out.println(\"penalty \" + posData.backPos[idx] + \" to \" + pos);\n            //cost += computePenalty(posData.backPos[idx], pos - posData.backPos[idx]);\n            if (cost < leastCost) {\n              //System.out.println(\"      ** \");\n              leastCost = cost;\n              leastIDX = idx;\n            }\n          }\n          //System.out.println(\"  leastIDX=\" + leastIDX);\n\n          if (VERBOSE) {\n            System.out.println(\"  afterPrune: \" + posData.count + \" arcs arriving; leastCost=\" + leastCost + \" vs threshold=\" + maxCost + \" lastLeftWordID=\" + lastLeftWordID);\n          }\n\n          if (leastIDX != -1 && leastCost <= maxCost && posData.backPos[leastIDX] != backPos) {\n            // We should have pruned the altToken from the graph:\n            assert posData.backPos[leastIDX] != backPos;\n\n            // Save the current compound token, to output when\n            // this alternate path joins back:\n            altToken = new Token(backID,\n                                 fragment,\n                                 backPos - lastBackTracePos,\n                                 length,\n                                 backType,\n                                 backPos,\n                                 getDict(backType));\n\n            // Redirect our backtrace to 2nd best:\n            bestIDX = leastIDX;\n            nextBestIDX = posData.backIndex[bestIDX];\n\n            backPos = posData.backPos[bestIDX];\n            length = pos - backPos;\n            backType = posData.backType[bestIDX];\n            backID = posData.backID[bestIDX];\n            backCount = 0;\n            //System.out.println(\"  do alt token!\");\n            \n          } else {\n            // I think in theory it's possible there is no\n            // 2nd best path, which is fine; in this case we\n            // only output the compound token:\n            //System.out.println(\"  no alt token! bestIDX=\" + bestIDX);\n          }\n        }\n      }\n\n      final int offset = backPos - lastBackTracePos;\n      assert offset >= 0;\n\n      if (altToken != null && altToken.getPosition() >= backPos) {\n\n        // We've backtraced to the position where the\n        // compound token starts; add it now:\n\n        // The pruning we did when we created the altToken\n        // ensures that the back trace will align back with\n        // the start of the altToken:\n        // cannot assert...\n        //assert altToken.getPosition() == backPos: altToken.getPosition() + \" vs \" + backPos;\n\n        if (VERBOSE) {\n          System.out.println(\"    add altToken=\" + altToken);\n        }\n        if (backCount > 0) {\n          backCount++;\n          altToken.setPositionLength(backCount);\n          pending.add(altToken);\n        } else {\n          // This means alt token was all punct tokens:\n          assert discardPunctuation;\n        }\n        altToken = null;\n      }\n\n      final Dictionary dict = getDict(backType);\n\n      if (backType == Type.USER) {\n\n        // Expand the phraseID we recorded into the actual\n        // segmentation:\n        final int[] wordIDAndLength = userDictionary.lookupSegmentation(backID);\n        int wordID = wordIDAndLength[0];\n        int current = 0;\n        for(int j=1; j < wordIDAndLength.length; j++) {\n          final int len = wordIDAndLength[j];\n          //System.out.println(\"    add user: len=\" + len);\n          pending.add(new Token(wordID+j-1,\n                                fragment,\n                                current + offset,\n                                len,\n                                Type.USER,\n                                current + backPos,\n                                dict));\n          if (VERBOSE) {\n            System.out.println(\"    add USER token=\" + pending.get(pending.size()-1));\n          }\n          current += len;\n        }\n\n        // Reverse the tokens we just added, because when we\n        // serve them up from incrementToken we serve in\n        // reverse:\n        Collections.reverse(pending.subList(pending.size() - (wordIDAndLength.length - 1),\n                                            pending.size()));\n\n        backCount += wordIDAndLength.length-1;\n      } else {\n\n        if (extendedMode && backType == Type.UNKNOWN) {\n          // In EXTENDED mode we convert unknown word into\n          // unigrams:\n          int unigramTokenCount = 0;\n          for(int i=length-1;i>=0;i--) {\n            int charLen = 1;\n            if (i > 0 && Character.isLowSurrogate(fragment[offset+i])) {\n              i--;\n              charLen = 2;\n            }\n            //System.out.println(\"    extended tok offset=\"\n            //+ (offset + i));\n            if (!discardPunctuation || !isPunctuation(fragment[offset+i])) {\n              pending.add(new Token(CharacterDefinition.NGRAM,\n                                    fragment,\n                                    offset + i,\n                                    charLen,\n                                    Type.UNKNOWN,\n                                    backPos + i,\n                                    unkDictionary));\n              unigramTokenCount++;\n            }\n          }\n          backCount += unigramTokenCount;\n          \n        } else if (!discardPunctuation || length == 0 || !isPunctuation(fragment[offset])) {\n          pending.add(new Token(backID,\n                                fragment,\n                                offset,\n                                length,\n                                backType,\n                                backPos,\n                                dict));\n          if (VERBOSE) {\n            System.out.println(\"    add token=\" + pending.get(pending.size()-1));\n          }\n          backCount++;\n        } else {\n          if (VERBOSE) {\n            System.out.println(\"    skip punctuation token=\" + new String(fragment, offset, length));\n          }\n        }\n      }\n\n      lastLeftWordID = dict.getLeftId(backID);\n      pos = backPos;\n      bestIDX = nextBestIDX;\n    }\n\n    lastBackTracePos = endPos;\n\n    if (VERBOSE) {\n      System.out.println(\"  freeBefore pos=\" + endPos);\n    }\n    // Notify the circular buffers that we are done with\n    // these positions:\n    buffer.freeBefore(endPos);\n    positions.freeBefore(endPos);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"790bdefa98dd40c18848b1915db6ba17dabe4b61","date":1332711475,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"modules/analysis/kuromoji/src/java/org/apache/lucene/analysis/kuromoji/KuromojiTokenizer#backtrace(Position,int).mjava","pathOld":"modules/analysis/kuromoji/src/java/org/apache/lucene/analysis/kuromoji/KuromojiTokenizer#backtrace(Position,int).mjava","sourceNew":"  // Backtrace from the provided position, back to the last\n  // time we back-traced, accumulating the resulting tokens to\n  // the pending list.  The pending list is then in-reverse\n  // (last token should be returned first).\n  private void backtrace(final Position endPosData, final int fromIDX) throws IOException {\n    final int endPos = endPosData.pos;\n\n    if (VERBOSE) {\n      System.out.println(\"\\n  backtrace: endPos=\" + endPos + \" pos=\" + pos + \"; \" + (pos - lastBackTracePos) + \" characters; last=\" + lastBackTracePos + \" cost=\" + endPosData.costs[fromIDX]);\n    }\n\n    final char[] fragment = buffer.get(lastBackTracePos, endPos-lastBackTracePos);\n\n    if (dotOut != null) {\n      dotOut.onBacktrace(this, positions, lastBackTracePos, endPosData, fromIDX, fragment, end);\n    }\n\n    int pos = endPos;\n    int bestIDX = fromIDX;\n    Token altToken = null;\n\n    // We trace backwards, so this will be the leftWordID of\n    // the token after the one we are now on:\n    int lastLeftWordID = -1;\n\n    int backCount = 0;\n\n    // TODO: sort of silly to make Token instances here; the\n    // back trace has all info needed to generate the\n    // token.  So, we could just directly set the attrs,\n    // from the backtrace, in incrementToken w/o ever\n    // creating Token; we'd have to defer calling freeBefore\n    // until after the bactrace was fully \"consumed\" by\n    // incrementToken.\n\n    while (pos > lastBackTracePos) {\n      //System.out.println(\"BT: back pos=\" + pos + \" bestIDX=\" + bestIDX);\n      final Position posData = positions.get(pos);\n      assert bestIDX < posData.count;\n\n      int backPos = posData.backPos[bestIDX];\n      assert backPos >= lastBackTracePos: \"backPos=\" + backPos + \" vs lastBackTracePos=\" + lastBackTracePos;\n      int length = pos - backPos;\n      Type backType = posData.backType[bestIDX];\n      int backID = posData.backID[bestIDX];\n      int nextBestIDX = posData.backIndex[bestIDX];\n\n      if (outputCompounds && searchMode && altToken == null && backType != Type.USER) {\n        \n        // In searchMode, if best path had picked a too-long\n        // token, we use the \"penalty\" to compute the allowed\n        // max cost of an alternate back-trace.  If we find an\n        // alternate back trace with cost below that\n        // threshold, we pursue it instead (but also output\n        // the long token).\n        //System.out.println(\"    2nd best backPos=\" + backPos + \" pos=\" + pos);\n\n        final int penalty = computeSecondBestThreshold(backPos, pos-backPos);\n        \n        if (penalty > 0) {\n          if (VERBOSE) {\n            System.out.println(\"  compound=\" + new String(buffer.get(backPos, pos-backPos)) + \" backPos=\" + backPos + \" pos=\" + pos + \" penalty=\" + penalty + \" cost=\" + posData.costs[bestIDX] + \" bestIDX=\" + bestIDX + \" lastLeftID=\" + lastLeftWordID);\n          }\n\n          // Use the penalty to set maxCost on the 2nd best\n          // segmentation:\n          int maxCost = posData.costs[bestIDX] + penalty;\n          if (lastLeftWordID != -1) {\n            maxCost += costs.get(getDict(backType).getRightId(backID), lastLeftWordID);\n          }\n\n          // Now, prune all too-long tokens from the graph:\n          pruneAndRescore(backPos, pos,\n                          posData.backIndex[bestIDX]);\n\n          // Finally, find 2nd best back-trace and resume\n          // backtrace there:\n          int leastCost = Integer.MAX_VALUE;\n          int leastIDX = -1;\n          for(int idx=0;idx<posData.count;idx++) {\n            int cost = posData.costs[idx];\n            //System.out.println(\"    idx=\" + idx + \" prevCost=\" + cost);\n            \n            if (lastLeftWordID != -1) {\n              cost += costs.get(getDict(posData.backType[idx]).getRightId(posData.backID[idx]),\n                                lastLeftWordID);\n              //System.out.println(\"      += bgCost=\" + costs.get(getDict(posData.backType[idx]).getRightId(posData.backID[idx]),\n              //lastLeftWordID) + \" -> \" + cost);\n            }\n            //System.out.println(\"penalty \" + posData.backPos[idx] + \" to \" + pos);\n            //cost += computePenalty(posData.backPos[idx], pos - posData.backPos[idx]);\n            if (cost < leastCost) {\n              //System.out.println(\"      ** \");\n              leastCost = cost;\n              leastIDX = idx;\n            }\n          }\n          //System.out.println(\"  leastIDX=\" + leastIDX);\n\n          if (VERBOSE) {\n            System.out.println(\"  afterPrune: \" + posData.count + \" arcs arriving; leastCost=\" + leastCost + \" vs threshold=\" + maxCost + \" lastLeftWordID=\" + lastLeftWordID);\n          }\n\n          if (leastIDX != -1 && leastCost <= maxCost && posData.backPos[leastIDX] != backPos) {\n            // We should have pruned the altToken from the graph:\n            assert posData.backPos[leastIDX] != backPos;\n\n            // Save the current compound token, to output when\n            // this alternate path joins back:\n            altToken = new Token(backID,\n                                 fragment,\n                                 backPos - lastBackTracePos,\n                                 length,\n                                 backType,\n                                 backPos,\n                                 getDict(backType));\n\n            // Redirect our backtrace to 2nd best:\n            bestIDX = leastIDX;\n            nextBestIDX = posData.backIndex[bestIDX];\n\n            backPos = posData.backPos[bestIDX];\n            length = pos - backPos;\n            backType = posData.backType[bestIDX];\n            backID = posData.backID[bestIDX];\n            backCount = 0;\n            //System.out.println(\"  do alt token!\");\n            \n          } else {\n            // I think in theory it's possible there is no\n            // 2nd best path, which is fine; in this case we\n            // only output the compound token:\n            //System.out.println(\"  no alt token! bestIDX=\" + bestIDX);\n          }\n        }\n      }\n\n      final int offset = backPos - lastBackTracePos;\n      assert offset >= 0;\n\n      if (altToken != null && altToken.getPosition() >= backPos) {\n\n        // We've backtraced to the position where the\n        // compound token starts; add it now:\n\n        // The pruning we did when we created the altToken\n        // ensures that the back trace will align back with\n        // the start of the altToken:\n        // cannot assert...\n        //assert altToken.getPosition() == backPos: altToken.getPosition() + \" vs \" + backPos;\n\n        if (VERBOSE) {\n          System.out.println(\"    add altToken=\" + altToken);\n        }\n        if (backCount > 0) {\n          backCount++;\n          altToken.setPositionLength(backCount);\n          pending.add(altToken);\n        } else {\n          // This means alt token was all punct tokens:\n          assert discardPunctuation;\n        }\n        altToken = null;\n      }\n\n      final Dictionary dict = getDict(backType);\n\n      if (backType == Type.USER) {\n\n        // Expand the phraseID we recorded into the actual\n        // segmentation:\n        final int[] wordIDAndLength = userDictionary.lookupSegmentation(backID);\n        int wordID = wordIDAndLength[0];\n        int current = 0;\n        for(int j=1; j < wordIDAndLength.length; j++) {\n          final int len = wordIDAndLength[j];\n          //System.out.println(\"    add user: len=\" + len);\n          pending.add(new Token(wordID+j-1,\n                                fragment,\n                                current + offset,\n                                len,\n                                Type.USER,\n                                current + backPos,\n                                dict));\n          if (VERBOSE) {\n            System.out.println(\"    add USER token=\" + pending.get(pending.size()-1));\n          }\n          current += len;\n        }\n\n        // Reverse the tokens we just added, because when we\n        // serve them up from incrementToken we serve in\n        // reverse:\n        Collections.reverse(pending.subList(pending.size() - (wordIDAndLength.length - 1),\n                                            pending.size()));\n\n        backCount += wordIDAndLength.length-1;\n      } else {\n\n        if (extendedMode && backType == Type.UNKNOWN) {\n          // In EXTENDED mode we convert unknown word into\n          // unigrams:\n          int unigramTokenCount = 0;\n          for(int i=length-1;i>=0;i--) {\n            int charLen = 1;\n            if (i > 0 && Character.isLowSurrogate(fragment[offset+i])) {\n              i--;\n              charLen = 2;\n            }\n            //System.out.println(\"    extended tok offset=\"\n            //+ (offset + i));\n            if (!discardPunctuation || !isPunctuation(fragment[offset+i])) {\n              pending.add(new Token(CharacterDefinition.NGRAM,\n                                    fragment,\n                                    offset + i,\n                                    charLen,\n                                    Type.UNKNOWN,\n                                    backPos + i,\n                                    unkDictionary));\n              unigramTokenCount++;\n            }\n          }\n          backCount += unigramTokenCount;\n          \n        } else if (!discardPunctuation || length == 0 || !isPunctuation(fragment[offset])) {\n          pending.add(new Token(backID,\n                                fragment,\n                                offset,\n                                length,\n                                backType,\n                                backPos,\n                                dict));\n          if (VERBOSE) {\n            System.out.println(\"    add token=\" + pending.get(pending.size()-1));\n          }\n          backCount++;\n        } else {\n          if (VERBOSE) {\n            System.out.println(\"    skip punctuation token=\" + new String(fragment, offset, length));\n          }\n        }\n      }\n\n      lastLeftWordID = dict.getLeftId(backID);\n      pos = backPos;\n      bestIDX = nextBestIDX;\n    }\n\n    lastBackTracePos = endPos;\n\n    if (VERBOSE) {\n      System.out.println(\"  freeBefore pos=\" + endPos);\n    }\n    // Notify the circular buffers that we are done with\n    // these positions:\n    buffer.freeBefore(endPos);\n    positions.freeBefore(endPos);\n  }\n\n","sourceOld":"  // Backtrace from the provided position, back to the last\n  // time we back-traced, accumulating the resulting tokens to\n  // the pending list.  The pending list is then in-reverse\n  // (last token should be returned first).\n  private void backtrace(final Position endPosData, final int fromIDX) throws IOException {\n    if (VERBOSE) {\n      System.out.println(\"\\n  backtrace: pos=\" + pos + \"; \" + (pos - lastBackTracePos) + \" characters; last=\" + lastBackTracePos + \" cost=\" + endPosData.costs[fromIDX]);\n    }\n    final int endPos = endPosData.pos;\n\n    final char[] fragment = buffer.get(lastBackTracePos, endPos-lastBackTracePos);\n\n    if (dotOut != null) {\n      dotOut.onBacktrace(this, positions, lastBackTracePos, endPosData, fromIDX, fragment, end);\n    }\n\n    int pos = endPos;\n    int bestIDX = fromIDX;\n    Token altToken = null;\n\n    // We trace backwards, so this will be the leftWordID of\n    // the token after the one we are now on:\n    int lastLeftWordID = -1;\n\n    int backCount = 0;\n\n    // TODO: sort of silly to make Token instances here; the\n    // back trace has all info needed to generate the\n    // token.  So, we could just directly set the attrs,\n    // from the backtrace, in incrementToken w/o ever\n    // creating Token; we'd have to defer calling freeBefore\n    // until after the bactrace was fully \"consumed\" by\n    // incrementToken.\n\n    while (pos > lastBackTracePos) {\n      //System.out.println(\"BT: back pos=\" + pos + \" bestIDX=\" + bestIDX);\n      final Position posData = positions.get(pos);\n      assert bestIDX < posData.count;\n\n      int backPos = posData.backPos[bestIDX];\n      assert backPos >= lastBackTracePos: \"backPos=\" + backPos + \" vs lastBackTracePos=\" + lastBackTracePos;\n      int length = pos - backPos;\n      Type backType = posData.backType[bestIDX];\n      int backID = posData.backID[bestIDX];\n      int nextBestIDX = posData.backIndex[bestIDX];\n\n      if (outputCompounds && searchMode && altToken == null && backType != Type.USER) {\n        \n        // In searchMode, if best path had picked a too-long\n        // token, we use the \"penalty\" to compute the allowed\n        // max cost of an alternate back-trace.  If we find an\n        // alternate back trace with cost below that\n        // threshold, we pursue it instead (but also output\n        // the long token).\n        //System.out.println(\"    2nd best backPos=\" + backPos + \" pos=\" + pos);\n\n        final int penalty = computeSecondBestThreshold(backPos, pos-backPos);\n        \n        if (penalty > 0) {\n          if (VERBOSE) {\n            System.out.println(\"  compound=\" + new String(buffer.get(backPos, pos-backPos)) + \" backPos=\" + backPos + \" pos=\" + pos + \" penalty=\" + penalty + \" cost=\" + posData.costs[bestIDX] + \" bestIDX=\" + bestIDX + \" lastLeftID=\" + lastLeftWordID);\n          }\n\n          // Use the penalty to set maxCost on the 2nd best\n          // segmentation:\n          int maxCost = posData.costs[bestIDX] + penalty;\n          if (lastLeftWordID != -1) {\n            maxCost += costs.get(getDict(backType).getRightId(backID), lastLeftWordID);\n          }\n\n          // Now, prune all too-long tokens from the graph:\n          pruneAndRescore(backPos, pos,\n                          posData.backIndex[bestIDX]);\n\n          // Finally, find 2nd best back-trace and resume\n          // backtrace there:\n          int leastCost = Integer.MAX_VALUE;\n          int leastIDX = -1;\n          for(int idx=0;idx<posData.count;idx++) {\n            int cost = posData.costs[idx];\n            //System.out.println(\"    idx=\" + idx + \" prevCost=\" + cost);\n            \n            if (lastLeftWordID != -1) {\n              cost += costs.get(getDict(posData.backType[idx]).getRightId(posData.backID[idx]),\n                                lastLeftWordID);\n              //System.out.println(\"      += bgCost=\" + costs.get(getDict(posData.backType[idx]).getRightId(posData.backID[idx]),\n              //lastLeftWordID) + \" -> \" + cost);\n            }\n            //System.out.println(\"penalty \" + posData.backPos[idx] + \" to \" + pos);\n            //cost += computePenalty(posData.backPos[idx], pos - posData.backPos[idx]);\n            if (cost < leastCost) {\n              //System.out.println(\"      ** \");\n              leastCost = cost;\n              leastIDX = idx;\n            }\n          }\n          //System.out.println(\"  leastIDX=\" + leastIDX);\n\n          if (VERBOSE) {\n            System.out.println(\"  afterPrune: \" + posData.count + \" arcs arriving; leastCost=\" + leastCost + \" vs threshold=\" + maxCost + \" lastLeftWordID=\" + lastLeftWordID);\n          }\n\n          if (leastIDX != -1 && leastCost <= maxCost && posData.backPos[leastIDX] != backPos) {\n            // We should have pruned the altToken from the graph:\n            assert posData.backPos[leastIDX] != backPos;\n\n            // Save the current compound token, to output when\n            // this alternate path joins back:\n            altToken = new Token(backID,\n                                 fragment,\n                                 backPos - lastBackTracePos,\n                                 length,\n                                 backType,\n                                 backPos,\n                                 getDict(backType));\n\n            // Redirect our backtrace to 2nd best:\n            bestIDX = leastIDX;\n            nextBestIDX = posData.backIndex[bestIDX];\n\n            backPos = posData.backPos[bestIDX];\n            length = pos - backPos;\n            backType = posData.backType[bestIDX];\n            backID = posData.backID[bestIDX];\n            backCount = 0;\n            //System.out.println(\"  do alt token!\");\n            \n          } else {\n            // I think in theory it's possible there is no\n            // 2nd best path, which is fine; in this case we\n            // only output the compound token:\n            //System.out.println(\"  no alt token! bestIDX=\" + bestIDX);\n          }\n        }\n      }\n\n      final int offset = backPos - lastBackTracePos;\n      assert offset >= 0;\n\n      if (altToken != null && altToken.getPosition() >= backPos) {\n\n        // We've backtraced to the position where the\n        // compound token starts; add it now:\n\n        // The pruning we did when we created the altToken\n        // ensures that the back trace will align back with\n        // the start of the altToken:\n        // cannot assert...\n        //assert altToken.getPosition() == backPos: altToken.getPosition() + \" vs \" + backPos;\n\n        if (VERBOSE) {\n          System.out.println(\"    add altToken=\" + altToken);\n        }\n        if (backCount > 0) {\n          backCount++;\n          altToken.setPositionLength(backCount);\n          pending.add(altToken);\n        } else {\n          // This means alt token was all punct tokens:\n          assert discardPunctuation;\n        }\n        altToken = null;\n      }\n\n      final Dictionary dict = getDict(backType);\n\n      if (backType == Type.USER) {\n\n        // Expand the phraseID we recorded into the actual\n        // segmentation:\n        final int[] wordIDAndLength = userDictionary.lookupSegmentation(backID);\n        int wordID = wordIDAndLength[0];\n        int current = 0;\n        for(int j=1; j < wordIDAndLength.length; j++) {\n          final int len = wordIDAndLength[j];\n          //System.out.println(\"    add user: len=\" + len);\n          pending.add(new Token(wordID+j-1,\n                                fragment,\n                                current + offset,\n                                len,\n                                Type.USER,\n                                current + backPos,\n                                dict));\n          if (VERBOSE) {\n            System.out.println(\"    add USER token=\" + pending.get(pending.size()-1));\n          }\n          current += len;\n        }\n\n        // Reverse the tokens we just added, because when we\n        // serve them up from incrementToken we serve in\n        // reverse:\n        Collections.reverse(pending.subList(pending.size() - (wordIDAndLength.length - 1),\n                                            pending.size()));\n\n        backCount += wordIDAndLength.length-1;\n      } else {\n\n        if (extendedMode && backType == Type.UNKNOWN) {\n          // In EXTENDED mode we convert unknown word into\n          // unigrams:\n          int unigramTokenCount = 0;\n          for(int i=length-1;i>=0;i--) {\n            int charLen = 1;\n            if (i > 0 && Character.isLowSurrogate(fragment[offset+i])) {\n              i--;\n              charLen = 2;\n            }\n            //System.out.println(\"    extended tok offset=\"\n            //+ (offset + i));\n            if (!discardPunctuation || !isPunctuation(fragment[offset+i])) {\n              pending.add(new Token(CharacterDefinition.NGRAM,\n                                    fragment,\n                                    offset + i,\n                                    charLen,\n                                    Type.UNKNOWN,\n                                    backPos + i,\n                                    unkDictionary));\n              unigramTokenCount++;\n            }\n          }\n          backCount += unigramTokenCount;\n          \n        } else if (!discardPunctuation || length == 0 || !isPunctuation(fragment[offset])) {\n          pending.add(new Token(backID,\n                                fragment,\n                                offset,\n                                length,\n                                backType,\n                                backPos,\n                                dict));\n          if (VERBOSE) {\n            System.out.println(\"    add token=\" + pending.get(pending.size()-1));\n          }\n          backCount++;\n        } else {\n          if (VERBOSE) {\n            System.out.println(\"    skip punctuation token=\" + new String(fragment, offset, length));\n          }\n        }\n      }\n\n      lastLeftWordID = dict.getLeftId(backID);\n      pos = backPos;\n      bestIDX = nextBestIDX;\n    }\n\n    lastBackTracePos = endPos;\n\n    if (VERBOSE) {\n      System.out.println(\"  freeBefore pos=\" + endPos);\n    }\n    // Notify the circular buffers that we are done with\n    // these positions:\n    buffer.freeBefore(endPos);\n    positions.freeBefore(endPos);\n  }\n\n","bugFix":["0984ad47974c2d5d354519ddb2aa8358973a6271"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"98d45c1ff2c99694b6de2201175f9b8b8b27b597","date":1332757908,"type":5,"author":"Christian Moen","isMerge":false,"pathNew":"modules/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseTokenizer#backtrace(Position,int).mjava","pathOld":"modules/analysis/kuromoji/src/java/org/apache/lucene/analysis/kuromoji/KuromojiTokenizer#backtrace(Position,int).mjava","sourceNew":"  // Backtrace from the provided position, back to the last\n  // time we back-traced, accumulating the resulting tokens to\n  // the pending list.  The pending list is then in-reverse\n  // (last token should be returned first).\n  private void backtrace(final Position endPosData, final int fromIDX) throws IOException {\n    final int endPos = endPosData.pos;\n\n    if (VERBOSE) {\n      System.out.println(\"\\n  backtrace: endPos=\" + endPos + \" pos=\" + pos + \"; \" + (pos - lastBackTracePos) + \" characters; last=\" + lastBackTracePos + \" cost=\" + endPosData.costs[fromIDX]);\n    }\n\n    final char[] fragment = buffer.get(lastBackTracePos, endPos-lastBackTracePos);\n\n    if (dotOut != null) {\n      dotOut.onBacktrace(this, positions, lastBackTracePos, endPosData, fromIDX, fragment, end);\n    }\n\n    int pos = endPos;\n    int bestIDX = fromIDX;\n    Token altToken = null;\n\n    // We trace backwards, so this will be the leftWordID of\n    // the token after the one we are now on:\n    int lastLeftWordID = -1;\n\n    int backCount = 0;\n\n    // TODO: sort of silly to make Token instances here; the\n    // back trace has all info needed to generate the\n    // token.  So, we could just directly set the attrs,\n    // from the backtrace, in incrementToken w/o ever\n    // creating Token; we'd have to defer calling freeBefore\n    // until after the bactrace was fully \"consumed\" by\n    // incrementToken.\n\n    while (pos > lastBackTracePos) {\n      //System.out.println(\"BT: back pos=\" + pos + \" bestIDX=\" + bestIDX);\n      final Position posData = positions.get(pos);\n      assert bestIDX < posData.count;\n\n      int backPos = posData.backPos[bestIDX];\n      assert backPos >= lastBackTracePos: \"backPos=\" + backPos + \" vs lastBackTracePos=\" + lastBackTracePos;\n      int length = pos - backPos;\n      Type backType = posData.backType[bestIDX];\n      int backID = posData.backID[bestIDX];\n      int nextBestIDX = posData.backIndex[bestIDX];\n\n      if (outputCompounds && searchMode && altToken == null && backType != Type.USER) {\n        \n        // In searchMode, if best path had picked a too-long\n        // token, we use the \"penalty\" to compute the allowed\n        // max cost of an alternate back-trace.  If we find an\n        // alternate back trace with cost below that\n        // threshold, we pursue it instead (but also output\n        // the long token).\n        //System.out.println(\"    2nd best backPos=\" + backPos + \" pos=\" + pos);\n\n        final int penalty = computeSecondBestThreshold(backPos, pos-backPos);\n        \n        if (penalty > 0) {\n          if (VERBOSE) {\n            System.out.println(\"  compound=\" + new String(buffer.get(backPos, pos-backPos)) + \" backPos=\" + backPos + \" pos=\" + pos + \" penalty=\" + penalty + \" cost=\" + posData.costs[bestIDX] + \" bestIDX=\" + bestIDX + \" lastLeftID=\" + lastLeftWordID);\n          }\n\n          // Use the penalty to set maxCost on the 2nd best\n          // segmentation:\n          int maxCost = posData.costs[bestIDX] + penalty;\n          if (lastLeftWordID != -1) {\n            maxCost += costs.get(getDict(backType).getRightId(backID), lastLeftWordID);\n          }\n\n          // Now, prune all too-long tokens from the graph:\n          pruneAndRescore(backPos, pos,\n                          posData.backIndex[bestIDX]);\n\n          // Finally, find 2nd best back-trace and resume\n          // backtrace there:\n          int leastCost = Integer.MAX_VALUE;\n          int leastIDX = -1;\n          for(int idx=0;idx<posData.count;idx++) {\n            int cost = posData.costs[idx];\n            //System.out.println(\"    idx=\" + idx + \" prevCost=\" + cost);\n            \n            if (lastLeftWordID != -1) {\n              cost += costs.get(getDict(posData.backType[idx]).getRightId(posData.backID[idx]),\n                                lastLeftWordID);\n              //System.out.println(\"      += bgCost=\" + costs.get(getDict(posData.backType[idx]).getRightId(posData.backID[idx]),\n              //lastLeftWordID) + \" -> \" + cost);\n            }\n            //System.out.println(\"penalty \" + posData.backPos[idx] + \" to \" + pos);\n            //cost += computePenalty(posData.backPos[idx], pos - posData.backPos[idx]);\n            if (cost < leastCost) {\n              //System.out.println(\"      ** \");\n              leastCost = cost;\n              leastIDX = idx;\n            }\n          }\n          //System.out.println(\"  leastIDX=\" + leastIDX);\n\n          if (VERBOSE) {\n            System.out.println(\"  afterPrune: \" + posData.count + \" arcs arriving; leastCost=\" + leastCost + \" vs threshold=\" + maxCost + \" lastLeftWordID=\" + lastLeftWordID);\n          }\n\n          if (leastIDX != -1 && leastCost <= maxCost && posData.backPos[leastIDX] != backPos) {\n            // We should have pruned the altToken from the graph:\n            assert posData.backPos[leastIDX] != backPos;\n\n            // Save the current compound token, to output when\n            // this alternate path joins back:\n            altToken = new Token(backID,\n                                 fragment,\n                                 backPos - lastBackTracePos,\n                                 length,\n                                 backType,\n                                 backPos,\n                                 getDict(backType));\n\n            // Redirect our backtrace to 2nd best:\n            bestIDX = leastIDX;\n            nextBestIDX = posData.backIndex[bestIDX];\n\n            backPos = posData.backPos[bestIDX];\n            length = pos - backPos;\n            backType = posData.backType[bestIDX];\n            backID = posData.backID[bestIDX];\n            backCount = 0;\n            //System.out.println(\"  do alt token!\");\n            \n          } else {\n            // I think in theory it's possible there is no\n            // 2nd best path, which is fine; in this case we\n            // only output the compound token:\n            //System.out.println(\"  no alt token! bestIDX=\" + bestIDX);\n          }\n        }\n      }\n\n      final int offset = backPos - lastBackTracePos;\n      assert offset >= 0;\n\n      if (altToken != null && altToken.getPosition() >= backPos) {\n\n        // We've backtraced to the position where the\n        // compound token starts; add it now:\n\n        // The pruning we did when we created the altToken\n        // ensures that the back trace will align back with\n        // the start of the altToken:\n        // cannot assert...\n        //assert altToken.getPosition() == backPos: altToken.getPosition() + \" vs \" + backPos;\n\n        if (VERBOSE) {\n          System.out.println(\"    add altToken=\" + altToken);\n        }\n        if (backCount > 0) {\n          backCount++;\n          altToken.setPositionLength(backCount);\n          pending.add(altToken);\n        } else {\n          // This means alt token was all punct tokens:\n          assert discardPunctuation;\n        }\n        altToken = null;\n      }\n\n      final Dictionary dict = getDict(backType);\n\n      if (backType == Type.USER) {\n\n        // Expand the phraseID we recorded into the actual\n        // segmentation:\n        final int[] wordIDAndLength = userDictionary.lookupSegmentation(backID);\n        int wordID = wordIDAndLength[0];\n        int current = 0;\n        for(int j=1; j < wordIDAndLength.length; j++) {\n          final int len = wordIDAndLength[j];\n          //System.out.println(\"    add user: len=\" + len);\n          pending.add(new Token(wordID+j-1,\n                                fragment,\n                                current + offset,\n                                len,\n                                Type.USER,\n                                current + backPos,\n                                dict));\n          if (VERBOSE) {\n            System.out.println(\"    add USER token=\" + pending.get(pending.size()-1));\n          }\n          current += len;\n        }\n\n        // Reverse the tokens we just added, because when we\n        // serve them up from incrementToken we serve in\n        // reverse:\n        Collections.reverse(pending.subList(pending.size() - (wordIDAndLength.length - 1),\n                                            pending.size()));\n\n        backCount += wordIDAndLength.length-1;\n      } else {\n\n        if (extendedMode && backType == Type.UNKNOWN) {\n          // In EXTENDED mode we convert unknown word into\n          // unigrams:\n          int unigramTokenCount = 0;\n          for(int i=length-1;i>=0;i--) {\n            int charLen = 1;\n            if (i > 0 && Character.isLowSurrogate(fragment[offset+i])) {\n              i--;\n              charLen = 2;\n            }\n            //System.out.println(\"    extended tok offset=\"\n            //+ (offset + i));\n            if (!discardPunctuation || !isPunctuation(fragment[offset+i])) {\n              pending.add(new Token(CharacterDefinition.NGRAM,\n                                    fragment,\n                                    offset + i,\n                                    charLen,\n                                    Type.UNKNOWN,\n                                    backPos + i,\n                                    unkDictionary));\n              unigramTokenCount++;\n            }\n          }\n          backCount += unigramTokenCount;\n          \n        } else if (!discardPunctuation || length == 0 || !isPunctuation(fragment[offset])) {\n          pending.add(new Token(backID,\n                                fragment,\n                                offset,\n                                length,\n                                backType,\n                                backPos,\n                                dict));\n          if (VERBOSE) {\n            System.out.println(\"    add token=\" + pending.get(pending.size()-1));\n          }\n          backCount++;\n        } else {\n          if (VERBOSE) {\n            System.out.println(\"    skip punctuation token=\" + new String(fragment, offset, length));\n          }\n        }\n      }\n\n      lastLeftWordID = dict.getLeftId(backID);\n      pos = backPos;\n      bestIDX = nextBestIDX;\n    }\n\n    lastBackTracePos = endPos;\n\n    if (VERBOSE) {\n      System.out.println(\"  freeBefore pos=\" + endPos);\n    }\n    // Notify the circular buffers that we are done with\n    // these positions:\n    buffer.freeBefore(endPos);\n    positions.freeBefore(endPos);\n  }\n\n","sourceOld":"  // Backtrace from the provided position, back to the last\n  // time we back-traced, accumulating the resulting tokens to\n  // the pending list.  The pending list is then in-reverse\n  // (last token should be returned first).\n  private void backtrace(final Position endPosData, final int fromIDX) throws IOException {\n    final int endPos = endPosData.pos;\n\n    if (VERBOSE) {\n      System.out.println(\"\\n  backtrace: endPos=\" + endPos + \" pos=\" + pos + \"; \" + (pos - lastBackTracePos) + \" characters; last=\" + lastBackTracePos + \" cost=\" + endPosData.costs[fromIDX]);\n    }\n\n    final char[] fragment = buffer.get(lastBackTracePos, endPos-lastBackTracePos);\n\n    if (dotOut != null) {\n      dotOut.onBacktrace(this, positions, lastBackTracePos, endPosData, fromIDX, fragment, end);\n    }\n\n    int pos = endPos;\n    int bestIDX = fromIDX;\n    Token altToken = null;\n\n    // We trace backwards, so this will be the leftWordID of\n    // the token after the one we are now on:\n    int lastLeftWordID = -1;\n\n    int backCount = 0;\n\n    // TODO: sort of silly to make Token instances here; the\n    // back trace has all info needed to generate the\n    // token.  So, we could just directly set the attrs,\n    // from the backtrace, in incrementToken w/o ever\n    // creating Token; we'd have to defer calling freeBefore\n    // until after the bactrace was fully \"consumed\" by\n    // incrementToken.\n\n    while (pos > lastBackTracePos) {\n      //System.out.println(\"BT: back pos=\" + pos + \" bestIDX=\" + bestIDX);\n      final Position posData = positions.get(pos);\n      assert bestIDX < posData.count;\n\n      int backPos = posData.backPos[bestIDX];\n      assert backPos >= lastBackTracePos: \"backPos=\" + backPos + \" vs lastBackTracePos=\" + lastBackTracePos;\n      int length = pos - backPos;\n      Type backType = posData.backType[bestIDX];\n      int backID = posData.backID[bestIDX];\n      int nextBestIDX = posData.backIndex[bestIDX];\n\n      if (outputCompounds && searchMode && altToken == null && backType != Type.USER) {\n        \n        // In searchMode, if best path had picked a too-long\n        // token, we use the \"penalty\" to compute the allowed\n        // max cost of an alternate back-trace.  If we find an\n        // alternate back trace with cost below that\n        // threshold, we pursue it instead (but also output\n        // the long token).\n        //System.out.println(\"    2nd best backPos=\" + backPos + \" pos=\" + pos);\n\n        final int penalty = computeSecondBestThreshold(backPos, pos-backPos);\n        \n        if (penalty > 0) {\n          if (VERBOSE) {\n            System.out.println(\"  compound=\" + new String(buffer.get(backPos, pos-backPos)) + \" backPos=\" + backPos + \" pos=\" + pos + \" penalty=\" + penalty + \" cost=\" + posData.costs[bestIDX] + \" bestIDX=\" + bestIDX + \" lastLeftID=\" + lastLeftWordID);\n          }\n\n          // Use the penalty to set maxCost on the 2nd best\n          // segmentation:\n          int maxCost = posData.costs[bestIDX] + penalty;\n          if (lastLeftWordID != -1) {\n            maxCost += costs.get(getDict(backType).getRightId(backID), lastLeftWordID);\n          }\n\n          // Now, prune all too-long tokens from the graph:\n          pruneAndRescore(backPos, pos,\n                          posData.backIndex[bestIDX]);\n\n          // Finally, find 2nd best back-trace and resume\n          // backtrace there:\n          int leastCost = Integer.MAX_VALUE;\n          int leastIDX = -1;\n          for(int idx=0;idx<posData.count;idx++) {\n            int cost = posData.costs[idx];\n            //System.out.println(\"    idx=\" + idx + \" prevCost=\" + cost);\n            \n            if (lastLeftWordID != -1) {\n              cost += costs.get(getDict(posData.backType[idx]).getRightId(posData.backID[idx]),\n                                lastLeftWordID);\n              //System.out.println(\"      += bgCost=\" + costs.get(getDict(posData.backType[idx]).getRightId(posData.backID[idx]),\n              //lastLeftWordID) + \" -> \" + cost);\n            }\n            //System.out.println(\"penalty \" + posData.backPos[idx] + \" to \" + pos);\n            //cost += computePenalty(posData.backPos[idx], pos - posData.backPos[idx]);\n            if (cost < leastCost) {\n              //System.out.println(\"      ** \");\n              leastCost = cost;\n              leastIDX = idx;\n            }\n          }\n          //System.out.println(\"  leastIDX=\" + leastIDX);\n\n          if (VERBOSE) {\n            System.out.println(\"  afterPrune: \" + posData.count + \" arcs arriving; leastCost=\" + leastCost + \" vs threshold=\" + maxCost + \" lastLeftWordID=\" + lastLeftWordID);\n          }\n\n          if (leastIDX != -1 && leastCost <= maxCost && posData.backPos[leastIDX] != backPos) {\n            // We should have pruned the altToken from the graph:\n            assert posData.backPos[leastIDX] != backPos;\n\n            // Save the current compound token, to output when\n            // this alternate path joins back:\n            altToken = new Token(backID,\n                                 fragment,\n                                 backPos - lastBackTracePos,\n                                 length,\n                                 backType,\n                                 backPos,\n                                 getDict(backType));\n\n            // Redirect our backtrace to 2nd best:\n            bestIDX = leastIDX;\n            nextBestIDX = posData.backIndex[bestIDX];\n\n            backPos = posData.backPos[bestIDX];\n            length = pos - backPos;\n            backType = posData.backType[bestIDX];\n            backID = posData.backID[bestIDX];\n            backCount = 0;\n            //System.out.println(\"  do alt token!\");\n            \n          } else {\n            // I think in theory it's possible there is no\n            // 2nd best path, which is fine; in this case we\n            // only output the compound token:\n            //System.out.println(\"  no alt token! bestIDX=\" + bestIDX);\n          }\n        }\n      }\n\n      final int offset = backPos - lastBackTracePos;\n      assert offset >= 0;\n\n      if (altToken != null && altToken.getPosition() >= backPos) {\n\n        // We've backtraced to the position where the\n        // compound token starts; add it now:\n\n        // The pruning we did when we created the altToken\n        // ensures that the back trace will align back with\n        // the start of the altToken:\n        // cannot assert...\n        //assert altToken.getPosition() == backPos: altToken.getPosition() + \" vs \" + backPos;\n\n        if (VERBOSE) {\n          System.out.println(\"    add altToken=\" + altToken);\n        }\n        if (backCount > 0) {\n          backCount++;\n          altToken.setPositionLength(backCount);\n          pending.add(altToken);\n        } else {\n          // This means alt token was all punct tokens:\n          assert discardPunctuation;\n        }\n        altToken = null;\n      }\n\n      final Dictionary dict = getDict(backType);\n\n      if (backType == Type.USER) {\n\n        // Expand the phraseID we recorded into the actual\n        // segmentation:\n        final int[] wordIDAndLength = userDictionary.lookupSegmentation(backID);\n        int wordID = wordIDAndLength[0];\n        int current = 0;\n        for(int j=1; j < wordIDAndLength.length; j++) {\n          final int len = wordIDAndLength[j];\n          //System.out.println(\"    add user: len=\" + len);\n          pending.add(new Token(wordID+j-1,\n                                fragment,\n                                current + offset,\n                                len,\n                                Type.USER,\n                                current + backPos,\n                                dict));\n          if (VERBOSE) {\n            System.out.println(\"    add USER token=\" + pending.get(pending.size()-1));\n          }\n          current += len;\n        }\n\n        // Reverse the tokens we just added, because when we\n        // serve them up from incrementToken we serve in\n        // reverse:\n        Collections.reverse(pending.subList(pending.size() - (wordIDAndLength.length - 1),\n                                            pending.size()));\n\n        backCount += wordIDAndLength.length-1;\n      } else {\n\n        if (extendedMode && backType == Type.UNKNOWN) {\n          // In EXTENDED mode we convert unknown word into\n          // unigrams:\n          int unigramTokenCount = 0;\n          for(int i=length-1;i>=0;i--) {\n            int charLen = 1;\n            if (i > 0 && Character.isLowSurrogate(fragment[offset+i])) {\n              i--;\n              charLen = 2;\n            }\n            //System.out.println(\"    extended tok offset=\"\n            //+ (offset + i));\n            if (!discardPunctuation || !isPunctuation(fragment[offset+i])) {\n              pending.add(new Token(CharacterDefinition.NGRAM,\n                                    fragment,\n                                    offset + i,\n                                    charLen,\n                                    Type.UNKNOWN,\n                                    backPos + i,\n                                    unkDictionary));\n              unigramTokenCount++;\n            }\n          }\n          backCount += unigramTokenCount;\n          \n        } else if (!discardPunctuation || length == 0 || !isPunctuation(fragment[offset])) {\n          pending.add(new Token(backID,\n                                fragment,\n                                offset,\n                                length,\n                                backType,\n                                backPos,\n                                dict));\n          if (VERBOSE) {\n            System.out.println(\"    add token=\" + pending.get(pending.size()-1));\n          }\n          backCount++;\n        } else {\n          if (VERBOSE) {\n            System.out.println(\"    skip punctuation token=\" + new String(fragment, offset, length));\n          }\n        }\n      }\n\n      lastLeftWordID = dict.getLeftId(backID);\n      pos = backPos;\n      bestIDX = nextBestIDX;\n    }\n\n    lastBackTracePos = endPos;\n\n    if (VERBOSE) {\n      System.out.println(\"  freeBefore pos=\" + endPos);\n    }\n    // Notify the circular buffers that we are done with\n    // these positions:\n    buffer.freeBefore(endPos);\n    positions.freeBefore(endPos);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"790bdefa98dd40c18848b1915db6ba17dabe4b61":["0984ad47974c2d5d354519ddb2aa8358973a6271"],"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","0984ad47974c2d5d354519ddb2aa8358973a6271"],"0984ad47974c2d5d354519ddb2aa8358973a6271":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"98d45c1ff2c99694b6de2201175f9b8b8b27b597":["790bdefa98dd40c18848b1915db6ba17dabe4b61"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["98d45c1ff2c99694b6de2201175f9b8b8b27b597"]},"commit2Childs":{"790bdefa98dd40c18848b1915db6ba17dabe4b61":["98d45c1ff2c99694b6de2201175f9b8b8b27b597"],"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab":[],"0984ad47974c2d5d354519ddb2aa8358973a6271":["790bdefa98dd40c18848b1915db6ba17dabe4b61","9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","0984ad47974c2d5d354519ddb2aa8358973a6271"],"98d45c1ff2c99694b6de2201175f9b8b8b27b597":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}