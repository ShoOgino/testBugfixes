{"path":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggesterTest#testStolenBytes().mjava","commits":[{"id":"6ce825e9276493231308229152c48f755ce1a0a5","date":1348871483,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggesterTest#testStolenBytes().mjava","pathOld":"/dev/null","sourceNew":"  public void testStolenBytes() throws Exception {\n    \n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.SIMPLE, true);\n        \n        // TokenStream stream = new SynonymFilter(tokenizer, map, true);\n        // return new TokenStreamComponents(tokenizer, new RemoveDuplicatesTokenFilter(stream));\n        return new TokenStreamComponents(tokenizer) {\n          int tokenStreamCounter = 0;\n          final TokenStream[] tokenStreams = new TokenStream[] {\n            new CannedBinaryTokenStream(new BinaryToken[] {\n                token(new BytesRef(new byte[] {0x61, (byte) 0xff, 0x61})),\n              }),\n            new CannedTokenStream(new Token[] {\n                token(\"a\",1,1),          \n                token(\"a\",1,1)\n              }),\n            new CannedTokenStream(new Token[] {\n                token(\"a\",1,1),\n                token(\"a\",1,1)\n              }),\n            new CannedBinaryTokenStream(new BinaryToken[] {\n                token(new BytesRef(new byte[] {0x61, (byte) 0xff, 0x61})),\n              })\n          };\n\n          @Override\n          public TokenStream getTokenStream() {\n            TokenStream result = tokenStreams[tokenStreamCounter];\n            tokenStreamCounter++;\n            return result;\n          }\n         \n          @Override\n          protected void setReader(final Reader reader) throws IOException {\n          }\n        };\n      }\n    };\n\n    TermFreq keys[] = new TermFreq[] {\n      new TermFreq(\"a a\", 50),\n      new TermFreq(\"a b\", 50),\n    };\n\n    AnalyzingSuggester suggester = new AnalyzingSuggester(analyzer);\n    suggester.build(new TermFreqArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(\"a a\", false, 5);\n    assertEquals(1, results.size());\n    assertEquals(\"a b\", results.get(0).key);\n    assertEquals(50, results.get(0).value);\n\n    results = suggester.lookup(\"a a\", false, 5);\n    assertEquals(1, results.size());\n    assertEquals(\"a a\", results.get(0).key);\n    assertEquals(50, results.get(0).value);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f903593885012bb5d79809c44cb6dfcebb4a2c66","date":1349971647,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggesterTest#testStolenBytes().mjava","pathOld":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggesterTest#testStolenBytes().mjava","sourceNew":"  public void testStolenBytes() throws Exception {\n\n    // First time w/ preserveSep, second time without:\n    for(int i=0;i<2;i++) {\n      \n      final Analyzer analyzer = new Analyzer() {\n          @Override\n          protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n            Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.SIMPLE, true);\n        \n            // TokenStream stream = new SynonymFilter(tokenizer, map, true);\n            // return new TokenStreamComponents(tokenizer, new RemoveDuplicatesTokenFilter(stream));\n            return new TokenStreamComponents(tokenizer) {\n              int tokenStreamCounter = 0;\n              final TokenStream[] tokenStreams = new TokenStream[] {\n                new CannedBinaryTokenStream(new BinaryToken[] {\n                    token(new BytesRef(new byte[] {0x61, (byte) 0xff, 0x61})),\n                  }),\n                new CannedTokenStream(new Token[] {\n                    token(\"a\",1,1),          \n                    token(\"a\",1,1)\n                  }),\n                new CannedTokenStream(new Token[] {\n                    token(\"a\",1,1),\n                    token(\"a\",1,1)\n                  }),\n                new CannedBinaryTokenStream(new BinaryToken[] {\n                    token(new BytesRef(new byte[] {0x61, (byte) 0xff, 0x61})),\n                  })\n              };\n\n              @Override\n              public TokenStream getTokenStream() {\n                TokenStream result = tokenStreams[tokenStreamCounter];\n                tokenStreamCounter++;\n                return result;\n              }\n         \n              @Override\n              protected void setReader(final Reader reader) throws IOException {\n              }\n            };\n          }\n        };\n\n      TermFreq keys[] = new TermFreq[] {\n        new TermFreq(\"a a\", 50),\n        new TermFreq(\"a b\", 50),\n      };\n\n      AnalyzingSuggester suggester = new AnalyzingSuggester(analyzer, analyzer, AnalyzingSuggester.EXACT_FIRST | (i==0 ? AnalyzingSuggester.PRESERVE_SEP : 0), 256, -1);\n      suggester.build(new TermFreqArrayIterator(keys));\n      List<LookupResult> results = suggester.lookup(\"a a\", false, 5);\n      assertEquals(1, results.size());\n      assertEquals(\"a b\", results.get(0).key);\n      assertEquals(50, results.get(0).value);\n\n      results = suggester.lookup(\"a a\", false, 5);\n      assertEquals(1, results.size());\n      assertEquals(\"a a\", results.get(0).key);\n      assertEquals(50, results.get(0).value);\n    }\n  }\n\n","sourceOld":"  public void testStolenBytes() throws Exception {\n    \n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.SIMPLE, true);\n        \n        // TokenStream stream = new SynonymFilter(tokenizer, map, true);\n        // return new TokenStreamComponents(tokenizer, new RemoveDuplicatesTokenFilter(stream));\n        return new TokenStreamComponents(tokenizer) {\n          int tokenStreamCounter = 0;\n          final TokenStream[] tokenStreams = new TokenStream[] {\n            new CannedBinaryTokenStream(new BinaryToken[] {\n                token(new BytesRef(new byte[] {0x61, (byte) 0xff, 0x61})),\n              }),\n            new CannedTokenStream(new Token[] {\n                token(\"a\",1,1),          \n                token(\"a\",1,1)\n              }),\n            new CannedTokenStream(new Token[] {\n                token(\"a\",1,1),\n                token(\"a\",1,1)\n              }),\n            new CannedBinaryTokenStream(new BinaryToken[] {\n                token(new BytesRef(new byte[] {0x61, (byte) 0xff, 0x61})),\n              })\n          };\n\n          @Override\n          public TokenStream getTokenStream() {\n            TokenStream result = tokenStreams[tokenStreamCounter];\n            tokenStreamCounter++;\n            return result;\n          }\n         \n          @Override\n          protected void setReader(final Reader reader) throws IOException {\n          }\n        };\n      }\n    };\n\n    TermFreq keys[] = new TermFreq[] {\n      new TermFreq(\"a a\", 50),\n      new TermFreq(\"a b\", 50),\n    };\n\n    AnalyzingSuggester suggester = new AnalyzingSuggester(analyzer);\n    suggester.build(new TermFreqArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(\"a a\", false, 5);\n    assertEquals(1, results.size());\n    assertEquals(\"a b\", results.get(0).key);\n    assertEquals(50, results.get(0).value);\n\n    results = suggester.lookup(\"a a\", false, 5);\n    assertEquals(1, results.size());\n    assertEquals(\"a a\", results.get(0).key);\n    assertEquals(50, results.get(0).value);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2efd1b8e67185b5bb2dbdfa435b9f085a0c659f6","date":1374158194,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggesterTest#testStolenBytes().mjava","sourceNew":null,"sourceOld":"  public void testStolenBytes() throws Exception {\n\n    // First time w/ preserveSep, second time without:\n    for(int i=0;i<2;i++) {\n      \n      final Analyzer analyzer = new Analyzer() {\n          @Override\n          protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n            Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.SIMPLE, true);\n        \n            // TokenStream stream = new SynonymFilter(tokenizer, map, true);\n            // return new TokenStreamComponents(tokenizer, new RemoveDuplicatesTokenFilter(stream));\n            return new TokenStreamComponents(tokenizer) {\n              int tokenStreamCounter = 0;\n              final TokenStream[] tokenStreams = new TokenStream[] {\n                new CannedBinaryTokenStream(new BinaryToken[] {\n                    token(new BytesRef(new byte[] {0x61, (byte) 0xff, 0x61})),\n                  }),\n                new CannedTokenStream(new Token[] {\n                    token(\"a\",1,1),          \n                    token(\"a\",1,1)\n                  }),\n                new CannedTokenStream(new Token[] {\n                    token(\"a\",1,1),\n                    token(\"a\",1,1)\n                  }),\n                new CannedBinaryTokenStream(new BinaryToken[] {\n                    token(new BytesRef(new byte[] {0x61, (byte) 0xff, 0x61})),\n                  })\n              };\n\n              @Override\n              public TokenStream getTokenStream() {\n                TokenStream result = tokenStreams[tokenStreamCounter];\n                tokenStreamCounter++;\n                return result;\n              }\n         \n              @Override\n              protected void setReader(final Reader reader) throws IOException {\n              }\n            };\n          }\n        };\n\n      TermFreq keys[] = new TermFreq[] {\n        new TermFreq(\"a a\", 50),\n        new TermFreq(\"a b\", 50),\n      };\n\n      AnalyzingSuggester suggester = new AnalyzingSuggester(analyzer, analyzer, AnalyzingSuggester.EXACT_FIRST | (i==0 ? AnalyzingSuggester.PRESERVE_SEP : 0), 256, -1);\n      suggester.build(new TermFreqArrayIterator(keys));\n      List<LookupResult> results = suggester.lookup(\"a a\", false, 5);\n      assertEquals(1, results.size());\n      assertEquals(\"a b\", results.get(0).key);\n      assertEquals(50, results.get(0).value);\n\n      results = suggester.lookup(\"a a\", false, 5);\n      assertEquals(1, results.size());\n      assertEquals(\"a a\", results.get(0).key);\n      assertEquals(50, results.get(0).value);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","date":1376366778,"type":4,"author":"Han Jiang","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggesterTest#testStolenBytes().mjava","sourceNew":null,"sourceOld":"  public void testStolenBytes() throws Exception {\n\n    // First time w/ preserveSep, second time without:\n    for(int i=0;i<2;i++) {\n      \n      final Analyzer analyzer = new Analyzer() {\n          @Override\n          protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n            Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.SIMPLE, true);\n        \n            // TokenStream stream = new SynonymFilter(tokenizer, map, true);\n            // return new TokenStreamComponents(tokenizer, new RemoveDuplicatesTokenFilter(stream));\n            return new TokenStreamComponents(tokenizer) {\n              int tokenStreamCounter = 0;\n              final TokenStream[] tokenStreams = new TokenStream[] {\n                new CannedBinaryTokenStream(new BinaryToken[] {\n                    token(new BytesRef(new byte[] {0x61, (byte) 0xff, 0x61})),\n                  }),\n                new CannedTokenStream(new Token[] {\n                    token(\"a\",1,1),          \n                    token(\"a\",1,1)\n                  }),\n                new CannedTokenStream(new Token[] {\n                    token(\"a\",1,1),\n                    token(\"a\",1,1)\n                  }),\n                new CannedBinaryTokenStream(new BinaryToken[] {\n                    token(new BytesRef(new byte[] {0x61, (byte) 0xff, 0x61})),\n                  })\n              };\n\n              @Override\n              public TokenStream getTokenStream() {\n                TokenStream result = tokenStreams[tokenStreamCounter];\n                tokenStreamCounter++;\n                return result;\n              }\n         \n              @Override\n              protected void setReader(final Reader reader) throws IOException {\n              }\n            };\n          }\n        };\n\n      TermFreq keys[] = new TermFreq[] {\n        new TermFreq(\"a a\", 50),\n        new TermFreq(\"a b\", 50),\n      };\n\n      AnalyzingSuggester suggester = new AnalyzingSuggester(analyzer, analyzer, AnalyzingSuggester.EXACT_FIRST | (i==0 ? AnalyzingSuggester.PRESERVE_SEP : 0), 256, -1);\n      suggester.build(new TermFreqArrayIterator(keys));\n      List<LookupResult> results = suggester.lookup(\"a a\", false, 5);\n      assertEquals(1, results.size());\n      assertEquals(\"a b\", results.get(0).key);\n      assertEquals(50, results.get(0).value);\n\n      results = suggester.lookup(\"a a\", false, 5);\n      assertEquals(1, results.size());\n      assertEquals(\"a a\", results.get(0).key);\n      assertEquals(50, results.get(0).value);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"2efd1b8e67185b5bb2dbdfa435b9f085a0c659f6":["f903593885012bb5d79809c44cb6dfcebb4a2c66"],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":["f903593885012bb5d79809c44cb6dfcebb4a2c66"],"6ce825e9276493231308229152c48f755ce1a0a5":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"f903593885012bb5d79809c44cb6dfcebb4a2c66":["6ce825e9276493231308229152c48f755ce1a0a5"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["2efd1b8e67185b5bb2dbdfa435b9f085a0c659f6"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["6ce825e9276493231308229152c48f755ce1a0a5"],"2efd1b8e67185b5bb2dbdfa435b9f085a0c659f6":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":[],"6ce825e9276493231308229152c48f755ce1a0a5":["f903593885012bb5d79809c44cb6dfcebb4a2c66"],"f903593885012bb5d79809c44cb6dfcebb4a2c66":["2efd1b8e67185b5bb2dbdfa435b9f085a0c659f6","8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}