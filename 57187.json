{"path":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","commits":[{"id":"91109046a59c58ee0ee5d0d2767b08d1f30d6702","date":1000830588,"type":0,"author":"Jason van Zyl","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"/dev/null","sourceNew":"  private final void mergeFields() throws IOException {\n    fieldInfos = new FieldInfos();\t\t  // merge field names\n    for (int i = 0; i < readers.size(); i++) {\n      SegmentReader reader = (SegmentReader)readers.elementAt(i);\n      fieldInfos.add(reader.fieldInfos);\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n    \n    FieldsWriter fieldsWriter =\t\t\t  // merge field values\n      new FieldsWriter(directory, segment, fieldInfos);\n    try {\n      for (int i = 0; i < readers.size(); i++) {\n\tSegmentReader reader = (SegmentReader)readers.elementAt(i);\n\tBitVector deletedDocs = reader.deletedDocs;\n\tint maxDoc = reader.maxDoc();\n\tfor (int j = 0; j < maxDoc; j++)\n\t  if (deletedDocs == null || !deletedDocs.get(j)) // skip deleted docs\n\t    fieldsWriter.addDocument(reader.document(j));\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8fb95844e4ba5160067c64c5eb1cd8a09f7a94f1","date":1064527311,"type":3,"author":"Dmitry Serebrennikov","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  private final void mergeFields() throws IOException {\n    fieldInfos = new FieldInfos();\t\t  // merge field names\n    for (int i = 0; i < readers.size(); i++) {\n      SegmentReader reader = (SegmentReader)readers.elementAt(i);\n      fieldInfos.add(reader.fieldInfos);\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n    \n    FieldsWriter fieldsWriter =\t\t\t  // merge field values\n      new FieldsWriter(directory, segment, fieldInfos);\n    try {\n      for (int i = 0; i < readers.size(); i++) {\n        SegmentReader reader = (SegmentReader)readers.elementAt(i);\n        BitVector deletedDocs = reader.deletedDocs;\n        int maxDoc = reader.maxDoc();\n        for (int j = 0; j < maxDoc; j++)\n          if (deletedDocs == null || !deletedDocs.get(j)) // skip deleted docs\n            fieldsWriter.addDocument(reader.document(j));\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n  }\n\n","sourceOld":"  private final void mergeFields() throws IOException {\n    fieldInfos = new FieldInfos();\t\t  // merge field names\n    for (int i = 0; i < readers.size(); i++) {\n      SegmentReader reader = (SegmentReader)readers.elementAt(i);\n      fieldInfos.add(reader.fieldInfos);\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n    \n    FieldsWriter fieldsWriter =\t\t\t  // merge field values\n      new FieldsWriter(directory, segment, fieldInfos);\n    try {\n      for (int i = 0; i < readers.size(); i++) {\n\tSegmentReader reader = (SegmentReader)readers.elementAt(i);\n\tBitVector deletedDocs = reader.deletedDocs;\n\tint maxDoc = reader.maxDoc();\n\tfor (int j = 0; j < maxDoc; j++)\n\t  if (deletedDocs == null || !deletedDocs.get(j)) // skip deleted docs\n\t    fieldsWriter.addDocument(reader.document(j));\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7fb6d70db034a5456ae560175dd1b821eea9ff4","date":1066759157,"type":3,"author":"Doug Cutting","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  private final void mergeFields() throws IOException {\n    fieldInfos = new FieldInfos();\t\t  // merge field names\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader)readers.elementAt(i);\n      fieldInfos.add(reader.getFieldNames(true), true);\n      fieldInfos.add(reader.getFieldNames(false), false);\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n    \n    FieldsWriter fieldsWriter =\t\t\t  // merge field values\n      new FieldsWriter(directory, segment, fieldInfos);\n    try {\n      for (int i = 0; i < readers.size(); i++) {\n\tIndexReader reader = (IndexReader)readers.elementAt(i);\n\tint maxDoc = reader.maxDoc();\n\tfor (int j = 0; j < maxDoc; j++)\n\t  if (!reader.isDeleted(j))               // skip deleted docs\n\t    fieldsWriter.addDocument(reader.document(j));\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n  }\n\n","sourceOld":"  private final void mergeFields() throws IOException {\n    fieldInfos = new FieldInfos();\t\t  // merge field names\n    for (int i = 0; i < readers.size(); i++) {\n      SegmentReader reader = (SegmentReader)readers.elementAt(i);\n      fieldInfos.add(reader.fieldInfos);\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n    \n    FieldsWriter fieldsWriter =\t\t\t  // merge field values\n      new FieldsWriter(directory, segment, fieldInfos);\n    try {\n      for (int i = 0; i < readers.size(); i++) {\n        SegmentReader reader = (SegmentReader)readers.elementAt(i);\n        BitVector deletedDocs = reader.deletedDocs;\n        int maxDoc = reader.maxDoc();\n        for (int j = 0; j < maxDoc; j++)\n          if (deletedDocs == null || !deletedDocs.get(j)) // skip deleted docs\n            fieldsWriter.addDocument(reader.document(j));\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f22ffa72cdb7463ca3e818b15f8fb8b0330e7cf5","date":1067592524,"type":3,"author":"Christoph Goller","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  private final int mergeFields() throws IOException {\n    fieldInfos = new FieldInfos();\t\t  // merge field names\n    int docCount = 0;\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader)readers.elementAt(i);\n      fieldInfos.add(reader.getFieldNames(true), true);\n      fieldInfos.add(reader.getFieldNames(false), false);\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n    \n    FieldsWriter fieldsWriter =\t\t\t  // merge field values\n      new FieldsWriter(directory, segment, fieldInfos);\n    try {\n      for (int i = 0; i < readers.size(); i++) {\n\tIndexReader reader = (IndexReader)readers.elementAt(i);\n\tint maxDoc = reader.maxDoc();\n\tfor (int j = 0; j < maxDoc; j++)\n\t  if (!reader.isDeleted(j)){               // skip deleted docs\n            fieldsWriter.addDocument(reader.document(j));\n            docCount++;\n\t  }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n    return docCount;\n  }\n\n","sourceOld":"  private final void mergeFields() throws IOException {\n    fieldInfos = new FieldInfos();\t\t  // merge field names\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader)readers.elementAt(i);\n      fieldInfos.add(reader.getFieldNames(true), true);\n      fieldInfos.add(reader.getFieldNames(false), false);\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n    \n    FieldsWriter fieldsWriter =\t\t\t  // merge field values\n      new FieldsWriter(directory, segment, fieldInfos);\n    try {\n      for (int i = 0; i < readers.size(); i++) {\n\tIndexReader reader = (IndexReader)readers.elementAt(i);\n\tint maxDoc = reader.maxDoc();\n\tfor (int j = 0; j < maxDoc; j++)\n\t  if (!reader.isDeleted(j))               // skip deleted docs\n\t    fieldsWriter.addDocument(reader.document(j));\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["1b54a9bc667895a2095a886184bf69a3179e63df"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"770281b8a8459cafcdd2354b6a06078fea2d83c9","date":1077308096,"type":3,"author":"Doug Cutting","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws IOException\n   */\n  private final int mergeFields() throws IOException {\n    fieldInfos = new FieldInfos();\t\t  // merge field names\n    int docCount = 0;\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.elementAt(i);\n      fieldInfos.addIndexed(reader.getIndexedFieldNames(true), true);\n      fieldInfos.addIndexed(reader.getIndexedFieldNames(false), false);\n      fieldInfos.add(reader.getFieldNames(false), false);\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    FieldsWriter fieldsWriter = // merge field values\n            new FieldsWriter(directory, segment, fieldInfos);\n    try {\n      for (int i = 0; i < readers.size(); i++) {\n        IndexReader reader = (IndexReader) readers.elementAt(i);\n        int maxDoc = reader.maxDoc();\n        for (int j = 0; j < maxDoc; j++)\n          if (!reader.isDeleted(j)) {               // skip deleted docs\n            fieldsWriter.addDocument(reader.document(j));\n            docCount++;\n          }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n    return docCount;\n  }\n\n","sourceOld":"  private final int mergeFields() throws IOException {\n    fieldInfos = new FieldInfos();\t\t  // merge field names\n    int docCount = 0;\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader)readers.elementAt(i);\n      fieldInfos.add(reader.getFieldNames(true), true);\n      fieldInfos.add(reader.getFieldNames(false), false);\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n    \n    FieldsWriter fieldsWriter =\t\t\t  // merge field values\n      new FieldsWriter(directory, segment, fieldInfos);\n    try {\n      for (int i = 0; i < readers.size(); i++) {\n\tIndexReader reader = (IndexReader)readers.elementAt(i);\n\tint maxDoc = reader.maxDoc();\n\tfor (int j = 0; j < maxDoc; j++)\n\t  if (!reader.isDeleted(j)){               // skip deleted docs\n            fieldsWriter.addDocument(reader.document(j));\n            docCount++;\n\t  }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":["1b54a9bc667895a2095a886184bf69a3179e63df"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6177f0f28ace66d1538b1e6ac5f1773e5449a0b0","date":1096997448,"type":3,"author":"Christoph Goller","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws IOException\n   */\n  private final int mergeFields() throws IOException {\n    fieldInfos = new FieldInfos();\t\t  // merge field names\n    int docCount = 0;\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.elementAt(i);\n      fieldInfos.addIndexed(reader.getIndexedFieldNames(Field.TermVector.WITH_POSITIONS_OFFSETS), true, true, true);\n      fieldInfos.addIndexed(reader.getIndexedFieldNames(Field.TermVector.WITH_POSITIONS), true, true, false);\n      fieldInfos.addIndexed(reader.getIndexedFieldNames(Field.TermVector.WITH_OFFSETS), true, false, true);\n      fieldInfos.addIndexed(reader.getIndexedFieldNames(Field.TermVector.YES), true, false, false);\n      fieldInfos.addIndexed(reader.getIndexedFieldNames(Field.TermVector.NO), false, false, false);\n      fieldInfos.add(reader.getFieldNames(false), false);\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    FieldsWriter fieldsWriter = // merge field values\n            new FieldsWriter(directory, segment, fieldInfos);\n    try {\n      for (int i = 0; i < readers.size(); i++) {\n        IndexReader reader = (IndexReader) readers.elementAt(i);\n        int maxDoc = reader.maxDoc();\n        for (int j = 0; j < maxDoc; j++)\n          if (!reader.isDeleted(j)) {               // skip deleted docs\n            fieldsWriter.addDocument(reader.document(j));\n            docCount++;\n          }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws IOException\n   */\n  private final int mergeFields() throws IOException {\n    fieldInfos = new FieldInfos();\t\t  // merge field names\n    int docCount = 0;\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.elementAt(i);\n      fieldInfos.addIndexed(reader.getIndexedFieldNames(true), true);\n      fieldInfos.addIndexed(reader.getIndexedFieldNames(false), false);\n      fieldInfos.add(reader.getFieldNames(false), false);\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    FieldsWriter fieldsWriter = // merge field values\n            new FieldsWriter(directory, segment, fieldInfos);\n    try {\n      for (int i = 0; i < readers.size(); i++) {\n        IndexReader reader = (IndexReader) readers.elementAt(i);\n        int maxDoc = reader.maxDoc();\n        for (int j = 0; j < maxDoc; j++)\n          if (!reader.isDeleted(j)) {               // skip deleted docs\n            fieldsWriter.addDocument(reader.document(j));\n            docCount++;\n          }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6c9ae72f0c7aa48fa47b0c33986b516a2e71d578","date":1104356267,"type":3,"author":"Bernhard Messer","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws IOException\n   */\n  private final int mergeFields() throws IOException {\n    fieldInfos = new FieldInfos();\t\t  // merge field names\n    int docCount = 0;\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.elementAt(i);\n      fieldInfos.addIndexed(reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true);\n      fieldInfos.addIndexed(reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false);\n      fieldInfos.addIndexed(reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true);\n      fieldInfos.addIndexed(reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false);\n      fieldInfos.addIndexed(reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false);\n      fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    FieldsWriter fieldsWriter = // merge field values\n            new FieldsWriter(directory, segment, fieldInfos);\n    try {\n      for (int i = 0; i < readers.size(); i++) {\n        IndexReader reader = (IndexReader) readers.elementAt(i);\n        int maxDoc = reader.maxDoc();\n        for (int j = 0; j < maxDoc; j++)\n          if (!reader.isDeleted(j)) {               // skip deleted docs\n            fieldsWriter.addDocument(reader.document(j));\n            docCount++;\n          }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws IOException\n   */\n  private final int mergeFields() throws IOException {\n    fieldInfos = new FieldInfos();\t\t  // merge field names\n    int docCount = 0;\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.elementAt(i);\n      fieldInfos.addIndexed(reader.getIndexedFieldNames(Field.TermVector.WITH_POSITIONS_OFFSETS), true, true, true);\n      fieldInfos.addIndexed(reader.getIndexedFieldNames(Field.TermVector.WITH_POSITIONS), true, true, false);\n      fieldInfos.addIndexed(reader.getIndexedFieldNames(Field.TermVector.WITH_OFFSETS), true, false, true);\n      fieldInfos.addIndexed(reader.getIndexedFieldNames(Field.TermVector.YES), true, false, false);\n      fieldInfos.addIndexed(reader.getIndexedFieldNames(Field.TermVector.NO), false, false, false);\n      fieldInfos.add(reader.getFieldNames(false), false);\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    FieldsWriter fieldsWriter = // merge field values\n            new FieldsWriter(directory, segment, fieldInfos);\n    try {\n      for (int i = 0; i < readers.size(); i++) {\n        IndexReader reader = (IndexReader) readers.elementAt(i);\n        int maxDoc = reader.maxDoc();\n        for (int j = 0; j < maxDoc; j++)\n          if (!reader.isDeleted(j)) {               // skip deleted docs\n            fieldsWriter.addDocument(reader.document(j));\n            docCount++;\n          }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"950f3c7592cb559e2534e5089c78833250e156a3","date":1130557968,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws IOException\n   */\n  private final int mergeFields() throws IOException {\n    fieldInfos = new FieldInfos();\t\t  // merge field names\n    int docCount = 0;\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.elementAt(i);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false);\n      fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    FieldsWriter fieldsWriter = // merge field values\n            new FieldsWriter(directory, segment, fieldInfos);\n    try {\n      for (int i = 0; i < readers.size(); i++) {\n        IndexReader reader = (IndexReader) readers.elementAt(i);\n        int maxDoc = reader.maxDoc();\n        for (int j = 0; j < maxDoc; j++)\n          if (!reader.isDeleted(j)) {               // skip deleted docs\n            fieldsWriter.addDocument(reader.document(j));\n            docCount++;\n          }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws IOException\n   */\n  private final int mergeFields() throws IOException {\n    fieldInfos = new FieldInfos();\t\t  // merge field names\n    int docCount = 0;\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.elementAt(i);\n      fieldInfos.addIndexed(reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true);\n      fieldInfos.addIndexed(reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false);\n      fieldInfos.addIndexed(reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true);\n      fieldInfos.addIndexed(reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false);\n      fieldInfos.addIndexed(reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false);\n      fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    FieldsWriter fieldsWriter = // merge field values\n            new FieldsWriter(directory, segment, fieldInfos);\n    try {\n      for (int i = 0; i < readers.size(); i++) {\n        IndexReader reader = (IndexReader) readers.elementAt(i);\n        int maxDoc = reader.maxDoc();\n        for (int j = 0; j < maxDoc; j++)\n          if (!reader.isDeleted(j)) {               // skip deleted docs\n            fieldsWriter.addDocument(reader.document(j));\n            docCount++;\n          }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0c0427a35a68b16ec73d3bf5f27e57719915f166","date":1155449527,"type":3,"author":"Otis Gospodnetic","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws IOException\n   */\n  private final int mergeFields() throws IOException {\n    fieldInfos = new FieldInfos();\t\t  // merge field names\n    int docCount = 0;\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.elementAt(i);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false);\n      fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    FieldsWriter fieldsWriter = // merge field values\n            new FieldsWriter(directory, segment, fieldInfos);\n    \n    // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n    // in  merge mode, we use this FieldSelector\n    FieldSelector fieldSelectorMerge = new FieldSelector() {\n      public FieldSelectorResult accept(String fieldName) {\n        return FieldSelectorResult.LOAD_FOR_MERGE;\n      }        \n    };\n    \n    try {\n      for (int i = 0; i < readers.size(); i++) {\n        IndexReader reader = (IndexReader) readers.elementAt(i);\n        int maxDoc = reader.maxDoc();\n        for (int j = 0; j < maxDoc; j++)\n          if (!reader.isDeleted(j)) {               // skip deleted docs\n            fieldsWriter.addDocument(reader.document(j, fieldSelectorMerge));\n            docCount++;\n          }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws IOException\n   */\n  private final int mergeFields() throws IOException {\n    fieldInfos = new FieldInfos();\t\t  // merge field names\n    int docCount = 0;\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.elementAt(i);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false);\n      fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    FieldsWriter fieldsWriter = // merge field values\n            new FieldsWriter(directory, segment, fieldInfos);\n    try {\n      for (int i = 0; i < readers.size(); i++) {\n        IndexReader reader = (IndexReader) readers.elementAt(i);\n        int maxDoc = reader.maxDoc();\n        for (int j = 0; j < maxDoc; j++)\n          if (!reader.isDeleted(j)) {               // skip deleted docs\n            fieldsWriter.addDocument(reader.document(j));\n            docCount++;\n          }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1b54a9bc667895a2095a886184bf69a3179e63df","date":1172088096,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n    fieldInfos = new FieldInfos();\t\t  // merge field names\n    int docCount = 0;\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.elementAt(i);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false);\n      fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    FieldsWriter fieldsWriter = // merge field values\n            new FieldsWriter(directory, segment, fieldInfos);\n    \n    // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n    // in  merge mode, we use this FieldSelector\n    FieldSelector fieldSelectorMerge = new FieldSelector() {\n      public FieldSelectorResult accept(String fieldName) {\n        return FieldSelectorResult.LOAD_FOR_MERGE;\n      }        \n    };\n    \n    try {\n      for (int i = 0; i < readers.size(); i++) {\n        IndexReader reader = (IndexReader) readers.elementAt(i);\n        int maxDoc = reader.maxDoc();\n        for (int j = 0; j < maxDoc; j++)\n          if (!reader.isDeleted(j)) {               // skip deleted docs\n            fieldsWriter.addDocument(reader.document(j, fieldSelectorMerge));\n            docCount++;\n          }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws IOException\n   */\n  private final int mergeFields() throws IOException {\n    fieldInfos = new FieldInfos();\t\t  // merge field names\n    int docCount = 0;\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.elementAt(i);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false);\n      fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    FieldsWriter fieldsWriter = // merge field values\n            new FieldsWriter(directory, segment, fieldInfos);\n    \n    // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n    // in  merge mode, we use this FieldSelector\n    FieldSelector fieldSelectorMerge = new FieldSelector() {\n      public FieldSelectorResult accept(String fieldName) {\n        return FieldSelectorResult.LOAD_FOR_MERGE;\n      }        \n    };\n    \n    try {\n      for (int i = 0; i < readers.size(); i++) {\n        IndexReader reader = (IndexReader) readers.elementAt(i);\n        int maxDoc = reader.maxDoc();\n        for (int j = 0; j < maxDoc; j++)\n          if (!reader.isDeleted(j)) {               // skip deleted docs\n            fieldsWriter.addDocument(reader.document(j, fieldSelectorMerge));\n            docCount++;\n          }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n    return docCount;\n  }\n\n","bugFix":["770281b8a8459cafcdd2354b6a06078fea2d83c9","f22ffa72cdb7463ca3e818b15f8fb8b0330e7cf5"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8522ae207a56c6db28ca06fe6cc33e70911c3600","date":1173935743,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n    fieldInfos = new FieldInfos();\t\t  // merge field names\n    int docCount = 0;\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.elementAt(i);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false, false);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.STORES_PAYLOADS), false, false, false, true);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false, false);\n      fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    FieldsWriter fieldsWriter = // merge field values\n            new FieldsWriter(directory, segment, fieldInfos);\n    \n    // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n    // in  merge mode, we use this FieldSelector\n    FieldSelector fieldSelectorMerge = new FieldSelector() {\n      public FieldSelectorResult accept(String fieldName) {\n        return FieldSelectorResult.LOAD_FOR_MERGE;\n      }        \n    };\n    \n    try {\n      for (int i = 0; i < readers.size(); i++) {\n        IndexReader reader = (IndexReader) readers.elementAt(i);\n        int maxDoc = reader.maxDoc();\n        for (int j = 0; j < maxDoc; j++)\n          if (!reader.isDeleted(j)) {               // skip deleted docs\n            fieldsWriter.addDocument(reader.document(j, fieldSelectorMerge));\n            docCount++;\n          }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n    fieldInfos = new FieldInfos();\t\t  // merge field names\n    int docCount = 0;\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.elementAt(i);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false);\n      fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    FieldsWriter fieldsWriter = // merge field values\n            new FieldsWriter(directory, segment, fieldInfos);\n    \n    // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n    // in  merge mode, we use this FieldSelector\n    FieldSelector fieldSelectorMerge = new FieldSelector() {\n      public FieldSelectorResult accept(String fieldName) {\n        return FieldSelectorResult.LOAD_FOR_MERGE;\n      }        \n    };\n    \n    try {\n      for (int i = 0; i < readers.size(); i++) {\n        IndexReader reader = (IndexReader) readers.elementAt(i);\n        int maxDoc = reader.maxDoc();\n        for (int j = 0; j < maxDoc; j++)\n          if (!reader.isDeleted(j)) {               // skip deleted docs\n            fieldsWriter.addDocument(reader.document(j, fieldSelectorMerge));\n            docCount++;\n          }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4350b17bd363cd13a95171b8df1ca62ea4c3e71c","date":1183562198,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, that means\n      // all segments were written as part of a single\n      // autoCommit=false IndexWriter session, so their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.elementAt(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    int docCount = 0;\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.elementAt(i);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false, false);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.STORES_PAYLOADS), false, false, false, true);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false, false);\n      fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    if (mergeDocStores) {\n\n      FieldsWriter fieldsWriter = // merge field values\n        new FieldsWriter(directory, segment, fieldInfos);\n    \n      // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n      // in  merge mode, we use this FieldSelector\n      FieldSelector fieldSelectorMerge = new FieldSelector() {\n          public FieldSelectorResult accept(String fieldName) {\n            return FieldSelectorResult.LOAD_FOR_MERGE;\n          }        \n        };\n\n      try {\n        for (int i = 0; i < readers.size(); i++) {\n          IndexReader reader = (IndexReader) readers.elementAt(i);\n          int maxDoc = reader.maxDoc();\n          for (int j = 0; j < maxDoc; j++)\n            if (!reader.isDeleted(j)) {               // skip deleted docs\n              fieldsWriter.addDocument(reader.document(j, fieldSelectorMerge));\n              docCount++;\n            }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (int i = 0; i < readers.size(); i++)\n        docCount += ((IndexReader) readers.elementAt(i)).numDocs();\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n    fieldInfos = new FieldInfos();\t\t  // merge field names\n    int docCount = 0;\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.elementAt(i);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false, false);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.STORES_PAYLOADS), false, false, false, true);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false, false);\n      fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    FieldsWriter fieldsWriter = // merge field values\n            new FieldsWriter(directory, segment, fieldInfos);\n    \n    // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n    // in  merge mode, we use this FieldSelector\n    FieldSelector fieldSelectorMerge = new FieldSelector() {\n      public FieldSelectorResult accept(String fieldName) {\n        return FieldSelectorResult.LOAD_FOR_MERGE;\n      }        \n    };\n    \n    try {\n      for (int i = 0; i < readers.size(); i++) {\n        IndexReader reader = (IndexReader) readers.elementAt(i);\n        int maxDoc = reader.maxDoc();\n        for (int j = 0; j < maxDoc; j++)\n          if (!reader.isDeleted(j)) {               // skip deleted docs\n            fieldsWriter.addDocument(reader.document(j, fieldSelectorMerge));\n            docCount++;\n          }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3b248d9b80f6b2fceade80b3c8683d1cca6e4c98","date":1194520024,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, that means\n      // all segments were written as part of a single\n      // autoCommit=false IndexWriter session, so their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.elementAt(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.elementAt(i);\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        for (int j = 0; j < segmentReader.getFieldInfos().size(); j++) {\n          FieldInfo fi = segmentReader.getFieldInfos().fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector, fi.storePositionWithTermVector, fi.storeOffsetWithTermVector, !reader.hasNorms(fi.name));\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.STORES_PAYLOADS), false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    if (mergeDocStores) {\n\n      // If the i'th reader is a SegmentReader and has\n      // identical fieldName -> number mapping, then this\n      // array will be non-null at position i:\n      SegmentReader[] matchingSegmentReaders = new SegmentReader[readers.size()];\n\n      for (int i = 0; i < readers.size(); i++) {\n        IndexReader reader = (IndexReader) readers.elementAt(i);\n        boolean same = reader.getFieldNames(IndexReader.FieldOption.ALL).size() == fieldInfos.size() && reader instanceof SegmentReader;\n        if (same) {\n          SegmentReader segmentReader = (SegmentReader) reader;\n          for (int j = 0; same && j < fieldInfos.size(); j++)\n            same = fieldInfos.fieldName(j).equals(segmentReader.getFieldInfos().fieldName(j));\n          if (same)\n            matchingSegmentReaders[i] = segmentReader;\n        }\n      }\n\t\n      // Used for bulk-reading raw bytes for stored fields\n      final int[] rawDocLengths = new int[MAX_RAW_MERGE_DOCS];\n\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n      // in  merge mode, we use this FieldSelector\n      FieldSelector fieldSelectorMerge = new FieldSelector() {\n          public FieldSelectorResult accept(String fieldName) {\n            return FieldSelectorResult.LOAD_FOR_MERGE;\n          }        \n        };\n\n      try {\n        for (int i = 0; i < readers.size(); i++) {\n          final IndexReader reader = (IndexReader) readers.elementAt(i);\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[i];\n          final FieldsReader matchingFieldsReader;\n          if (matchingSegmentReader != null)\n            matchingFieldsReader = matchingSegmentReader.getFieldsReader();\n          else\n            matchingFieldsReader = null;\n          final int maxDoc = reader.maxDoc();\n          for (int j = 0; j < maxDoc;) {\n            if (!reader.isDeleted(j)) { // skip deleted docs\n              if (matchingSegmentReader != null) {\n                // We can optimize this case (doing a bulk\n                // byte copy) since the field numbers are\n                // identical\n                int start = j;\n                int numDocs = 0;\n                do {\n                  j++;\n                  numDocs++;\n                } while(j < maxDoc && !matchingSegmentReader.isDeleted(j) && numDocs < MAX_RAW_MERGE_DOCS);\n\n                IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n                fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n                docCount += numDocs;\n              } else {\n                fieldsWriter.addDocument(reader.document(j, fieldSelectorMerge));\n                j++;\n                docCount++;\n              }\n            } else\n              j++;\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (int i = 0; i < readers.size(); i++)\n        docCount += ((IndexReader) readers.elementAt(i)).numDocs();\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, that means\n      // all segments were written as part of a single\n      // autoCommit=false IndexWriter session, so their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.elementAt(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    int docCount = 0;\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.elementAt(i);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false, false);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.STORES_PAYLOADS), false, false, false, true);\n      addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false, false);\n      fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    if (mergeDocStores) {\n\n      FieldsWriter fieldsWriter = // merge field values\n        new FieldsWriter(directory, segment, fieldInfos);\n    \n      // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n      // in  merge mode, we use this FieldSelector\n      FieldSelector fieldSelectorMerge = new FieldSelector() {\n          public FieldSelectorResult accept(String fieldName) {\n            return FieldSelectorResult.LOAD_FOR_MERGE;\n          }        \n        };\n\n      try {\n        for (int i = 0; i < readers.size(); i++) {\n          IndexReader reader = (IndexReader) readers.elementAt(i);\n          int maxDoc = reader.maxDoc();\n          for (int j = 0; j < maxDoc; j++)\n            if (!reader.isDeleted(j)) {               // skip deleted docs\n              fieldsWriter.addDocument(reader.document(j, fieldSelectorMerge));\n              docCount++;\n            }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (int i = 0; i < readers.size(); i++)\n        docCount += ((IndexReader) readers.elementAt(i)).numDocs();\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":["c9a0deca56efc5191d6b3c41047fd538f3fae1d8","346d5897e4c4e77ed5dbd31f7730ff30973d5971","f271ff507bf58806fc089e76e38057c95e391dcd"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f271ff507bf58806fc089e76e38057c95e391dcd","date":1196311383,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, that means\n      // all segments were written as part of a single\n      // autoCommit=false IndexWriter session, so their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.elementAt(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.elementAt(i);\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        for (int j = 0; j < segmentReader.getFieldInfos().size(); j++) {\n          FieldInfo fi = segmentReader.getFieldInfos().fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector, fi.storePositionWithTermVector, fi.storeOffsetWithTermVector, !reader.hasNorms(fi.name), fi.storePayloads);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.STORES_PAYLOADS), false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    if (mergeDocStores) {\n\n      // If the i'th reader is a SegmentReader and has\n      // identical fieldName -> number mapping, then this\n      // array will be non-null at position i:\n      SegmentReader[] matchingSegmentReaders = new SegmentReader[readers.size()];\n\n      for (int i = 0; i < readers.size(); i++) {\n        IndexReader reader = (IndexReader) readers.elementAt(i);\n        boolean same = reader.getFieldNames(IndexReader.FieldOption.ALL).size() == fieldInfos.size() && reader instanceof SegmentReader;\n        if (same) {\n          SegmentReader segmentReader = (SegmentReader) reader;\n          for (int j = 0; same && j < fieldInfos.size(); j++)\n            same = fieldInfos.fieldName(j).equals(segmentReader.getFieldInfos().fieldName(j));\n          if (same)\n            matchingSegmentReaders[i] = segmentReader;\n        }\n      }\n\t\n      // Used for bulk-reading raw bytes for stored fields\n      final int[] rawDocLengths = new int[MAX_RAW_MERGE_DOCS];\n\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n      // in  merge mode, we use this FieldSelector\n      FieldSelector fieldSelectorMerge = new FieldSelector() {\n          public FieldSelectorResult accept(String fieldName) {\n            return FieldSelectorResult.LOAD_FOR_MERGE;\n          }        \n        };\n\n      try {\n        for (int i = 0; i < readers.size(); i++) {\n          final IndexReader reader = (IndexReader) readers.elementAt(i);\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[i];\n          final FieldsReader matchingFieldsReader;\n          if (matchingSegmentReader != null)\n            matchingFieldsReader = matchingSegmentReader.getFieldsReader();\n          else\n            matchingFieldsReader = null;\n          final int maxDoc = reader.maxDoc();\n          for (int j = 0; j < maxDoc;) {\n            if (!reader.isDeleted(j)) { // skip deleted docs\n              if (matchingSegmentReader != null) {\n                // We can optimize this case (doing a bulk\n                // byte copy) since the field numbers are\n                // identical\n                int start = j;\n                int numDocs = 0;\n                do {\n                  j++;\n                  numDocs++;\n                } while(j < maxDoc && !matchingSegmentReader.isDeleted(j) && numDocs < MAX_RAW_MERGE_DOCS);\n\n                IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n                fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n                docCount += numDocs;\n              } else {\n                fieldsWriter.addDocument(reader.document(j, fieldSelectorMerge));\n                j++;\n                docCount++;\n              }\n            } else\n              j++;\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (int i = 0; i < readers.size(); i++)\n        docCount += ((IndexReader) readers.elementAt(i)).numDocs();\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, that means\n      // all segments were written as part of a single\n      // autoCommit=false IndexWriter session, so their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.elementAt(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.elementAt(i);\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        for (int j = 0; j < segmentReader.getFieldInfos().size(); j++) {\n          FieldInfo fi = segmentReader.getFieldInfos().fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector, fi.storePositionWithTermVector, fi.storeOffsetWithTermVector, !reader.hasNorms(fi.name));\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.STORES_PAYLOADS), false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    if (mergeDocStores) {\n\n      // If the i'th reader is a SegmentReader and has\n      // identical fieldName -> number mapping, then this\n      // array will be non-null at position i:\n      SegmentReader[] matchingSegmentReaders = new SegmentReader[readers.size()];\n\n      for (int i = 0; i < readers.size(); i++) {\n        IndexReader reader = (IndexReader) readers.elementAt(i);\n        boolean same = reader.getFieldNames(IndexReader.FieldOption.ALL).size() == fieldInfos.size() && reader instanceof SegmentReader;\n        if (same) {\n          SegmentReader segmentReader = (SegmentReader) reader;\n          for (int j = 0; same && j < fieldInfos.size(); j++)\n            same = fieldInfos.fieldName(j).equals(segmentReader.getFieldInfos().fieldName(j));\n          if (same)\n            matchingSegmentReaders[i] = segmentReader;\n        }\n      }\n\t\n      // Used for bulk-reading raw bytes for stored fields\n      final int[] rawDocLengths = new int[MAX_RAW_MERGE_DOCS];\n\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n      // in  merge mode, we use this FieldSelector\n      FieldSelector fieldSelectorMerge = new FieldSelector() {\n          public FieldSelectorResult accept(String fieldName) {\n            return FieldSelectorResult.LOAD_FOR_MERGE;\n          }        \n        };\n\n      try {\n        for (int i = 0; i < readers.size(); i++) {\n          final IndexReader reader = (IndexReader) readers.elementAt(i);\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[i];\n          final FieldsReader matchingFieldsReader;\n          if (matchingSegmentReader != null)\n            matchingFieldsReader = matchingSegmentReader.getFieldsReader();\n          else\n            matchingFieldsReader = null;\n          final int maxDoc = reader.maxDoc();\n          for (int j = 0; j < maxDoc;) {\n            if (!reader.isDeleted(j)) { // skip deleted docs\n              if (matchingSegmentReader != null) {\n                // We can optimize this case (doing a bulk\n                // byte copy) since the field numbers are\n                // identical\n                int start = j;\n                int numDocs = 0;\n                do {\n                  j++;\n                  numDocs++;\n                } while(j < maxDoc && !matchingSegmentReader.isDeleted(j) && numDocs < MAX_RAW_MERGE_DOCS);\n\n                IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n                fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n                docCount += numDocs;\n              } else {\n                fieldsWriter.addDocument(reader.document(j, fieldSelectorMerge));\n                j++;\n                docCount++;\n              }\n            } else\n              j++;\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (int i = 0; i < readers.size(); i++)\n        docCount += ((IndexReader) readers.elementAt(i)).numDocs();\n\n    return docCount;\n  }\n\n","bugFix":["3b248d9b80f6b2fceade80b3c8683d1cca6e4c98"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c9a0deca56efc5191d6b3c41047fd538f3fae1d8","date":1198156049,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, that means\n      // all segments were written as part of a single\n      // autoCommit=false IndexWriter session, so their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.elementAt(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.elementAt(i);\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        for (int j = 0; j < segmentReader.getFieldInfos().size(); j++) {\n          FieldInfo fi = segmentReader.getFieldInfos().fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector, fi.storePositionWithTermVector, fi.storeOffsetWithTermVector, !reader.hasNorms(fi.name), fi.storePayloads);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.STORES_PAYLOADS), false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    if (mergeDocStores) {\n\n      // If the i'th reader is a SegmentReader and has\n      // identical fieldName -> number mapping, then this\n      // array will be non-null at position i:\n      SegmentReader[] matchingSegmentReaders = new SegmentReader[readers.size()];\n\n      // If this reader is a SegmentReader, and all of its\n      // field name -> number mappings match the \"merged\"\n      // FieldInfos, then we can do a bulk copy of the\n      // stored fields:\n      for (int i = 0; i < readers.size(); i++) {\n        IndexReader reader = (IndexReader) readers.elementAt(i);\n        if (reader instanceof SegmentReader) {\n          SegmentReader segmentReader = (SegmentReader) reader;\n          boolean same = true;\n          FieldInfos segmentFieldInfos = segmentReader.getFieldInfos();\n          for (int j = 0; same && j < segmentFieldInfos.size(); j++)\n            same = fieldInfos.fieldName(j).equals(segmentFieldInfos.fieldName(j));\n          if (same) {\n            matchingSegmentReaders[i] = segmentReader;\n          }\n        }\n      }\n\t\n      // Used for bulk-reading raw bytes for stored fields\n      final int[] rawDocLengths = new int[MAX_RAW_MERGE_DOCS];\n\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n      // in  merge mode, we use this FieldSelector\n      FieldSelector fieldSelectorMerge = new FieldSelector() {\n          public FieldSelectorResult accept(String fieldName) {\n            return FieldSelectorResult.LOAD_FOR_MERGE;\n          }        \n        };\n\n      try {\n        for (int i = 0; i < readers.size(); i++) {\n          final IndexReader reader = (IndexReader) readers.elementAt(i);\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[i];\n          final FieldsReader matchingFieldsReader;\n          if (matchingSegmentReader != null)\n            matchingFieldsReader = matchingSegmentReader.getFieldsReader();\n          else\n            matchingFieldsReader = null;\n          final int maxDoc = reader.maxDoc();\n          for (int j = 0; j < maxDoc;) {\n            if (!reader.isDeleted(j)) { // skip deleted docs\n              if (matchingSegmentReader != null) {\n                // We can optimize this case (doing a bulk\n                // byte copy) since the field numbers are\n                // identical\n                int start = j;\n                int numDocs = 0;\n                do {\n                  j++;\n                  numDocs++;\n                } while(j < maxDoc && !matchingSegmentReader.isDeleted(j) && numDocs < MAX_RAW_MERGE_DOCS);\n\n                IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n                fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n                docCount += numDocs;\n              } else {\n                fieldsWriter.addDocument(reader.document(j, fieldSelectorMerge));\n                j++;\n                docCount++;\n              }\n            } else\n              j++;\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (int i = 0; i < readers.size(); i++)\n        docCount += ((IndexReader) readers.elementAt(i)).numDocs();\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, that means\n      // all segments were written as part of a single\n      // autoCommit=false IndexWriter session, so their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.elementAt(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.elementAt(i);\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        for (int j = 0; j < segmentReader.getFieldInfos().size(); j++) {\n          FieldInfo fi = segmentReader.getFieldInfos().fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector, fi.storePositionWithTermVector, fi.storeOffsetWithTermVector, !reader.hasNorms(fi.name), fi.storePayloads);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.STORES_PAYLOADS), false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    if (mergeDocStores) {\n\n      // If the i'th reader is a SegmentReader and has\n      // identical fieldName -> number mapping, then this\n      // array will be non-null at position i:\n      SegmentReader[] matchingSegmentReaders = new SegmentReader[readers.size()];\n\n      for (int i = 0; i < readers.size(); i++) {\n        IndexReader reader = (IndexReader) readers.elementAt(i);\n        boolean same = reader.getFieldNames(IndexReader.FieldOption.ALL).size() == fieldInfos.size() && reader instanceof SegmentReader;\n        if (same) {\n          SegmentReader segmentReader = (SegmentReader) reader;\n          for (int j = 0; same && j < fieldInfos.size(); j++)\n            same = fieldInfos.fieldName(j).equals(segmentReader.getFieldInfos().fieldName(j));\n          if (same)\n            matchingSegmentReaders[i] = segmentReader;\n        }\n      }\n\t\n      // Used for bulk-reading raw bytes for stored fields\n      final int[] rawDocLengths = new int[MAX_RAW_MERGE_DOCS];\n\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n      // in  merge mode, we use this FieldSelector\n      FieldSelector fieldSelectorMerge = new FieldSelector() {\n          public FieldSelectorResult accept(String fieldName) {\n            return FieldSelectorResult.LOAD_FOR_MERGE;\n          }        \n        };\n\n      try {\n        for (int i = 0; i < readers.size(); i++) {\n          final IndexReader reader = (IndexReader) readers.elementAt(i);\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[i];\n          final FieldsReader matchingFieldsReader;\n          if (matchingSegmentReader != null)\n            matchingFieldsReader = matchingSegmentReader.getFieldsReader();\n          else\n            matchingFieldsReader = null;\n          final int maxDoc = reader.maxDoc();\n          for (int j = 0; j < maxDoc;) {\n            if (!reader.isDeleted(j)) { // skip deleted docs\n              if (matchingSegmentReader != null) {\n                // We can optimize this case (doing a bulk\n                // byte copy) since the field numbers are\n                // identical\n                int start = j;\n                int numDocs = 0;\n                do {\n                  j++;\n                  numDocs++;\n                } while(j < maxDoc && !matchingSegmentReader.isDeleted(j) && numDocs < MAX_RAW_MERGE_DOCS);\n\n                IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n                fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n                docCount += numDocs;\n              } else {\n                fieldsWriter.addDocument(reader.document(j, fieldSelectorMerge));\n                j++;\n                docCount++;\n              }\n            } else\n              j++;\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (int i = 0; i < readers.size(); i++)\n        docCount += ((IndexReader) readers.elementAt(i)).numDocs();\n\n    return docCount;\n  }\n\n","bugFix":["3b248d9b80f6b2fceade80b3c8683d1cca6e4c98"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"346d5897e4c4e77ed5dbd31f7730ff30973d5971","date":1198317988,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, that means\n      // all segments were written as part of a single\n      // autoCommit=false IndexWriter session, so their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.elementAt(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.elementAt(i);\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        for (int j = 0; j < segmentReader.getFieldInfos().size(); j++) {\n          FieldInfo fi = segmentReader.getFieldInfos().fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector, fi.storePositionWithTermVector, fi.storeOffsetWithTermVector, !reader.hasNorms(fi.name), fi.storePayloads);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.STORES_PAYLOADS), false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    if (mergeDocStores) {\n\n      // If the i'th reader is a SegmentReader and has\n      // identical fieldName -> number mapping, then this\n      // array will be non-null at position i:\n      SegmentReader[] matchingSegmentReaders = new SegmentReader[readers.size()];\n\n      // If this reader is a SegmentReader, and all of its\n      // field name -> number mappings match the \"merged\"\n      // FieldInfos, then we can do a bulk copy of the\n      // stored fields:\n      for (int i = 0; i < readers.size(); i++) {\n        IndexReader reader = (IndexReader) readers.elementAt(i);\n        if (reader instanceof SegmentReader) {\n          SegmentReader segmentReader = (SegmentReader) reader;\n          boolean same = true;\n          FieldInfos segmentFieldInfos = segmentReader.getFieldInfos();\n          for (int j = 0; same && j < segmentFieldInfos.size(); j++)\n            same = fieldInfos.fieldName(j).equals(segmentFieldInfos.fieldName(j));\n          if (same) {\n            matchingSegmentReaders[i] = segmentReader;\n          }\n        }\n      }\n\t\n      // Used for bulk-reading raw bytes for stored fields\n      final int[] rawDocLengths = new int[MAX_RAW_MERGE_DOCS];\n\n      // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n      // in  merge mode, we use this FieldSelector\n      FieldSelector fieldSelectorMerge = new FieldSelector() {\n          public FieldSelectorResult accept(String fieldName) {\n            return FieldSelectorResult.LOAD_FOR_MERGE;\n          }        \n        };\n\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        for (int i = 0; i < readers.size(); i++) {\n          final IndexReader reader = (IndexReader) readers.elementAt(i);\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[i];\n          final FieldsReader matchingFieldsReader;\n          if (matchingSegmentReader != null)\n            matchingFieldsReader = matchingSegmentReader.getFieldsReader();\n          else\n            matchingFieldsReader = null;\n          final int maxDoc = reader.maxDoc();\n          for (int j = 0; j < maxDoc;) {\n            if (!reader.isDeleted(j)) { // skip deleted docs\n              if (matchingSegmentReader != null) {\n                // We can optimize this case (doing a bulk\n                // byte copy) since the field numbers are\n                // identical\n                int start = j;\n                int numDocs = 0;\n                do {\n                  j++;\n                  numDocs++;\n                } while(j < maxDoc && !matchingSegmentReader.isDeleted(j) && numDocs < MAX_RAW_MERGE_DOCS);\n\n                IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n                fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n                docCount += numDocs;\n                if (checkAbort != null)\n                  checkAbort.work(300*numDocs);\n              } else {\n                fieldsWriter.addDocument(reader.document(j, fieldSelectorMerge));\n                j++;\n                docCount++;\n                if (checkAbort != null)\n                  checkAbort.work(300);\n              }\n            } else\n              j++;\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (int i = 0; i < readers.size(); i++)\n        docCount += ((IndexReader) readers.elementAt(i)).numDocs();\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, that means\n      // all segments were written as part of a single\n      // autoCommit=false IndexWriter session, so their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.elementAt(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.elementAt(i);\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        for (int j = 0; j < segmentReader.getFieldInfos().size(); j++) {\n          FieldInfo fi = segmentReader.getFieldInfos().fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector, fi.storePositionWithTermVector, fi.storeOffsetWithTermVector, !reader.hasNorms(fi.name), fi.storePayloads);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.STORES_PAYLOADS), false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    if (mergeDocStores) {\n\n      // If the i'th reader is a SegmentReader and has\n      // identical fieldName -> number mapping, then this\n      // array will be non-null at position i:\n      SegmentReader[] matchingSegmentReaders = new SegmentReader[readers.size()];\n\n      // If this reader is a SegmentReader, and all of its\n      // field name -> number mappings match the \"merged\"\n      // FieldInfos, then we can do a bulk copy of the\n      // stored fields:\n      for (int i = 0; i < readers.size(); i++) {\n        IndexReader reader = (IndexReader) readers.elementAt(i);\n        if (reader instanceof SegmentReader) {\n          SegmentReader segmentReader = (SegmentReader) reader;\n          boolean same = true;\n          FieldInfos segmentFieldInfos = segmentReader.getFieldInfos();\n          for (int j = 0; same && j < segmentFieldInfos.size(); j++)\n            same = fieldInfos.fieldName(j).equals(segmentFieldInfos.fieldName(j));\n          if (same) {\n            matchingSegmentReaders[i] = segmentReader;\n          }\n        }\n      }\n\t\n      // Used for bulk-reading raw bytes for stored fields\n      final int[] rawDocLengths = new int[MAX_RAW_MERGE_DOCS];\n\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n      // in  merge mode, we use this FieldSelector\n      FieldSelector fieldSelectorMerge = new FieldSelector() {\n          public FieldSelectorResult accept(String fieldName) {\n            return FieldSelectorResult.LOAD_FOR_MERGE;\n          }        \n        };\n\n      try {\n        for (int i = 0; i < readers.size(); i++) {\n          final IndexReader reader = (IndexReader) readers.elementAt(i);\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[i];\n          final FieldsReader matchingFieldsReader;\n          if (matchingSegmentReader != null)\n            matchingFieldsReader = matchingSegmentReader.getFieldsReader();\n          else\n            matchingFieldsReader = null;\n          final int maxDoc = reader.maxDoc();\n          for (int j = 0; j < maxDoc;) {\n            if (!reader.isDeleted(j)) { // skip deleted docs\n              if (matchingSegmentReader != null) {\n                // We can optimize this case (doing a bulk\n                // byte copy) since the field numbers are\n                // identical\n                int start = j;\n                int numDocs = 0;\n                do {\n                  j++;\n                  numDocs++;\n                } while(j < maxDoc && !matchingSegmentReader.isDeleted(j) && numDocs < MAX_RAW_MERGE_DOCS);\n\n                IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n                fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n                docCount += numDocs;\n              } else {\n                fieldsWriter.addDocument(reader.document(j, fieldSelectorMerge));\n                j++;\n                docCount++;\n              }\n            } else\n              j++;\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (int i = 0; i < readers.size(); i++)\n        docCount += ((IndexReader) readers.elementAt(i)).numDocs();\n\n    return docCount;\n  }\n\n","bugFix":["3b248d9b80f6b2fceade80b3c8683d1cca6e4c98"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b3d08461c77d39c25ea6ff0cd05b32f948fa2a33","date":1201260752,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, that means\n      // all segments were written as part of a single\n      // autoCommit=false IndexWriter session, so their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.elementAt(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.elementAt(i);\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        for (int j = 0; j < segmentReader.getFieldInfos().size(); j++) {\n          FieldInfo fi = segmentReader.getFieldInfos().fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector, fi.storePositionWithTermVector, fi.storeOffsetWithTermVector, !reader.hasNorms(fi.name), fi.storePayloads);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.STORES_PAYLOADS), false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n\n      // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n      // in  merge mode, we use this FieldSelector\n      FieldSelector fieldSelectorMerge = new FieldSelector() {\n          public FieldSelectorResult accept(String fieldName) {\n            return FieldSelectorResult.LOAD_FOR_MERGE;\n          }        \n        };\n\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        for (int i = 0; i < readers.size(); i++) {\n          final IndexReader reader = (IndexReader) readers.elementAt(i);\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[i];\n          final FieldsReader matchingFieldsReader;\n          if (matchingSegmentReader != null)\n            matchingFieldsReader = matchingSegmentReader.getFieldsReader();\n          else\n            matchingFieldsReader = null;\n          final int maxDoc = reader.maxDoc();\n          for (int j = 0; j < maxDoc;) {\n            if (!reader.isDeleted(j)) { // skip deleted docs\n              if (matchingSegmentReader != null) {\n                // We can optimize this case (doing a bulk\n                // byte copy) since the field numbers are\n                // identical\n                int start = j;\n                int numDocs = 0;\n                do {\n                  j++;\n                  numDocs++;\n                } while(j < maxDoc && !matchingSegmentReader.isDeleted(j) && numDocs < MAX_RAW_MERGE_DOCS);\n\n                IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n                fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n                docCount += numDocs;\n                if (checkAbort != null)\n                  checkAbort.work(300*numDocs);\n              } else {\n                fieldsWriter.addDocument(reader.document(j, fieldSelectorMerge));\n                j++;\n                docCount++;\n                if (checkAbort != null)\n                  checkAbort.work(300);\n              }\n            } else\n              j++;\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (int i = 0; i < readers.size(); i++)\n        docCount += ((IndexReader) readers.elementAt(i)).numDocs();\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, that means\n      // all segments were written as part of a single\n      // autoCommit=false IndexWriter session, so their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.elementAt(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.elementAt(i);\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        for (int j = 0; j < segmentReader.getFieldInfos().size(); j++) {\n          FieldInfo fi = segmentReader.getFieldInfos().fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector, fi.storePositionWithTermVector, fi.storeOffsetWithTermVector, !reader.hasNorms(fi.name), fi.storePayloads);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.STORES_PAYLOADS), false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    if (mergeDocStores) {\n\n      // If the i'th reader is a SegmentReader and has\n      // identical fieldName -> number mapping, then this\n      // array will be non-null at position i:\n      SegmentReader[] matchingSegmentReaders = new SegmentReader[readers.size()];\n\n      // If this reader is a SegmentReader, and all of its\n      // field name -> number mappings match the \"merged\"\n      // FieldInfos, then we can do a bulk copy of the\n      // stored fields:\n      for (int i = 0; i < readers.size(); i++) {\n        IndexReader reader = (IndexReader) readers.elementAt(i);\n        if (reader instanceof SegmentReader) {\n          SegmentReader segmentReader = (SegmentReader) reader;\n          boolean same = true;\n          FieldInfos segmentFieldInfos = segmentReader.getFieldInfos();\n          for (int j = 0; same && j < segmentFieldInfos.size(); j++)\n            same = fieldInfos.fieldName(j).equals(segmentFieldInfos.fieldName(j));\n          if (same) {\n            matchingSegmentReaders[i] = segmentReader;\n          }\n        }\n      }\n\t\n      // Used for bulk-reading raw bytes for stored fields\n      final int[] rawDocLengths = new int[MAX_RAW_MERGE_DOCS];\n\n      // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n      // in  merge mode, we use this FieldSelector\n      FieldSelector fieldSelectorMerge = new FieldSelector() {\n          public FieldSelectorResult accept(String fieldName) {\n            return FieldSelectorResult.LOAD_FOR_MERGE;\n          }        \n        };\n\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        for (int i = 0; i < readers.size(); i++) {\n          final IndexReader reader = (IndexReader) readers.elementAt(i);\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[i];\n          final FieldsReader matchingFieldsReader;\n          if (matchingSegmentReader != null)\n            matchingFieldsReader = matchingSegmentReader.getFieldsReader();\n          else\n            matchingFieldsReader = null;\n          final int maxDoc = reader.maxDoc();\n          for (int j = 0; j < maxDoc;) {\n            if (!reader.isDeleted(j)) { // skip deleted docs\n              if (matchingSegmentReader != null) {\n                // We can optimize this case (doing a bulk\n                // byte copy) since the field numbers are\n                // identical\n                int start = j;\n                int numDocs = 0;\n                do {\n                  j++;\n                  numDocs++;\n                } while(j < maxDoc && !matchingSegmentReader.isDeleted(j) && numDocs < MAX_RAW_MERGE_DOCS);\n\n                IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n                fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n                docCount += numDocs;\n                if (checkAbort != null)\n                  checkAbort.work(300*numDocs);\n              } else {\n                fieldsWriter.addDocument(reader.document(j, fieldSelectorMerge));\n                j++;\n                docCount++;\n                if (checkAbort != null)\n                  checkAbort.work(300);\n              }\n            } else\n              j++;\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (int i = 0; i < readers.size(); i++)\n        docCount += ((IndexReader) readers.elementAt(i)).numDocs();\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"da3e8c2fef4ea558379c4c0879b3bcdecde41bcd","date":1206037293,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, that means\n      // all segments were written as part of a single\n      // autoCommit=false IndexWriter session, so their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.elementAt(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.elementAt(i);\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        for (int j = 0; j < segmentReader.getFieldInfos().size(); j++) {\n          FieldInfo fi = segmentReader.getFieldInfos().fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector, fi.storePositionWithTermVector, fi.storeOffsetWithTermVector, !reader.hasNorms(fi.name), fi.storePayloads);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.STORES_PAYLOADS), false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n\n      // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n      // in  merge mode, we use this FieldSelector\n      FieldSelector fieldSelectorMerge = new FieldSelector() {\n          public FieldSelectorResult accept(String fieldName) {\n            return FieldSelectorResult.LOAD_FOR_MERGE;\n          }        \n        };\n\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        for (int i = 0; i < readers.size(); i++) {\n          final IndexReader reader = (IndexReader) readers.elementAt(i);\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[i];\n          final FieldsReader matchingFieldsReader;\n          if (matchingSegmentReader != null)\n            matchingFieldsReader = matchingSegmentReader.getFieldsReader();\n          else\n            matchingFieldsReader = null;\n          final int maxDoc = reader.maxDoc();\n          for (int j = 0; j < maxDoc;) {\n            if (!reader.isDeleted(j)) { // skip deleted docs\n              if (matchingSegmentReader != null) {\n                // We can optimize this case (doing a bulk\n                // byte copy) since the field numbers are\n                // identical\n                int start = j;\n                int numDocs = 0;\n                do {\n                  j++;\n                  numDocs++;\n                } while(j < maxDoc && !matchingSegmentReader.isDeleted(j) && numDocs < MAX_RAW_MERGE_DOCS);\n\n                IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n                fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n                docCount += numDocs;\n                if (checkAbort != null)\n                  checkAbort.work(300*numDocs);\n              } else {\n                fieldsWriter.addDocument(reader.document(j, fieldSelectorMerge));\n                j++;\n                docCount++;\n                if (checkAbort != null)\n                  checkAbort.work(300);\n              }\n            } else\n              j++;\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      assert docCount*8 == directory.fileLength(segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION) :\n        \"after mergeFields: fdx size mismatch: \" + docCount + \" docs vs \" + directory.fileLength(segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION) + \" length in bytes of \" + segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION;\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (int i = 0; i < readers.size(); i++)\n        docCount += ((IndexReader) readers.elementAt(i)).numDocs();\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, that means\n      // all segments were written as part of a single\n      // autoCommit=false IndexWriter session, so their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.elementAt(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.elementAt(i);\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        for (int j = 0; j < segmentReader.getFieldInfos().size(); j++) {\n          FieldInfo fi = segmentReader.getFieldInfos().fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector, fi.storePositionWithTermVector, fi.storeOffsetWithTermVector, !reader.hasNorms(fi.name), fi.storePayloads);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.STORES_PAYLOADS), false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n\n      // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n      // in  merge mode, we use this FieldSelector\n      FieldSelector fieldSelectorMerge = new FieldSelector() {\n          public FieldSelectorResult accept(String fieldName) {\n            return FieldSelectorResult.LOAD_FOR_MERGE;\n          }        \n        };\n\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        for (int i = 0; i < readers.size(); i++) {\n          final IndexReader reader = (IndexReader) readers.elementAt(i);\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[i];\n          final FieldsReader matchingFieldsReader;\n          if (matchingSegmentReader != null)\n            matchingFieldsReader = matchingSegmentReader.getFieldsReader();\n          else\n            matchingFieldsReader = null;\n          final int maxDoc = reader.maxDoc();\n          for (int j = 0; j < maxDoc;) {\n            if (!reader.isDeleted(j)) { // skip deleted docs\n              if (matchingSegmentReader != null) {\n                // We can optimize this case (doing a bulk\n                // byte copy) since the field numbers are\n                // identical\n                int start = j;\n                int numDocs = 0;\n                do {\n                  j++;\n                  numDocs++;\n                } while(j < maxDoc && !matchingSegmentReader.isDeleted(j) && numDocs < MAX_RAW_MERGE_DOCS);\n\n                IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n                fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n                docCount += numDocs;\n                if (checkAbort != null)\n                  checkAbort.work(300*numDocs);\n              } else {\n                fieldsWriter.addDocument(reader.document(j, fieldSelectorMerge));\n                j++;\n                docCount++;\n                if (checkAbort != null)\n                  checkAbort.work(300);\n              }\n            } else\n              j++;\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (int i = 0; i < readers.size(); i++)\n        docCount += ((IndexReader) readers.elementAt(i)).numDocs();\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":["33bd4a1f823fcca0940fb58c35873adeab11c761"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"84acdfa12c18361ff932244db20470fce117e52d","date":1206384355,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, that means\n      // all segments were written as part of a single\n      // autoCommit=false IndexWriter session, so their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.elementAt(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.elementAt(i);\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        for (int j = 0; j < segmentReader.getFieldInfos().size(); j++) {\n          FieldInfo fi = segmentReader.getFieldInfos().fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector, fi.storePositionWithTermVector, fi.storeOffsetWithTermVector, !reader.hasNorms(fi.name), fi.storePayloads);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.STORES_PAYLOADS), false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n\n      // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n      // in  merge mode, we use this FieldSelector\n      FieldSelector fieldSelectorMerge = new FieldSelector() {\n          public FieldSelectorResult accept(String fieldName) {\n            return FieldSelectorResult.LOAD_FOR_MERGE;\n          }        \n        };\n\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        for (int i = 0; i < readers.size(); i++) {\n          final IndexReader reader = (IndexReader) readers.elementAt(i);\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[i];\n          final FieldsReader matchingFieldsReader;\n          final boolean hasMatchingReader;\n          if (matchingSegmentReader != null) {\n            hasMatchingReader = true;\n            matchingFieldsReader = matchingSegmentReader.getFieldsReader();\n          } else {\n            hasMatchingReader = false;\n            matchingFieldsReader = null;\n          }\n          final int maxDoc = reader.maxDoc();\n          final boolean hasDeletions = reader.hasDeletions();\n          for (int j = 0; j < maxDoc;) {\n            if (!hasDeletions || !reader.isDeleted(j)) { // skip deleted docs\n              if (hasMatchingReader) {\n                // We can optimize this case (doing a bulk\n                // byte copy) since the field numbers are\n                // identical\n                int start = j;\n                int numDocs = 0;\n                do {\n                  j++;\n                  numDocs++;\n                  if (j >= maxDoc)\n                    break;\n                  if (hasDeletions && matchingSegmentReader.isDeleted(j)) {\n                    j++;\n                    break;\n                  }\n                } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n                IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n                fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n                docCount += numDocs;\n                if (checkAbort != null)\n                  checkAbort.work(300*numDocs);\n              } else {\n                fieldsWriter.addDocument(reader.document(j, fieldSelectorMerge));\n                j++;\n                docCount++;\n                if (checkAbort != null)\n                  checkAbort.work(300);\n              }\n            } else\n              j++;\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      assert docCount*8 == directory.fileLength(segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION) :\n        \"after mergeFields: fdx size mismatch: \" + docCount + \" docs vs \" + directory.fileLength(segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION) + \" length in bytes of \" + segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION;\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (int i = 0; i < readers.size(); i++)\n        docCount += ((IndexReader) readers.elementAt(i)).numDocs();\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, that means\n      // all segments were written as part of a single\n      // autoCommit=false IndexWriter session, so their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.elementAt(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.elementAt(i);\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        for (int j = 0; j < segmentReader.getFieldInfos().size(); j++) {\n          FieldInfo fi = segmentReader.getFieldInfos().fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector, fi.storePositionWithTermVector, fi.storeOffsetWithTermVector, !reader.hasNorms(fi.name), fi.storePayloads);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.STORES_PAYLOADS), false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n\n      // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n      // in  merge mode, we use this FieldSelector\n      FieldSelector fieldSelectorMerge = new FieldSelector() {\n          public FieldSelectorResult accept(String fieldName) {\n            return FieldSelectorResult.LOAD_FOR_MERGE;\n          }        \n        };\n\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        for (int i = 0; i < readers.size(); i++) {\n          final IndexReader reader = (IndexReader) readers.elementAt(i);\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[i];\n          final FieldsReader matchingFieldsReader;\n          if (matchingSegmentReader != null)\n            matchingFieldsReader = matchingSegmentReader.getFieldsReader();\n          else\n            matchingFieldsReader = null;\n          final int maxDoc = reader.maxDoc();\n          for (int j = 0; j < maxDoc;) {\n            if (!reader.isDeleted(j)) { // skip deleted docs\n              if (matchingSegmentReader != null) {\n                // We can optimize this case (doing a bulk\n                // byte copy) since the field numbers are\n                // identical\n                int start = j;\n                int numDocs = 0;\n                do {\n                  j++;\n                  numDocs++;\n                } while(j < maxDoc && !matchingSegmentReader.isDeleted(j) && numDocs < MAX_RAW_MERGE_DOCS);\n\n                IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n                fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n                docCount += numDocs;\n                if (checkAbort != null)\n                  checkAbort.work(300*numDocs);\n              } else {\n                fieldsWriter.addDocument(reader.document(j, fieldSelectorMerge));\n                j++;\n                docCount++;\n                if (checkAbort != null)\n                  checkAbort.work(300);\n              }\n            } else\n              j++;\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      assert docCount*8 == directory.fileLength(segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION) :\n        \"after mergeFields: fdx size mismatch: \" + docCount + \" docs vs \" + directory.fileLength(segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION) + \" length in bytes of \" + segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION;\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (int i = 0; i < readers.size(); i++)\n        docCount += ((IndexReader) readers.elementAt(i)).numDocs();\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"dbb7b6f10bff9eedd5c9bc6cf9222ffbb2df74d4","date":1206538765,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, that means\n      // all segments were written as part of a single\n      // autoCommit=false IndexWriter session, so their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.elementAt(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.elementAt(i);\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        for (int j = 0; j < segmentReader.getFieldInfos().size(); j++) {\n          FieldInfo fi = segmentReader.getFieldInfos().fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector, fi.storePositionWithTermVector, fi.storeOffsetWithTermVector, !reader.hasNorms(fi.name), fi.storePayloads);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.STORES_PAYLOADS), false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n\n      // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n      // in  merge mode, we use this FieldSelector\n      FieldSelector fieldSelectorMerge = new FieldSelector() {\n          public FieldSelectorResult accept(String fieldName) {\n            return FieldSelectorResult.LOAD_FOR_MERGE;\n          }        \n        };\n\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        for (int i = 0; i < readers.size(); i++) {\n          final IndexReader reader = (IndexReader) readers.elementAt(i);\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[i];\n          final FieldsReader matchingFieldsReader;\n          final boolean hasMatchingReader;\n          if (matchingSegmentReader != null) {\n            hasMatchingReader = true;\n            matchingFieldsReader = matchingSegmentReader.getFieldsReader();\n          } else {\n            hasMatchingReader = false;\n            matchingFieldsReader = null;\n          }\n          final int maxDoc = reader.maxDoc();\n          final boolean hasDeletions = reader.hasDeletions();\n          for (int j = 0; j < maxDoc;) {\n            if (!hasDeletions || !reader.isDeleted(j)) { // skip deleted docs\n              if (hasMatchingReader) {\n                // We can optimize this case (doing a bulk\n                // byte copy) since the field numbers are\n                // identical\n                int start = j;\n                int numDocs = 0;\n                do {\n                  j++;\n                  numDocs++;\n                  if (j >= maxDoc)\n                    break;\n                  if (hasDeletions && matchingSegmentReader.isDeleted(j)) {\n                    j++;\n                    break;\n                  }\n                } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n                IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n                fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n                docCount += numDocs;\n                if (checkAbort != null)\n                  checkAbort.work(300*numDocs);\n              } else {\n                fieldsWriter.addDocument(reader.document(j, fieldSelectorMerge));\n                j++;\n                docCount++;\n                if (checkAbort != null)\n                  checkAbort.work(300);\n              }\n            } else\n              j++;\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      assert 4+docCount*8 == directory.fileLength(segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION) :\n        \"after mergeFields: fdx size mismatch: \" + docCount + \" docs vs \" + directory.fileLength(segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION) + \" length in bytes of \" + segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION;\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (int i = 0; i < readers.size(); i++)\n        docCount += ((IndexReader) readers.elementAt(i)).numDocs();\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, that means\n      // all segments were written as part of a single\n      // autoCommit=false IndexWriter session, so their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.elementAt(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.elementAt(i);\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        for (int j = 0; j < segmentReader.getFieldInfos().size(); j++) {\n          FieldInfo fi = segmentReader.getFieldInfos().fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector, fi.storePositionWithTermVector, fi.storeOffsetWithTermVector, !reader.hasNorms(fi.name), fi.storePayloads);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.STORES_PAYLOADS), false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n\n      // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n      // in  merge mode, we use this FieldSelector\n      FieldSelector fieldSelectorMerge = new FieldSelector() {\n          public FieldSelectorResult accept(String fieldName) {\n            return FieldSelectorResult.LOAD_FOR_MERGE;\n          }        \n        };\n\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        for (int i = 0; i < readers.size(); i++) {\n          final IndexReader reader = (IndexReader) readers.elementAt(i);\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[i];\n          final FieldsReader matchingFieldsReader;\n          final boolean hasMatchingReader;\n          if (matchingSegmentReader != null) {\n            hasMatchingReader = true;\n            matchingFieldsReader = matchingSegmentReader.getFieldsReader();\n          } else {\n            hasMatchingReader = false;\n            matchingFieldsReader = null;\n          }\n          final int maxDoc = reader.maxDoc();\n          final boolean hasDeletions = reader.hasDeletions();\n          for (int j = 0; j < maxDoc;) {\n            if (!hasDeletions || !reader.isDeleted(j)) { // skip deleted docs\n              if (hasMatchingReader) {\n                // We can optimize this case (doing a bulk\n                // byte copy) since the field numbers are\n                // identical\n                int start = j;\n                int numDocs = 0;\n                do {\n                  j++;\n                  numDocs++;\n                  if (j >= maxDoc)\n                    break;\n                  if (hasDeletions && matchingSegmentReader.isDeleted(j)) {\n                    j++;\n                    break;\n                  }\n                } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n                IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n                fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n                docCount += numDocs;\n                if (checkAbort != null)\n                  checkAbort.work(300*numDocs);\n              } else {\n                fieldsWriter.addDocument(reader.document(j, fieldSelectorMerge));\n                j++;\n                docCount++;\n                if (checkAbort != null)\n                  checkAbort.work(300);\n              }\n            } else\n              j++;\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      assert docCount*8 == directory.fileLength(segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION) :\n        \"after mergeFields: fdx size mismatch: \" + docCount + \" docs vs \" + directory.fileLength(segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION) + \" length in bytes of \" + segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION;\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (int i = 0; i < readers.size(); i++)\n        docCount += ((IndexReader) readers.elementAt(i)).numDocs();\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":["33bd4a1f823fcca0940fb58c35873adeab11c761"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f4ba1ab90d9a427e7f2c7d1e65a3ce5869ed8e5d","date":1210334686,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, that means\n      // all segments were written as part of a single\n      // autoCommit=false IndexWriter session, so their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.elementAt(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.elementAt(i);\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        for (int j = 0; j < segmentReader.getFieldInfos().size(); j++) {\n          FieldInfo fi = segmentReader.getFieldInfos().fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector, fi.storePositionWithTermVector, fi.storeOffsetWithTermVector, !reader.hasNorms(fi.name), fi.storePayloads);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.STORES_PAYLOADS), false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n\n      // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n      // in  merge mode, we use this FieldSelector\n      FieldSelector fieldSelectorMerge = new FieldSelector() {\n          public FieldSelectorResult accept(String fieldName) {\n            return FieldSelectorResult.LOAD_FOR_MERGE;\n          }        \n        };\n\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        for (int i = 0; i < readers.size(); i++) {\n          final IndexReader reader = (IndexReader) readers.elementAt(i);\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[i];\n          final FieldsReader matchingFieldsReader;\n          final boolean hasMatchingReader;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null && !fieldsReader.canReadRawDocs()) {            \n              matchingFieldsReader = null;\n              hasMatchingReader = false;\n            } else {\n              matchingFieldsReader = fieldsReader;\n              hasMatchingReader = true;\n            }\n          } else {\n            hasMatchingReader = false;\n            matchingFieldsReader = null;\n          }\n          final int maxDoc = reader.maxDoc();\n          final boolean hasDeletions = reader.hasDeletions();\n          for (int j = 0; j < maxDoc;) {\n            if (!hasDeletions || !reader.isDeleted(j)) { // skip deleted docs\n              if (hasMatchingReader) {\n                // We can optimize this case (doing a bulk\n                // byte copy) since the field numbers are\n                // identical\n                int start = j;\n                int numDocs = 0;\n                do {\n                  j++;\n                  numDocs++;\n                  if (j >= maxDoc)\n                    break;\n                  if (hasDeletions && matchingSegmentReader.isDeleted(j)) {\n                    j++;\n                    break;\n                  }\n                } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n                IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n                fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n                docCount += numDocs;\n                if (checkAbort != null)\n                  checkAbort.work(300*numDocs);\n              } else {\n                fieldsWriter.addDocument(reader.document(j, fieldSelectorMerge));\n                j++;\n                docCount++;\n                if (checkAbort != null)\n                  checkAbort.work(300);\n              }\n            } else\n              j++;\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      assert 4+docCount*8 == directory.fileLength(segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION) :\n        \"after mergeFields: fdx size mismatch: \" + docCount + \" docs vs \" + directory.fileLength(segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION) + \" length in bytes of \" + segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION;\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (int i = 0; i < readers.size(); i++)\n        docCount += ((IndexReader) readers.elementAt(i)).numDocs();\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, that means\n      // all segments were written as part of a single\n      // autoCommit=false IndexWriter session, so their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.elementAt(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.elementAt(i);\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        for (int j = 0; j < segmentReader.getFieldInfos().size(); j++) {\n          FieldInfo fi = segmentReader.getFieldInfos().fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector, fi.storePositionWithTermVector, fi.storeOffsetWithTermVector, !reader.hasNorms(fi.name), fi.storePayloads);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.STORES_PAYLOADS), false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n\n      // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n      // in  merge mode, we use this FieldSelector\n      FieldSelector fieldSelectorMerge = new FieldSelector() {\n          public FieldSelectorResult accept(String fieldName) {\n            return FieldSelectorResult.LOAD_FOR_MERGE;\n          }        \n        };\n\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        for (int i = 0; i < readers.size(); i++) {\n          final IndexReader reader = (IndexReader) readers.elementAt(i);\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[i];\n          final FieldsReader matchingFieldsReader;\n          final boolean hasMatchingReader;\n          if (matchingSegmentReader != null) {\n            hasMatchingReader = true;\n            matchingFieldsReader = matchingSegmentReader.getFieldsReader();\n          } else {\n            hasMatchingReader = false;\n            matchingFieldsReader = null;\n          }\n          final int maxDoc = reader.maxDoc();\n          final boolean hasDeletions = reader.hasDeletions();\n          for (int j = 0; j < maxDoc;) {\n            if (!hasDeletions || !reader.isDeleted(j)) { // skip deleted docs\n              if (hasMatchingReader) {\n                // We can optimize this case (doing a bulk\n                // byte copy) since the field numbers are\n                // identical\n                int start = j;\n                int numDocs = 0;\n                do {\n                  j++;\n                  numDocs++;\n                  if (j >= maxDoc)\n                    break;\n                  if (hasDeletions && matchingSegmentReader.isDeleted(j)) {\n                    j++;\n                    break;\n                  }\n                } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n                IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n                fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n                docCount += numDocs;\n                if (checkAbort != null)\n                  checkAbort.work(300*numDocs);\n              } else {\n                fieldsWriter.addDocument(reader.document(j, fieldSelectorMerge));\n                j++;\n                docCount++;\n                if (checkAbort != null)\n                  checkAbort.work(300);\n              }\n            } else\n              j++;\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      assert 4+docCount*8 == directory.fileLength(segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION) :\n        \"after mergeFields: fdx size mismatch: \" + docCount + \" docs vs \" + directory.fileLength(segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION) + \" length in bytes of \" + segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION;\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (int i = 0; i < readers.size(); i++)\n        docCount += ((IndexReader) readers.elementAt(i)).numDocs();\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"33bd4a1f823fcca0940fb58c35873adeab11c761","date":1210409330,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, that means\n      // all segments were written as part of a single\n      // autoCommit=false IndexWriter session, so their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.elementAt(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.elementAt(i);\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        for (int j = 0; j < segmentReader.getFieldInfos().size(); j++) {\n          FieldInfo fi = segmentReader.getFieldInfos().fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector, fi.storePositionWithTermVector, fi.storeOffsetWithTermVector, !reader.hasNorms(fi.name), fi.storePayloads);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.STORES_PAYLOADS), false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n\n      // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n      // in  merge mode, we use this FieldSelector\n      FieldSelector fieldSelectorMerge = new FieldSelector() {\n          public FieldSelectorResult accept(String fieldName) {\n            return FieldSelectorResult.LOAD_FOR_MERGE;\n          }        \n        };\n\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        for (int i = 0; i < readers.size(); i++) {\n          final IndexReader reader = (IndexReader) readers.elementAt(i);\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[i];\n          final FieldsReader matchingFieldsReader;\n          final boolean hasMatchingReader;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null && !fieldsReader.canReadRawDocs()) {            \n              matchingFieldsReader = null;\n              hasMatchingReader = false;\n            } else {\n              matchingFieldsReader = fieldsReader;\n              hasMatchingReader = true;\n            }\n          } else {\n            hasMatchingReader = false;\n            matchingFieldsReader = null;\n          }\n          final int maxDoc = reader.maxDoc();\n          final boolean hasDeletions = reader.hasDeletions();\n          for (int j = 0; j < maxDoc;) {\n            if (!hasDeletions || !reader.isDeleted(j)) { // skip deleted docs\n              if (hasMatchingReader) {\n                // We can optimize this case (doing a bulk\n                // byte copy) since the field numbers are\n                // identical\n                int start = j;\n                int numDocs = 0;\n                do {\n                  j++;\n                  numDocs++;\n                  if (j >= maxDoc)\n                    break;\n                  if (hasDeletions && matchingSegmentReader.isDeleted(j)) {\n                    j++;\n                    break;\n                  }\n                } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n                IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n                fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n                docCount += numDocs;\n                if (checkAbort != null)\n                  checkAbort.work(300*numDocs);\n              } else {\n                fieldsWriter.addDocument(reader.document(j, fieldSelectorMerge));\n                j++;\n                docCount++;\n                if (checkAbort != null)\n                  checkAbort.work(300);\n              }\n            } else\n              j++;\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final long fdxFileLength = directory.fileLength(segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION);\n\n      if (4+docCount*8 != fdxFileLength)\n        // This is most like a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \"; now aborting this merge to prevent index corruption\");\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (int i = 0; i < readers.size(); i++)\n        docCount += ((IndexReader) readers.elementAt(i)).numDocs();\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, that means\n      // all segments were written as part of a single\n      // autoCommit=false IndexWriter session, so their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.elementAt(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.elementAt(i);\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        for (int j = 0; j < segmentReader.getFieldInfos().size(); j++) {\n          FieldInfo fi = segmentReader.getFieldInfos().fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector, fi.storePositionWithTermVector, fi.storeOffsetWithTermVector, !reader.hasNorms(fi.name), fi.storePayloads);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.STORES_PAYLOADS), false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n\n      // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n      // in  merge mode, we use this FieldSelector\n      FieldSelector fieldSelectorMerge = new FieldSelector() {\n          public FieldSelectorResult accept(String fieldName) {\n            return FieldSelectorResult.LOAD_FOR_MERGE;\n          }        \n        };\n\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        for (int i = 0; i < readers.size(); i++) {\n          final IndexReader reader = (IndexReader) readers.elementAt(i);\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[i];\n          final FieldsReader matchingFieldsReader;\n          final boolean hasMatchingReader;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null && !fieldsReader.canReadRawDocs()) {            \n              matchingFieldsReader = null;\n              hasMatchingReader = false;\n            } else {\n              matchingFieldsReader = fieldsReader;\n              hasMatchingReader = true;\n            }\n          } else {\n            hasMatchingReader = false;\n            matchingFieldsReader = null;\n          }\n          final int maxDoc = reader.maxDoc();\n          final boolean hasDeletions = reader.hasDeletions();\n          for (int j = 0; j < maxDoc;) {\n            if (!hasDeletions || !reader.isDeleted(j)) { // skip deleted docs\n              if (hasMatchingReader) {\n                // We can optimize this case (doing a bulk\n                // byte copy) since the field numbers are\n                // identical\n                int start = j;\n                int numDocs = 0;\n                do {\n                  j++;\n                  numDocs++;\n                  if (j >= maxDoc)\n                    break;\n                  if (hasDeletions && matchingSegmentReader.isDeleted(j)) {\n                    j++;\n                    break;\n                  }\n                } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n                IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n                fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n                docCount += numDocs;\n                if (checkAbort != null)\n                  checkAbort.work(300*numDocs);\n              } else {\n                fieldsWriter.addDocument(reader.document(j, fieldSelectorMerge));\n                j++;\n                docCount++;\n                if (checkAbort != null)\n                  checkAbort.work(300);\n              }\n            } else\n              j++;\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      assert 4+docCount*8 == directory.fileLength(segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION) :\n        \"after mergeFields: fdx size mismatch: \" + docCount + \" docs vs \" + directory.fileLength(segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION) + \" length in bytes of \" + segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION;\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (int i = 0; i < readers.size(); i++)\n        docCount += ((IndexReader) readers.elementAt(i)).numDocs();\n\n    return docCount;\n  }\n\n","bugFix":["dbb7b6f10bff9eedd5c9bc6cf9222ffbb2df74d4","da3e8c2fef4ea558379c4c0879b3bcdecde41bcd"],"bugIntro":["a3a7f858bb80cdcedf1e39222961d53f11e1f6ed"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4630870c8c0e172c76b902376619806cc2ec5e38","date":1211408056,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, that means\n      // all segments were written as part of a single\n      // autoCommit=false IndexWriter session, so their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.elementAt(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.elementAt(i);\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        for (int j = 0; j < segmentReader.getFieldInfos().size(); j++) {\n          FieldInfo fi = segmentReader.getFieldInfos().fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector, fi.storePositionWithTermVector, fi.storeOffsetWithTermVector, !reader.hasNorms(fi.name), fi.storePayloads);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.STORES_PAYLOADS), false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n\n      // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n      // in  merge mode, we use this FieldSelector\n      FieldSelector fieldSelectorMerge = new FieldSelector() {\n          public FieldSelectorResult accept(String fieldName) {\n            return FieldSelectorResult.LOAD_FOR_MERGE;\n          }        \n        };\n\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        for (int i = 0; i < readers.size(); i++) {\n          final IndexReader reader = (IndexReader) readers.elementAt(i);\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[i];\n          final FieldsReader matchingFieldsReader;\n          final boolean hasMatchingReader;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null && !fieldsReader.canReadRawDocs()) {            \n              matchingFieldsReader = null;\n              hasMatchingReader = false;\n            } else {\n              matchingFieldsReader = fieldsReader;\n              hasMatchingReader = true;\n            }\n          } else {\n            hasMatchingReader = false;\n            matchingFieldsReader = null;\n          }\n          final int maxDoc = reader.maxDoc();\n          final boolean hasDeletions = reader.hasDeletions();\n          for (int j = 0; j < maxDoc;) {\n            if (!hasDeletions || !reader.isDeleted(j)) { // skip deleted docs\n              if (hasMatchingReader) {\n                // We can optimize this case (doing a bulk\n                // byte copy) since the field numbers are\n                // identical\n                int start = j;\n                int numDocs = 0;\n                do {\n                  j++;\n                  numDocs++;\n                  if (j >= maxDoc)\n                    break;\n                  if (hasDeletions && matchingSegmentReader.isDeleted(j)) {\n                    j++;\n                    break;\n                  }\n                } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n                IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n                fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n                docCount += numDocs;\n                if (checkAbort != null)\n                  checkAbort.work(300*numDocs);\n              } else {\n                // NOTE: it's very important to first assign\n                // to doc then pass it to\n                // termVectorsWriter.addAllDocVectors; see\n                // LUCENE-1282\n                Document doc = reader.document(j, fieldSelectorMerge);\n                fieldsWriter.addDocument(doc);\n                j++;\n                docCount++;\n                if (checkAbort != null)\n                  checkAbort.work(300);\n              }\n            } else\n              j++;\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final long fdxFileLength = directory.fileLength(segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION);\n\n      if (4+docCount*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \"; now aborting this merge to prevent index corruption\");\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (int i = 0; i < readers.size(); i++)\n        docCount += ((IndexReader) readers.elementAt(i)).numDocs();\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, that means\n      // all segments were written as part of a single\n      // autoCommit=false IndexWriter session, so their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.elementAt(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.elementAt(i);\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        for (int j = 0; j < segmentReader.getFieldInfos().size(); j++) {\n          FieldInfo fi = segmentReader.getFieldInfos().fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector, fi.storePositionWithTermVector, fi.storeOffsetWithTermVector, !reader.hasNorms(fi.name), fi.storePayloads);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.STORES_PAYLOADS), false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n\n      // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n      // in  merge mode, we use this FieldSelector\n      FieldSelector fieldSelectorMerge = new FieldSelector() {\n          public FieldSelectorResult accept(String fieldName) {\n            return FieldSelectorResult.LOAD_FOR_MERGE;\n          }        \n        };\n\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        for (int i = 0; i < readers.size(); i++) {\n          final IndexReader reader = (IndexReader) readers.elementAt(i);\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[i];\n          final FieldsReader matchingFieldsReader;\n          final boolean hasMatchingReader;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null && !fieldsReader.canReadRawDocs()) {            \n              matchingFieldsReader = null;\n              hasMatchingReader = false;\n            } else {\n              matchingFieldsReader = fieldsReader;\n              hasMatchingReader = true;\n            }\n          } else {\n            hasMatchingReader = false;\n            matchingFieldsReader = null;\n          }\n          final int maxDoc = reader.maxDoc();\n          final boolean hasDeletions = reader.hasDeletions();\n          for (int j = 0; j < maxDoc;) {\n            if (!hasDeletions || !reader.isDeleted(j)) { // skip deleted docs\n              if (hasMatchingReader) {\n                // We can optimize this case (doing a bulk\n                // byte copy) since the field numbers are\n                // identical\n                int start = j;\n                int numDocs = 0;\n                do {\n                  j++;\n                  numDocs++;\n                  if (j >= maxDoc)\n                    break;\n                  if (hasDeletions && matchingSegmentReader.isDeleted(j)) {\n                    j++;\n                    break;\n                  }\n                } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n                IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n                fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n                docCount += numDocs;\n                if (checkAbort != null)\n                  checkAbort.work(300*numDocs);\n              } else {\n                fieldsWriter.addDocument(reader.document(j, fieldSelectorMerge));\n                j++;\n                docCount++;\n                if (checkAbort != null)\n                  checkAbort.work(300);\n              }\n            } else\n              j++;\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final long fdxFileLength = directory.fileLength(segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION);\n\n      if (4+docCount*8 != fdxFileLength)\n        // This is most like a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \"; now aborting this merge to prevent index corruption\");\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (int i = 0; i < readers.size(); i++)\n        docCount += ((IndexReader) readers.elementAt(i)).numDocs();\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"902ba79f4590a41c663c447756d2e5041cbbdda9","date":1217956662,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, that means\n      // all segments were written as part of a single\n      // autoCommit=false IndexWriter session, so their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.elementAt(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.elementAt(i);\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        for (int j = 0; j < segmentReader.getFieldInfos().size(); j++) {\n          FieldInfo fi = segmentReader.getFieldInfos().fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector, fi.storePositionWithTermVector, fi.storeOffsetWithTermVector, !reader.hasNorms(fi.name), fi.storePayloads, fi.omitTf);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.OMIT_TF), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n\n      // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n      // in  merge mode, we use this FieldSelector\n      FieldSelector fieldSelectorMerge = new FieldSelector() {\n          public FieldSelectorResult accept(String fieldName) {\n            return FieldSelectorResult.LOAD_FOR_MERGE;\n          }        \n        };\n\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        for (int i = 0; i < readers.size(); i++) {\n          final IndexReader reader = (IndexReader) readers.elementAt(i);\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[i];\n          final FieldsReader matchingFieldsReader;\n          final boolean hasMatchingReader;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null && !fieldsReader.canReadRawDocs()) {            \n              matchingFieldsReader = null;\n              hasMatchingReader = false;\n            } else {\n              matchingFieldsReader = fieldsReader;\n              hasMatchingReader = true;\n            }\n          } else {\n            hasMatchingReader = false;\n            matchingFieldsReader = null;\n          }\n          final int maxDoc = reader.maxDoc();\n          final boolean hasDeletions = reader.hasDeletions();\n          for (int j = 0; j < maxDoc;) {\n            if (!hasDeletions || !reader.isDeleted(j)) { // skip deleted docs\n              if (hasMatchingReader) {\n                // We can optimize this case (doing a bulk\n                // byte copy) since the field numbers are\n                // identical\n                int start = j;\n                int numDocs = 0;\n                do {\n                  j++;\n                  numDocs++;\n                  if (j >= maxDoc)\n                    break;\n                  if (hasDeletions && matchingSegmentReader.isDeleted(j)) {\n                    j++;\n                    break;\n                  }\n                } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n                IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n                fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n                docCount += numDocs;\n                if (checkAbort != null)\n                  checkAbort.work(300*numDocs);\n              } else {\n                // NOTE: it's very important to first assign\n                // to doc then pass it to\n                // termVectorsWriter.addAllDocVectors; see\n                // LUCENE-1282\n                Document doc = reader.document(j, fieldSelectorMerge);\n                fieldsWriter.addDocument(doc);\n                j++;\n                docCount++;\n                if (checkAbort != null)\n                  checkAbort.work(300);\n              }\n            } else\n              j++;\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final long fdxFileLength = directory.fileLength(segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION);\n\n      if (4+docCount*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \"; now aborting this merge to prevent index corruption\");\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (int i = 0; i < readers.size(); i++)\n        docCount += ((IndexReader) readers.elementAt(i)).numDocs();\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, that means\n      // all segments were written as part of a single\n      // autoCommit=false IndexWriter session, so their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.elementAt(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.elementAt(i);\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        for (int j = 0; j < segmentReader.getFieldInfos().size(); j++) {\n          FieldInfo fi = segmentReader.getFieldInfos().fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector, fi.storePositionWithTermVector, fi.storeOffsetWithTermVector, !reader.hasNorms(fi.name), fi.storePayloads);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.STORES_PAYLOADS), false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n\n      // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n      // in  merge mode, we use this FieldSelector\n      FieldSelector fieldSelectorMerge = new FieldSelector() {\n          public FieldSelectorResult accept(String fieldName) {\n            return FieldSelectorResult.LOAD_FOR_MERGE;\n          }        \n        };\n\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        for (int i = 0; i < readers.size(); i++) {\n          final IndexReader reader = (IndexReader) readers.elementAt(i);\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[i];\n          final FieldsReader matchingFieldsReader;\n          final boolean hasMatchingReader;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null && !fieldsReader.canReadRawDocs()) {            \n              matchingFieldsReader = null;\n              hasMatchingReader = false;\n            } else {\n              matchingFieldsReader = fieldsReader;\n              hasMatchingReader = true;\n            }\n          } else {\n            hasMatchingReader = false;\n            matchingFieldsReader = null;\n          }\n          final int maxDoc = reader.maxDoc();\n          final boolean hasDeletions = reader.hasDeletions();\n          for (int j = 0; j < maxDoc;) {\n            if (!hasDeletions || !reader.isDeleted(j)) { // skip deleted docs\n              if (hasMatchingReader) {\n                // We can optimize this case (doing a bulk\n                // byte copy) since the field numbers are\n                // identical\n                int start = j;\n                int numDocs = 0;\n                do {\n                  j++;\n                  numDocs++;\n                  if (j >= maxDoc)\n                    break;\n                  if (hasDeletions && matchingSegmentReader.isDeleted(j)) {\n                    j++;\n                    break;\n                  }\n                } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n                IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n                fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n                docCount += numDocs;\n                if (checkAbort != null)\n                  checkAbort.work(300*numDocs);\n              } else {\n                // NOTE: it's very important to first assign\n                // to doc then pass it to\n                // termVectorsWriter.addAllDocVectors; see\n                // LUCENE-1282\n                Document doc = reader.document(j, fieldSelectorMerge);\n                fieldsWriter.addDocument(doc);\n                j++;\n                docCount++;\n                if (checkAbort != null)\n                  checkAbort.work(300);\n              }\n            } else\n              j++;\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final long fdxFileLength = directory.fileLength(segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION);\n\n      if (4+docCount*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \"; now aborting this merge to prevent index corruption\");\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (int i = 0; i < readers.size(); i++)\n        docCount += ((IndexReader) readers.elementAt(i)).numDocs();\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c7b6cdc70e097da94da79a655ed8f94477ff69f5","date":1220815360,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, that means\n      // all segments were written as part of a single\n      // autoCommit=false IndexWriter session, so their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.get(i);\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        for (int j = 0; j < segmentReader.getFieldInfos().size(); j++) {\n          FieldInfo fi = segmentReader.getFieldInfos().fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector, fi.storePositionWithTermVector, fi.storeOffsetWithTermVector, !reader.hasNorms(fi.name), fi.storePayloads, fi.omitTf);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.OMIT_TF), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n\n      // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n      // in  merge mode, we use this FieldSelector\n      FieldSelector fieldSelectorMerge = new FieldSelector() {\n          public FieldSelectorResult accept(String fieldName) {\n            return FieldSelectorResult.LOAD_FOR_MERGE;\n          }        \n        };\n\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        for (int i = 0; i < readers.size(); i++) {\n          final IndexReader reader = (IndexReader) readers.get(i);\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[i];\n          final FieldsReader matchingFieldsReader;\n          final boolean hasMatchingReader;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null && !fieldsReader.canReadRawDocs()) {            \n              matchingFieldsReader = null;\n              hasMatchingReader = false;\n            } else {\n              matchingFieldsReader = fieldsReader;\n              hasMatchingReader = true;\n            }\n          } else {\n            hasMatchingReader = false;\n            matchingFieldsReader = null;\n          }\n          final int maxDoc = reader.maxDoc();\n          final boolean hasDeletions = reader.hasDeletions();\n          for (int j = 0; j < maxDoc;) {\n            if (!hasDeletions || !reader.isDeleted(j)) { // skip deleted docs\n              if (hasMatchingReader) {\n                // We can optimize this case (doing a bulk\n                // byte copy) since the field numbers are\n                // identical\n                int start = j;\n                int numDocs = 0;\n                do {\n                  j++;\n                  numDocs++;\n                  if (j >= maxDoc)\n                    break;\n                  if (hasDeletions && matchingSegmentReader.isDeleted(j)) {\n                    j++;\n                    break;\n                  }\n                } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n                IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n                fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n                docCount += numDocs;\n                if (checkAbort != null)\n                  checkAbort.work(300*numDocs);\n              } else {\n                // NOTE: it's very important to first assign\n                // to doc then pass it to\n                // termVectorsWriter.addAllDocVectors; see\n                // LUCENE-1282\n                Document doc = reader.document(j, fieldSelectorMerge);\n                fieldsWriter.addDocument(doc);\n                j++;\n                docCount++;\n                if (checkAbort != null)\n                  checkAbort.work(300);\n              }\n            } else\n              j++;\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final long fdxFileLength = directory.fileLength(segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION);\n\n      if (4+docCount*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \"; now aborting this merge to prevent index corruption\");\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (int i = 0; i < readers.size(); i++)\n        docCount += ((IndexReader) readers.get(i)).numDocs();\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, that means\n      // all segments were written as part of a single\n      // autoCommit=false IndexWriter session, so their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.elementAt(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.elementAt(i);\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        for (int j = 0; j < segmentReader.getFieldInfos().size(); j++) {\n          FieldInfo fi = segmentReader.getFieldInfos().fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector, fi.storePositionWithTermVector, fi.storeOffsetWithTermVector, !reader.hasNorms(fi.name), fi.storePayloads, fi.omitTf);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.OMIT_TF), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n\n      // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n      // in  merge mode, we use this FieldSelector\n      FieldSelector fieldSelectorMerge = new FieldSelector() {\n          public FieldSelectorResult accept(String fieldName) {\n            return FieldSelectorResult.LOAD_FOR_MERGE;\n          }        \n        };\n\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        for (int i = 0; i < readers.size(); i++) {\n          final IndexReader reader = (IndexReader) readers.elementAt(i);\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[i];\n          final FieldsReader matchingFieldsReader;\n          final boolean hasMatchingReader;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null && !fieldsReader.canReadRawDocs()) {            \n              matchingFieldsReader = null;\n              hasMatchingReader = false;\n            } else {\n              matchingFieldsReader = fieldsReader;\n              hasMatchingReader = true;\n            }\n          } else {\n            hasMatchingReader = false;\n            matchingFieldsReader = null;\n          }\n          final int maxDoc = reader.maxDoc();\n          final boolean hasDeletions = reader.hasDeletions();\n          for (int j = 0; j < maxDoc;) {\n            if (!hasDeletions || !reader.isDeleted(j)) { // skip deleted docs\n              if (hasMatchingReader) {\n                // We can optimize this case (doing a bulk\n                // byte copy) since the field numbers are\n                // identical\n                int start = j;\n                int numDocs = 0;\n                do {\n                  j++;\n                  numDocs++;\n                  if (j >= maxDoc)\n                    break;\n                  if (hasDeletions && matchingSegmentReader.isDeleted(j)) {\n                    j++;\n                    break;\n                  }\n                } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n                IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n                fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n                docCount += numDocs;\n                if (checkAbort != null)\n                  checkAbort.work(300*numDocs);\n              } else {\n                // NOTE: it's very important to first assign\n                // to doc then pass it to\n                // termVectorsWriter.addAllDocVectors; see\n                // LUCENE-1282\n                Document doc = reader.document(j, fieldSelectorMerge);\n                fieldsWriter.addDocument(doc);\n                j++;\n                docCount++;\n                if (checkAbort != null)\n                  checkAbort.work(300);\n              }\n            } else\n              j++;\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final long fdxFileLength = directory.fileLength(segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION);\n\n      if (4+docCount*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \"; now aborting this merge to prevent index corruption\");\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (int i = 0; i < readers.size(); i++)\n        docCount += ((IndexReader) readers.elementAt(i)).numDocs();\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a3a7f858bb80cdcedf1e39222961d53f11e1f6ed","date":1231945797,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, that means\n      // all segments were written as part of a single\n      // autoCommit=false IndexWriter session, so their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.get(i);\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        for (int j = 0; j < segmentReader.getFieldInfos().size(); j++) {\n          FieldInfo fi = segmentReader.getFieldInfos().fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector, fi.storePositionWithTermVector, fi.storeOffsetWithTermVector, !reader.hasNorms(fi.name), fi.storePayloads, fi.omitTf);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.OMIT_TF), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n\n      // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n      // in  merge mode, we use this FieldSelector\n      FieldSelector fieldSelectorMerge = new FieldSelector() {\n          public FieldSelectorResult accept(String fieldName) {\n            return FieldSelectorResult.LOAD_FOR_MERGE;\n          }        \n        };\n\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        for (int i = 0; i < readers.size(); i++) {\n          final IndexReader reader = (IndexReader) readers.get(i);\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[i];\n          final FieldsReader matchingFieldsReader;\n          final boolean hasMatchingReader;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null && !fieldsReader.canReadRawDocs()) {            \n              matchingFieldsReader = null;\n              hasMatchingReader = false;\n            } else {\n              matchingFieldsReader = fieldsReader;\n              hasMatchingReader = true;\n            }\n          } else {\n            hasMatchingReader = false;\n            matchingFieldsReader = null;\n          }\n          final int maxDoc = reader.maxDoc();\n          final boolean hasDeletions = reader.hasDeletions();\n          for (int j = 0; j < maxDoc;) {\n            if (!hasDeletions || !reader.isDeleted(j)) { // skip deleted docs\n              if (hasMatchingReader) {\n                // We can optimize this case (doing a bulk\n                // byte copy) since the field numbers are\n                // identical\n                int start = j;\n                int numDocs = 0;\n                do {\n                  j++;\n                  numDocs++;\n                  if (j >= maxDoc)\n                    break;\n                  if (hasDeletions && matchingSegmentReader.isDeleted(j)) {\n                    j++;\n                    break;\n                  }\n                } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n                IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n                fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n                docCount += numDocs;\n                if (checkAbort != null)\n                  checkAbort.work(300*numDocs);\n              } else {\n                // NOTE: it's very important to first assign\n                // to doc then pass it to\n                // termVectorsWriter.addAllDocVectors; see\n                // LUCENE-1282\n                Document doc = reader.document(j, fieldSelectorMerge);\n                fieldsWriter.addDocument(doc);\n                j++;\n                docCount++;\n                if (checkAbort != null)\n                  checkAbort.work(300);\n              }\n            } else\n              j++;\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final long fdxFileLength = directory.fileLength(segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \"; now aborting this merge to prevent index corruption\");\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (int i = 0; i < readers.size(); i++)\n        docCount += ((IndexReader) readers.get(i)).numDocs();\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, that means\n      // all segments were written as part of a single\n      // autoCommit=false IndexWriter session, so their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.get(i);\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        for (int j = 0; j < segmentReader.getFieldInfos().size(); j++) {\n          FieldInfo fi = segmentReader.getFieldInfos().fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector, fi.storePositionWithTermVector, fi.storeOffsetWithTermVector, !reader.hasNorms(fi.name), fi.storePayloads, fi.omitTf);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.OMIT_TF), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n\n      // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n      // in  merge mode, we use this FieldSelector\n      FieldSelector fieldSelectorMerge = new FieldSelector() {\n          public FieldSelectorResult accept(String fieldName) {\n            return FieldSelectorResult.LOAD_FOR_MERGE;\n          }        \n        };\n\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        for (int i = 0; i < readers.size(); i++) {\n          final IndexReader reader = (IndexReader) readers.get(i);\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[i];\n          final FieldsReader matchingFieldsReader;\n          final boolean hasMatchingReader;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null && !fieldsReader.canReadRawDocs()) {            \n              matchingFieldsReader = null;\n              hasMatchingReader = false;\n            } else {\n              matchingFieldsReader = fieldsReader;\n              hasMatchingReader = true;\n            }\n          } else {\n            hasMatchingReader = false;\n            matchingFieldsReader = null;\n          }\n          final int maxDoc = reader.maxDoc();\n          final boolean hasDeletions = reader.hasDeletions();\n          for (int j = 0; j < maxDoc;) {\n            if (!hasDeletions || !reader.isDeleted(j)) { // skip deleted docs\n              if (hasMatchingReader) {\n                // We can optimize this case (doing a bulk\n                // byte copy) since the field numbers are\n                // identical\n                int start = j;\n                int numDocs = 0;\n                do {\n                  j++;\n                  numDocs++;\n                  if (j >= maxDoc)\n                    break;\n                  if (hasDeletions && matchingSegmentReader.isDeleted(j)) {\n                    j++;\n                    break;\n                  }\n                } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n                IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n                fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n                docCount += numDocs;\n                if (checkAbort != null)\n                  checkAbort.work(300*numDocs);\n              } else {\n                // NOTE: it's very important to first assign\n                // to doc then pass it to\n                // termVectorsWriter.addAllDocVectors; see\n                // LUCENE-1282\n                Document doc = reader.document(j, fieldSelectorMerge);\n                fieldsWriter.addDocument(doc);\n                j++;\n                docCount++;\n                if (checkAbort != null)\n                  checkAbort.work(300);\n              }\n            } else\n              j++;\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final long fdxFileLength = directory.fileLength(segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION);\n\n      if (4+docCount*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \"; now aborting this merge to prevent index corruption\");\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (int i = 0; i < readers.size(); i++)\n        docCount += ((IndexReader) readers.get(i)).numDocs();\n\n    return docCount;\n  }\n\n","bugFix":["33bd4a1f823fcca0940fb58c35873adeab11c761"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"066b6ff5a08e35c3b6880e7c3ddda79526acdab1","date":1237569961,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, that means\n      // all segments were written as part of a single\n      // autoCommit=false IndexWriter session, so their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.get(i);\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        for (int j = 0; j < segmentReader.getFieldInfos().size(); j++) {\n          FieldInfo fi = segmentReader.getFieldInfos().fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector, fi.storePositionWithTermVector, fi.storeOffsetWithTermVector, !reader.hasNorms(fi.name), fi.storePayloads, fi.omitTermFreqAndPositions);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.OMIT_TF), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n\n      // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n      // in  merge mode, we use this FieldSelector\n      FieldSelector fieldSelectorMerge = new FieldSelector() {\n          public FieldSelectorResult accept(String fieldName) {\n            return FieldSelectorResult.LOAD_FOR_MERGE;\n          }        \n        };\n\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        for (int i = 0; i < readers.size(); i++) {\n          final IndexReader reader = (IndexReader) readers.get(i);\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[i];\n          final FieldsReader matchingFieldsReader;\n          final boolean hasMatchingReader;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null && !fieldsReader.canReadRawDocs()) {            \n              matchingFieldsReader = null;\n              hasMatchingReader = false;\n            } else {\n              matchingFieldsReader = fieldsReader;\n              hasMatchingReader = true;\n            }\n          } else {\n            hasMatchingReader = false;\n            matchingFieldsReader = null;\n          }\n          final int maxDoc = reader.maxDoc();\n          final boolean hasDeletions = reader.hasDeletions();\n          for (int j = 0; j < maxDoc;) {\n            if (!hasDeletions || !reader.isDeleted(j)) { // skip deleted docs\n              if (hasMatchingReader) {\n                // We can optimize this case (doing a bulk\n                // byte copy) since the field numbers are\n                // identical\n                int start = j;\n                int numDocs = 0;\n                do {\n                  j++;\n                  numDocs++;\n                  if (j >= maxDoc)\n                    break;\n                  if (hasDeletions && matchingSegmentReader.isDeleted(j)) {\n                    j++;\n                    break;\n                  }\n                } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n                IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n                fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n                docCount += numDocs;\n                if (checkAbort != null)\n                  checkAbort.work(300*numDocs);\n              } else {\n                // NOTE: it's very important to first assign\n                // to doc then pass it to\n                // termVectorsWriter.addAllDocVectors; see\n                // LUCENE-1282\n                Document doc = reader.document(j, fieldSelectorMerge);\n                fieldsWriter.addDocument(doc);\n                j++;\n                docCount++;\n                if (checkAbort != null)\n                  checkAbort.work(300);\n              }\n            } else\n              j++;\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final long fdxFileLength = directory.fileLength(segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \"; now aborting this merge to prevent index corruption\");\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (int i = 0; i < readers.size(); i++)\n        docCount += ((IndexReader) readers.get(i)).numDocs();\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, that means\n      // all segments were written as part of a single\n      // autoCommit=false IndexWriter session, so their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.get(i);\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        for (int j = 0; j < segmentReader.getFieldInfos().size(); j++) {\n          FieldInfo fi = segmentReader.getFieldInfos().fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector, fi.storePositionWithTermVector, fi.storeOffsetWithTermVector, !reader.hasNorms(fi.name), fi.storePayloads, fi.omitTf);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.OMIT_TF), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n\n      // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n      // in  merge mode, we use this FieldSelector\n      FieldSelector fieldSelectorMerge = new FieldSelector() {\n          public FieldSelectorResult accept(String fieldName) {\n            return FieldSelectorResult.LOAD_FOR_MERGE;\n          }        \n        };\n\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        for (int i = 0; i < readers.size(); i++) {\n          final IndexReader reader = (IndexReader) readers.get(i);\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[i];\n          final FieldsReader matchingFieldsReader;\n          final boolean hasMatchingReader;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null && !fieldsReader.canReadRawDocs()) {            \n              matchingFieldsReader = null;\n              hasMatchingReader = false;\n            } else {\n              matchingFieldsReader = fieldsReader;\n              hasMatchingReader = true;\n            }\n          } else {\n            hasMatchingReader = false;\n            matchingFieldsReader = null;\n          }\n          final int maxDoc = reader.maxDoc();\n          final boolean hasDeletions = reader.hasDeletions();\n          for (int j = 0; j < maxDoc;) {\n            if (!hasDeletions || !reader.isDeleted(j)) { // skip deleted docs\n              if (hasMatchingReader) {\n                // We can optimize this case (doing a bulk\n                // byte copy) since the field numbers are\n                // identical\n                int start = j;\n                int numDocs = 0;\n                do {\n                  j++;\n                  numDocs++;\n                  if (j >= maxDoc)\n                    break;\n                  if (hasDeletions && matchingSegmentReader.isDeleted(j)) {\n                    j++;\n                    break;\n                  }\n                } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n                IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n                fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n                docCount += numDocs;\n                if (checkAbort != null)\n                  checkAbort.work(300*numDocs);\n              } else {\n                // NOTE: it's very important to first assign\n                // to doc then pass it to\n                // termVectorsWriter.addAllDocVectors; see\n                // LUCENE-1282\n                Document doc = reader.document(j, fieldSelectorMerge);\n                fieldsWriter.addDocument(doc);\n                j++;\n                docCount++;\n                if (checkAbort != null)\n                  checkAbort.work(300);\n              }\n            } else\n              j++;\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final long fdxFileLength = directory.fileLength(segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \"; now aborting this merge to prevent index corruption\");\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (int i = 0; i < readers.size(); i++)\n        docCount += ((IndexReader) readers.get(i)).numDocs();\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d736930237c54e1516a9e3bae803c92ff19ec4e5","date":1245789156,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, that means\n      // all segments were written as part of a single\n      // autoCommit=false IndexWriter session, so their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (Iterator iter = readers.iterator(); iter.hasNext();) {\n      IndexReader reader = (IndexReader) iter.next();\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.getFieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n              fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n              !reader.hasNorms(fi.name), fi.storePayloads,\n              fi.omitTermFreqAndPositions);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n\n      // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n      // in  merge mode, we use this FieldSelector\n      FieldSelector fieldSelectorMerge = new FieldSelector() {\n          public FieldSelectorResult accept(String fieldName) {\n            return FieldSelectorResult.LOAD_FOR_MERGE;\n          }        \n        };\n\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (Iterator iter = readers.iterator(); iter.hasNext();) {\n          final IndexReader reader = (IndexReader) iter.next();\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null && fieldsReader.canReadRawDocs()) {            \n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldSelectorMerge, fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldSelectorMerge, fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final long fdxFileLength = directory.fileLength(segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \"; now aborting this merge to prevent index corruption\");\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (Iterator iter = readers.iterator(); iter.hasNext();) {\n        docCount += ((IndexReader) iter.next()).numDocs();\n      }\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, that means\n      // all segments were written as part of a single\n      // autoCommit=false IndexWriter session, so their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (int i = 0; i < readers.size(); i++) {\n      IndexReader reader = (IndexReader) readers.get(i);\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        for (int j = 0; j < segmentReader.getFieldInfos().size(); j++) {\n          FieldInfo fi = segmentReader.getFieldInfos().fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector, fi.storePositionWithTermVector, fi.storeOffsetWithTermVector, !reader.hasNorms(fi.name), fi.storePayloads, fi.omitTermFreqAndPositions);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.OMIT_TF), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n\n      // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n      // in  merge mode, we use this FieldSelector\n      FieldSelector fieldSelectorMerge = new FieldSelector() {\n          public FieldSelectorResult accept(String fieldName) {\n            return FieldSelectorResult.LOAD_FOR_MERGE;\n          }        \n        };\n\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        for (int i = 0; i < readers.size(); i++) {\n          final IndexReader reader = (IndexReader) readers.get(i);\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[i];\n          final FieldsReader matchingFieldsReader;\n          final boolean hasMatchingReader;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null && !fieldsReader.canReadRawDocs()) {            \n              matchingFieldsReader = null;\n              hasMatchingReader = false;\n            } else {\n              matchingFieldsReader = fieldsReader;\n              hasMatchingReader = true;\n            }\n          } else {\n            hasMatchingReader = false;\n            matchingFieldsReader = null;\n          }\n          final int maxDoc = reader.maxDoc();\n          final boolean hasDeletions = reader.hasDeletions();\n          for (int j = 0; j < maxDoc;) {\n            if (!hasDeletions || !reader.isDeleted(j)) { // skip deleted docs\n              if (hasMatchingReader) {\n                // We can optimize this case (doing a bulk\n                // byte copy) since the field numbers are\n                // identical\n                int start = j;\n                int numDocs = 0;\n                do {\n                  j++;\n                  numDocs++;\n                  if (j >= maxDoc)\n                    break;\n                  if (hasDeletions && matchingSegmentReader.isDeleted(j)) {\n                    j++;\n                    break;\n                  }\n                } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n                IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n                fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n                docCount += numDocs;\n                if (checkAbort != null)\n                  checkAbort.work(300*numDocs);\n              } else {\n                // NOTE: it's very important to first assign\n                // to doc then pass it to\n                // termVectorsWriter.addAllDocVectors; see\n                // LUCENE-1282\n                Document doc = reader.document(j, fieldSelectorMerge);\n                fieldsWriter.addDocument(doc);\n                j++;\n                docCount++;\n                if (checkAbort != null)\n                  checkAbort.work(300);\n              }\n            } else\n              j++;\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final long fdxFileLength = directory.fileLength(segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \"; now aborting this merge to prevent index corruption\");\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (int i = 0; i < readers.size(); i++)\n        docCount += ((IndexReader) readers.get(i)).numDocs();\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"66f3dadb253a44f4cccc81c8a21b685b18b201fb","date":1247245699,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, that means\n      // all segments were written as part of a single\n      // autoCommit=false IndexWriter session, so their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (Iterator iter = readers.iterator(); iter.hasNext();) {\n      IndexReader reader = (IndexReader) iter.next();\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n              fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n              !reader.hasNorms(fi.name), fi.storePayloads,\n              fi.omitTermFreqAndPositions);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n\n      // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n      // in  merge mode, we use this FieldSelector\n      FieldSelector fieldSelectorMerge = new FieldSelector() {\n          public FieldSelectorResult accept(String fieldName) {\n            return FieldSelectorResult.LOAD_FOR_MERGE;\n          }        \n        };\n\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (Iterator iter = readers.iterator(); iter.hasNext();) {\n          final IndexReader reader = (IndexReader) iter.next();\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null && fieldsReader.canReadRawDocs()) {            \n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldSelectorMerge, fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldSelectorMerge, fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final long fdxFileLength = directory.fileLength(segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \"; now aborting this merge to prevent index corruption\");\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (Iterator iter = readers.iterator(); iter.hasNext();) {\n        docCount += ((IndexReader) iter.next()).numDocs();\n      }\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, that means\n      // all segments were written as part of a single\n      // autoCommit=false IndexWriter session, so their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (Iterator iter = readers.iterator(); iter.hasNext();) {\n      IndexReader reader = (IndexReader) iter.next();\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.getFieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n              fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n              !reader.hasNorms(fi.name), fi.storePayloads,\n              fi.omitTermFreqAndPositions);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n\n      // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n      // in  merge mode, we use this FieldSelector\n      FieldSelector fieldSelectorMerge = new FieldSelector() {\n          public FieldSelectorResult accept(String fieldName) {\n            return FieldSelectorResult.LOAD_FOR_MERGE;\n          }        \n        };\n\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (Iterator iter = readers.iterator(); iter.hasNext();) {\n          final IndexReader reader = (IndexReader) iter.next();\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null && fieldsReader.canReadRawDocs()) {            \n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldSelectorMerge, fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldSelectorMerge, fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final long fdxFileLength = directory.fileLength(segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \"; now aborting this merge to prevent index corruption\");\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (Iterator iter = readers.iterator(); iter.hasNext();) {\n        docCount += ((IndexReader) iter.next()).numDocs();\n      }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"55ddd0022d1d26b52ed711e90ab77de2b36130c6","date":1251466592,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, that means\n      // all segments were written as part of a single\n      // autoCommit=false IndexWriter session, so their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (Iterator iter = readers.iterator(); iter.hasNext();) {\n      IndexReader reader = (IndexReader) iter.next();\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n              fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n              !reader.hasNorms(fi.name), fi.storePayloads,\n              fi.omitTermFreqAndPositions);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n\n      // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n      // in  merge mode, we use this FieldSelector\n      FieldSelector fieldSelectorMerge = new FieldSelector() {\n          public FieldSelectorResult accept(String fieldName) {\n            return FieldSelectorResult.LOAD_FOR_MERGE;\n          }        \n        };\n\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (Iterator iter = readers.iterator(); iter.hasNext();) {\n          final IndexReader reader = (IndexReader) iter.next();\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null && fieldsReader.canReadRawDocs()) {            \n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldSelectorMerge, fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldSelectorMerge, fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final String fileName = segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION;\n      final long fdxFileLength = directory.fileLength(fileName);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (Iterator iter = readers.iterator(); iter.hasNext();) {\n        docCount += ((IndexReader) iter.next()).numDocs();\n      }\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, that means\n      // all segments were written as part of a single\n      // autoCommit=false IndexWriter session, so their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (Iterator iter = readers.iterator(); iter.hasNext();) {\n      IndexReader reader = (IndexReader) iter.next();\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n              fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n              !reader.hasNorms(fi.name), fi.storePayloads,\n              fi.omitTermFreqAndPositions);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n\n      // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n      // in  merge mode, we use this FieldSelector\n      FieldSelector fieldSelectorMerge = new FieldSelector() {\n          public FieldSelectorResult accept(String fieldName) {\n            return FieldSelectorResult.LOAD_FOR_MERGE;\n          }        \n        };\n\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (Iterator iter = readers.iterator(); iter.hasNext();) {\n          final IndexReader reader = (IndexReader) iter.next();\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null && fieldsReader.canReadRawDocs()) {            \n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldSelectorMerge, fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldSelectorMerge, fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final long fdxFileLength = directory.fileLength(segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \"; now aborting this merge to prevent index corruption\");\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (Iterator iter = readers.iterator(); iter.hasNext();) {\n        docCount += ((IndexReader) iter.next()).numDocs();\n      }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e0c804f7aa477229414a7e12882af490c241f64d","date":1254963299,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, that means\n      // all segments were written as part of a single\n      // autoCommit=false IndexWriter session, so their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (Iterator iter = readers.iterator(); iter.hasNext();) {\n      IndexReader reader = (IndexReader) iter.next();\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n              fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n              !reader.hasNorms(fi.name), fi.storePayloads,\n              fi.omitTermFreqAndPositions);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (Iterator iter = readers.iterator(); iter.hasNext();) {\n          final IndexReader reader = (IndexReader) iter.next();\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null && fieldsReader.canReadRawDocs()) {            \n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final String fileName = segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION;\n      final long fdxFileLength = directory.fileLength(fileName);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (Iterator iter = readers.iterator(); iter.hasNext();) {\n        docCount += ((IndexReader) iter.next()).numDocs();\n      }\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, that means\n      // all segments were written as part of a single\n      // autoCommit=false IndexWriter session, so their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (Iterator iter = readers.iterator(); iter.hasNext();) {\n      IndexReader reader = (IndexReader) iter.next();\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n              fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n              !reader.hasNorms(fi.name), fi.storePayloads,\n              fi.omitTermFreqAndPositions);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n\n      // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're\n      // in  merge mode, we use this FieldSelector\n      FieldSelector fieldSelectorMerge = new FieldSelector() {\n          public FieldSelectorResult accept(String fieldName) {\n            return FieldSelectorResult.LOAD_FOR_MERGE;\n          }        \n        };\n\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (Iterator iter = readers.iterator(); iter.hasNext();) {\n          final IndexReader reader = (IndexReader) iter.next();\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null && fieldsReader.canReadRawDocs()) {            \n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldSelectorMerge, fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldSelectorMerge, fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final String fileName = segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION;\n      final long fdxFileLength = directory.fileLength(fileName);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (Iterator iter = readers.iterator(); iter.hasNext();) {\n        docCount += ((IndexReader) iter.next()).numDocs();\n      }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"87c966e9308847938a7c905c2e46a56d8df788b8","date":1255035452,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (Iterator iter = readers.iterator(); iter.hasNext();) {\n      IndexReader reader = (IndexReader) iter.next();\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n              fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n              !reader.hasNorms(fi.name), fi.storePayloads,\n              fi.omitTermFreqAndPositions);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (Iterator iter = readers.iterator(); iter.hasNext();) {\n          final IndexReader reader = (IndexReader) iter.next();\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null && fieldsReader.canReadRawDocs()) {            \n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final String fileName = segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION;\n      final long fdxFileLength = directory.fileLength(fileName);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (Iterator iter = readers.iterator(); iter.hasNext();) {\n        docCount += ((IndexReader) iter.next()).numDocs();\n      }\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, that means\n      // all segments were written as part of a single\n      // autoCommit=false IndexWriter session, so their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (Iterator iter = readers.iterator(); iter.hasNext();) {\n      IndexReader reader = (IndexReader) iter.next();\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n              fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n              !reader.hasNorms(fi.name), fi.storePayloads,\n              fi.omitTermFreqAndPositions);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (Iterator iter = readers.iterator(); iter.hasNext();) {\n          final IndexReader reader = (IndexReader) iter.next();\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null && fieldsReader.canReadRawDocs()) {            \n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final String fileName = segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION;\n      final long fdxFileLength = directory.fileLength(fileName);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (Iterator iter = readers.iterator(); iter.hasNext();) {\n        docCount += ((IndexReader) iter.next()).numDocs();\n      }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fa27b750ee9a51ec4bed93ef328aef9ca1e2153d","date":1255859449,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n              fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n              !reader.hasNorms(fi.name), fi.storePayloads,\n              fi.omitTermFreqAndPositions);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (IndexReader reader : readers) {\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null && fieldsReader.canReadRawDocs()) {            \n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final String fileName = segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION;\n      final long fdxFileLength = directory.fileLength(fileName);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (final IndexReader reader : readers) {\n        docCount += reader.numDocs();\n      }\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (Iterator iter = readers.iterator(); iter.hasNext();) {\n      IndexReader reader = (IndexReader) iter.next();\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n              fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n              !reader.hasNorms(fi.name), fi.storePayloads,\n              fi.omitTermFreqAndPositions);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (Iterator iter = readers.iterator(); iter.hasNext();) {\n          final IndexReader reader = (IndexReader) iter.next();\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null && fieldsReader.canReadRawDocs()) {            \n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final String fileName = segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION;\n      final long fdxFileLength = directory.fileLength(fileName);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (Iterator iter = readers.iterator(); iter.hasNext();) {\n        docCount += ((IndexReader) iter.next()).numDocs();\n      }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"775efee7f959e0dd3df7960b93767d9e00b78751","date":1267203159,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n              fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n              !reader.hasNorms(fi.name), fi.storePayloads,\n              fi.omitTermFreqAndPositions);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (IndexReader reader : readers) {\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null && fieldsReader.canReadRawDocs()) {            \n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final String fileName = IndexFileNames.segmentFileName(segment, IndexFileNames.FIELDS_INDEX_EXTENSION);\n      final long fdxFileLength = directory.fileLength(fileName);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (final IndexReader reader : readers) {\n        docCount += reader.numDocs();\n      }\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n              fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n              !reader.hasNorms(fi.name), fi.storePayloads,\n              fi.omitTermFreqAndPositions);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (IndexReader reader : readers) {\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null && fieldsReader.canReadRawDocs()) {            \n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final String fileName = segment + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION;\n      final long fdxFileLength = directory.fileLength(fileName);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (final IndexReader reader : readers) {\n        docCount += reader.numDocs();\n      }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n              fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n              !reader.hasNorms(fi.name), fi.storePayloads,\n              fi.omitTermFreqAndPositions);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (IndexReader reader : readers) {\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null && fieldsReader.canReadRawDocs()) {            \n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final String fileName = IndexFileNames.segmentFileName(segment, IndexFileNames.FIELDS_INDEX_EXTENSION);\n      final long fdxFileLength = directory.fileLength(fileName);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (final IndexReader reader : readers) {\n        docCount += reader.numDocs();\n      }\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n              fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n              !reader.hasNorms(fi.name), fi.storePayloads,\n              fi.omitTermFreqAndPositions);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (IndexReader reader : readers) {\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null && fieldsReader.canReadRawDocs()) {            \n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final String fileName = IndexFileNames.segmentFileName(segment, IndexFileNames.FIELDS_INDEX_EXTENSION);\n      final long fdxFileLength = directory.fileLength(fileName);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (final IndexReader reader : readers) {\n        docCount += reader.numDocs();\n      }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b3d08461c77d39c25ea6ff0cd05b32f948fa2a33":["346d5897e4c4e77ed5dbd31f7730ff30973d5971"],"950f3c7592cb559e2534e5089c78833250e156a3":["6c9ae72f0c7aa48fa47b0c33986b516a2e71d578"],"e7fb6d70db034a5456ae560175dd1b821eea9ff4":["8fb95844e4ba5160067c64c5eb1cd8a09f7a94f1"],"d736930237c54e1516a9e3bae803c92ff19ec4e5":["066b6ff5a08e35c3b6880e7c3ddda79526acdab1"],"902ba79f4590a41c663c447756d2e5041cbbdda9":["4630870c8c0e172c76b902376619806cc2ec5e38"],"c7b6cdc70e097da94da79a655ed8f94477ff69f5":["902ba79f4590a41c663c447756d2e5041cbbdda9"],"84acdfa12c18361ff932244db20470fce117e52d":["da3e8c2fef4ea558379c4c0879b3bcdecde41bcd"],"4350b17bd363cd13a95171b8df1ca62ea4c3e71c":["8522ae207a56c6db28ca06fe6cc33e70911c3600"],"fa27b750ee9a51ec4bed93ef328aef9ca1e2153d":["87c966e9308847938a7c905c2e46a56d8df788b8"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3b248d9b80f6b2fceade80b3c8683d1cca6e4c98":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"6c9ae72f0c7aa48fa47b0c33986b516a2e71d578":["6177f0f28ace66d1538b1e6ac5f1773e5449a0b0"],"87c966e9308847938a7c905c2e46a56d8df788b8":["e0c804f7aa477229414a7e12882af490c241f64d"],"f271ff507bf58806fc089e76e38057c95e391dcd":["3b248d9b80f6b2fceade80b3c8683d1cca6e4c98"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["775efee7f959e0dd3df7960b93767d9e00b78751"],"346d5897e4c4e77ed5dbd31f7730ff30973d5971":["c9a0deca56efc5191d6b3c41047fd538f3fae1d8"],"91109046a59c58ee0ee5d0d2767b08d1f30d6702":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"0c0427a35a68b16ec73d3bf5f27e57719915f166":["950f3c7592cb559e2534e5089c78833250e156a3"],"775efee7f959e0dd3df7960b93767d9e00b78751":["fa27b750ee9a51ec4bed93ef328aef9ca1e2153d"],"8522ae207a56c6db28ca06fe6cc33e70911c3600":["1b54a9bc667895a2095a886184bf69a3179e63df"],"33bd4a1f823fcca0940fb58c35873adeab11c761":["f4ba1ab90d9a427e7f2c7d1e65a3ce5869ed8e5d"],"1b54a9bc667895a2095a886184bf69a3179e63df":["0c0427a35a68b16ec73d3bf5f27e57719915f166"],"dbb7b6f10bff9eedd5c9bc6cf9222ffbb2df74d4":["84acdfa12c18361ff932244db20470fce117e52d"],"8fb95844e4ba5160067c64c5eb1cd8a09f7a94f1":["91109046a59c58ee0ee5d0d2767b08d1f30d6702"],"6177f0f28ace66d1538b1e6ac5f1773e5449a0b0":["770281b8a8459cafcdd2354b6a06078fea2d83c9"],"a3a7f858bb80cdcedf1e39222961d53f11e1f6ed":["c7b6cdc70e097da94da79a655ed8f94477ff69f5"],"66f3dadb253a44f4cccc81c8a21b685b18b201fb":["d736930237c54e1516a9e3bae803c92ff19ec4e5"],"770281b8a8459cafcdd2354b6a06078fea2d83c9":["f22ffa72cdb7463ca3e818b15f8fb8b0330e7cf5"],"c9a0deca56efc5191d6b3c41047fd538f3fae1d8":["f271ff507bf58806fc089e76e38057c95e391dcd"],"55ddd0022d1d26b52ed711e90ab77de2b36130c6":["66f3dadb253a44f4cccc81c8a21b685b18b201fb"],"f4ba1ab90d9a427e7f2c7d1e65a3ce5869ed8e5d":["dbb7b6f10bff9eedd5c9bc6cf9222ffbb2df74d4"],"f22ffa72cdb7463ca3e818b15f8fb8b0330e7cf5":["e7fb6d70db034a5456ae560175dd1b821eea9ff4"],"066b6ff5a08e35c3b6880e7c3ddda79526acdab1":["a3a7f858bb80cdcedf1e39222961d53f11e1f6ed"],"da3e8c2fef4ea558379c4c0879b3bcdecde41bcd":["b3d08461c77d39c25ea6ff0cd05b32f948fa2a33"],"4630870c8c0e172c76b902376619806cc2ec5e38":["33bd4a1f823fcca0940fb58c35873adeab11c761"],"e0c804f7aa477229414a7e12882af490c241f64d":["55ddd0022d1d26b52ed711e90ab77de2b36130c6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"]},"commit2Childs":{"b3d08461c77d39c25ea6ff0cd05b32f948fa2a33":["da3e8c2fef4ea558379c4c0879b3bcdecde41bcd"],"950f3c7592cb559e2534e5089c78833250e156a3":["0c0427a35a68b16ec73d3bf5f27e57719915f166"],"e7fb6d70db034a5456ae560175dd1b821eea9ff4":["f22ffa72cdb7463ca3e818b15f8fb8b0330e7cf5"],"d736930237c54e1516a9e3bae803c92ff19ec4e5":["66f3dadb253a44f4cccc81c8a21b685b18b201fb"],"902ba79f4590a41c663c447756d2e5041cbbdda9":["c7b6cdc70e097da94da79a655ed8f94477ff69f5"],"c7b6cdc70e097da94da79a655ed8f94477ff69f5":["a3a7f858bb80cdcedf1e39222961d53f11e1f6ed"],"84acdfa12c18361ff932244db20470fce117e52d":["dbb7b6f10bff9eedd5c9bc6cf9222ffbb2df74d4"],"4350b17bd363cd13a95171b8df1ca62ea4c3e71c":["3b248d9b80f6b2fceade80b3c8683d1cca6e4c98"],"fa27b750ee9a51ec4bed93ef328aef9ca1e2153d":["775efee7f959e0dd3df7960b93767d9e00b78751"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["91109046a59c58ee0ee5d0d2767b08d1f30d6702"],"6c9ae72f0c7aa48fa47b0c33986b516a2e71d578":["950f3c7592cb559e2534e5089c78833250e156a3"],"3b248d9b80f6b2fceade80b3c8683d1cca6e4c98":["f271ff507bf58806fc089e76e38057c95e391dcd"],"87c966e9308847938a7c905c2e46a56d8df788b8":["fa27b750ee9a51ec4bed93ef328aef9ca1e2153d"],"f271ff507bf58806fc089e76e38057c95e391dcd":["c9a0deca56efc5191d6b3c41047fd538f3fae1d8"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"346d5897e4c4e77ed5dbd31f7730ff30973d5971":["b3d08461c77d39c25ea6ff0cd05b32f948fa2a33"],"91109046a59c58ee0ee5d0d2767b08d1f30d6702":["8fb95844e4ba5160067c64c5eb1cd8a09f7a94f1"],"0c0427a35a68b16ec73d3bf5f27e57719915f166":["1b54a9bc667895a2095a886184bf69a3179e63df"],"8522ae207a56c6db28ca06fe6cc33e70911c3600":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"775efee7f959e0dd3df7960b93767d9e00b78751":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"1b54a9bc667895a2095a886184bf69a3179e63df":["8522ae207a56c6db28ca06fe6cc33e70911c3600"],"33bd4a1f823fcca0940fb58c35873adeab11c761":["4630870c8c0e172c76b902376619806cc2ec5e38"],"dbb7b6f10bff9eedd5c9bc6cf9222ffbb2df74d4":["f4ba1ab90d9a427e7f2c7d1e65a3ce5869ed8e5d"],"8fb95844e4ba5160067c64c5eb1cd8a09f7a94f1":["e7fb6d70db034a5456ae560175dd1b821eea9ff4"],"6177f0f28ace66d1538b1e6ac5f1773e5449a0b0":["6c9ae72f0c7aa48fa47b0c33986b516a2e71d578"],"a3a7f858bb80cdcedf1e39222961d53f11e1f6ed":["066b6ff5a08e35c3b6880e7c3ddda79526acdab1"],"66f3dadb253a44f4cccc81c8a21b685b18b201fb":["55ddd0022d1d26b52ed711e90ab77de2b36130c6"],"770281b8a8459cafcdd2354b6a06078fea2d83c9":["6177f0f28ace66d1538b1e6ac5f1773e5449a0b0"],"c9a0deca56efc5191d6b3c41047fd538f3fae1d8":["346d5897e4c4e77ed5dbd31f7730ff30973d5971"],"55ddd0022d1d26b52ed711e90ab77de2b36130c6":["e0c804f7aa477229414a7e12882af490c241f64d"],"f4ba1ab90d9a427e7f2c7d1e65a3ce5869ed8e5d":["33bd4a1f823fcca0940fb58c35873adeab11c761"],"f22ffa72cdb7463ca3e818b15f8fb8b0330e7cf5":["770281b8a8459cafcdd2354b6a06078fea2d83c9"],"066b6ff5a08e35c3b6880e7c3ddda79526acdab1":["d736930237c54e1516a9e3bae803c92ff19ec4e5"],"da3e8c2fef4ea558379c4c0879b3bcdecde41bcd":["84acdfa12c18361ff932244db20470fce117e52d"],"4630870c8c0e172c76b902376619806cc2ec5e38":["902ba79f4590a41c663c447756d2e5041cbbdda9"],"e0c804f7aa477229414a7e12882af490c241f64d":["87c966e9308847938a7c905c2e46a56d8df788b8"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}