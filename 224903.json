{"path":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#doFindMerges(List[SegmentSizeAndDocs],long,int,int,MERGE_TYPE,MergeContext,boolean).mjava","commits":[{"id":"56fb5e4e4b239474721e13b4cd9542ea2d215451","date":1529091182,"type":0,"author":"Erick","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#doFindMerges(List[SegmentSizeAndDocs],long,int,int,MERGE_TYPE,MergeContext,boolean).mjava","pathOld":"/dev/null","sourceNew":"  private MergeSpecification doFindMerges(List<SegmentSizeAndDocs> sortedEligibleInfos,\n                                          final long maxMergedSegmentBytes,\n                                          final int maxMergeAtOnce, final int allowedSegCount,\n                                          final MERGE_TYPE mergeType,\n                                          MergeContext mergeContext,\n                                          boolean maxMergeIsRunning) throws IOException {\n\n    List<SegmentSizeAndDocs> sortedEligible = new ArrayList<>(sortedEligibleInfos);\n\n    Map<SegmentCommitInfo, SegmentSizeAndDocs> segInfosSizes = new HashMap<>();\n    for (SegmentSizeAndDocs segSizeDocs : sortedEligible) {\n      segInfosSizes.put(segSizeDocs.segInfo, segSizeDocs);\n    }\n\n    int originalSortedSize = sortedEligible.size();\n    if (verbose(mergeContext)) {\n      message(\"findMerges: \" + originalSortedSize + \" segments\", mergeContext);\n    }\n    if (originalSortedSize == 0) {\n      return null;\n    }\n\n    final Set<SegmentCommitInfo> toBeMerged = new HashSet<>();\n\n    MergeSpecification spec = null;\n\n    // Cycle to possibly select more than one merge:\n    // The trigger point for total deleted documents in the index leads to a bunch of large segment\n    // merges at the same time. So only put one large merge in the list of merges per cycle. We'll pick up another\n    // merge next time around.\n    boolean haveOneLargeMerge = false;\n\n    while (true) {\n\n      // Gather eligible segments for merging, ie segments\n      // not already being merged and not already picked (by\n      // prior iteration of this loop) for merging:\n\n      // Remove ineligible segments. These are either already being merged or already picked by prior iterations\n      Iterator<SegmentSizeAndDocs> iter = sortedEligible.iterator();\n      while (iter.hasNext()) {\n        SegmentSizeAndDocs segSizeDocs = iter.next();\n        if (toBeMerged.contains(segSizeDocs.segInfo)) {\n          iter.remove();\n        }\n      }\n\n      if (verbose(mergeContext)) {\n        message(\"  allowedSegmentCount=\" + allowedSegCount + \" vs count=\" + originalSortedSize + \" (eligible count=\" + sortedEligible.size() + \")\", mergeContext);\n      }\n\n      if (sortedEligible.size() == 0) {\n        return spec;\n      }\n\n      if (allowedSegCount != Integer.MAX_VALUE && sortedEligible.size() <= allowedSegCount && mergeType == MERGE_TYPE.NATURAL) {\n        return spec;\n      }\n\n      // OK we are over budget -- find best merge!\n      MergeScore bestScore = null;\n      List<SegmentCommitInfo> best = null;\n      boolean bestTooLarge = false;\n      long bestMergeBytes = 0;\n\n    // Consider all merge starts.\n    int lim = sortedEligible.size() - maxMergeAtOnce; // assume the usual case of background merging.\n\n    if (mergeType != MERGE_TYPE.NATURAL) { // The unusual case of forceMerge or expungeDeletes.\n      // The incoming eligible list will have only segments with > forceMergeDeletesPctAllowed in the case of\n      // findForcedDeletesMerges and segments with < max allowed size in the case of optimize.\n      // If forcing, we must allow singleton merges.\n      lim = sortedEligible.size() - 1;\n    }\n\n    for (int startIdx = 0; startIdx <= lim; startIdx++) {\n\n        long totAfterMergeBytes = 0;\n\n        final List<SegmentCommitInfo> candidate = new ArrayList<>();\n        boolean hitTooLarge = false;\n      long bytesThisMerge = 0;\n      for (int idx = startIdx; idx < sortedEligible.size() && candidate.size() < maxMergeAtOnce && bytesThisMerge < maxMergedSegmentBytes; idx++) {\n        final SegmentSizeAndDocs segSizeDocs = sortedEligible.get(idx);\n        final long segBytes = segSizeDocs.sizeInBytes;\n\n          if (totAfterMergeBytes + segBytes > maxMergedSegmentBytes) {\n            hitTooLarge = true;\n            if (candidate.size() == 0) {\n              // We should never have something coming in that _cannot_ be merged, so handle singleton merges\n              candidate.add(segSizeDocs.segInfo);\n              bytesThisMerge += segBytes;\n            }\n            // NOTE: we continue, so that we can try\n            // \"packing\" smaller segments into this merge\n            // to see if we can get closer to the max\n            // size; this in general is not perfect since\n            // this is really \"bin packing\" and we'd have\n            // to try different permutations.\n            continue;\n          }\n        candidate.add(segSizeDocs.segInfo);\n        bytesThisMerge += segBytes;\n          totAfterMergeBytes += segBytes;\n        }\n\n        // We should never see an empty candidate: we iterated over maxMergeAtOnce\n        // segments, and already pre-excluded the too-large segments:\n        assert candidate.size() > 0;\n\n      // A singleton merge with no deletes makes no sense. We can get here when forceMerge is looping around...\n      if (candidate.size() == 1) {\n        SegmentSizeAndDocs segSizeDocs = segInfosSizes.get(candidate.get(0));\n        if (segSizeDocs.delCount == 0) {\n          continue;\n        }\n      }\n\n      final MergeScore score = score(candidate, hitTooLarge, segInfosSizes);\n        if (verbose(mergeContext)) {\n          message(\"  maybe=\" + segString(mergeContext, candidate) + \" score=\" + score.getScore() + \" \" + score.getExplanation() + \" tooLarge=\" + hitTooLarge + \" size=\" + String.format(Locale.ROOT, \"%.3f MB\", totAfterMergeBytes/1024./1024.), mergeContext);\n        }\n\n        if ((bestScore == null || score.getScore() < bestScore.getScore()) && (!hitTooLarge || !maxMergeIsRunning)) {\n          best = candidate;\n          bestScore = score;\n          bestTooLarge = hitTooLarge;\n          bestMergeBytes = totAfterMergeBytes;\n        }\n      }\n\n      if (best == null) {\n        return spec;\n      }\n      // The mergeType == FORCE_MERGE_DELETES behaves as the code does currently and can create a large number of\n      // concurrent big merges. If we make findForcedDeletesMerges behave as findForcedMerges and cycle through\n      // we should remove this.\n      if (haveOneLargeMerge == false || bestTooLarge == false || mergeType == MERGE_TYPE.FORCE_MERGE_DELETES) {\n\n        haveOneLargeMerge |= bestTooLarge;\n\n        if (spec == null) {\n          spec = new MergeSpecification();\n        }\n        final OneMerge merge = new OneMerge(best);\n        spec.add(merge);\n\n        if (verbose(mergeContext)) {\n          message(\"  add merge=\" + segString(mergeContext, merge.segments) + \" size=\" + String.format(Locale.ROOT, \"%.3f MB\", bestMergeBytes / 1024. / 1024.) + \" score=\" + String.format(Locale.ROOT, \"%.3f\", bestScore.getScore()) + \" \" + bestScore.getExplanation() + (bestTooLarge ? \" [max merge]\" : \"\"), mergeContext);\n        }\n      }\n      // whether we're going to return this list in the spec of not, we need to remove it from\n      // consideration on the next loop.\n      toBeMerged.addAll(best);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"514d769c85cddcfafabba24fb50a628dfbcc416f","date":1530704214,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#doFindMerges(List[SegmentSizeAndDocs],long,int,int,MERGE_TYPE,MergeContext,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#doFindMerges(List[SegmentSizeAndDocs],long,int,int,MERGE_TYPE,MergeContext,boolean).mjava","sourceNew":"  private MergeSpecification doFindMerges(List<SegmentSizeAndDocs> sortedEligibleInfos,\n                                          final long maxMergedSegmentBytes,\n                                          final int maxMergeAtOnce, final int allowedSegCount,\n                                          final MERGE_TYPE mergeType,\n                                          MergeContext mergeContext,\n                                          boolean maxMergeIsRunning) throws IOException {\n\n    List<SegmentSizeAndDocs> sortedEligible = new ArrayList<>(sortedEligibleInfos);\n\n    Map<SegmentCommitInfo, SegmentSizeAndDocs> segInfosSizes = new HashMap<>();\n    for (SegmentSizeAndDocs segSizeDocs : sortedEligible) {\n      segInfosSizes.put(segSizeDocs.segInfo, segSizeDocs);\n    }\n\n    int originalSortedSize = sortedEligible.size();\n    if (verbose(mergeContext)) {\n      message(\"findMerges: \" + originalSortedSize + \" segments\", mergeContext);\n    }\n    if (originalSortedSize == 0) {\n      return null;\n    }\n\n    final Set<SegmentCommitInfo> toBeMerged = new HashSet<>();\n\n    MergeSpecification spec = null;\n\n    // Cycle to possibly select more than one merge:\n    // The trigger point for total deleted documents in the index leads to a bunch of large segment\n    // merges at the same time. So only put one large merge in the list of merges per cycle. We'll pick up another\n    // merge next time around.\n    boolean haveOneLargeMerge = false;\n\n    while (true) {\n\n      // Gather eligible segments for merging, ie segments\n      // not already being merged and not already picked (by\n      // prior iteration of this loop) for merging:\n\n      // Remove ineligible segments. These are either already being merged or already picked by prior iterations\n      Iterator<SegmentSizeAndDocs> iter = sortedEligible.iterator();\n      while (iter.hasNext()) {\n        SegmentSizeAndDocs segSizeDocs = iter.next();\n        if (toBeMerged.contains(segSizeDocs.segInfo)) {\n          iter.remove();\n        }\n      }\n\n      if (verbose(mergeContext)) {\n        message(\"  allowedSegmentCount=\" + allowedSegCount + \" vs count=\" + originalSortedSize + \" (eligible count=\" + sortedEligible.size() + \")\", mergeContext);\n      }\n\n      if (sortedEligible.size() == 0) {\n        return spec;\n      }\n\n      if (allowedSegCount != Integer.MAX_VALUE && sortedEligible.size() <= allowedSegCount && mergeType == MERGE_TYPE.NATURAL) {\n        return spec;\n      }\n\n      // OK we are over budget -- find best merge!\n      MergeScore bestScore = null;\n      List<SegmentCommitInfo> best = null;\n      boolean bestTooLarge = false;\n      long bestMergeBytes = 0;\n\n      // Consider all merge starts.\n      int lim = sortedEligible.size() - maxMergeAtOnce; // assume the usual case of background merging.\n\n      if (mergeType != MERGE_TYPE.NATURAL) { // The unusual case of forceMerge or expungeDeletes.\n        // The incoming eligible list will have only segments with > forceMergeDeletesPctAllowed in the case of\n        // findForcedDeletesMerges and segments with < max allowed size in the case of optimize.\n        // If forcing, we must allow singleton merges.\n        lim = sortedEligible.size() - 1;\n      }\n\n      for (int startIdx = 0; startIdx <= lim; startIdx++) {\n\n        long totAfterMergeBytes = 0;\n\n        final List<SegmentCommitInfo> candidate = new ArrayList<>();\n        boolean hitTooLarge = false;\n        long bytesThisMerge = 0;\n        for (int idx = startIdx; idx < sortedEligible.size() && candidate.size() < maxMergeAtOnce && bytesThisMerge < maxMergedSegmentBytes; idx++) {\n          final SegmentSizeAndDocs segSizeDocs = sortedEligible.get(idx);\n          final long segBytes = segSizeDocs.sizeInBytes;\n\n          if (totAfterMergeBytes + segBytes > maxMergedSegmentBytes) {\n            hitTooLarge = true;\n            if (candidate.size() == 0) {\n              // We should never have something coming in that _cannot_ be merged, so handle singleton merges\n              candidate.add(segSizeDocs.segInfo);\n              bytesThisMerge += segBytes;\n            }\n            // NOTE: we continue, so that we can try\n            // \"packing\" smaller segments into this merge\n            // to see if we can get closer to the max\n            // size; this in general is not perfect since\n            // this is really \"bin packing\" and we'd have\n            // to try different permutations.\n            continue;\n          }\n          candidate.add(segSizeDocs.segInfo);\n          bytesThisMerge += segBytes;\n          totAfterMergeBytes += segBytes;\n        }\n\n        // We should never see an empty candidate: we iterated over maxMergeAtOnce\n        // segments, and already pre-excluded the too-large segments:\n        assert candidate.size() > 0;\n\n        // A singleton merge with no deletes makes no sense. We can get here when forceMerge is looping around...\n        if (candidate.size() == 1) {\n          SegmentSizeAndDocs segSizeDocs = segInfosSizes.get(candidate.get(0));\n          if (segSizeDocs.delCount == 0) {\n            continue;\n          }\n        }\n\n        final MergeScore score = score(candidate, hitTooLarge, segInfosSizes);\n        if (verbose(mergeContext)) {\n          message(\"  maybe=\" + segString(mergeContext, candidate) + \" score=\" + score.getScore() + \" \" + score.getExplanation() + \" tooLarge=\" + hitTooLarge + \" size=\" + String.format(Locale.ROOT, \"%.3f MB\", totAfterMergeBytes/1024./1024.), mergeContext);\n        }\n\n        if ((bestScore == null || score.getScore() < bestScore.getScore()) && (!hitTooLarge || !maxMergeIsRunning)) {\n          best = candidate;\n          bestScore = score;\n          bestTooLarge = hitTooLarge;\n          bestMergeBytes = totAfterMergeBytes;\n        }\n      }\n\n      if (best == null) {\n        return spec;\n      }\n      // The mergeType == FORCE_MERGE_DELETES behaves as the code does currently and can create a large number of\n      // concurrent big merges. If we make findForcedDeletesMerges behave as findForcedMerges and cycle through\n      // we should remove this.\n      if (haveOneLargeMerge == false || bestTooLarge == false || mergeType == MERGE_TYPE.FORCE_MERGE_DELETES) {\n\n        haveOneLargeMerge |= bestTooLarge;\n\n        if (spec == null) {\n          spec = new MergeSpecification();\n        }\n        final OneMerge merge = new OneMerge(best);\n        spec.add(merge);\n\n        if (verbose(mergeContext)) {\n          message(\"  add merge=\" + segString(mergeContext, merge.segments) + \" size=\" + String.format(Locale.ROOT, \"%.3f MB\", bestMergeBytes / 1024. / 1024.) + \" score=\" + String.format(Locale.ROOT, \"%.3f\", bestScore.getScore()) + \" \" + bestScore.getExplanation() + (bestTooLarge ? \" [max merge]\" : \"\"), mergeContext);\n        }\n      }\n      // whether we're going to return this list in the spec of not, we need to remove it from\n      // consideration on the next loop.\n      toBeMerged.addAll(best);\n    }\n  }\n\n","sourceOld":"  private MergeSpecification doFindMerges(List<SegmentSizeAndDocs> sortedEligibleInfos,\n                                          final long maxMergedSegmentBytes,\n                                          final int maxMergeAtOnce, final int allowedSegCount,\n                                          final MERGE_TYPE mergeType,\n                                          MergeContext mergeContext,\n                                          boolean maxMergeIsRunning) throws IOException {\n\n    List<SegmentSizeAndDocs> sortedEligible = new ArrayList<>(sortedEligibleInfos);\n\n    Map<SegmentCommitInfo, SegmentSizeAndDocs> segInfosSizes = new HashMap<>();\n    for (SegmentSizeAndDocs segSizeDocs : sortedEligible) {\n      segInfosSizes.put(segSizeDocs.segInfo, segSizeDocs);\n    }\n\n    int originalSortedSize = sortedEligible.size();\n    if (verbose(mergeContext)) {\n      message(\"findMerges: \" + originalSortedSize + \" segments\", mergeContext);\n    }\n    if (originalSortedSize == 0) {\n      return null;\n    }\n\n    final Set<SegmentCommitInfo> toBeMerged = new HashSet<>();\n\n    MergeSpecification spec = null;\n\n    // Cycle to possibly select more than one merge:\n    // The trigger point for total deleted documents in the index leads to a bunch of large segment\n    // merges at the same time. So only put one large merge in the list of merges per cycle. We'll pick up another\n    // merge next time around.\n    boolean haveOneLargeMerge = false;\n\n    while (true) {\n\n      // Gather eligible segments for merging, ie segments\n      // not already being merged and not already picked (by\n      // prior iteration of this loop) for merging:\n\n      // Remove ineligible segments. These are either already being merged or already picked by prior iterations\n      Iterator<SegmentSizeAndDocs> iter = sortedEligible.iterator();\n      while (iter.hasNext()) {\n        SegmentSizeAndDocs segSizeDocs = iter.next();\n        if (toBeMerged.contains(segSizeDocs.segInfo)) {\n          iter.remove();\n        }\n      }\n\n      if (verbose(mergeContext)) {\n        message(\"  allowedSegmentCount=\" + allowedSegCount + \" vs count=\" + originalSortedSize + \" (eligible count=\" + sortedEligible.size() + \")\", mergeContext);\n      }\n\n      if (sortedEligible.size() == 0) {\n        return spec;\n      }\n\n      if (allowedSegCount != Integer.MAX_VALUE && sortedEligible.size() <= allowedSegCount && mergeType == MERGE_TYPE.NATURAL) {\n        return spec;\n      }\n\n      // OK we are over budget -- find best merge!\n      MergeScore bestScore = null;\n      List<SegmentCommitInfo> best = null;\n      boolean bestTooLarge = false;\n      long bestMergeBytes = 0;\n\n    // Consider all merge starts.\n    int lim = sortedEligible.size() - maxMergeAtOnce; // assume the usual case of background merging.\n\n    if (mergeType != MERGE_TYPE.NATURAL) { // The unusual case of forceMerge or expungeDeletes.\n      // The incoming eligible list will have only segments with > forceMergeDeletesPctAllowed in the case of\n      // findForcedDeletesMerges and segments with < max allowed size in the case of optimize.\n      // If forcing, we must allow singleton merges.\n      lim = sortedEligible.size() - 1;\n    }\n\n    for (int startIdx = 0; startIdx <= lim; startIdx++) {\n\n        long totAfterMergeBytes = 0;\n\n        final List<SegmentCommitInfo> candidate = new ArrayList<>();\n        boolean hitTooLarge = false;\n      long bytesThisMerge = 0;\n      for (int idx = startIdx; idx < sortedEligible.size() && candidate.size() < maxMergeAtOnce && bytesThisMerge < maxMergedSegmentBytes; idx++) {\n        final SegmentSizeAndDocs segSizeDocs = sortedEligible.get(idx);\n        final long segBytes = segSizeDocs.sizeInBytes;\n\n          if (totAfterMergeBytes + segBytes > maxMergedSegmentBytes) {\n            hitTooLarge = true;\n            if (candidate.size() == 0) {\n              // We should never have something coming in that _cannot_ be merged, so handle singleton merges\n              candidate.add(segSizeDocs.segInfo);\n              bytesThisMerge += segBytes;\n            }\n            // NOTE: we continue, so that we can try\n            // \"packing\" smaller segments into this merge\n            // to see if we can get closer to the max\n            // size; this in general is not perfect since\n            // this is really \"bin packing\" and we'd have\n            // to try different permutations.\n            continue;\n          }\n        candidate.add(segSizeDocs.segInfo);\n        bytesThisMerge += segBytes;\n          totAfterMergeBytes += segBytes;\n        }\n\n        // We should never see an empty candidate: we iterated over maxMergeAtOnce\n        // segments, and already pre-excluded the too-large segments:\n        assert candidate.size() > 0;\n\n      // A singleton merge with no deletes makes no sense. We can get here when forceMerge is looping around...\n      if (candidate.size() == 1) {\n        SegmentSizeAndDocs segSizeDocs = segInfosSizes.get(candidate.get(0));\n        if (segSizeDocs.delCount == 0) {\n          continue;\n        }\n      }\n\n      final MergeScore score = score(candidate, hitTooLarge, segInfosSizes);\n        if (verbose(mergeContext)) {\n          message(\"  maybe=\" + segString(mergeContext, candidate) + \" score=\" + score.getScore() + \" \" + score.getExplanation() + \" tooLarge=\" + hitTooLarge + \" size=\" + String.format(Locale.ROOT, \"%.3f MB\", totAfterMergeBytes/1024./1024.), mergeContext);\n        }\n\n        if ((bestScore == null || score.getScore() < bestScore.getScore()) && (!hitTooLarge || !maxMergeIsRunning)) {\n          best = candidate;\n          bestScore = score;\n          bestTooLarge = hitTooLarge;\n          bestMergeBytes = totAfterMergeBytes;\n        }\n      }\n\n      if (best == null) {\n        return spec;\n      }\n      // The mergeType == FORCE_MERGE_DELETES behaves as the code does currently and can create a large number of\n      // concurrent big merges. If we make findForcedDeletesMerges behave as findForcedMerges and cycle through\n      // we should remove this.\n      if (haveOneLargeMerge == false || bestTooLarge == false || mergeType == MERGE_TYPE.FORCE_MERGE_DELETES) {\n\n        haveOneLargeMerge |= bestTooLarge;\n\n        if (spec == null) {\n          spec = new MergeSpecification();\n        }\n        final OneMerge merge = new OneMerge(best);\n        spec.add(merge);\n\n        if (verbose(mergeContext)) {\n          message(\"  add merge=\" + segString(mergeContext, merge.segments) + \" size=\" + String.format(Locale.ROOT, \"%.3f MB\", bestMergeBytes / 1024. / 1024.) + \" score=\" + String.format(Locale.ROOT, \"%.3f\", bestScore.getScore()) + \" \" + bestScore.getExplanation() + (bestTooLarge ? \" [max merge]\" : \"\"), mergeContext);\n        }\n      }\n      // whether we're going to return this list in the spec of not, we need to remove it from\n      // consideration on the next loop.\n      toBeMerged.addAll(best);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9707a68fe260631e514201dbf24e9afc9a3a4ba1","date":1531207054,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#doFindMerges(List[SegmentSizeAndDocs],long,int,int,MERGE_TYPE,MergeContext,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#doFindMerges(List[SegmentSizeAndDocs],long,int,int,MERGE_TYPE,MergeContext,boolean).mjava","sourceNew":"  private MergeSpecification doFindMerges(List<SegmentSizeAndDocs> sortedEligibleInfos,\n                                          final long maxMergedSegmentBytes,\n                                          final int mergeFactor, final int allowedSegCount,\n                                          final MERGE_TYPE mergeType,\n                                          MergeContext mergeContext,\n                                          boolean maxMergeIsRunning) throws IOException {\n\n    List<SegmentSizeAndDocs> sortedEligible = new ArrayList<>(sortedEligibleInfos);\n\n    Map<SegmentCommitInfo, SegmentSizeAndDocs> segInfosSizes = new HashMap<>();\n    for (SegmentSizeAndDocs segSizeDocs : sortedEligible) {\n      segInfosSizes.put(segSizeDocs.segInfo, segSizeDocs);\n    }\n\n    int originalSortedSize = sortedEligible.size();\n    if (verbose(mergeContext)) {\n      message(\"findMerges: \" + originalSortedSize + \" segments\", mergeContext);\n    }\n    if (originalSortedSize == 0) {\n      return null;\n    }\n\n    final Set<SegmentCommitInfo> toBeMerged = new HashSet<>();\n\n    MergeSpecification spec = null;\n\n    // Cycle to possibly select more than one merge:\n    // The trigger point for total deleted documents in the index leads to a bunch of large segment\n    // merges at the same time. So only put one large merge in the list of merges per cycle. We'll pick up another\n    // merge next time around.\n    boolean haveOneLargeMerge = false;\n\n    while (true) {\n\n      // Gather eligible segments for merging, ie segments\n      // not already being merged and not already picked (by\n      // prior iteration of this loop) for merging:\n\n      // Remove ineligible segments. These are either already being merged or already picked by prior iterations\n      Iterator<SegmentSizeAndDocs> iter = sortedEligible.iterator();\n      while (iter.hasNext()) {\n        SegmentSizeAndDocs segSizeDocs = iter.next();\n        if (toBeMerged.contains(segSizeDocs.segInfo)) {\n          iter.remove();\n        }\n      }\n\n      if (verbose(mergeContext)) {\n        message(\"  allowedSegmentCount=\" + allowedSegCount + \" vs count=\" + originalSortedSize + \" (eligible count=\" + sortedEligible.size() + \")\", mergeContext);\n      }\n\n      if (sortedEligible.size() == 0) {\n        return spec;\n      }\n\n      if (allowedSegCount != Integer.MAX_VALUE && sortedEligible.size() <= allowedSegCount && mergeType == MERGE_TYPE.NATURAL) {\n        return spec;\n      }\n\n      // OK we are over budget -- find best merge!\n      MergeScore bestScore = null;\n      List<SegmentCommitInfo> best = null;\n      boolean bestTooLarge = false;\n      long bestMergeBytes = 0;\n\n      // Consider all merge starts.\n      int lim = sortedEligible.size() - mergeFactor; // assume the usual case of background merging.\n\n      if (mergeType != MERGE_TYPE.NATURAL) { // The unusual case of forceMerge or expungeDeletes.\n        // The incoming eligible list will have only segments with > forceMergeDeletesPctAllowed in the case of\n        // findForcedDeletesMerges and segments with < max allowed size in the case of optimize.\n        // If forcing, we must allow singleton merges.\n        lim = sortedEligible.size() - 1;\n      }\n\n      for (int startIdx = 0; startIdx <= lim; startIdx++) {\n\n        long totAfterMergeBytes = 0;\n\n        final List<SegmentCommitInfo> candidate = new ArrayList<>();\n        boolean hitTooLarge = false;\n        long bytesThisMerge = 0;\n        for (int idx = startIdx; idx < sortedEligible.size() && candidate.size() < mergeFactor && bytesThisMerge < maxMergedSegmentBytes; idx++) {\n          final SegmentSizeAndDocs segSizeDocs = sortedEligible.get(idx);\n          final long segBytes = segSizeDocs.sizeInBytes;\n\n          if (totAfterMergeBytes + segBytes > maxMergedSegmentBytes) {\n            hitTooLarge = true;\n            if (candidate.size() == 0) {\n              // We should never have something coming in that _cannot_ be merged, so handle singleton merges\n              candidate.add(segSizeDocs.segInfo);\n              bytesThisMerge += segBytes;\n            }\n            // NOTE: we continue, so that we can try\n            // \"packing\" smaller segments into this merge\n            // to see if we can get closer to the max\n            // size; this in general is not perfect since\n            // this is really \"bin packing\" and we'd have\n            // to try different permutations.\n            continue;\n          }\n          candidate.add(segSizeDocs.segInfo);\n          bytesThisMerge += segBytes;\n          totAfterMergeBytes += segBytes;\n        }\n\n        // We should never see an empty candidate: we iterated over maxMergeAtOnce\n        // segments, and already pre-excluded the too-large segments:\n        assert candidate.size() > 0;\n\n        // A singleton merge with no deletes makes no sense. We can get here when forceMerge is looping around...\n        if (candidate.size() == 1) {\n          SegmentSizeAndDocs segSizeDocs = segInfosSizes.get(candidate.get(0));\n          if (segSizeDocs.delCount == 0) {\n            continue;\n          }\n        }\n\n        final MergeScore score = score(candidate, hitTooLarge, segInfosSizes);\n        if (verbose(mergeContext)) {\n          message(\"  maybe=\" + segString(mergeContext, candidate) + \" score=\" + score.getScore() + \" \" + score.getExplanation() + \" tooLarge=\" + hitTooLarge + \" size=\" + String.format(Locale.ROOT, \"%.3f MB\", totAfterMergeBytes/1024./1024.), mergeContext);\n        }\n\n        if ((bestScore == null || score.getScore() < bestScore.getScore()) && (!hitTooLarge || !maxMergeIsRunning)) {\n          best = candidate;\n          bestScore = score;\n          bestTooLarge = hitTooLarge;\n          bestMergeBytes = totAfterMergeBytes;\n        }\n      }\n\n      if (best == null) {\n        return spec;\n      }\n      // The mergeType == FORCE_MERGE_DELETES behaves as the code does currently and can create a large number of\n      // concurrent big merges. If we make findForcedDeletesMerges behave as findForcedMerges and cycle through\n      // we should remove this.\n      if (haveOneLargeMerge == false || bestTooLarge == false || mergeType == MERGE_TYPE.FORCE_MERGE_DELETES) {\n\n        haveOneLargeMerge |= bestTooLarge;\n\n        if (spec == null) {\n          spec = new MergeSpecification();\n        }\n        final OneMerge merge = new OneMerge(best);\n        spec.add(merge);\n\n        if (verbose(mergeContext)) {\n          message(\"  add merge=\" + segString(mergeContext, merge.segments) + \" size=\" + String.format(Locale.ROOT, \"%.3f MB\", bestMergeBytes / 1024. / 1024.) + \" score=\" + String.format(Locale.ROOT, \"%.3f\", bestScore.getScore()) + \" \" + bestScore.getExplanation() + (bestTooLarge ? \" [max merge]\" : \"\"), mergeContext);\n        }\n      }\n      // whether we're going to return this list in the spec of not, we need to remove it from\n      // consideration on the next loop.\n      toBeMerged.addAll(best);\n    }\n  }\n\n","sourceOld":"  private MergeSpecification doFindMerges(List<SegmentSizeAndDocs> sortedEligibleInfos,\n                                          final long maxMergedSegmentBytes,\n                                          final int maxMergeAtOnce, final int allowedSegCount,\n                                          final MERGE_TYPE mergeType,\n                                          MergeContext mergeContext,\n                                          boolean maxMergeIsRunning) throws IOException {\n\n    List<SegmentSizeAndDocs> sortedEligible = new ArrayList<>(sortedEligibleInfos);\n\n    Map<SegmentCommitInfo, SegmentSizeAndDocs> segInfosSizes = new HashMap<>();\n    for (SegmentSizeAndDocs segSizeDocs : sortedEligible) {\n      segInfosSizes.put(segSizeDocs.segInfo, segSizeDocs);\n    }\n\n    int originalSortedSize = sortedEligible.size();\n    if (verbose(mergeContext)) {\n      message(\"findMerges: \" + originalSortedSize + \" segments\", mergeContext);\n    }\n    if (originalSortedSize == 0) {\n      return null;\n    }\n\n    final Set<SegmentCommitInfo> toBeMerged = new HashSet<>();\n\n    MergeSpecification spec = null;\n\n    // Cycle to possibly select more than one merge:\n    // The trigger point for total deleted documents in the index leads to a bunch of large segment\n    // merges at the same time. So only put one large merge in the list of merges per cycle. We'll pick up another\n    // merge next time around.\n    boolean haveOneLargeMerge = false;\n\n    while (true) {\n\n      // Gather eligible segments for merging, ie segments\n      // not already being merged and not already picked (by\n      // prior iteration of this loop) for merging:\n\n      // Remove ineligible segments. These are either already being merged or already picked by prior iterations\n      Iterator<SegmentSizeAndDocs> iter = sortedEligible.iterator();\n      while (iter.hasNext()) {\n        SegmentSizeAndDocs segSizeDocs = iter.next();\n        if (toBeMerged.contains(segSizeDocs.segInfo)) {\n          iter.remove();\n        }\n      }\n\n      if (verbose(mergeContext)) {\n        message(\"  allowedSegmentCount=\" + allowedSegCount + \" vs count=\" + originalSortedSize + \" (eligible count=\" + sortedEligible.size() + \")\", mergeContext);\n      }\n\n      if (sortedEligible.size() == 0) {\n        return spec;\n      }\n\n      if (allowedSegCount != Integer.MAX_VALUE && sortedEligible.size() <= allowedSegCount && mergeType == MERGE_TYPE.NATURAL) {\n        return spec;\n      }\n\n      // OK we are over budget -- find best merge!\n      MergeScore bestScore = null;\n      List<SegmentCommitInfo> best = null;\n      boolean bestTooLarge = false;\n      long bestMergeBytes = 0;\n\n      // Consider all merge starts.\n      int lim = sortedEligible.size() - maxMergeAtOnce; // assume the usual case of background merging.\n\n      if (mergeType != MERGE_TYPE.NATURAL) { // The unusual case of forceMerge or expungeDeletes.\n        // The incoming eligible list will have only segments with > forceMergeDeletesPctAllowed in the case of\n        // findForcedDeletesMerges and segments with < max allowed size in the case of optimize.\n        // If forcing, we must allow singleton merges.\n        lim = sortedEligible.size() - 1;\n      }\n\n      for (int startIdx = 0; startIdx <= lim; startIdx++) {\n\n        long totAfterMergeBytes = 0;\n\n        final List<SegmentCommitInfo> candidate = new ArrayList<>();\n        boolean hitTooLarge = false;\n        long bytesThisMerge = 0;\n        for (int idx = startIdx; idx < sortedEligible.size() && candidate.size() < maxMergeAtOnce && bytesThisMerge < maxMergedSegmentBytes; idx++) {\n          final SegmentSizeAndDocs segSizeDocs = sortedEligible.get(idx);\n          final long segBytes = segSizeDocs.sizeInBytes;\n\n          if (totAfterMergeBytes + segBytes > maxMergedSegmentBytes) {\n            hitTooLarge = true;\n            if (candidate.size() == 0) {\n              // We should never have something coming in that _cannot_ be merged, so handle singleton merges\n              candidate.add(segSizeDocs.segInfo);\n              bytesThisMerge += segBytes;\n            }\n            // NOTE: we continue, so that we can try\n            // \"packing\" smaller segments into this merge\n            // to see if we can get closer to the max\n            // size; this in general is not perfect since\n            // this is really \"bin packing\" and we'd have\n            // to try different permutations.\n            continue;\n          }\n          candidate.add(segSizeDocs.segInfo);\n          bytesThisMerge += segBytes;\n          totAfterMergeBytes += segBytes;\n        }\n\n        // We should never see an empty candidate: we iterated over maxMergeAtOnce\n        // segments, and already pre-excluded the too-large segments:\n        assert candidate.size() > 0;\n\n        // A singleton merge with no deletes makes no sense. We can get here when forceMerge is looping around...\n        if (candidate.size() == 1) {\n          SegmentSizeAndDocs segSizeDocs = segInfosSizes.get(candidate.get(0));\n          if (segSizeDocs.delCount == 0) {\n            continue;\n          }\n        }\n\n        final MergeScore score = score(candidate, hitTooLarge, segInfosSizes);\n        if (verbose(mergeContext)) {\n          message(\"  maybe=\" + segString(mergeContext, candidate) + \" score=\" + score.getScore() + \" \" + score.getExplanation() + \" tooLarge=\" + hitTooLarge + \" size=\" + String.format(Locale.ROOT, \"%.3f MB\", totAfterMergeBytes/1024./1024.), mergeContext);\n        }\n\n        if ((bestScore == null || score.getScore() < bestScore.getScore()) && (!hitTooLarge || !maxMergeIsRunning)) {\n          best = candidate;\n          bestScore = score;\n          bestTooLarge = hitTooLarge;\n          bestMergeBytes = totAfterMergeBytes;\n        }\n      }\n\n      if (best == null) {\n        return spec;\n      }\n      // The mergeType == FORCE_MERGE_DELETES behaves as the code does currently and can create a large number of\n      // concurrent big merges. If we make findForcedDeletesMerges behave as findForcedMerges and cycle through\n      // we should remove this.\n      if (haveOneLargeMerge == false || bestTooLarge == false || mergeType == MERGE_TYPE.FORCE_MERGE_DELETES) {\n\n        haveOneLargeMerge |= bestTooLarge;\n\n        if (spec == null) {\n          spec = new MergeSpecification();\n        }\n        final OneMerge merge = new OneMerge(best);\n        spec.add(merge);\n\n        if (verbose(mergeContext)) {\n          message(\"  add merge=\" + segString(mergeContext, merge.segments) + \" size=\" + String.format(Locale.ROOT, \"%.3f MB\", bestMergeBytes / 1024. / 1024.) + \" score=\" + String.format(Locale.ROOT, \"%.3f\", bestScore.getScore()) + \" \" + bestScore.getExplanation() + (bestTooLarge ? \" [max merge]\" : \"\"), mergeContext);\n        }\n      }\n      // whether we're going to return this list in the spec of not, we need to remove it from\n      // consideration on the next loop.\n      toBeMerged.addAll(best);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26","date":1531589977,"type":0,"author":"Michael Braun","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#doFindMerges(List[SegmentSizeAndDocs],long,int,int,MERGE_TYPE,MergeContext,boolean).mjava","pathOld":"/dev/null","sourceNew":"  private MergeSpecification doFindMerges(List<SegmentSizeAndDocs> sortedEligibleInfos,\n                                          final long maxMergedSegmentBytes,\n                                          final int mergeFactor, final int allowedSegCount,\n                                          final MERGE_TYPE mergeType,\n                                          MergeContext mergeContext,\n                                          boolean maxMergeIsRunning) throws IOException {\n\n    List<SegmentSizeAndDocs> sortedEligible = new ArrayList<>(sortedEligibleInfos);\n\n    Map<SegmentCommitInfo, SegmentSizeAndDocs> segInfosSizes = new HashMap<>();\n    for (SegmentSizeAndDocs segSizeDocs : sortedEligible) {\n      segInfosSizes.put(segSizeDocs.segInfo, segSizeDocs);\n    }\n\n    int originalSortedSize = sortedEligible.size();\n    if (verbose(mergeContext)) {\n      message(\"findMerges: \" + originalSortedSize + \" segments\", mergeContext);\n    }\n    if (originalSortedSize == 0) {\n      return null;\n    }\n\n    final Set<SegmentCommitInfo> toBeMerged = new HashSet<>();\n\n    MergeSpecification spec = null;\n\n    // Cycle to possibly select more than one merge:\n    // The trigger point for total deleted documents in the index leads to a bunch of large segment\n    // merges at the same time. So only put one large merge in the list of merges per cycle. We'll pick up another\n    // merge next time around.\n    boolean haveOneLargeMerge = false;\n\n    while (true) {\n\n      // Gather eligible segments for merging, ie segments\n      // not already being merged and not already picked (by\n      // prior iteration of this loop) for merging:\n\n      // Remove ineligible segments. These are either already being merged or already picked by prior iterations\n      Iterator<SegmentSizeAndDocs> iter = sortedEligible.iterator();\n      while (iter.hasNext()) {\n        SegmentSizeAndDocs segSizeDocs = iter.next();\n        if (toBeMerged.contains(segSizeDocs.segInfo)) {\n          iter.remove();\n        }\n      }\n\n      if (verbose(mergeContext)) {\n        message(\"  allowedSegmentCount=\" + allowedSegCount + \" vs count=\" + originalSortedSize + \" (eligible count=\" + sortedEligible.size() + \")\", mergeContext);\n      }\n\n      if (sortedEligible.size() == 0) {\n        return spec;\n      }\n\n      if (allowedSegCount != Integer.MAX_VALUE && sortedEligible.size() <= allowedSegCount && mergeType == MERGE_TYPE.NATURAL) {\n        return spec;\n      }\n\n      // OK we are over budget -- find best merge!\n      MergeScore bestScore = null;\n      List<SegmentCommitInfo> best = null;\n      boolean bestTooLarge = false;\n      long bestMergeBytes = 0;\n\n      // Consider all merge starts.\n      int lim = sortedEligible.size() - mergeFactor; // assume the usual case of background merging.\n\n      if (mergeType != MERGE_TYPE.NATURAL) { // The unusual case of forceMerge or expungeDeletes.\n        // The incoming eligible list will have only segments with > forceMergeDeletesPctAllowed in the case of\n        // findForcedDeletesMerges and segments with < max allowed size in the case of optimize.\n        // If forcing, we must allow singleton merges.\n        lim = sortedEligible.size() - 1;\n      }\n\n      for (int startIdx = 0; startIdx <= lim; startIdx++) {\n\n        long totAfterMergeBytes = 0;\n\n        final List<SegmentCommitInfo> candidate = new ArrayList<>();\n        boolean hitTooLarge = false;\n        long bytesThisMerge = 0;\n        for (int idx = startIdx; idx < sortedEligible.size() && candidate.size() < mergeFactor && bytesThisMerge < maxMergedSegmentBytes; idx++) {\n          final SegmentSizeAndDocs segSizeDocs = sortedEligible.get(idx);\n          final long segBytes = segSizeDocs.sizeInBytes;\n\n          if (totAfterMergeBytes + segBytes > maxMergedSegmentBytes) {\n            hitTooLarge = true;\n            if (candidate.size() == 0) {\n              // We should never have something coming in that _cannot_ be merged, so handle singleton merges\n              candidate.add(segSizeDocs.segInfo);\n              bytesThisMerge += segBytes;\n            }\n            // NOTE: we continue, so that we can try\n            // \"packing\" smaller segments into this merge\n            // to see if we can get closer to the max\n            // size; this in general is not perfect since\n            // this is really \"bin packing\" and we'd have\n            // to try different permutations.\n            continue;\n          }\n          candidate.add(segSizeDocs.segInfo);\n          bytesThisMerge += segBytes;\n          totAfterMergeBytes += segBytes;\n        }\n\n        // We should never see an empty candidate: we iterated over maxMergeAtOnce\n        // segments, and already pre-excluded the too-large segments:\n        assert candidate.size() > 0;\n\n        // A singleton merge with no deletes makes no sense. We can get here when forceMerge is looping around...\n        if (candidate.size() == 1) {\n          SegmentSizeAndDocs segSizeDocs = segInfosSizes.get(candidate.get(0));\n          if (segSizeDocs.delCount == 0) {\n            continue;\n          }\n        }\n\n        final MergeScore score = score(candidate, hitTooLarge, segInfosSizes);\n        if (verbose(mergeContext)) {\n          message(\"  maybe=\" + segString(mergeContext, candidate) + \" score=\" + score.getScore() + \" \" + score.getExplanation() + \" tooLarge=\" + hitTooLarge + \" size=\" + String.format(Locale.ROOT, \"%.3f MB\", totAfterMergeBytes/1024./1024.), mergeContext);\n        }\n\n        if ((bestScore == null || score.getScore() < bestScore.getScore()) && (!hitTooLarge || !maxMergeIsRunning)) {\n          best = candidate;\n          bestScore = score;\n          bestTooLarge = hitTooLarge;\n          bestMergeBytes = totAfterMergeBytes;\n        }\n      }\n\n      if (best == null) {\n        return spec;\n      }\n      // The mergeType == FORCE_MERGE_DELETES behaves as the code does currently and can create a large number of\n      // concurrent big merges. If we make findForcedDeletesMerges behave as findForcedMerges and cycle through\n      // we should remove this.\n      if (haveOneLargeMerge == false || bestTooLarge == false || mergeType == MERGE_TYPE.FORCE_MERGE_DELETES) {\n\n        haveOneLargeMerge |= bestTooLarge;\n\n        if (spec == null) {\n          spec = new MergeSpecification();\n        }\n        final OneMerge merge = new OneMerge(best);\n        spec.add(merge);\n\n        if (verbose(mergeContext)) {\n          message(\"  add merge=\" + segString(mergeContext, merge.segments) + \" size=\" + String.format(Locale.ROOT, \"%.3f MB\", bestMergeBytes / 1024. / 1024.) + \" score=\" + String.format(Locale.ROOT, \"%.3f\", bestScore.getScore()) + \" \" + bestScore.getExplanation() + (bestTooLarge ? \" [max merge]\" : \"\"), mergeContext);\n        }\n      }\n      // whether we're going to return this list in the spec of not, we need to remove it from\n      // consideration on the next loop.\n      toBeMerged.addAll(best);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4a90cc8c90aa53ddf51fbd15019989ac269514a3","date":1531845066,"type":5,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#doFindMerges(List[SegmentSizeAndDocs],long,int,int,int,MERGE_TYPE,MergeContext,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#doFindMerges(List[SegmentSizeAndDocs],long,int,int,MERGE_TYPE,MergeContext,boolean).mjava","sourceNew":"  private MergeSpecification doFindMerges(List<SegmentSizeAndDocs> sortedEligibleInfos,\n                                          final long maxMergedSegmentBytes,\n                                          final int mergeFactor, final int allowedSegCount,\n                                          final int allowedDelCount, final MERGE_TYPE mergeType,\n                                          MergeContext mergeContext,\n                                          boolean maxMergeIsRunning) throws IOException {\n\n    List<SegmentSizeAndDocs> sortedEligible = new ArrayList<>(sortedEligibleInfos);\n\n    Map<SegmentCommitInfo, SegmentSizeAndDocs> segInfosSizes = new HashMap<>();\n    for (SegmentSizeAndDocs segSizeDocs : sortedEligible) {\n      segInfosSizes.put(segSizeDocs.segInfo, segSizeDocs);\n    }\n\n    int originalSortedSize = sortedEligible.size();\n    if (verbose(mergeContext)) {\n      message(\"findMerges: \" + originalSortedSize + \" segments\", mergeContext);\n    }\n    if (originalSortedSize == 0) {\n      return null;\n    }\n\n    final Set<SegmentCommitInfo> toBeMerged = new HashSet<>();\n\n    MergeSpecification spec = null;\n\n    // Cycle to possibly select more than one merge:\n    // The trigger point for total deleted documents in the index leads to a bunch of large segment\n    // merges at the same time. So only put one large merge in the list of merges per cycle. We'll pick up another\n    // merge next time around.\n    boolean haveOneLargeMerge = false;\n\n    while (true) {\n\n      // Gather eligible segments for merging, ie segments\n      // not already being merged and not already picked (by\n      // prior iteration of this loop) for merging:\n\n      // Remove ineligible segments. These are either already being merged or already picked by prior iterations\n      Iterator<SegmentSizeAndDocs> iter = sortedEligible.iterator();\n      while (iter.hasNext()) {\n        SegmentSizeAndDocs segSizeDocs = iter.next();\n        if (toBeMerged.contains(segSizeDocs.segInfo)) {\n          iter.remove();\n        }\n      }\n\n      if (verbose(mergeContext)) {\n        message(\"  allowedSegmentCount=\" + allowedSegCount + \" vs count=\" + originalSortedSize + \" (eligible count=\" + sortedEligible.size() + \")\", mergeContext);\n      }\n\n      if (sortedEligible.size() == 0) {\n        return spec;\n      }\n\n      final int remainingDelCount = sortedEligible.stream().mapToInt(c -> c.delCount).sum();\n      if (mergeType == MERGE_TYPE.NATURAL &&\n          sortedEligible.size() <= allowedSegCount &&\n          remainingDelCount <= allowedDelCount) {\n        return spec;\n      }\n\n      // OK we are over budget -- find best merge!\n      MergeScore bestScore = null;\n      List<SegmentCommitInfo> best = null;\n      boolean bestTooLarge = false;\n      long bestMergeBytes = 0;\n\n      for (int startIdx = 0; startIdx < sortedEligible.size(); startIdx++) {\n\n        long totAfterMergeBytes = 0;\n\n        final List<SegmentCommitInfo> candidate = new ArrayList<>();\n        boolean hitTooLarge = false;\n        long bytesThisMerge = 0;\n        for (int idx = startIdx; idx < sortedEligible.size() && candidate.size() < mergeFactor && bytesThisMerge < maxMergedSegmentBytes; idx++) {\n          final SegmentSizeAndDocs segSizeDocs = sortedEligible.get(idx);\n          final long segBytes = segSizeDocs.sizeInBytes;\n\n          if (totAfterMergeBytes + segBytes > maxMergedSegmentBytes) {\n            hitTooLarge = true;\n            if (candidate.size() == 0) {\n              // We should never have something coming in that _cannot_ be merged, so handle singleton merges\n              candidate.add(segSizeDocs.segInfo);\n              bytesThisMerge += segBytes;\n            }\n            // NOTE: we continue, so that we can try\n            // \"packing\" smaller segments into this merge\n            // to see if we can get closer to the max\n            // size; this in general is not perfect since\n            // this is really \"bin packing\" and we'd have\n            // to try different permutations.\n            continue;\n          }\n          candidate.add(segSizeDocs.segInfo);\n          bytesThisMerge += segBytes;\n          totAfterMergeBytes += segBytes;\n        }\n\n        // We should never see an empty candidate: we iterated over maxMergeAtOnce\n        // segments, and already pre-excluded the too-large segments:\n        assert candidate.size() > 0;\n\n        // A singleton merge with no deletes makes no sense. We can get here when forceMerge is looping around...\n        if (candidate.size() == 1) {\n          SegmentSizeAndDocs segSizeDocs = segInfosSizes.get(candidate.get(0));\n          if (segSizeDocs.delCount == 0) {\n            continue;\n          }\n        }\n\n        // If we didn't find a too-large merge and have a list of candidates\n        // whose length is less than the merge factor, it means we are reaching\n        // the tail of the list of segments and will only find smaller merges.\n        // Stop here.\n        if (bestScore != null &&\n            hitTooLarge == false &&\n            candidate.size() < mergeFactor) {\n          break;\n        }\n\n        final MergeScore score = score(candidate, hitTooLarge, segInfosSizes);\n        if (verbose(mergeContext)) {\n          message(\"  maybe=\" + segString(mergeContext, candidate) + \" score=\" + score.getScore() + \" \" + score.getExplanation() + \" tooLarge=\" + hitTooLarge + \" size=\" + String.format(Locale.ROOT, \"%.3f MB\", totAfterMergeBytes/1024./1024.), mergeContext);\n        }\n\n        if ((bestScore == null || score.getScore() < bestScore.getScore()) && (!hitTooLarge || !maxMergeIsRunning)) {\n          best = candidate;\n          bestScore = score;\n          bestTooLarge = hitTooLarge;\n          bestMergeBytes = totAfterMergeBytes;\n        }\n      }\n\n      if (best == null) {\n        return spec;\n      }\n      // The mergeType == FORCE_MERGE_DELETES behaves as the code does currently and can create a large number of\n      // concurrent big merges. If we make findForcedDeletesMerges behave as findForcedMerges and cycle through\n      // we should remove this.\n      if (haveOneLargeMerge == false || bestTooLarge == false || mergeType == MERGE_TYPE.FORCE_MERGE_DELETES) {\n\n        haveOneLargeMerge |= bestTooLarge;\n\n        if (spec == null) {\n          spec = new MergeSpecification();\n        }\n        final OneMerge merge = new OneMerge(best);\n        spec.add(merge);\n\n        if (verbose(mergeContext)) {\n          message(\"  add merge=\" + segString(mergeContext, merge.segments) + \" size=\" + String.format(Locale.ROOT, \"%.3f MB\", bestMergeBytes / 1024. / 1024.) + \" score=\" + String.format(Locale.ROOT, \"%.3f\", bestScore.getScore()) + \" \" + bestScore.getExplanation() + (bestTooLarge ? \" [max merge]\" : \"\"), mergeContext);\n        }\n      }\n      // whether we're going to return this list in the spec of not, we need to remove it from\n      // consideration on the next loop.\n      toBeMerged.addAll(best);\n    }\n  }\n\n","sourceOld":"  private MergeSpecification doFindMerges(List<SegmentSizeAndDocs> sortedEligibleInfos,\n                                          final long maxMergedSegmentBytes,\n                                          final int mergeFactor, final int allowedSegCount,\n                                          final MERGE_TYPE mergeType,\n                                          MergeContext mergeContext,\n                                          boolean maxMergeIsRunning) throws IOException {\n\n    List<SegmentSizeAndDocs> sortedEligible = new ArrayList<>(sortedEligibleInfos);\n\n    Map<SegmentCommitInfo, SegmentSizeAndDocs> segInfosSizes = new HashMap<>();\n    for (SegmentSizeAndDocs segSizeDocs : sortedEligible) {\n      segInfosSizes.put(segSizeDocs.segInfo, segSizeDocs);\n    }\n\n    int originalSortedSize = sortedEligible.size();\n    if (verbose(mergeContext)) {\n      message(\"findMerges: \" + originalSortedSize + \" segments\", mergeContext);\n    }\n    if (originalSortedSize == 0) {\n      return null;\n    }\n\n    final Set<SegmentCommitInfo> toBeMerged = new HashSet<>();\n\n    MergeSpecification spec = null;\n\n    // Cycle to possibly select more than one merge:\n    // The trigger point for total deleted documents in the index leads to a bunch of large segment\n    // merges at the same time. So only put one large merge in the list of merges per cycle. We'll pick up another\n    // merge next time around.\n    boolean haveOneLargeMerge = false;\n\n    while (true) {\n\n      // Gather eligible segments for merging, ie segments\n      // not already being merged and not already picked (by\n      // prior iteration of this loop) for merging:\n\n      // Remove ineligible segments. These are either already being merged or already picked by prior iterations\n      Iterator<SegmentSizeAndDocs> iter = sortedEligible.iterator();\n      while (iter.hasNext()) {\n        SegmentSizeAndDocs segSizeDocs = iter.next();\n        if (toBeMerged.contains(segSizeDocs.segInfo)) {\n          iter.remove();\n        }\n      }\n\n      if (verbose(mergeContext)) {\n        message(\"  allowedSegmentCount=\" + allowedSegCount + \" vs count=\" + originalSortedSize + \" (eligible count=\" + sortedEligible.size() + \")\", mergeContext);\n      }\n\n      if (sortedEligible.size() == 0) {\n        return spec;\n      }\n\n      if (allowedSegCount != Integer.MAX_VALUE && sortedEligible.size() <= allowedSegCount && mergeType == MERGE_TYPE.NATURAL) {\n        return spec;\n      }\n\n      // OK we are over budget -- find best merge!\n      MergeScore bestScore = null;\n      List<SegmentCommitInfo> best = null;\n      boolean bestTooLarge = false;\n      long bestMergeBytes = 0;\n\n      // Consider all merge starts.\n      int lim = sortedEligible.size() - mergeFactor; // assume the usual case of background merging.\n\n      if (mergeType != MERGE_TYPE.NATURAL) { // The unusual case of forceMerge or expungeDeletes.\n        // The incoming eligible list will have only segments with > forceMergeDeletesPctAllowed in the case of\n        // findForcedDeletesMerges and segments with < max allowed size in the case of optimize.\n        // If forcing, we must allow singleton merges.\n        lim = sortedEligible.size() - 1;\n      }\n\n      for (int startIdx = 0; startIdx <= lim; startIdx++) {\n\n        long totAfterMergeBytes = 0;\n\n        final List<SegmentCommitInfo> candidate = new ArrayList<>();\n        boolean hitTooLarge = false;\n        long bytesThisMerge = 0;\n        for (int idx = startIdx; idx < sortedEligible.size() && candidate.size() < mergeFactor && bytesThisMerge < maxMergedSegmentBytes; idx++) {\n          final SegmentSizeAndDocs segSizeDocs = sortedEligible.get(idx);\n          final long segBytes = segSizeDocs.sizeInBytes;\n\n          if (totAfterMergeBytes + segBytes > maxMergedSegmentBytes) {\n            hitTooLarge = true;\n            if (candidate.size() == 0) {\n              // We should never have something coming in that _cannot_ be merged, so handle singleton merges\n              candidate.add(segSizeDocs.segInfo);\n              bytesThisMerge += segBytes;\n            }\n            // NOTE: we continue, so that we can try\n            // \"packing\" smaller segments into this merge\n            // to see if we can get closer to the max\n            // size; this in general is not perfect since\n            // this is really \"bin packing\" and we'd have\n            // to try different permutations.\n            continue;\n          }\n          candidate.add(segSizeDocs.segInfo);\n          bytesThisMerge += segBytes;\n          totAfterMergeBytes += segBytes;\n        }\n\n        // We should never see an empty candidate: we iterated over maxMergeAtOnce\n        // segments, and already pre-excluded the too-large segments:\n        assert candidate.size() > 0;\n\n        // A singleton merge with no deletes makes no sense. We can get here when forceMerge is looping around...\n        if (candidate.size() == 1) {\n          SegmentSizeAndDocs segSizeDocs = segInfosSizes.get(candidate.get(0));\n          if (segSizeDocs.delCount == 0) {\n            continue;\n          }\n        }\n\n        final MergeScore score = score(candidate, hitTooLarge, segInfosSizes);\n        if (verbose(mergeContext)) {\n          message(\"  maybe=\" + segString(mergeContext, candidate) + \" score=\" + score.getScore() + \" \" + score.getExplanation() + \" tooLarge=\" + hitTooLarge + \" size=\" + String.format(Locale.ROOT, \"%.3f MB\", totAfterMergeBytes/1024./1024.), mergeContext);\n        }\n\n        if ((bestScore == null || score.getScore() < bestScore.getScore()) && (!hitTooLarge || !maxMergeIsRunning)) {\n          best = candidate;\n          bestScore = score;\n          bestTooLarge = hitTooLarge;\n          bestMergeBytes = totAfterMergeBytes;\n        }\n      }\n\n      if (best == null) {\n        return spec;\n      }\n      // The mergeType == FORCE_MERGE_DELETES behaves as the code does currently and can create a large number of\n      // concurrent big merges. If we make findForcedDeletesMerges behave as findForcedMerges and cycle through\n      // we should remove this.\n      if (haveOneLargeMerge == false || bestTooLarge == false || mergeType == MERGE_TYPE.FORCE_MERGE_DELETES) {\n\n        haveOneLargeMerge |= bestTooLarge;\n\n        if (spec == null) {\n          spec = new MergeSpecification();\n        }\n        final OneMerge merge = new OneMerge(best);\n        spec.add(merge);\n\n        if (verbose(mergeContext)) {\n          message(\"  add merge=\" + segString(mergeContext, merge.segments) + \" size=\" + String.format(Locale.ROOT, \"%.3f MB\", bestMergeBytes / 1024. / 1024.) + \" score=\" + String.format(Locale.ROOT, \"%.3f\", bestScore.getScore()) + \" \" + bestScore.getExplanation() + (bestTooLarge ? \" [max merge]\" : \"\"), mergeContext);\n        }\n      }\n      // whether we're going to return this list in the spec of not, we need to remove it from\n      // consideration on the next loop.\n      toBeMerged.addAll(best);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"9707a68fe260631e514201dbf24e9afc9a3a4ba1":["514d769c85cddcfafabba24fb50a628dfbcc416f"],"56fb5e4e4b239474721e13b4cd9542ea2d215451":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"4a90cc8c90aa53ddf51fbd15019989ac269514a3":["9707a68fe260631e514201dbf24e9afc9a3a4ba1"],"514d769c85cddcfafabba24fb50a628dfbcc416f":["56fb5e4e4b239474721e13b4cd9542ea2d215451"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["4a90cc8c90aa53ddf51fbd15019989ac269514a3"],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","9707a68fe260631e514201dbf24e9afc9a3a4ba1"]},"commit2Childs":{"9707a68fe260631e514201dbf24e9afc9a3a4ba1":["4a90cc8c90aa53ddf51fbd15019989ac269514a3","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"56fb5e4e4b239474721e13b4cd9542ea2d215451":["514d769c85cddcfafabba24fb50a628dfbcc416f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["56fb5e4e4b239474721e13b4cd9542ea2d215451","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"4a90cc8c90aa53ddf51fbd15019989ac269514a3":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"514d769c85cddcfafabba24fb50a628dfbcc416f":["9707a68fe260631e514201dbf24e9afc9a3a4ba1"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}