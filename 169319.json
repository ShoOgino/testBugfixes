{"path":"solr/core/src/test/org/apache/solr/cloud/autoscaling/IndexSizeTriggerTest#testEstimatedIndexSize().mjava","commits":[{"id":"2d80c1ad9241ae005a167d7ee8ac473601b0e57c","date":1559036097,"type":0,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/autoscaling/IndexSizeTriggerTest#testEstimatedIndexSize().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  public void testEstimatedIndexSize() throws Exception {\n    if (!realCluster) {\n      log.info(\"This test doesn't work with a simulated cluster\");\n      return;\n    }\n    String collectionName = \"testEstimatedIndexSize_collection\";\n    CollectionAdminRequest.Create create = CollectionAdminRequest.createCollection(collectionName,\n        \"conf\", 2, 2).setMaxShardsPerNode(2);\n    create.process(solrClient);\n\n    CloudUtil.waitForState(cloudManager, \"failed to create \" + collectionName, collectionName,\n        CloudUtil.clusterShape(2, 2, false, true));\n\n    int NUM_DOCS = 20;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      SolrInputDocument doc = new SolrInputDocument(\"id\", \"id-\" + (i * 100));\n      solrClient.add(collectionName, doc);\n    }\n    solrClient.commit(collectionName);\n\n    // get the size of the leader's index\n    DocCollection coll = cloudManager.getClusterStateProvider().getCollection(collectionName);\n    Replica leader = coll.getSlice(\"shard1\").getLeader();\n    String replicaName = Utils.parseMetricsReplicaName(collectionName, leader.getCoreName());\n    assertNotNull(\"replicaName could not be constructed from \" + leader, replicaName);\n    final String registry = SolrCoreMetricManager.createRegistryName(true, collectionName, \"shard1\", replicaName, null);\n    Set<String> tags = SimUtils.COMMON_REPLICA_TAGS.stream()\n        .map(s -> \"metrics:\" + registry + \":\" + s).collect(Collectors.toSet());\n    Map<String, Object> sizes = cloudManager.getNodeStateProvider().getNodeValues(leader.getNodeName(), tags);\n    String commitSizeTag = \"metrics:\" + registry + \":SEARCHER.searcher.indexCommitSize\";\n    String numDocsTag = \"metrics:\" + registry + \":SEARCHER.searcher.numDocs\";\n    String maxDocTag = \"metrics:\" + registry + \":SEARCHER.searcher.maxDoc\";\n    assertNotNull(sizes.toString(), sizes.get(commitSizeTag));\n    assertNotNull(sizes.toString(), sizes.get(numDocsTag));\n    assertNotNull(sizes.toString(), sizes.get(maxDocTag));\n    long commitSize = ((Number)sizes.get(commitSizeTag)).longValue();\n    long maxDoc = ((Number)sizes.get(maxDocTag)).longValue();\n    long numDocs = ((Number)sizes.get(numDocsTag)).longValue();\n\n    assertEquals(\"maxDoc != numDocs\", maxDoc, numDocs);\n    assertTrue(\"unexpected numDocs=\" + numDocs, numDocs > NUM_DOCS / 3);\n\n    long aboveBytes = commitSize * 9 / 10;\n    long waitForSeconds = 3 + random().nextInt(5);\n    String setTriggerCommand = \"{\" +\n        \"'set-trigger' : {\" +\n        \"'name' : 'index_size_trigger7',\" +\n        \"'event' : 'indexSize',\" +\n        \"'waitFor' : '\" + waitForSeconds + \"s',\" +\n        \"'splitMethod' : 'link',\" +\n        \"'aboveBytes' : \" + aboveBytes + \",\" +\n        \"'enabled' : false,\" +\n        \"'actions' : [{'name' : 'compute_plan', 'class' : 'solr.ComputePlanAction'},\" +\n        \"{'name' : 'execute_plan', 'class' : '\" + ExecutePlanAction.class.getName() + \"'}]\" +\n        \"}}\";\n    SolrRequest req = AutoScalingRequest.create(SolrRequest.METHOD.POST, setTriggerCommand);\n    NamedList<Object> response = solrClient.request(req);\n    assertEquals(response.get(\"result\").toString(), \"success\");\n\n    String setListenerCommand = \"{\" +\n        \"'set-listener' : \" +\n        \"{\" +\n        \"'name' : 'capturing7',\" +\n        \"'trigger' : 'index_size_trigger7',\" +\n        \"'stage' : ['STARTED','ABORTED','SUCCEEDED','FAILED'],\" +\n        \"'beforeAction' : ['compute_plan','execute_plan'],\" +\n        \"'afterAction' : ['compute_plan','execute_plan'],\" +\n        \"'class' : '\" + CapturingTriggerListener.class.getName() + \"'\" +\n        \"}\" +\n        \"}\";\n    req = AutoScalingRequest.create(SolrRequest.METHOD.POST, setListenerCommand);\n    response = solrClient.request(req);\n    assertEquals(response.get(\"result\").toString(), \"success\");\n\n    setListenerCommand = \"{\" +\n        \"'set-listener' : \" +\n        \"{\" +\n        \"'name' : 'finished',\" +\n        \"'trigger' : 'index_size_trigger7',\" +\n        \"'stage' : ['SUCCEEDED'],\" +\n        \"'class' : '\" + FinishedProcessingListener.class.getName() + \"'\" +\n        \"}\" +\n        \"}\";\n    req = AutoScalingRequest.create(SolrRequest.METHOD.POST, setListenerCommand);\n    response = solrClient.request(req);\n    assertEquals(response.get(\"result\").toString(), \"success\");\n\n    // enable the trigger\n    String resumeTriggerCommand = \"{\" +\n        \"'resume-trigger' : {\" +\n        \"'name' : 'index_size_trigger7'\" +\n        \"}\" +\n        \"}\";\n    req = AutoScalingRequest.create(SolrRequest.METHOD.POST, resumeTriggerCommand);\n    response = solrClient.request(req);\n    assertEquals(\"success\", response.get(\"result\").toString());\n\n    // aboveBytes was set to be slightly lower than the actual size of at least one shard, so\n    // we're expecting a SPLITSHARD - but with 'link' method the actual size of the resulting shards\n    // will likely not go down. However, the estimated size of the latest commit point will go down\n    // (see SOLR-12941).\n\n    timeSource.sleep(TimeUnit.MILLISECONDS.convert(waitForSeconds + 1, TimeUnit.SECONDS));\n\n    boolean await = finished.await(90000 / SPEED, TimeUnit.MILLISECONDS);\n    assertTrue(\"did not finish processing in time\", await);\n    // suspend the trigger\n    String suspendTriggerCommand = \"{\" +\n        \"'suspend-trigger' : {\" +\n        \"'name' : 'index_size_trigger7'\" +\n        \"}\" +\n        \"}\";\n    req = AutoScalingRequest.create(SolrRequest.METHOD.POST, resumeTriggerCommand);\n    response = solrClient.request(req);\n    assertEquals(\"success\", response.get(\"result\").toString());\n\n    assertEquals(1, listenerEvents.size());\n    List<CapturedEvent> events = listenerEvents.get(\"capturing7\");\n    assertNotNull(listenerEvents.toString(), events);\n    assertFalse(\"empty events?\", events.isEmpty());\n    CapturedEvent ev = events.get(0);\n    List<TriggerEvent.Op> ops = (List< TriggerEvent.Op>)ev.event.properties.get(TriggerEvent.REQUESTED_OPS);\n    assertNotNull(\"no requested ops in \" + ev, ops);\n    assertFalse(\"empty list of ops in \" + ev, ops.isEmpty());\n    Set<String> parentShards = new HashSet<>();\n    ops.forEach(op -> {\n      assertTrue(op.toString(), op.getAction() == CollectionParams.CollectionAction.SPLITSHARD);\n      Collection<Pair<String, String>> hints = (Collection<Pair<String, String>>)op.getHints().get(Suggester.Hint.COLL_SHARD);\n      assertNotNull(\"no hints in op \" + op, hints);\n      hints.forEach(h -> parentShards.add(h.second()));\n    });\n\n    // allow for recovery of at least some sub-shards\n    timeSource.sleep(TimeUnit.MILLISECONDS.convert(waitForSeconds + 1, TimeUnit.SECONDS));\n\n    coll = cloudManager.getClusterStateProvider().getCollection(collectionName);\n\n    int checkedSubShards = 0;\n\n    for (String parentShard : parentShards) {\n      for (String subShard : Arrays.asList(parentShard + \"_0\", parentShard + \"_1\")) {\n        leader = coll.getSlice(subShard).getLeader();\n        if (leader == null) {\n          // no leader yet - skip it\n        }\n        checkedSubShards++;\n        replicaName = Utils.parseMetricsReplicaName(collectionName, leader.getCoreName());\n        assertNotNull(\"replicaName could not be constructed from \" + leader, replicaName);\n        final String subregistry = SolrCoreMetricManager.createRegistryName(true, collectionName, subShard, replicaName, null);\n        Set<String> subtags = SimUtils.COMMON_REPLICA_TAGS.stream()\n            .map(s -> \"metrics:\" + subregistry + \":\" + s).collect(Collectors.toSet());\n        sizes = cloudManager.getNodeStateProvider().getNodeValues(leader.getNodeName(), subtags);\n        commitSizeTag = \"metrics:\" + subregistry + \":SEARCHER.searcher.indexCommitSize\";\n        numDocsTag = \"metrics:\" + subregistry + \":SEARCHER.searcher.numDocs\";\n        maxDocTag = \"metrics:\" + subregistry + \":SEARCHER.searcher.maxDoc\";\n        assertNotNull(sizes.toString(), sizes.get(commitSizeTag));\n        assertNotNull(sizes.toString(), sizes.get(numDocsTag));\n        assertNotNull(sizes.toString(), sizes.get(maxDocTag));\n        long subCommitSize = ((Number)sizes.get(commitSizeTag)).longValue();\n        long subMaxDoc = ((Number)sizes.get(maxDocTag)).longValue();\n        long subNumDocs = ((Number)sizes.get(numDocsTag)).longValue();\n        assertTrue(\"subNumDocs=\" + subNumDocs + \" should be less than subMaxDoc=\" + subMaxDoc +\n            \" due to link split\", subNumDocs < subMaxDoc);\n        assertTrue(\"subCommitSize=\" + subCommitSize + \" should be still greater than aboveBytes=\" + aboveBytes +\n            \" due to link split\", subCommitSize > aboveBytes);\n        // calculate estimated size using the same formula\n        long estimatedSize = IndexSizeTrigger.estimatedSize(subMaxDoc, subNumDocs, subCommitSize);\n        assertTrue(\"estimatedSize=\" + estimatedSize + \" should be lower than aboveBytes=\" + aboveBytes,\n            estimatedSize < aboveBytes);\n      }\n    }\n\n    assertTrue(\"didn't find any leaders in new sub-shards\", checkedSubShards > 0);\n\n    // reset & resume\n    listenerEvents.clear();\n    finished = new CountDownLatch(1);\n    req = AutoScalingRequest.create(SolrRequest.METHOD.POST, resumeTriggerCommand);\n    response = solrClient.request(req);\n    assertEquals(\"success\", response.get(\"result\").toString());\n    timeSource.sleep(TimeUnit.MILLISECONDS.convert(waitForSeconds + 1, TimeUnit.SECONDS));\n\n    // estimated shard size should fall well below the aboveBytes, even though the real commitSize\n    // still remains larger due to the splitMethod=link side-effects\n    await = finished.await(10000 / SPEED, TimeUnit.MILLISECONDS);\n    assertFalse(\"should not fire the trigger again! \" + listenerEvents, await);\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a250c410a74277b5acb7ef5d8b5cb3e60fbd71a4","date":1576125737,"type":5,"author":"Ishan Chattopadhyaya","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/autoscaling/IndexSizeTriggerSizeEstimationTest#testEstimatedIndexSize().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/autoscaling/IndexSizeTriggerTest#testEstimatedIndexSize().mjava","sourceNew":"  @Test\n  public void testEstimatedIndexSize() throws Exception {\n    String collectionName = \"testEstimatedIndexSize_collection\";\n    CollectionAdminRequest.Create create = CollectionAdminRequest.createCollection(collectionName,\n        \"conf\", 2, 2).setMaxShardsPerNode(2);\n    create.process(solrClient);\n\n    CloudUtil.waitForState(cloudManager, \"failed to create \" + collectionName, collectionName,\n        CloudUtil.clusterShape(2, 2, false, true));\n\n    int NUM_DOCS = 20;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      SolrInputDocument doc = new SolrInputDocument(\"id\", \"id-\" + (i * 100));\n      solrClient.add(collectionName, doc);\n    }\n    solrClient.commit(collectionName);\n\n    // get the size of the leader's index\n    DocCollection coll = cloudManager.getClusterStateProvider().getCollection(collectionName);\n    Replica leader = coll.getSlice(\"shard1\").getLeader();\n    String replicaName = Utils.parseMetricsReplicaName(collectionName, leader.getCoreName());\n    assertNotNull(\"replicaName could not be constructed from \" + leader, replicaName);\n    final String registry = SolrCoreMetricManager.createRegistryName(true, collectionName, \"shard1\", replicaName, null);\n    Set<String> tags = SimUtils.COMMON_REPLICA_TAGS.stream()\n        .map(s -> \"metrics:\" + registry + \":\" + s).collect(Collectors.toSet());\n    Map<String, Object> sizes = cloudManager.getNodeStateProvider().getNodeValues(leader.getNodeName(), tags);\n    String commitSizeTag = \"metrics:\" + registry + \":SEARCHER.searcher.indexCommitSize\";\n    String numDocsTag = \"metrics:\" + registry + \":SEARCHER.searcher.numDocs\";\n    String maxDocTag = \"metrics:\" + registry + \":SEARCHER.searcher.maxDoc\";\n    assertNotNull(sizes.toString(), sizes.get(commitSizeTag));\n    assertNotNull(sizes.toString(), sizes.get(numDocsTag));\n    assertNotNull(sizes.toString(), sizes.get(maxDocTag));\n    long commitSize = ((Number)sizes.get(commitSizeTag)).longValue();\n    long maxDoc = ((Number)sizes.get(maxDocTag)).longValue();\n    long numDocs = ((Number)sizes.get(numDocsTag)).longValue();\n\n    assertEquals(\"maxDoc != numDocs\", maxDoc, numDocs);\n    assertTrue(\"unexpected numDocs=\" + numDocs, numDocs > NUM_DOCS / 3);\n\n    long aboveBytes = commitSize * 9 / 10;\n    long waitForSeconds = 3 + random().nextInt(5);\n    String setTriggerCommand = \"{\" +\n        \"'set-trigger' : {\" +\n        \"'name' : 'index_size_trigger7',\" +\n        \"'event' : 'indexSize',\" +\n        \"'waitFor' : '\" + waitForSeconds + \"s',\" +\n        \"'splitMethod' : 'link',\" +\n        \"'aboveBytes' : \" + aboveBytes + \",\" +\n        \"'enabled' : false,\" +\n        \"'actions' : [{'name' : 'compute_plan', 'class' : 'solr.ComputePlanAction'},\" +\n        \"{'name' : 'execute_plan', 'class' : '\" + ExecutePlanAction.class.getName() + \"'}]\" +\n        \"}}\";\n    SolrRequest req = AutoScalingRequest.create(SolrRequest.METHOD.POST, setTriggerCommand);\n    NamedList<Object> response = solrClient.request(req);\n    assertEquals(response.get(\"result\").toString(), \"success\");\n\n    String setListenerCommand = \"{\" +\n        \"'set-listener' : \" +\n        \"{\" +\n        \"'name' : 'capturing7',\" +\n        \"'trigger' : 'index_size_trigger7',\" +\n        \"'stage' : ['STARTED','ABORTED','SUCCEEDED','FAILED'],\" +\n        \"'beforeAction' : ['compute_plan','execute_plan'],\" +\n        \"'afterAction' : ['compute_plan','execute_plan'],\" +\n        \"'class' : '\" + CapturingTriggerListener.class.getName() + \"'\" +\n        \"}\" +\n        \"}\";\n    req = AutoScalingRequest.create(SolrRequest.METHOD.POST, setListenerCommand);\n    response = solrClient.request(req);\n    assertEquals(response.get(\"result\").toString(), \"success\");\n\n    setListenerCommand = \"{\" +\n        \"'set-listener' : \" +\n        \"{\" +\n        \"'name' : 'finished',\" +\n        \"'trigger' : 'index_size_trigger7',\" +\n        \"'stage' : ['SUCCEEDED'],\" +\n        \"'class' : '\" + FinishedProcessingListener.class.getName() + \"'\" +\n        \"}\" +\n        \"}\";\n    req = AutoScalingRequest.create(SolrRequest.METHOD.POST, setListenerCommand);\n    response = solrClient.request(req);\n    assertEquals(response.get(\"result\").toString(), \"success\");\n\n    // enable the trigger\n    String resumeTriggerCommand = \"{\" +\n        \"'resume-trigger' : {\" +\n        \"'name' : 'index_size_trigger7'\" +\n        \"}\" +\n        \"}\";\n    req = AutoScalingRequest.create(SolrRequest.METHOD.POST, resumeTriggerCommand);\n    response = solrClient.request(req);\n    assertEquals(\"success\", response.get(\"result\").toString());\n\n    // aboveBytes was set to be slightly lower than the actual size of at least one shard, so\n    // we're expecting a SPLITSHARD - but with 'link' method the actual size of the resulting shards\n    // will likely not go down. However, the estimated size of the latest commit point will go down\n    // (see SOLR-12941).\n\n    timeSource.sleep(TimeUnit.MILLISECONDS.convert(waitForSeconds + 1, TimeUnit.SECONDS));\n\n    boolean await = finished.await(90000 / SPEED, TimeUnit.MILLISECONDS);\n    assertTrue(\"did not finish processing in time\", await);\n    // suspend the trigger\n    String suspendTriggerCommand = \"{\" +\n        \"'suspend-trigger' : {\" +\n        \"'name' : 'index_size_trigger7'\" +\n        \"}\" +\n        \"}\";\n    req = AutoScalingRequest.create(SolrRequest.METHOD.POST, resumeTriggerCommand);\n    response = solrClient.request(req);\n    assertEquals(\"success\", response.get(\"result\").toString());\n\n    assertEquals(1, listenerEvents.size());\n    List<CapturedEvent> events = listenerEvents.get(\"capturing7\");\n    assertNotNull(listenerEvents.toString(), events);\n    assertFalse(\"empty events?\", events.isEmpty());\n    CapturedEvent ev = events.get(0);\n    List<TriggerEvent.Op> ops = (List< TriggerEvent.Op>)ev.event.properties.get(TriggerEvent.REQUESTED_OPS);\n    assertNotNull(\"no requested ops in \" + ev, ops);\n    assertFalse(\"empty list of ops in \" + ev, ops.isEmpty());\n    Set<String> parentShards = new HashSet<>();\n    ops.forEach(op -> {\n      assertTrue(op.toString(), op.getAction() == CollectionParams.CollectionAction.SPLITSHARD);\n      Collection<Pair<String, String>> hints = (Collection<Pair<String, String>>)op.getHints().get(Suggester.Hint.COLL_SHARD);\n      assertNotNull(\"no hints in op \" + op, hints);\n      hints.forEach(h -> parentShards.add(h.second()));\n    });\n\n    // allow for recovery of at least some sub-shards\n    timeSource.sleep(TimeUnit.MILLISECONDS.convert(waitForSeconds + 1, TimeUnit.SECONDS));\n\n    coll = cloudManager.getClusterStateProvider().getCollection(collectionName);\n\n    int checkedSubShards = 0;\n\n    for (String parentShard : parentShards) {\n      for (String subShard : Arrays.asList(parentShard + \"_0\", parentShard + \"_1\")) {\n        leader = coll.getSlice(subShard).getLeader();\n        if (leader == null) {\n          // no leader yet - skip it\n        }\n        checkedSubShards++;\n        replicaName = Utils.parseMetricsReplicaName(collectionName, leader.getCoreName());\n        assertNotNull(\"replicaName could not be constructed from \" + leader, replicaName);\n        final String subregistry = SolrCoreMetricManager.createRegistryName(true, collectionName, subShard, replicaName, null);\n        Set<String> subtags = SimUtils.COMMON_REPLICA_TAGS.stream()\n            .map(s -> \"metrics:\" + subregistry + \":\" + s).collect(Collectors.toSet());\n        sizes = cloudManager.getNodeStateProvider().getNodeValues(leader.getNodeName(), subtags);\n        commitSizeTag = \"metrics:\" + subregistry + \":SEARCHER.searcher.indexCommitSize\";\n        numDocsTag = \"metrics:\" + subregistry + \":SEARCHER.searcher.numDocs\";\n        maxDocTag = \"metrics:\" + subregistry + \":SEARCHER.searcher.maxDoc\";\n        assertNotNull(sizes.toString(), sizes.get(commitSizeTag));\n        assertNotNull(sizes.toString(), sizes.get(numDocsTag));\n        assertNotNull(sizes.toString(), sizes.get(maxDocTag));\n        long subCommitSize = ((Number)sizes.get(commitSizeTag)).longValue();\n        long subMaxDoc = ((Number)sizes.get(maxDocTag)).longValue();\n        long subNumDocs = ((Number)sizes.get(numDocsTag)).longValue();\n        assertTrue(\"subNumDocs=\" + subNumDocs + \" should be less than subMaxDoc=\" + subMaxDoc +\n            \" due to link split\", subNumDocs < subMaxDoc);\n        assertTrue(\"subCommitSize=\" + subCommitSize + \" should be still greater than aboveBytes=\" + aboveBytes +\n            \" due to link split\", subCommitSize > aboveBytes);\n        // calculate estimated size using the same formula\n        long estimatedSize = IndexSizeTrigger.estimatedSize(subMaxDoc, subNumDocs, subCommitSize);\n        assertTrue(\"estimatedSize=\" + estimatedSize + \" should be lower than aboveBytes=\" + aboveBytes,\n            estimatedSize < aboveBytes);\n      }\n    }\n\n    assertTrue(\"didn't find any leaders in new sub-shards\", checkedSubShards > 0);\n\n    // reset & resume\n    listenerEvents.clear();\n    finished = new CountDownLatch(1);\n    req = AutoScalingRequest.create(SolrRequest.METHOD.POST, resumeTriggerCommand);\n    response = solrClient.request(req);\n    assertEquals(\"success\", response.get(\"result\").toString());\n    timeSource.sleep(TimeUnit.MILLISECONDS.convert(waitForSeconds + 1, TimeUnit.SECONDS));\n\n    // estimated shard size should fall well below the aboveBytes, even though the real commitSize\n    // still remains larger due to the splitMethod=link side-effects\n    await = finished.await(10000 / SPEED, TimeUnit.MILLISECONDS);\n    assertFalse(\"should not fire the trigger again! \" + listenerEvents, await);\n\n  }\n\n","sourceOld":"  @Test\n  public void testEstimatedIndexSize() throws Exception {\n    if (!realCluster) {\n      log.info(\"This test doesn't work with a simulated cluster\");\n      return;\n    }\n    String collectionName = \"testEstimatedIndexSize_collection\";\n    CollectionAdminRequest.Create create = CollectionAdminRequest.createCollection(collectionName,\n        \"conf\", 2, 2).setMaxShardsPerNode(2);\n    create.process(solrClient);\n\n    CloudUtil.waitForState(cloudManager, \"failed to create \" + collectionName, collectionName,\n        CloudUtil.clusterShape(2, 2, false, true));\n\n    int NUM_DOCS = 20;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      SolrInputDocument doc = new SolrInputDocument(\"id\", \"id-\" + (i * 100));\n      solrClient.add(collectionName, doc);\n    }\n    solrClient.commit(collectionName);\n\n    // get the size of the leader's index\n    DocCollection coll = cloudManager.getClusterStateProvider().getCollection(collectionName);\n    Replica leader = coll.getSlice(\"shard1\").getLeader();\n    String replicaName = Utils.parseMetricsReplicaName(collectionName, leader.getCoreName());\n    assertNotNull(\"replicaName could not be constructed from \" + leader, replicaName);\n    final String registry = SolrCoreMetricManager.createRegistryName(true, collectionName, \"shard1\", replicaName, null);\n    Set<String> tags = SimUtils.COMMON_REPLICA_TAGS.stream()\n        .map(s -> \"metrics:\" + registry + \":\" + s).collect(Collectors.toSet());\n    Map<String, Object> sizes = cloudManager.getNodeStateProvider().getNodeValues(leader.getNodeName(), tags);\n    String commitSizeTag = \"metrics:\" + registry + \":SEARCHER.searcher.indexCommitSize\";\n    String numDocsTag = \"metrics:\" + registry + \":SEARCHER.searcher.numDocs\";\n    String maxDocTag = \"metrics:\" + registry + \":SEARCHER.searcher.maxDoc\";\n    assertNotNull(sizes.toString(), sizes.get(commitSizeTag));\n    assertNotNull(sizes.toString(), sizes.get(numDocsTag));\n    assertNotNull(sizes.toString(), sizes.get(maxDocTag));\n    long commitSize = ((Number)sizes.get(commitSizeTag)).longValue();\n    long maxDoc = ((Number)sizes.get(maxDocTag)).longValue();\n    long numDocs = ((Number)sizes.get(numDocsTag)).longValue();\n\n    assertEquals(\"maxDoc != numDocs\", maxDoc, numDocs);\n    assertTrue(\"unexpected numDocs=\" + numDocs, numDocs > NUM_DOCS / 3);\n\n    long aboveBytes = commitSize * 9 / 10;\n    long waitForSeconds = 3 + random().nextInt(5);\n    String setTriggerCommand = \"{\" +\n        \"'set-trigger' : {\" +\n        \"'name' : 'index_size_trigger7',\" +\n        \"'event' : 'indexSize',\" +\n        \"'waitFor' : '\" + waitForSeconds + \"s',\" +\n        \"'splitMethod' : 'link',\" +\n        \"'aboveBytes' : \" + aboveBytes + \",\" +\n        \"'enabled' : false,\" +\n        \"'actions' : [{'name' : 'compute_plan', 'class' : 'solr.ComputePlanAction'},\" +\n        \"{'name' : 'execute_plan', 'class' : '\" + ExecutePlanAction.class.getName() + \"'}]\" +\n        \"}}\";\n    SolrRequest req = AutoScalingRequest.create(SolrRequest.METHOD.POST, setTriggerCommand);\n    NamedList<Object> response = solrClient.request(req);\n    assertEquals(response.get(\"result\").toString(), \"success\");\n\n    String setListenerCommand = \"{\" +\n        \"'set-listener' : \" +\n        \"{\" +\n        \"'name' : 'capturing7',\" +\n        \"'trigger' : 'index_size_trigger7',\" +\n        \"'stage' : ['STARTED','ABORTED','SUCCEEDED','FAILED'],\" +\n        \"'beforeAction' : ['compute_plan','execute_plan'],\" +\n        \"'afterAction' : ['compute_plan','execute_plan'],\" +\n        \"'class' : '\" + CapturingTriggerListener.class.getName() + \"'\" +\n        \"}\" +\n        \"}\";\n    req = AutoScalingRequest.create(SolrRequest.METHOD.POST, setListenerCommand);\n    response = solrClient.request(req);\n    assertEquals(response.get(\"result\").toString(), \"success\");\n\n    setListenerCommand = \"{\" +\n        \"'set-listener' : \" +\n        \"{\" +\n        \"'name' : 'finished',\" +\n        \"'trigger' : 'index_size_trigger7',\" +\n        \"'stage' : ['SUCCEEDED'],\" +\n        \"'class' : '\" + FinishedProcessingListener.class.getName() + \"'\" +\n        \"}\" +\n        \"}\";\n    req = AutoScalingRequest.create(SolrRequest.METHOD.POST, setListenerCommand);\n    response = solrClient.request(req);\n    assertEquals(response.get(\"result\").toString(), \"success\");\n\n    // enable the trigger\n    String resumeTriggerCommand = \"{\" +\n        \"'resume-trigger' : {\" +\n        \"'name' : 'index_size_trigger7'\" +\n        \"}\" +\n        \"}\";\n    req = AutoScalingRequest.create(SolrRequest.METHOD.POST, resumeTriggerCommand);\n    response = solrClient.request(req);\n    assertEquals(\"success\", response.get(\"result\").toString());\n\n    // aboveBytes was set to be slightly lower than the actual size of at least one shard, so\n    // we're expecting a SPLITSHARD - but with 'link' method the actual size of the resulting shards\n    // will likely not go down. However, the estimated size of the latest commit point will go down\n    // (see SOLR-12941).\n\n    timeSource.sleep(TimeUnit.MILLISECONDS.convert(waitForSeconds + 1, TimeUnit.SECONDS));\n\n    boolean await = finished.await(90000 / SPEED, TimeUnit.MILLISECONDS);\n    assertTrue(\"did not finish processing in time\", await);\n    // suspend the trigger\n    String suspendTriggerCommand = \"{\" +\n        \"'suspend-trigger' : {\" +\n        \"'name' : 'index_size_trigger7'\" +\n        \"}\" +\n        \"}\";\n    req = AutoScalingRequest.create(SolrRequest.METHOD.POST, resumeTriggerCommand);\n    response = solrClient.request(req);\n    assertEquals(\"success\", response.get(\"result\").toString());\n\n    assertEquals(1, listenerEvents.size());\n    List<CapturedEvent> events = listenerEvents.get(\"capturing7\");\n    assertNotNull(listenerEvents.toString(), events);\n    assertFalse(\"empty events?\", events.isEmpty());\n    CapturedEvent ev = events.get(0);\n    List<TriggerEvent.Op> ops = (List< TriggerEvent.Op>)ev.event.properties.get(TriggerEvent.REQUESTED_OPS);\n    assertNotNull(\"no requested ops in \" + ev, ops);\n    assertFalse(\"empty list of ops in \" + ev, ops.isEmpty());\n    Set<String> parentShards = new HashSet<>();\n    ops.forEach(op -> {\n      assertTrue(op.toString(), op.getAction() == CollectionParams.CollectionAction.SPLITSHARD);\n      Collection<Pair<String, String>> hints = (Collection<Pair<String, String>>)op.getHints().get(Suggester.Hint.COLL_SHARD);\n      assertNotNull(\"no hints in op \" + op, hints);\n      hints.forEach(h -> parentShards.add(h.second()));\n    });\n\n    // allow for recovery of at least some sub-shards\n    timeSource.sleep(TimeUnit.MILLISECONDS.convert(waitForSeconds + 1, TimeUnit.SECONDS));\n\n    coll = cloudManager.getClusterStateProvider().getCollection(collectionName);\n\n    int checkedSubShards = 0;\n\n    for (String parentShard : parentShards) {\n      for (String subShard : Arrays.asList(parentShard + \"_0\", parentShard + \"_1\")) {\n        leader = coll.getSlice(subShard).getLeader();\n        if (leader == null) {\n          // no leader yet - skip it\n        }\n        checkedSubShards++;\n        replicaName = Utils.parseMetricsReplicaName(collectionName, leader.getCoreName());\n        assertNotNull(\"replicaName could not be constructed from \" + leader, replicaName);\n        final String subregistry = SolrCoreMetricManager.createRegistryName(true, collectionName, subShard, replicaName, null);\n        Set<String> subtags = SimUtils.COMMON_REPLICA_TAGS.stream()\n            .map(s -> \"metrics:\" + subregistry + \":\" + s).collect(Collectors.toSet());\n        sizes = cloudManager.getNodeStateProvider().getNodeValues(leader.getNodeName(), subtags);\n        commitSizeTag = \"metrics:\" + subregistry + \":SEARCHER.searcher.indexCommitSize\";\n        numDocsTag = \"metrics:\" + subregistry + \":SEARCHER.searcher.numDocs\";\n        maxDocTag = \"metrics:\" + subregistry + \":SEARCHER.searcher.maxDoc\";\n        assertNotNull(sizes.toString(), sizes.get(commitSizeTag));\n        assertNotNull(sizes.toString(), sizes.get(numDocsTag));\n        assertNotNull(sizes.toString(), sizes.get(maxDocTag));\n        long subCommitSize = ((Number)sizes.get(commitSizeTag)).longValue();\n        long subMaxDoc = ((Number)sizes.get(maxDocTag)).longValue();\n        long subNumDocs = ((Number)sizes.get(numDocsTag)).longValue();\n        assertTrue(\"subNumDocs=\" + subNumDocs + \" should be less than subMaxDoc=\" + subMaxDoc +\n            \" due to link split\", subNumDocs < subMaxDoc);\n        assertTrue(\"subCommitSize=\" + subCommitSize + \" should be still greater than aboveBytes=\" + aboveBytes +\n            \" due to link split\", subCommitSize > aboveBytes);\n        // calculate estimated size using the same formula\n        long estimatedSize = IndexSizeTrigger.estimatedSize(subMaxDoc, subNumDocs, subCommitSize);\n        assertTrue(\"estimatedSize=\" + estimatedSize + \" should be lower than aboveBytes=\" + aboveBytes,\n            estimatedSize < aboveBytes);\n      }\n    }\n\n    assertTrue(\"didn't find any leaders in new sub-shards\", checkedSubShards > 0);\n\n    // reset & resume\n    listenerEvents.clear();\n    finished = new CountDownLatch(1);\n    req = AutoScalingRequest.create(SolrRequest.METHOD.POST, resumeTriggerCommand);\n    response = solrClient.request(req);\n    assertEquals(\"success\", response.get(\"result\").toString());\n    timeSource.sleep(TimeUnit.MILLISECONDS.convert(waitForSeconds + 1, TimeUnit.SECONDS));\n\n    // estimated shard size should fall well below the aboveBytes, even though the real commitSize\n    // still remains larger due to the splitMethod=link side-effects\n    await = finished.await(10000 / SPEED, TimeUnit.MILLISECONDS);\n    assertFalse(\"should not fire the trigger again! \" + listenerEvents, await);\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"df724d84dab24a0cc54bec95a8680867adc7f171","date":1576156608,"type":5,"author":"Dawid Weiss","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/autoscaling/IndexSizeTriggerSizeEstimationTest#testEstimatedIndexSize().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/autoscaling/IndexSizeTriggerTest#testEstimatedIndexSize().mjava","sourceNew":"  @Test\n  public void testEstimatedIndexSize() throws Exception {\n    String collectionName = \"testEstimatedIndexSize_collection\";\n    CollectionAdminRequest.Create create = CollectionAdminRequest.createCollection(collectionName,\n        \"conf\", 2, 2).setMaxShardsPerNode(2);\n    create.process(solrClient);\n\n    CloudUtil.waitForState(cloudManager, \"failed to create \" + collectionName, collectionName,\n        CloudUtil.clusterShape(2, 2, false, true));\n\n    int NUM_DOCS = 20;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      SolrInputDocument doc = new SolrInputDocument(\"id\", \"id-\" + (i * 100));\n      solrClient.add(collectionName, doc);\n    }\n    solrClient.commit(collectionName);\n\n    // get the size of the leader's index\n    DocCollection coll = cloudManager.getClusterStateProvider().getCollection(collectionName);\n    Replica leader = coll.getSlice(\"shard1\").getLeader();\n    String replicaName = Utils.parseMetricsReplicaName(collectionName, leader.getCoreName());\n    assertNotNull(\"replicaName could not be constructed from \" + leader, replicaName);\n    final String registry = SolrCoreMetricManager.createRegistryName(true, collectionName, \"shard1\", replicaName, null);\n    Set<String> tags = SimUtils.COMMON_REPLICA_TAGS.stream()\n        .map(s -> \"metrics:\" + registry + \":\" + s).collect(Collectors.toSet());\n    Map<String, Object> sizes = cloudManager.getNodeStateProvider().getNodeValues(leader.getNodeName(), tags);\n    String commitSizeTag = \"metrics:\" + registry + \":SEARCHER.searcher.indexCommitSize\";\n    String numDocsTag = \"metrics:\" + registry + \":SEARCHER.searcher.numDocs\";\n    String maxDocTag = \"metrics:\" + registry + \":SEARCHER.searcher.maxDoc\";\n    assertNotNull(sizes.toString(), sizes.get(commitSizeTag));\n    assertNotNull(sizes.toString(), sizes.get(numDocsTag));\n    assertNotNull(sizes.toString(), sizes.get(maxDocTag));\n    long commitSize = ((Number)sizes.get(commitSizeTag)).longValue();\n    long maxDoc = ((Number)sizes.get(maxDocTag)).longValue();\n    long numDocs = ((Number)sizes.get(numDocsTag)).longValue();\n\n    assertEquals(\"maxDoc != numDocs\", maxDoc, numDocs);\n    assertTrue(\"unexpected numDocs=\" + numDocs, numDocs > NUM_DOCS / 3);\n\n    long aboveBytes = commitSize * 9 / 10;\n    long waitForSeconds = 3 + random().nextInt(5);\n    String setTriggerCommand = \"{\" +\n        \"'set-trigger' : {\" +\n        \"'name' : 'index_size_trigger7',\" +\n        \"'event' : 'indexSize',\" +\n        \"'waitFor' : '\" + waitForSeconds + \"s',\" +\n        \"'splitMethod' : 'link',\" +\n        \"'aboveBytes' : \" + aboveBytes + \",\" +\n        \"'enabled' : false,\" +\n        \"'actions' : [{'name' : 'compute_plan', 'class' : 'solr.ComputePlanAction'},\" +\n        \"{'name' : 'execute_plan', 'class' : '\" + ExecutePlanAction.class.getName() + \"'}]\" +\n        \"}}\";\n    SolrRequest req = AutoScalingRequest.create(SolrRequest.METHOD.POST, setTriggerCommand);\n    NamedList<Object> response = solrClient.request(req);\n    assertEquals(response.get(\"result\").toString(), \"success\");\n\n    String setListenerCommand = \"{\" +\n        \"'set-listener' : \" +\n        \"{\" +\n        \"'name' : 'capturing7',\" +\n        \"'trigger' : 'index_size_trigger7',\" +\n        \"'stage' : ['STARTED','ABORTED','SUCCEEDED','FAILED'],\" +\n        \"'beforeAction' : ['compute_plan','execute_plan'],\" +\n        \"'afterAction' : ['compute_plan','execute_plan'],\" +\n        \"'class' : '\" + CapturingTriggerListener.class.getName() + \"'\" +\n        \"}\" +\n        \"}\";\n    req = AutoScalingRequest.create(SolrRequest.METHOD.POST, setListenerCommand);\n    response = solrClient.request(req);\n    assertEquals(response.get(\"result\").toString(), \"success\");\n\n    setListenerCommand = \"{\" +\n        \"'set-listener' : \" +\n        \"{\" +\n        \"'name' : 'finished',\" +\n        \"'trigger' : 'index_size_trigger7',\" +\n        \"'stage' : ['SUCCEEDED'],\" +\n        \"'class' : '\" + FinishedProcessingListener.class.getName() + \"'\" +\n        \"}\" +\n        \"}\";\n    req = AutoScalingRequest.create(SolrRequest.METHOD.POST, setListenerCommand);\n    response = solrClient.request(req);\n    assertEquals(response.get(\"result\").toString(), \"success\");\n\n    // enable the trigger\n    String resumeTriggerCommand = \"{\" +\n        \"'resume-trigger' : {\" +\n        \"'name' : 'index_size_trigger7'\" +\n        \"}\" +\n        \"}\";\n    req = AutoScalingRequest.create(SolrRequest.METHOD.POST, resumeTriggerCommand);\n    response = solrClient.request(req);\n    assertEquals(\"success\", response.get(\"result\").toString());\n\n    // aboveBytes was set to be slightly lower than the actual size of at least one shard, so\n    // we're expecting a SPLITSHARD - but with 'link' method the actual size of the resulting shards\n    // will likely not go down. However, the estimated size of the latest commit point will go down\n    // (see SOLR-12941).\n\n    timeSource.sleep(TimeUnit.MILLISECONDS.convert(waitForSeconds + 1, TimeUnit.SECONDS));\n\n    boolean await = finished.await(90000 / SPEED, TimeUnit.MILLISECONDS);\n    assertTrue(\"did not finish processing in time\", await);\n    // suspend the trigger\n    String suspendTriggerCommand = \"{\" +\n        \"'suspend-trigger' : {\" +\n        \"'name' : 'index_size_trigger7'\" +\n        \"}\" +\n        \"}\";\n    req = AutoScalingRequest.create(SolrRequest.METHOD.POST, resumeTriggerCommand);\n    response = solrClient.request(req);\n    assertEquals(\"success\", response.get(\"result\").toString());\n\n    assertEquals(1, listenerEvents.size());\n    List<CapturedEvent> events = listenerEvents.get(\"capturing7\");\n    assertNotNull(listenerEvents.toString(), events);\n    assertFalse(\"empty events?\", events.isEmpty());\n    CapturedEvent ev = events.get(0);\n    List<TriggerEvent.Op> ops = (List< TriggerEvent.Op>)ev.event.properties.get(TriggerEvent.REQUESTED_OPS);\n    assertNotNull(\"no requested ops in \" + ev, ops);\n    assertFalse(\"empty list of ops in \" + ev, ops.isEmpty());\n    Set<String> parentShards = new HashSet<>();\n    ops.forEach(op -> {\n      assertTrue(op.toString(), op.getAction() == CollectionParams.CollectionAction.SPLITSHARD);\n      Collection<Pair<String, String>> hints = (Collection<Pair<String, String>>)op.getHints().get(Suggester.Hint.COLL_SHARD);\n      assertNotNull(\"no hints in op \" + op, hints);\n      hints.forEach(h -> parentShards.add(h.second()));\n    });\n\n    // allow for recovery of at least some sub-shards\n    timeSource.sleep(TimeUnit.MILLISECONDS.convert(waitForSeconds + 1, TimeUnit.SECONDS));\n\n    coll = cloudManager.getClusterStateProvider().getCollection(collectionName);\n\n    int checkedSubShards = 0;\n\n    for (String parentShard : parentShards) {\n      for (String subShard : Arrays.asList(parentShard + \"_0\", parentShard + \"_1\")) {\n        leader = coll.getSlice(subShard).getLeader();\n        if (leader == null) {\n          // no leader yet - skip it\n        }\n        checkedSubShards++;\n        replicaName = Utils.parseMetricsReplicaName(collectionName, leader.getCoreName());\n        assertNotNull(\"replicaName could not be constructed from \" + leader, replicaName);\n        final String subregistry = SolrCoreMetricManager.createRegistryName(true, collectionName, subShard, replicaName, null);\n        Set<String> subtags = SimUtils.COMMON_REPLICA_TAGS.stream()\n            .map(s -> \"metrics:\" + subregistry + \":\" + s).collect(Collectors.toSet());\n        sizes = cloudManager.getNodeStateProvider().getNodeValues(leader.getNodeName(), subtags);\n        commitSizeTag = \"metrics:\" + subregistry + \":SEARCHER.searcher.indexCommitSize\";\n        numDocsTag = \"metrics:\" + subregistry + \":SEARCHER.searcher.numDocs\";\n        maxDocTag = \"metrics:\" + subregistry + \":SEARCHER.searcher.maxDoc\";\n        assertNotNull(sizes.toString(), sizes.get(commitSizeTag));\n        assertNotNull(sizes.toString(), sizes.get(numDocsTag));\n        assertNotNull(sizes.toString(), sizes.get(maxDocTag));\n        long subCommitSize = ((Number)sizes.get(commitSizeTag)).longValue();\n        long subMaxDoc = ((Number)sizes.get(maxDocTag)).longValue();\n        long subNumDocs = ((Number)sizes.get(numDocsTag)).longValue();\n        assertTrue(\"subNumDocs=\" + subNumDocs + \" should be less than subMaxDoc=\" + subMaxDoc +\n            \" due to link split\", subNumDocs < subMaxDoc);\n        assertTrue(\"subCommitSize=\" + subCommitSize + \" should be still greater than aboveBytes=\" + aboveBytes +\n            \" due to link split\", subCommitSize > aboveBytes);\n        // calculate estimated size using the same formula\n        long estimatedSize = IndexSizeTrigger.estimatedSize(subMaxDoc, subNumDocs, subCommitSize);\n        assertTrue(\"estimatedSize=\" + estimatedSize + \" should be lower than aboveBytes=\" + aboveBytes,\n            estimatedSize < aboveBytes);\n      }\n    }\n\n    assertTrue(\"didn't find any leaders in new sub-shards\", checkedSubShards > 0);\n\n    // reset & resume\n    listenerEvents.clear();\n    finished = new CountDownLatch(1);\n    req = AutoScalingRequest.create(SolrRequest.METHOD.POST, resumeTriggerCommand);\n    response = solrClient.request(req);\n    assertEquals(\"success\", response.get(\"result\").toString());\n    timeSource.sleep(TimeUnit.MILLISECONDS.convert(waitForSeconds + 1, TimeUnit.SECONDS));\n\n    // estimated shard size should fall well below the aboveBytes, even though the real commitSize\n    // still remains larger due to the splitMethod=link side-effects\n    await = finished.await(10000 / SPEED, TimeUnit.MILLISECONDS);\n    assertFalse(\"should not fire the trigger again! \" + listenerEvents, await);\n\n  }\n\n","sourceOld":"  @Test\n  public void testEstimatedIndexSize() throws Exception {\n    if (!realCluster) {\n      log.info(\"This test doesn't work with a simulated cluster\");\n      return;\n    }\n    String collectionName = \"testEstimatedIndexSize_collection\";\n    CollectionAdminRequest.Create create = CollectionAdminRequest.createCollection(collectionName,\n        \"conf\", 2, 2).setMaxShardsPerNode(2);\n    create.process(solrClient);\n\n    CloudUtil.waitForState(cloudManager, \"failed to create \" + collectionName, collectionName,\n        CloudUtil.clusterShape(2, 2, false, true));\n\n    int NUM_DOCS = 20;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      SolrInputDocument doc = new SolrInputDocument(\"id\", \"id-\" + (i * 100));\n      solrClient.add(collectionName, doc);\n    }\n    solrClient.commit(collectionName);\n\n    // get the size of the leader's index\n    DocCollection coll = cloudManager.getClusterStateProvider().getCollection(collectionName);\n    Replica leader = coll.getSlice(\"shard1\").getLeader();\n    String replicaName = Utils.parseMetricsReplicaName(collectionName, leader.getCoreName());\n    assertNotNull(\"replicaName could not be constructed from \" + leader, replicaName);\n    final String registry = SolrCoreMetricManager.createRegistryName(true, collectionName, \"shard1\", replicaName, null);\n    Set<String> tags = SimUtils.COMMON_REPLICA_TAGS.stream()\n        .map(s -> \"metrics:\" + registry + \":\" + s).collect(Collectors.toSet());\n    Map<String, Object> sizes = cloudManager.getNodeStateProvider().getNodeValues(leader.getNodeName(), tags);\n    String commitSizeTag = \"metrics:\" + registry + \":SEARCHER.searcher.indexCommitSize\";\n    String numDocsTag = \"metrics:\" + registry + \":SEARCHER.searcher.numDocs\";\n    String maxDocTag = \"metrics:\" + registry + \":SEARCHER.searcher.maxDoc\";\n    assertNotNull(sizes.toString(), sizes.get(commitSizeTag));\n    assertNotNull(sizes.toString(), sizes.get(numDocsTag));\n    assertNotNull(sizes.toString(), sizes.get(maxDocTag));\n    long commitSize = ((Number)sizes.get(commitSizeTag)).longValue();\n    long maxDoc = ((Number)sizes.get(maxDocTag)).longValue();\n    long numDocs = ((Number)sizes.get(numDocsTag)).longValue();\n\n    assertEquals(\"maxDoc != numDocs\", maxDoc, numDocs);\n    assertTrue(\"unexpected numDocs=\" + numDocs, numDocs > NUM_DOCS / 3);\n\n    long aboveBytes = commitSize * 9 / 10;\n    long waitForSeconds = 3 + random().nextInt(5);\n    String setTriggerCommand = \"{\" +\n        \"'set-trigger' : {\" +\n        \"'name' : 'index_size_trigger7',\" +\n        \"'event' : 'indexSize',\" +\n        \"'waitFor' : '\" + waitForSeconds + \"s',\" +\n        \"'splitMethod' : 'link',\" +\n        \"'aboveBytes' : \" + aboveBytes + \",\" +\n        \"'enabled' : false,\" +\n        \"'actions' : [{'name' : 'compute_plan', 'class' : 'solr.ComputePlanAction'},\" +\n        \"{'name' : 'execute_plan', 'class' : '\" + ExecutePlanAction.class.getName() + \"'}]\" +\n        \"}}\";\n    SolrRequest req = AutoScalingRequest.create(SolrRequest.METHOD.POST, setTriggerCommand);\n    NamedList<Object> response = solrClient.request(req);\n    assertEquals(response.get(\"result\").toString(), \"success\");\n\n    String setListenerCommand = \"{\" +\n        \"'set-listener' : \" +\n        \"{\" +\n        \"'name' : 'capturing7',\" +\n        \"'trigger' : 'index_size_trigger7',\" +\n        \"'stage' : ['STARTED','ABORTED','SUCCEEDED','FAILED'],\" +\n        \"'beforeAction' : ['compute_plan','execute_plan'],\" +\n        \"'afterAction' : ['compute_plan','execute_plan'],\" +\n        \"'class' : '\" + CapturingTriggerListener.class.getName() + \"'\" +\n        \"}\" +\n        \"}\";\n    req = AutoScalingRequest.create(SolrRequest.METHOD.POST, setListenerCommand);\n    response = solrClient.request(req);\n    assertEquals(response.get(\"result\").toString(), \"success\");\n\n    setListenerCommand = \"{\" +\n        \"'set-listener' : \" +\n        \"{\" +\n        \"'name' : 'finished',\" +\n        \"'trigger' : 'index_size_trigger7',\" +\n        \"'stage' : ['SUCCEEDED'],\" +\n        \"'class' : '\" + FinishedProcessingListener.class.getName() + \"'\" +\n        \"}\" +\n        \"}\";\n    req = AutoScalingRequest.create(SolrRequest.METHOD.POST, setListenerCommand);\n    response = solrClient.request(req);\n    assertEquals(response.get(\"result\").toString(), \"success\");\n\n    // enable the trigger\n    String resumeTriggerCommand = \"{\" +\n        \"'resume-trigger' : {\" +\n        \"'name' : 'index_size_trigger7'\" +\n        \"}\" +\n        \"}\";\n    req = AutoScalingRequest.create(SolrRequest.METHOD.POST, resumeTriggerCommand);\n    response = solrClient.request(req);\n    assertEquals(\"success\", response.get(\"result\").toString());\n\n    // aboveBytes was set to be slightly lower than the actual size of at least one shard, so\n    // we're expecting a SPLITSHARD - but with 'link' method the actual size of the resulting shards\n    // will likely not go down. However, the estimated size of the latest commit point will go down\n    // (see SOLR-12941).\n\n    timeSource.sleep(TimeUnit.MILLISECONDS.convert(waitForSeconds + 1, TimeUnit.SECONDS));\n\n    boolean await = finished.await(90000 / SPEED, TimeUnit.MILLISECONDS);\n    assertTrue(\"did not finish processing in time\", await);\n    // suspend the trigger\n    String suspendTriggerCommand = \"{\" +\n        \"'suspend-trigger' : {\" +\n        \"'name' : 'index_size_trigger7'\" +\n        \"}\" +\n        \"}\";\n    req = AutoScalingRequest.create(SolrRequest.METHOD.POST, resumeTriggerCommand);\n    response = solrClient.request(req);\n    assertEquals(\"success\", response.get(\"result\").toString());\n\n    assertEquals(1, listenerEvents.size());\n    List<CapturedEvent> events = listenerEvents.get(\"capturing7\");\n    assertNotNull(listenerEvents.toString(), events);\n    assertFalse(\"empty events?\", events.isEmpty());\n    CapturedEvent ev = events.get(0);\n    List<TriggerEvent.Op> ops = (List< TriggerEvent.Op>)ev.event.properties.get(TriggerEvent.REQUESTED_OPS);\n    assertNotNull(\"no requested ops in \" + ev, ops);\n    assertFalse(\"empty list of ops in \" + ev, ops.isEmpty());\n    Set<String> parentShards = new HashSet<>();\n    ops.forEach(op -> {\n      assertTrue(op.toString(), op.getAction() == CollectionParams.CollectionAction.SPLITSHARD);\n      Collection<Pair<String, String>> hints = (Collection<Pair<String, String>>)op.getHints().get(Suggester.Hint.COLL_SHARD);\n      assertNotNull(\"no hints in op \" + op, hints);\n      hints.forEach(h -> parentShards.add(h.second()));\n    });\n\n    // allow for recovery of at least some sub-shards\n    timeSource.sleep(TimeUnit.MILLISECONDS.convert(waitForSeconds + 1, TimeUnit.SECONDS));\n\n    coll = cloudManager.getClusterStateProvider().getCollection(collectionName);\n\n    int checkedSubShards = 0;\n\n    for (String parentShard : parentShards) {\n      for (String subShard : Arrays.asList(parentShard + \"_0\", parentShard + \"_1\")) {\n        leader = coll.getSlice(subShard).getLeader();\n        if (leader == null) {\n          // no leader yet - skip it\n        }\n        checkedSubShards++;\n        replicaName = Utils.parseMetricsReplicaName(collectionName, leader.getCoreName());\n        assertNotNull(\"replicaName could not be constructed from \" + leader, replicaName);\n        final String subregistry = SolrCoreMetricManager.createRegistryName(true, collectionName, subShard, replicaName, null);\n        Set<String> subtags = SimUtils.COMMON_REPLICA_TAGS.stream()\n            .map(s -> \"metrics:\" + subregistry + \":\" + s).collect(Collectors.toSet());\n        sizes = cloudManager.getNodeStateProvider().getNodeValues(leader.getNodeName(), subtags);\n        commitSizeTag = \"metrics:\" + subregistry + \":SEARCHER.searcher.indexCommitSize\";\n        numDocsTag = \"metrics:\" + subregistry + \":SEARCHER.searcher.numDocs\";\n        maxDocTag = \"metrics:\" + subregistry + \":SEARCHER.searcher.maxDoc\";\n        assertNotNull(sizes.toString(), sizes.get(commitSizeTag));\n        assertNotNull(sizes.toString(), sizes.get(numDocsTag));\n        assertNotNull(sizes.toString(), sizes.get(maxDocTag));\n        long subCommitSize = ((Number)sizes.get(commitSizeTag)).longValue();\n        long subMaxDoc = ((Number)sizes.get(maxDocTag)).longValue();\n        long subNumDocs = ((Number)sizes.get(numDocsTag)).longValue();\n        assertTrue(\"subNumDocs=\" + subNumDocs + \" should be less than subMaxDoc=\" + subMaxDoc +\n            \" due to link split\", subNumDocs < subMaxDoc);\n        assertTrue(\"subCommitSize=\" + subCommitSize + \" should be still greater than aboveBytes=\" + aboveBytes +\n            \" due to link split\", subCommitSize > aboveBytes);\n        // calculate estimated size using the same formula\n        long estimatedSize = IndexSizeTrigger.estimatedSize(subMaxDoc, subNumDocs, subCommitSize);\n        assertTrue(\"estimatedSize=\" + estimatedSize + \" should be lower than aboveBytes=\" + aboveBytes,\n            estimatedSize < aboveBytes);\n      }\n    }\n\n    assertTrue(\"didn't find any leaders in new sub-shards\", checkedSubShards > 0);\n\n    // reset & resume\n    listenerEvents.clear();\n    finished = new CountDownLatch(1);\n    req = AutoScalingRequest.create(SolrRequest.METHOD.POST, resumeTriggerCommand);\n    response = solrClient.request(req);\n    assertEquals(\"success\", response.get(\"result\").toString());\n    timeSource.sleep(TimeUnit.MILLISECONDS.convert(waitForSeconds + 1, TimeUnit.SECONDS));\n\n    // estimated shard size should fall well below the aboveBytes, even though the real commitSize\n    // still remains larger due to the splitMethod=link side-effects\n    await = finished.await(10000 / SPEED, TimeUnit.MILLISECONDS);\n    assertFalse(\"should not fire the trigger again! \" + listenerEvents, await);\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a250c410a74277b5acb7ef5d8b5cb3e60fbd71a4":["2d80c1ad9241ae005a167d7ee8ac473601b0e57c"],"df724d84dab24a0cc54bec95a8680867adc7f171":["2d80c1ad9241ae005a167d7ee8ac473601b0e57c","a250c410a74277b5acb7ef5d8b5cb3e60fbd71a4"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"2d80c1ad9241ae005a167d7ee8ac473601b0e57c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a250c410a74277b5acb7ef5d8b5cb3e60fbd71a4"]},"commit2Childs":{"a250c410a74277b5acb7ef5d8b5cb3e60fbd71a4":["df724d84dab24a0cc54bec95a8680867adc7f171","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"df724d84dab24a0cc54bec95a8680867adc7f171":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["2d80c1ad9241ae005a167d7ee8ac473601b0e57c"],"2d80c1ad9241ae005a167d7ee8ac473601b0e57c":["a250c410a74277b5acb7ef5d8b5cb3e60fbd71a4","df724d84dab24a0cc54bec95a8680867adc7f171"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["df724d84dab24a0cc54bec95a8680867adc7f171","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}