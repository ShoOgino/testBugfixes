{"path":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(StoredDocument,int,SchemaField,Query,IndexReader,SolrQueryRequest).mjava","commits":[{"id":"1ba83a7997a13459d756c436cc76ee2570d2128f","date":1432215022,"type":1,"author":"David Wayne Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(StoredDocument,int,SchemaField,Query,IndexReader,SolrQueryRequest).mjava","pathOld":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","sourceNew":"  /** Highlights and returns the highlight object for this field -- a String[] by default. Null if none. */\n  @SuppressWarnings(\"unchecked\")\n  protected Object doHighlightingByHighlighter(StoredDocument doc, int docId, SchemaField schemaField, Query query,\n                                               IndexReader reader, SolrQueryRequest req) throws IOException {\n    final SolrParams params = req.getParams();\n    final String fieldName = schemaField.getName();\n\n    final int mvToExamine =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n            (schemaField.multiValued()) ? Integer.MAX_VALUE : 1);\n\n    // Technically this is the max *fragments* (snippets), not max values:\n    int mvToMatch =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_MATCH, Integer.MAX_VALUE);\n    if (mvToExamine <= 0 || mvToMatch <= 0) {\n      return null;\n    }\n\n    int maxCharsToAnalyze = params.getFieldInt(fieldName,\n        HighlightParams.MAX_CHARS,\n        Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n    if (maxCharsToAnalyze < 0) {//e.g. -1\n      maxCharsToAnalyze = Integer.MAX_VALUE;\n    }\n\n    List<String> fieldValues = getFieldValues(doc, fieldName, mvToExamine, maxCharsToAnalyze, req);\n    if (fieldValues.isEmpty()) {\n      return null;\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    //  note: offsets are minimally sufficient for this HL.\n    final Fields tvFields = schemaField.storeTermOffsets() ? reader.getTermVectors(docId) : null;\n    final TokenStream tvStream =\n        TokenSources.getTermVectorTokenStreamOrNull(fieldName, tvFields, maxCharsToAnalyze - 1);\n    //  We need to wrap in OffsetWindowTokenFilter if multi-valued\n    final OffsetWindowTokenFilter tvWindowStream;\n    if (tvStream != null && fieldValues.size() > 1) {\n      tvWindowStream = new OffsetWindowTokenFilter(tvStream);\n    } else {\n      tvWindowStream = null;\n    }\n\n    for (String thisText : fieldValues) {\n      if (mvToMatch <= 0 || maxCharsToAnalyze <= 0) {\n        break;\n      }\n\n      TokenStream tstream;\n      if (tvWindowStream != null) {\n        // if we have a multi-valued field with term vectors, then get the next offset window\n        tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n      } else if (tvStream != null) {\n        tstream = tvStream; // single-valued with term vectors\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schemaField, thisText);\n      }\n\n      Highlighter highlighter;\n      if (req.getParams().getFieldBool(fieldName, HighlightParams.USE_PHRASE_HIGHLIGHTER, true)) {\n        // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n        // needs to implement reset() efficiently.\n\n        //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n        //  It should be okay if OffsetLimit won't get applied in this case.\n        final TokenStream tempTokenStream;\n        if (tstream != tvStream) {\n          if (maxCharsToAnalyze >= thisText.length()) {\n            tempTokenStream = new CachingTokenFilter(tstream);\n          } else {\n            tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n          }\n        } else {\n          tempTokenStream = tstream;\n        }\n\n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n\n        // if the CachingTokenFilter was consumed then use it going forward.\n        if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter) tempTokenStream).isCached()) {\n          tstream = tempTokenStream;\n        }\n        //tstream.reset(); not needed; getBestTextFragments will reset it.\n      } else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n\n      highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      maxCharsToAnalyze -= thisText.length();\n\n      // Highlight!\n      try {\n        TextFragment[] bestTextFragments =\n            highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (TextFragment bestTextFragment : bestTextFragments) {\n          if (bestTextFragment == null)//can happen via mergeContiguousFragments\n            continue;\n          // normally we want a score (must be highlighted), but if preserveMulti then we return a snippet regardless.\n          if (bestTextFragment.getScore() > 0 || preserveMulti) {\n            frags.add(bestTextFragment);\n            if (bestTextFragment.getScore() > 0)\n              --mvToMatch; // note: limits fragments (for multi-valued fields), not quite the number of values\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }//end field value loop\n\n    // Put the fragments onto the Solr response (docSummaries)\n    if (frags.size() > 0) {\n      // sort such that the fragments with the highest score come first\n      if (!preserveMulti) {\n        Collections.sort(frags, new Comparator<TextFragment>() {//TODO make TextFragment Comparable\n          @Override\n          public int compare(TextFragment arg0, TextFragment arg1) {\n            return Float.compare(arg1.getScore(), arg0.getScore());\n          }\n        });\n      }\n\n      // Truncate list to hl.snippets, but not when hl.preserveMulti\n      if (frags.size() > numFragments && !preserveMulti) {\n        frags = frags.subList(0, numFragments);\n      }\n      return getResponseForFragments(frags, req);\n    }\n    return null;//no highlights for this field\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  protected void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n                                              int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    final SolrParams params = req.getParams();\n\n    // TODO: highlighting numeric fields is broken (Lucene) - so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && schemaField.getType() instanceof org.apache.solr.schema.TrieField) return;\n    // END: Hack\n\n    final int mvToExamine =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n            (schemaField != null && schemaField.multiValued()) ? Integer.MAX_VALUE : 1);\n    // Technically this is the max *fragments* (snippets), not max values:\n    int mvToMatch =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_MATCH, Integer.MAX_VALUE);\n    if (mvToExamine <= 0 || mvToMatch <= 0) {\n      return;\n    }\n\n    int maxCharsToAnalyze = params.getFieldInt(fieldName,\n        HighlightParams.MAX_CHARS,\n        Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n    if (maxCharsToAnalyze < 0) {//e.g. -1\n      maxCharsToAnalyze = Integer.MAX_VALUE;\n    }\n\n    List<String> fieldValues = getFieldValues(req, doc, fieldName, mvToExamine, maxCharsToAnalyze);\n    if (fieldValues.isEmpty()) {\n      return;\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    final Fields tvFields = searcher.getIndexReader().getTermVectors(docId); // TODO add as param; see SOLR-5855\n    final TokenStream tvStream =\n        TokenSources.getTermVectorTokenStreamOrNull(fieldName, tvFields, maxCharsToAnalyze - 1);\n    //  We need to wrap in OffsetWindowTokenFilter if multi-valued\n    final OffsetWindowTokenFilter tvWindowStream;\n    if (tvStream != null && fieldValues.size() > 1) {\n      tvWindowStream = new OffsetWindowTokenFilter(tvStream);\n    } else {\n      tvWindowStream = null;\n    }\n\n    for (String thisText : fieldValues) {\n      if (mvToMatch <= 0 || maxCharsToAnalyze <= 0) {\n        break;\n      }\n\n      TokenStream tstream;\n      if (tvWindowStream != null) {\n        // if we have a multi-valued field with term vectors, then get the next offset window\n        tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n      } else if (tvStream != null) {\n        tstream = tvStream; // single-valued with term vectors\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, thisText);\n      }\n\n      Highlighter highlighter;\n      if (req.getParams().getFieldBool(fieldName, HighlightParams.USE_PHRASE_HIGHLIGHTER, true)) {\n        // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n        // needs to implement reset() efficiently.\n\n        //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n        //  It should be okay if OffsetLimit won't get applied in this case.\n        final TokenStream tempTokenStream;\n        if (tstream != tvStream) {\n          if (maxCharsToAnalyze >= thisText.length()) {\n            tempTokenStream = new CachingTokenFilter(tstream);\n          } else {\n            tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n          }\n        } else {\n          tempTokenStream = tstream;\n        }\n\n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n\n        // if the CachingTokenFilter was consumed then use it going forward.\n        if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter)tempTokenStream).isCached()) {\n          tstream = tempTokenStream;\n        }\n        //tstream.reset(); not needed; getBestTextFragments will reset it.\n      } else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n\n      highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      maxCharsToAnalyze -= thisText.length();\n\n      // Highlight!\n      try {\n        TextFragment[] bestTextFragments =\n            highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (TextFragment bestTextFragment : bestTextFragments) {\n          if (bestTextFragment == null)//can happen via mergeContiguousFragments\n            continue;\n          // normally we want a score (must be highlighted), but if preserveMulti then we return a snippet regardless.\n          if (bestTextFragment.getScore() > 0 || preserveMulti) {\n            frags.add(bestTextFragment);\n            if (bestTextFragment.getScore() > 0)\n              --mvToMatch; // note: limits fragments (for multi-valued fields), not quite the number of values\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }//end field value loop\n\n    // Put the fragments onto the Solr response (docSummaries)\n    if (frags.size() > 0) {\n      // sort such that the fragments with the highest score come first\n      if (!preserveMulti) {\n        Collections.sort(frags, new Comparator<TextFragment>() {//TODO make TextFragment Comparable\n          @Override\n          public int compare(TextFragment arg0, TextFragment arg1) {\n            return Float.compare(arg1.getScore(), arg0.getScore());\n          }\n        });\n      }\n\n      // Truncate list to hl.snippets, but not when hl.preserveMulti\n      if (frags.size() > numFragments && !preserveMulti) {\n        frags = frags.subList(0, numFragments);\n      }\n      docSummaries.add(fieldName, getResponseForFragments(frags, req));\n    } else {\n      // no summaries made, copy text from alternate field\n      alternateField(docSummaries, params, doc, fieldName);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c","date":1453060490,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Document,int,SchemaField,Query,IndexReader,SolrQueryRequest).mjava","pathOld":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(StoredDocument,int,SchemaField,Query,IndexReader,SolrQueryRequest).mjava","sourceNew":"  /** Highlights and returns the highlight object for this field -- a String[] by default. Null if none. */\n  @SuppressWarnings(\"unchecked\")\n  protected Object doHighlightingByHighlighter(Document doc, int docId, SchemaField schemaField, Query query,\n                                               IndexReader reader, SolrQueryRequest req) throws IOException {\n    final SolrParams params = req.getParams();\n    final String fieldName = schemaField.getName();\n\n    final int mvToExamine =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n            (schemaField.multiValued()) ? Integer.MAX_VALUE : 1);\n\n    // Technically this is the max *fragments* (snippets), not max values:\n    int mvToMatch =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_MATCH, Integer.MAX_VALUE);\n    if (mvToExamine <= 0 || mvToMatch <= 0) {\n      return null;\n    }\n\n    int maxCharsToAnalyze = params.getFieldInt(fieldName,\n        HighlightParams.MAX_CHARS,\n        Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n    if (maxCharsToAnalyze < 0) {//e.g. -1\n      maxCharsToAnalyze = Integer.MAX_VALUE;\n    }\n\n    List<String> fieldValues = getFieldValues(doc, fieldName, mvToExamine, maxCharsToAnalyze, req);\n    if (fieldValues.isEmpty()) {\n      return null;\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    //  note: offsets are minimally sufficient for this HL.\n    final Fields tvFields = schemaField.storeTermOffsets() ? reader.getTermVectors(docId) : null;\n    final TokenStream tvStream =\n        TokenSources.getTermVectorTokenStreamOrNull(fieldName, tvFields, maxCharsToAnalyze - 1);\n    //  We need to wrap in OffsetWindowTokenFilter if multi-valued\n    final OffsetWindowTokenFilter tvWindowStream;\n    if (tvStream != null && fieldValues.size() > 1) {\n      tvWindowStream = new OffsetWindowTokenFilter(tvStream);\n    } else {\n      tvWindowStream = null;\n    }\n\n    for (String thisText : fieldValues) {\n      if (mvToMatch <= 0 || maxCharsToAnalyze <= 0) {\n        break;\n      }\n\n      TokenStream tstream;\n      if (tvWindowStream != null) {\n        // if we have a multi-valued field with term vectors, then get the next offset window\n        tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n      } else if (tvStream != null) {\n        tstream = tvStream; // single-valued with term vectors\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schemaField, thisText);\n      }\n\n      Highlighter highlighter;\n      if (req.getParams().getFieldBool(fieldName, HighlightParams.USE_PHRASE_HIGHLIGHTER, true)) {\n        // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n        // needs to implement reset() efficiently.\n\n        //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n        //  It should be okay if OffsetLimit won't get applied in this case.\n        final TokenStream tempTokenStream;\n        if (tstream != tvStream) {\n          if (maxCharsToAnalyze >= thisText.length()) {\n            tempTokenStream = new CachingTokenFilter(tstream);\n          } else {\n            tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n          }\n        } else {\n          tempTokenStream = tstream;\n        }\n\n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n\n        // if the CachingTokenFilter was consumed then use it going forward.\n        if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter) tempTokenStream).isCached()) {\n          tstream = tempTokenStream;\n        }\n        //tstream.reset(); not needed; getBestTextFragments will reset it.\n      } else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n\n      highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      maxCharsToAnalyze -= thisText.length();\n\n      // Highlight!\n      try {\n        TextFragment[] bestTextFragments =\n            highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (TextFragment bestTextFragment : bestTextFragments) {\n          if (bestTextFragment == null)//can happen via mergeContiguousFragments\n            continue;\n          // normally we want a score (must be highlighted), but if preserveMulti then we return a snippet regardless.\n          if (bestTextFragment.getScore() > 0 || preserveMulti) {\n            frags.add(bestTextFragment);\n            if (bestTextFragment.getScore() > 0)\n              --mvToMatch; // note: limits fragments (for multi-valued fields), not quite the number of values\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }//end field value loop\n\n    // Put the fragments onto the Solr response (docSummaries)\n    if (frags.size() > 0) {\n      // sort such that the fragments with the highest score come first\n      if (!preserveMulti) {\n        Collections.sort(frags, new Comparator<TextFragment>() {//TODO make TextFragment Comparable\n          @Override\n          public int compare(TextFragment arg0, TextFragment arg1) {\n            return Float.compare(arg1.getScore(), arg0.getScore());\n          }\n        });\n      }\n\n      // Truncate list to hl.snippets, but not when hl.preserveMulti\n      if (frags.size() > numFragments && !preserveMulti) {\n        frags = frags.subList(0, numFragments);\n      }\n      return getResponseForFragments(frags, req);\n    }\n    return null;//no highlights for this field\n  }\n\n","sourceOld":"  /** Highlights and returns the highlight object for this field -- a String[] by default. Null if none. */\n  @SuppressWarnings(\"unchecked\")\n  protected Object doHighlightingByHighlighter(StoredDocument doc, int docId, SchemaField schemaField, Query query,\n                                               IndexReader reader, SolrQueryRequest req) throws IOException {\n    final SolrParams params = req.getParams();\n    final String fieldName = schemaField.getName();\n\n    final int mvToExamine =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n            (schemaField.multiValued()) ? Integer.MAX_VALUE : 1);\n\n    // Technically this is the max *fragments* (snippets), not max values:\n    int mvToMatch =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_MATCH, Integer.MAX_VALUE);\n    if (mvToExamine <= 0 || mvToMatch <= 0) {\n      return null;\n    }\n\n    int maxCharsToAnalyze = params.getFieldInt(fieldName,\n        HighlightParams.MAX_CHARS,\n        Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n    if (maxCharsToAnalyze < 0) {//e.g. -1\n      maxCharsToAnalyze = Integer.MAX_VALUE;\n    }\n\n    List<String> fieldValues = getFieldValues(doc, fieldName, mvToExamine, maxCharsToAnalyze, req);\n    if (fieldValues.isEmpty()) {\n      return null;\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    //  note: offsets are minimally sufficient for this HL.\n    final Fields tvFields = schemaField.storeTermOffsets() ? reader.getTermVectors(docId) : null;\n    final TokenStream tvStream =\n        TokenSources.getTermVectorTokenStreamOrNull(fieldName, tvFields, maxCharsToAnalyze - 1);\n    //  We need to wrap in OffsetWindowTokenFilter if multi-valued\n    final OffsetWindowTokenFilter tvWindowStream;\n    if (tvStream != null && fieldValues.size() > 1) {\n      tvWindowStream = new OffsetWindowTokenFilter(tvStream);\n    } else {\n      tvWindowStream = null;\n    }\n\n    for (String thisText : fieldValues) {\n      if (mvToMatch <= 0 || maxCharsToAnalyze <= 0) {\n        break;\n      }\n\n      TokenStream tstream;\n      if (tvWindowStream != null) {\n        // if we have a multi-valued field with term vectors, then get the next offset window\n        tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n      } else if (tvStream != null) {\n        tstream = tvStream; // single-valued with term vectors\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schemaField, thisText);\n      }\n\n      Highlighter highlighter;\n      if (req.getParams().getFieldBool(fieldName, HighlightParams.USE_PHRASE_HIGHLIGHTER, true)) {\n        // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n        // needs to implement reset() efficiently.\n\n        //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n        //  It should be okay if OffsetLimit won't get applied in this case.\n        final TokenStream tempTokenStream;\n        if (tstream != tvStream) {\n          if (maxCharsToAnalyze >= thisText.length()) {\n            tempTokenStream = new CachingTokenFilter(tstream);\n          } else {\n            tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n          }\n        } else {\n          tempTokenStream = tstream;\n        }\n\n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n\n        // if the CachingTokenFilter was consumed then use it going forward.\n        if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter) tempTokenStream).isCached()) {\n          tstream = tempTokenStream;\n        }\n        //tstream.reset(); not needed; getBestTextFragments will reset it.\n      } else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n\n      highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      maxCharsToAnalyze -= thisText.length();\n\n      // Highlight!\n      try {\n        TextFragment[] bestTextFragments =\n            highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (TextFragment bestTextFragment : bestTextFragments) {\n          if (bestTextFragment == null)//can happen via mergeContiguousFragments\n            continue;\n          // normally we want a score (must be highlighted), but if preserveMulti then we return a snippet regardless.\n          if (bestTextFragment.getScore() > 0 || preserveMulti) {\n            frags.add(bestTextFragment);\n            if (bestTextFragment.getScore() > 0)\n              --mvToMatch; // note: limits fragments (for multi-valued fields), not quite the number of values\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }//end field value loop\n\n    // Put the fragments onto the Solr response (docSummaries)\n    if (frags.size() > 0) {\n      // sort such that the fragments with the highest score come first\n      if (!preserveMulti) {\n        Collections.sort(frags, new Comparator<TextFragment>() {//TODO make TextFragment Comparable\n          @Override\n          public int compare(TextFragment arg0, TextFragment arg1) {\n            return Float.compare(arg1.getScore(), arg0.getScore());\n          }\n        });\n      }\n\n      // Truncate list to hl.snippets, but not when hl.preserveMulti\n      if (frags.size() > numFragments && !preserveMulti) {\n        frags = frags.subList(0, numFragments);\n      }\n      return getResponseForFragments(frags, req);\n    }\n    return null;//no highlights for this field\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"1ba83a7997a13459d756c436cc76ee2570d2128f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c"],"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c":["1ba83a7997a13459d756c436cc76ee2570d2128f"]},"commit2Childs":{"1ba83a7997a13459d756c436cc76ee2570d2128f":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["1ba83a7997a13459d756c436cc76ee2570d2128f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}