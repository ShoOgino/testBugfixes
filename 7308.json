{"path":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,ScoreMode,float).mjava","commits":[{"id":"9fc47cb7b4346802411bb432f501ed0673d7119e","date":1512640179,"type":1,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,ScoreMode,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, scoreMode, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        // Only cache instances that have a reasonable size. Otherwise it might cause memory issues\n        // with the query cache if most memory ends up being spent on queries rather than doc id sets.\n        return ramBytesUsed() <= LRUQueryCache.QUERY_DEFAULT_RAM_BYTES_USED;\n      }\n\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        // Only cache instances that have a reasonable size. Otherwise it might cause memory issues\n        // with the query cache if most memory ends up being spent on queries rather than doc id sets.\n        return ramBytesUsed() <= LRUQueryCache.QUERY_DEFAULT_RAM_BYTES_USED;\n      }\n\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"417142ff08fda9cf0b72d5133e63097a166c6458","date":1512729693,"type":1,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,ScoreMode,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, scoreMode, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        // Only cache instances that have a reasonable size. Otherwise it might cause memory issues\n        // with the query cache if most memory ends up being spent on queries rather than doc id sets.\n        return ramBytesUsed() <= LRUQueryCache.QUERY_DEFAULT_RAM_BYTES_USED;\n      }\n\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        // Only cache instances that have a reasonable size. Otherwise it might cause memory issues\n        // with the query cache if most memory ends up being spent on queries rather than doc id sets.\n        return ramBytesUsed() <= LRUQueryCache.QUERY_DEFAULT_RAM_BYTES_USED;\n      }\n\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a6e9f769521480a623f897c0d59089b919fa4239","date":1515161835,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,ScoreMode,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,ScoreMode,float).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermStates termStates = new TermStates(searcher.getTopReaderContext());\n            termStates.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termStates), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, scoreMode, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        // Only cache instances that have a reasonable size. Otherwise it might cause memory issues\n        // with the query cache if most memory ends up being spent on queries rather than doc id sets.\n        return ramBytesUsed() <= LRUQueryCache.QUERY_DEFAULT_RAM_BYTES_USED;\n      }\n\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, scoreMode, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        // Only cache instances that have a reasonable size. Otherwise it might cause memory issues\n        // with the query cache if most memory ends up being spent on queries rather than doc id sets.\n        return ramBytesUsed() <= LRUQueryCache.QUERY_DEFAULT_RAM_BYTES_USED;\n      }\n\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b94236357aaa22b76c10629851fe4e376e0cea82","date":1516710914,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,ScoreMode,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,ScoreMode,float).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermStates termStates = new TermStates(searcher.getTopReaderContext());\n            termStates.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termStates), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, scoreMode, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        // Only cache instances that have a reasonable size. Otherwise it might cause memory issues\n        // with the query cache if most memory ends up being spent on queries rather than doc id sets.\n        return ramBytesUsed() <= LRUQueryCache.QUERY_DEFAULT_RAM_BYTES_USED;\n      }\n\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, scoreMode, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        // Only cache instances that have a reasonable size. Otherwise it might cause memory issues\n        // with the query cache if most memory ends up being spent on queries rather than doc id sets.\n        return ramBytesUsed() <= LRUQueryCache.QUERY_DEFAULT_RAM_BYTES_USED;\n      }\n\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"657704b225b01c6ff4bada5b6667f1f60aaaad0f","date":1523436207,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,ScoreMode,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,ScoreMode,float).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      @Override\n      public Matches matches(LeafReaderContext context, int doc) throws IOException {\n        Terms terms = context.reader().terms(field);\n        if (terms == null || terms.hasPositions() == false) {\n          return super.matches(context, doc);\n        }\n        return Matches.forField(field, () -> DisjunctionMatchesIterator.fromTermsEnum(context, doc, field, termData.iterator()));\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermStates termStates = new TermStates(searcher.getTopReaderContext());\n            termStates.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termStates), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, scoreMode, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        // Only cache instances that have a reasonable size. Otherwise it might cause memory issues\n        // with the query cache if most memory ends up being spent on queries rather than doc id sets.\n        return ramBytesUsed() <= LRUQueryCache.QUERY_DEFAULT_RAM_BYTES_USED;\n      }\n\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermStates termStates = new TermStates(searcher.getTopReaderContext());\n            termStates.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termStates), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, scoreMode, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        // Only cache instances that have a reasonable size. Otherwise it might cause memory issues\n        // with the query cache if most memory ends up being spent on queries rather than doc id sets.\n        return ramBytesUsed() <= LRUQueryCache.QUERY_DEFAULT_RAM_BYTES_USED;\n      }\n\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"43345f1452f9510f8aaadae6156fe0c834e7d957","date":1523483670,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,ScoreMode,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,ScoreMode,float).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      @Override\n      public Matches matches(LeafReaderContext context, int doc) throws IOException {\n        Terms terms = context.reader().terms(field);\n        if (terms == null || terms.hasPositions() == false) {\n          return super.matches(context, doc);\n        }\n        return Matches.forField(field, () -> DisjunctionMatchesIterator.fromTermsEnum(context, doc, field, termData.iterator()));\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermStates termStates = new TermStates(searcher.getTopReaderContext());\n            termStates.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termStates), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, scoreMode, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        // Only cache instances that have a reasonable size. Otherwise it might cause memory issues\n        // with the query cache if most memory ends up being spent on queries rather than doc id sets.\n        return ramBytesUsed() <= LRUQueryCache.QUERY_DEFAULT_RAM_BYTES_USED;\n      }\n\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermStates termStates = new TermStates(searcher.getTopReaderContext());\n            termStates.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termStates), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, scoreMode, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        // Only cache instances that have a reasonable size. Otherwise it might cause memory issues\n        // with the query cache if most memory ends up being spent on queries rather than doc id sets.\n        return ramBytesUsed() <= LRUQueryCache.QUERY_DEFAULT_RAM_BYTES_USED;\n      }\n\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"41ebc07bccf12a902ca6a0077910d18ee38b695f","date":1532336521,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,ScoreMode,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,ScoreMode,float).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      @Override\n      public Matches matches(LeafReaderContext context, int doc) throws IOException {\n        Terms terms = context.reader().terms(field);\n        if (terms == null || terms.hasPositions() == false) {\n          return super.matches(context, doc);\n        }\n        return Matches.forField(field, () -> DisjunctionMatchesIterator.fromTermsEnum(context, doc, getQuery(), field, termData.iterator()));\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermStates termStates = new TermStates(searcher.getTopReaderContext());\n            termStates.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termStates), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, scoreMode, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        // Only cache instances that have a reasonable size. Otherwise it might cause memory issues\n        // with the query cache if most memory ends up being spent on queries rather than doc id sets.\n        return ramBytesUsed() <= LRUQueryCache.QUERY_DEFAULT_RAM_BYTES_USED;\n      }\n\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      @Override\n      public Matches matches(LeafReaderContext context, int doc) throws IOException {\n        Terms terms = context.reader().terms(field);\n        if (terms == null || terms.hasPositions() == false) {\n          return super.matches(context, doc);\n        }\n        return Matches.forField(field, () -> DisjunctionMatchesIterator.fromTermsEnum(context, doc, field, termData.iterator()));\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermStates termStates = new TermStates(searcher.getTopReaderContext());\n            termStates.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termStates), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, scoreMode, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        // Only cache instances that have a reasonable size. Otherwise it might cause memory issues\n        // with the query cache if most memory ends up being spent on queries rather than doc id sets.\n        return ramBytesUsed() <= LRUQueryCache.QUERY_DEFAULT_RAM_BYTES_USED;\n      }\n\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2375622520a4e480775e3104a2f9a423536755b4","date":1536005521,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,ScoreMode,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,ScoreMode,float).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      @Override\n      public Matches matches(LeafReaderContext context, int doc) throws IOException {\n        Terms terms = context.reader().terms(field);\n        if (terms == null || terms.hasPositions() == false) {\n          return super.matches(context, doc);\n        }\n        return MatchesUtils.forField(field, () -> DisjunctionMatchesIterator.fromTermsEnum(context, doc, getQuery(), field, termData.iterator()));\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermStates termStates = new TermStates(searcher.getTopReaderContext());\n            termStates.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termStates), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, scoreMode, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        // Only cache instances that have a reasonable size. Otherwise it might cause memory issues\n        // with the query cache if most memory ends up being spent on queries rather than doc id sets.\n        return ramBytesUsed() <= LRUQueryCache.QUERY_DEFAULT_RAM_BYTES_USED;\n      }\n\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      @Override\n      public Matches matches(LeafReaderContext context, int doc) throws IOException {\n        Terms terms = context.reader().terms(field);\n        if (terms == null || terms.hasPositions() == false) {\n          return super.matches(context, doc);\n        }\n        return Matches.forField(field, () -> DisjunctionMatchesIterator.fromTermsEnum(context, doc, getQuery(), field, termData.iterator()));\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermStates termStates = new TermStates(searcher.getTopReaderContext());\n            termStates.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termStates), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, scoreMode, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        // Only cache instances that have a reasonable size. Otherwise it might cause memory issues\n        // with the query cache if most memory ends up being spent on queries rather than doc id sets.\n        return ramBytesUsed() <= LRUQueryCache.QUERY_DEFAULT_RAM_BYTES_USED;\n      }\n\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9c226b0eeb8b028f572020f459851a663a2c064e","date":1542377651,"type":3,"author":"Christophe Bismuth","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,ScoreMode,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,ScoreMode,float).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      @Override\n      public Matches matches(LeafReaderContext context, int doc) throws IOException {\n        Terms terms = context.reader().terms(field);\n        if (terms == null || terms.hasPositions() == false) {\n          return super.matches(context, doc);\n        }\n        return MatchesUtils.forField(field, () -> DisjunctionMatchesIterator.fromTermsEnum(context, doc, getQuery(), field, termData.iterator()));\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermStates termStates = new TermStates(searcher.getTopReaderContext());\n            termStates.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termStates), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, scoreMode, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), scoreMode, disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        // Only cache instances that have a reasonable size. Otherwise it might cause memory issues\n        // with the query cache if most memory ends up being spent on queries rather than doc id sets.\n        return ramBytesUsed() <= LRUQueryCache.QUERY_DEFAULT_RAM_BYTES_USED;\n      }\n\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      @Override\n      public Matches matches(LeafReaderContext context, int doc) throws IOException {\n        Terms terms = context.reader().terms(field);\n        if (terms == null || terms.hasPositions() == false) {\n          return super.matches(context, doc);\n        }\n        return MatchesUtils.forField(field, () -> DisjunctionMatchesIterator.fromTermsEnum(context, doc, getQuery(), field, termData.iterator()));\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermStates termStates = new TermStates(searcher.getTopReaderContext());\n            termStates.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termStates), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, scoreMode, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        // Only cache instances that have a reasonable size. Otherwise it might cause memory issues\n        // with the query cache if most memory ends up being spent on queries rather than doc id sets.\n        return ramBytesUsed() <= LRUQueryCache.QUERY_DEFAULT_RAM_BYTES_USED;\n      }\n\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5b9ffb60dc4bdc972b1403ad2ab2f5b4d9ce4cf7","date":1552575873,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,ScoreMode,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,ScoreMode,float).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public Matches matches(LeafReaderContext context, int doc) throws IOException {\n        Terms terms = context.reader().terms(field);\n        if (terms == null || terms.hasPositions() == false) {\n          return super.matches(context, doc);\n        }\n        return MatchesUtils.forField(field, () -> DisjunctionMatchesIterator.fromTermsEnum(context, doc, getQuery(), field, termData.iterator()));\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermStates termStates = new TermStates(searcher.getTopReaderContext());\n            termStates.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termStates), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, scoreMode, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), scoreMode, disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        // Only cache instances that have a reasonable size. Otherwise it might cause memory issues\n        // with the query cache if most memory ends up being spent on queries rather than doc id sets.\n        return ramBytesUsed() <= LRUQueryCache.QUERY_DEFAULT_RAM_BYTES_USED;\n      }\n\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      @Override\n      public Matches matches(LeafReaderContext context, int doc) throws IOException {\n        Terms terms = context.reader().terms(field);\n        if (terms == null || terms.hasPositions() == false) {\n          return super.matches(context, doc);\n        }\n        return MatchesUtils.forField(field, () -> DisjunctionMatchesIterator.fromTermsEnum(context, doc, getQuery(), field, termData.iterator()));\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermStates termStates = new TermStates(searcher.getTopReaderContext());\n            termStates.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termStates), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, scoreMode, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), scoreMode, disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        // Only cache instances that have a reasonable size. Otherwise it might cause memory issues\n        // with the query cache if most memory ends up being spent on queries rather than doc id sets.\n        return ramBytesUsed() <= LRUQueryCache.QUERY_DEFAULT_RAM_BYTES_USED;\n      }\n\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"36c70eb3b44de4355a7168b762cadc0f1cf194bc","date":1561542955,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,ScoreMode,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,ScoreMode,float).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public Matches matches(LeafReaderContext context, int doc) throws IOException {\n        Terms terms = context.reader().terms(field);\n        if (terms == null || terms.hasPositions() == false) {\n          return super.matches(context, doc);\n        }\n        return MatchesUtils.forField(field, () -> DisjunctionMatchesIterator.fromTermsEnum(context, doc, getQuery(), field, termData.iterator()));\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, IndexSearcher.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermStates termStates = new TermStates(searcher.getTopReaderContext());\n            termStates.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termStates), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, scoreMode, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), scoreMode, disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        // Only cache instances that have a reasonable size. Otherwise it might cause memory issues\n        // with the query cache if most memory ends up being spent on queries rather than doc id sets.\n        return ramBytesUsed() <= LRUQueryCache.QUERY_DEFAULT_RAM_BYTES_USED;\n      }\n\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public Matches matches(LeafReaderContext context, int doc) throws IOException {\n        Terms terms = context.reader().terms(field);\n        if (terms == null || terms.hasPositions() == false) {\n          return super.matches(context, doc);\n        }\n        return MatchesUtils.forField(field, () -> DisjunctionMatchesIterator.fromTermsEnum(context, doc, getQuery(), field, termData.iterator()));\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermStates termStates = new TermStates(searcher.getTopReaderContext());\n            termStates.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termStates), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, scoreMode, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), scoreMode, disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        // Only cache instances that have a reasonable size. Otherwise it might cause memory issues\n        // with the query cache if most memory ends up being spent on queries rather than doc id sets.\n        return ramBytesUsed() <= LRUQueryCache.QUERY_DEFAULT_RAM_BYTES_USED;\n      }\n\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a71ca10e7131e1f01868c80d228f26a855e79dd0","date":1562166223,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,ScoreMode,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,ScoreMode,float).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public Matches matches(LeafReaderContext context, int doc) throws IOException {\n        Terms terms = context.reader().terms(field);\n        if (terms == null || terms.hasPositions() == false) {\n          return super.matches(context, doc);\n        }\n        return MatchesUtils.forField(field, () -> DisjunctionMatchesIterator.fromTermsEnum(context, doc, getQuery(), field, termData.iterator()));\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, IndexSearcher.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermStates termStates = new TermStates(searcher.getTopReaderContext());\n            termStates.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termStates), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, scoreMode, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), scoreMode, disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        // Only cache instances that have a reasonable size. Otherwise it might cause memory issues\n        // with the query cache if most memory ends up being spent on queries rather than doc id sets.\n        return ramBytesUsed() <= RamUsageEstimator.QUERY_DEFAULT_RAM_BYTES_USED;\n      }\n\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public Matches matches(LeafReaderContext context, int doc) throws IOException {\n        Terms terms = context.reader().terms(field);\n        if (terms == null || terms.hasPositions() == false) {\n          return super.matches(context, doc);\n        }\n        return MatchesUtils.forField(field, () -> DisjunctionMatchesIterator.fromTermsEnum(context, doc, getQuery(), field, termData.iterator()));\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, IndexSearcher.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermStates termStates = new TermStates(searcher.getTopReaderContext());\n            termStates.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termStates), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, scoreMode, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), scoreMode, disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        // Only cache instances that have a reasonable size. Otherwise it might cause memory issues\n        // with the query cache if most memory ends up being spent on queries rather than doc id sets.\n        return ramBytesUsed() <= LRUQueryCache.QUERY_DEFAULT_RAM_BYTES_USED;\n      }\n\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b94236357aaa22b76c10629851fe4e376e0cea82":["417142ff08fda9cf0b72d5133e63097a166c6458","a6e9f769521480a623f897c0d59089b919fa4239"],"a71ca10e7131e1f01868c80d228f26a855e79dd0":["36c70eb3b44de4355a7168b762cadc0f1cf194bc"],"2375622520a4e480775e3104a2f9a423536755b4":["41ebc07bccf12a902ca6a0077910d18ee38b695f"],"43345f1452f9510f8aaadae6156fe0c834e7d957":["b94236357aaa22b76c10629851fe4e376e0cea82","657704b225b01c6ff4bada5b6667f1f60aaaad0f"],"36c70eb3b44de4355a7168b762cadc0f1cf194bc":["5b9ffb60dc4bdc972b1403ad2ab2f5b4d9ce4cf7"],"9c226b0eeb8b028f572020f459851a663a2c064e":["2375622520a4e480775e3104a2f9a423536755b4"],"5b9ffb60dc4bdc972b1403ad2ab2f5b4d9ce4cf7":["9c226b0eeb8b028f572020f459851a663a2c064e"],"417142ff08fda9cf0b72d5133e63097a166c6458":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","9fc47cb7b4346802411bb432f501ed0673d7119e"],"a6e9f769521480a623f897c0d59089b919fa4239":["417142ff08fda9cf0b72d5133e63097a166c6458"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"657704b225b01c6ff4bada5b6667f1f60aaaad0f":["b94236357aaa22b76c10629851fe4e376e0cea82"],"9fc47cb7b4346802411bb432f501ed0673d7119e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a71ca10e7131e1f01868c80d228f26a855e79dd0"],"41ebc07bccf12a902ca6a0077910d18ee38b695f":["43345f1452f9510f8aaadae6156fe0c834e7d957"]},"commit2Childs":{"b94236357aaa22b76c10629851fe4e376e0cea82":["43345f1452f9510f8aaadae6156fe0c834e7d957","657704b225b01c6ff4bada5b6667f1f60aaaad0f"],"a71ca10e7131e1f01868c80d228f26a855e79dd0":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"2375622520a4e480775e3104a2f9a423536755b4":["9c226b0eeb8b028f572020f459851a663a2c064e"],"43345f1452f9510f8aaadae6156fe0c834e7d957":["41ebc07bccf12a902ca6a0077910d18ee38b695f"],"36c70eb3b44de4355a7168b762cadc0f1cf194bc":["a71ca10e7131e1f01868c80d228f26a855e79dd0"],"9c226b0eeb8b028f572020f459851a663a2c064e":["5b9ffb60dc4bdc972b1403ad2ab2f5b4d9ce4cf7"],"417142ff08fda9cf0b72d5133e63097a166c6458":["b94236357aaa22b76c10629851fe4e376e0cea82","a6e9f769521480a623f897c0d59089b919fa4239"],"5b9ffb60dc4bdc972b1403ad2ab2f5b4d9ce4cf7":["36c70eb3b44de4355a7168b762cadc0f1cf194bc"],"a6e9f769521480a623f897c0d59089b919fa4239":["b94236357aaa22b76c10629851fe4e376e0cea82"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["417142ff08fda9cf0b72d5133e63097a166c6458","9fc47cb7b4346802411bb432f501ed0673d7119e"],"657704b225b01c6ff4bada5b6667f1f60aaaad0f":["43345f1452f9510f8aaadae6156fe0c834e7d957"],"9fc47cb7b4346802411bb432f501ed0673d7119e":["417142ff08fda9cf0b72d5133e63097a166c6458"],"41ebc07bccf12a902ca6a0077910d18ee38b695f":["2375622520a4e480775e3104a2f9a423536755b4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}