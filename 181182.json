{"path":"src/test/org/apache/lucene/analysis/TestStandardAnalyzer#testStandard().mjava","commits":[{"id":"5e067886e4ac03fc3c4df3d58054940a48ee6e85","date":1106054843,"type":0,"author":"Erik Hatcher","isMerge":false,"pathNew":"src/test/org/apache/lucene/analysis/TestStandardAnalyzer#testStandard().mjava","pathOld":"/dev/null","sourceNew":"  public void testStandard() throws Exception {\n    Analyzer a = new StandardAnalyzer();\n\n    // alphanumeric tokens\n    assertAnalyzesTo(a, \"B2B\", new String[]{\"b2b\"});\n    assertAnalyzesTo(a, \"2B\", new String[]{\"2b\"});\n\n    // underscores are delimiters, but not in email addresses (below)\n    assertAnalyzesTo(a, \"word_having_underscore\", new String[]{\"word\", \"having\", \"underscore\"});\n    assertAnalyzesTo(a, \"word_with_underscore_and_stopwords\", new String[]{\"word\", \"underscore\", \"stopwords\"});\n\n    // other delimiters: \"-\", \"/\", \",\"\n    assertAnalyzesTo(a, \"some-dashed-phrase\",   new String[]{\"some\", \"dashed\", \"phrase\" });\n    assertAnalyzesTo(a, \"dogs,chase,cats\", new String[]{\"dogs\", \"chase\", \"cats\"});\n    assertAnalyzesTo(a, \"ac/dc\", new String[]{\"ac\", \"dc\"});\n\n    // internal apostrophes: O'Reilly, you're, O'Reilly's\n    // possessives are actually removed by StardardFilter, not the tokenizer\n    assertAnalyzesTo(a, \"O'Reilly\", new String[]{\"o'reilly\"});\n    assertAnalyzesTo(a, \"you're\", new String[]{\"you're\"});\n    assertAnalyzesTo(a, \"O'Reilly's\", new String[]{\"o'reilly\"});\n\n    // company names\n    assertAnalyzesTo(a, \"AT&T\", new String[]{\"at&t\"});\n    assertAnalyzesTo(a, \"Excite@Home\", new String[]{\"excite@home\"});\n\n    // domain names\n    assertAnalyzesTo(a, \"www.nutch.org\",   new String[]{\"www.nutch.org\" });\n\n    // email addresses, possibly with underscores, periods, etc\n    assertAnalyzesTo(a, \"test@example.com\", new String[]{\"test@example.com\"});\n    assertAnalyzesTo(a, \"first.lastname@example.com\", new String[]{\"first.lastname@example.com\"});\n    assertAnalyzesTo(a, \"first_lastname@example.com\", new String[]{\"first_lastname@example.com\"});\n\n    // floating point, serial, model numbers, ip addresses, etc.\n    // every other segment must have at least one digit\n    assertAnalyzesTo(a, \"21.35\", new String[]{\"21.35\"});\n    assertAnalyzesTo(a, \"R2D2 C3PO\", new String[]{\"r2d2\", \"c3po\"});\n    assertAnalyzesTo(a, \"216.239.63.104\",   new String[]{\"216.239.63.104\"});\n    assertAnalyzesTo(a, \"1-2-3\",   new String[]{\"1-2-3\"});\n    assertAnalyzesTo(a, \"a1-b2-c3\",   new String[]{\"a1-b2-c3\"});\n    assertAnalyzesTo(a, \"a1-b-c3\",   new String[]{\"a1-b-c3\"});\n\n    // numbers\n    assertAnalyzesTo(a, \"David has 5000 bones\", new String[]{\"david\", \"has\", \"5000\", \"bones\"});\n\n    // various\n    assertAnalyzesTo(a, \"C embedded developers wanted\", new String[]{\"c\", \"embedded\", \"developers\", \"wanted\" });\n    assertAnalyzesTo(a, \"foo bar FOO BAR\", new String[]{\"foo\", \"bar\", \"foo\", \"bar\"});\n    assertAnalyzesTo(a, \"foo      bar .  FOO <> BAR\", new String[]{\"foo\", \"bar\", \"foo\", \"bar\"});\n    assertAnalyzesTo(a, \"\\\"QUOTED\\\" word\", new String[]{\"quoted\", \"word\"});\n\n    // acronyms have their dots stripped\n    assertAnalyzesTo(a, \"U.S.A.\", new String[]{ \"usa\" });\n\n    // It would be nice to change the grammar in StandardTokenizer.jj to make \"C#\" and \"C++\" end up as tokens.\n    assertAnalyzesTo(a, \"C++\", new String[]{\"c\"});\n    assertAnalyzesTo(a, \"C#\", new String[]{\"c\"});\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"71440939a99e296b53d3d64630c7355b914f55a2","date":1131784401,"type":3,"author":"Erik Hatcher","isMerge":false,"pathNew":"src/test/org/apache/lucene/analysis/TestStandardAnalyzer#testStandard().mjava","pathOld":"src/test/org/apache/lucene/analysis/TestStandardAnalyzer#testStandard().mjava","sourceNew":"  public void testStandard() throws Exception {\n    Analyzer a = new StandardAnalyzer();\n\n    // alphanumeric tokens\n    assertAnalyzesTo(a, \"B2B\", new String[]{\"b2b\"});\n    assertAnalyzesTo(a, \"2B\", new String[]{\"2b\"});\n\n    // underscores are delimiters, but not in email addresses (below)\n    assertAnalyzesTo(a, \"word_having_underscore\", new String[]{\"word\", \"having\", \"underscore\"});\n    assertAnalyzesTo(a, \"word_with_underscore_and_stopwords\", new String[]{\"word\", \"underscore\", \"stopwords\"});\n\n    // other delimiters: \"-\", \"/\", \",\"\n    assertAnalyzesTo(a, \"some-dashed-phrase\",   new String[]{\"some\", \"dashed\", \"phrase\" });\n    assertAnalyzesTo(a, \"dogs,chase,cats\", new String[]{\"dogs\", \"chase\", \"cats\"});\n    assertAnalyzesTo(a, \"ac/dc\", new String[]{\"ac\", \"dc\"});\n\n    // internal apostrophes: O'Reilly, you're, O'Reilly's\n    // possessives are actually removed by StardardFilter, not the tokenizer\n    assertAnalyzesTo(a, \"O'Reilly\", new String[]{\"o'reilly\"});\n    assertAnalyzesTo(a, \"you're\", new String[]{\"you're\"});\n    assertAnalyzesTo(a, \"O'Reilly's\", new String[]{\"o'reilly\"});\n\n    // company names\n    assertAnalyzesTo(a, \"AT&T\", new String[]{\"at&t\"});\n    assertAnalyzesTo(a, \"Excite@Home\", new String[]{\"excite@home\"});\n\n    // domain names\n    assertAnalyzesTo(a, \"www.nutch.org\",   new String[]{\"www.nutch.org\" });\n\n    // email addresses, possibly with underscores, periods, etc\n    assertAnalyzesTo(a, \"test@example.com\", new String[]{\"test@example.com\"});\n    assertAnalyzesTo(a, \"first.lastname@example.com\", new String[]{\"first.lastname@example.com\"});\n    assertAnalyzesTo(a, \"first_lastname@example.com\", new String[]{\"first_lastname@example.com\"});\n\n    // floating point, serial, model numbers, ip addresses, etc.\n    // every other segment must have at least one digit\n    assertAnalyzesTo(a, \"21.35\", new String[]{\"21.35\"});\n    assertAnalyzesTo(a, \"R2D2 C3PO\", new String[]{\"r2d2\", \"c3po\"});\n    assertAnalyzesTo(a, \"216.239.63.104\",   new String[]{\"216.239.63.104\"});\n    assertAnalyzesTo(a, \"1-2-3\",   new String[]{\"1-2-3\"});\n    assertAnalyzesTo(a, \"a1-b2-c3\",   new String[]{\"a1-b2-c3\"});\n    assertAnalyzesTo(a, \"a1-b-c3\",   new String[]{\"a1-b-c3\"});\n\n    // numbers\n    assertAnalyzesTo(a, \"David has 5000 bones\", new String[]{\"david\", \"has\", \"5000\", \"bones\"});\n\n    // various\n    assertAnalyzesTo(a, \"C embedded developers wanted\", new String[]{\"c\", \"embedded\", \"developers\", \"wanted\" });\n    assertAnalyzesTo(a, \"foo bar FOO BAR\", new String[]{\"foo\", \"bar\", \"foo\", \"bar\"});\n    assertAnalyzesTo(a, \"foo      bar .  FOO <> BAR\", new String[]{\"foo\", \"bar\", \"foo\", \"bar\"});\n    assertAnalyzesTo(a, \"\\\"QUOTED\\\" word\", new String[]{\"quoted\", \"word\"});\n\n    // acronyms have their dots stripped\n    assertAnalyzesTo(a, \"U.S.A.\", new String[]{ \"usa\" });\n\n    // It would be nice to change the grammar in StandardTokenizer.jj to make \"C#\" and \"C++\" end up as tokens.\n    assertAnalyzesTo(a, \"C++\", new String[]{\"c\"});\n    assertAnalyzesTo(a, \"C#\", new String[]{\"c\"});\n\n    // Korean words\n    assertAnalyzesTo(a, \"안녕하세요 한글입니다\", new String[]{\"안녕하세요\", \"한글입니다\"});\n\n  }\n\n","sourceOld":"  public void testStandard() throws Exception {\n    Analyzer a = new StandardAnalyzer();\n\n    // alphanumeric tokens\n    assertAnalyzesTo(a, \"B2B\", new String[]{\"b2b\"});\n    assertAnalyzesTo(a, \"2B\", new String[]{\"2b\"});\n\n    // underscores are delimiters, but not in email addresses (below)\n    assertAnalyzesTo(a, \"word_having_underscore\", new String[]{\"word\", \"having\", \"underscore\"});\n    assertAnalyzesTo(a, \"word_with_underscore_and_stopwords\", new String[]{\"word\", \"underscore\", \"stopwords\"});\n\n    // other delimiters: \"-\", \"/\", \",\"\n    assertAnalyzesTo(a, \"some-dashed-phrase\",   new String[]{\"some\", \"dashed\", \"phrase\" });\n    assertAnalyzesTo(a, \"dogs,chase,cats\", new String[]{\"dogs\", \"chase\", \"cats\"});\n    assertAnalyzesTo(a, \"ac/dc\", new String[]{\"ac\", \"dc\"});\n\n    // internal apostrophes: O'Reilly, you're, O'Reilly's\n    // possessives are actually removed by StardardFilter, not the tokenizer\n    assertAnalyzesTo(a, \"O'Reilly\", new String[]{\"o'reilly\"});\n    assertAnalyzesTo(a, \"you're\", new String[]{\"you're\"});\n    assertAnalyzesTo(a, \"O'Reilly's\", new String[]{\"o'reilly\"});\n\n    // company names\n    assertAnalyzesTo(a, \"AT&T\", new String[]{\"at&t\"});\n    assertAnalyzesTo(a, \"Excite@Home\", new String[]{\"excite@home\"});\n\n    // domain names\n    assertAnalyzesTo(a, \"www.nutch.org\",   new String[]{\"www.nutch.org\" });\n\n    // email addresses, possibly with underscores, periods, etc\n    assertAnalyzesTo(a, \"test@example.com\", new String[]{\"test@example.com\"});\n    assertAnalyzesTo(a, \"first.lastname@example.com\", new String[]{\"first.lastname@example.com\"});\n    assertAnalyzesTo(a, \"first_lastname@example.com\", new String[]{\"first_lastname@example.com\"});\n\n    // floating point, serial, model numbers, ip addresses, etc.\n    // every other segment must have at least one digit\n    assertAnalyzesTo(a, \"21.35\", new String[]{\"21.35\"});\n    assertAnalyzesTo(a, \"R2D2 C3PO\", new String[]{\"r2d2\", \"c3po\"});\n    assertAnalyzesTo(a, \"216.239.63.104\",   new String[]{\"216.239.63.104\"});\n    assertAnalyzesTo(a, \"1-2-3\",   new String[]{\"1-2-3\"});\n    assertAnalyzesTo(a, \"a1-b2-c3\",   new String[]{\"a1-b2-c3\"});\n    assertAnalyzesTo(a, \"a1-b-c3\",   new String[]{\"a1-b-c3\"});\n\n    // numbers\n    assertAnalyzesTo(a, \"David has 5000 bones\", new String[]{\"david\", \"has\", \"5000\", \"bones\"});\n\n    // various\n    assertAnalyzesTo(a, \"C embedded developers wanted\", new String[]{\"c\", \"embedded\", \"developers\", \"wanted\" });\n    assertAnalyzesTo(a, \"foo bar FOO BAR\", new String[]{\"foo\", \"bar\", \"foo\", \"bar\"});\n    assertAnalyzesTo(a, \"foo      bar .  FOO <> BAR\", new String[]{\"foo\", \"bar\", \"foo\", \"bar\"});\n    assertAnalyzesTo(a, \"\\\"QUOTED\\\" word\", new String[]{\"quoted\", \"word\"});\n\n    // acronyms have their dots stripped\n    assertAnalyzesTo(a, \"U.S.A.\", new String[]{ \"usa\" });\n\n    // It would be nice to change the grammar in StandardTokenizer.jj to make \"C#\" and \"C++\" end up as tokens.\n    assertAnalyzesTo(a, \"C++\", new String[]{\"c\"});\n    assertAnalyzesTo(a, \"C#\", new String[]{\"c\"});\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2fca70c0a9eadf498ff1ef998206337220b05b10","date":1154783469,"type":3,"author":"Daniel Naber","isMerge":false,"pathNew":"src/test/org/apache/lucene/analysis/TestStandardAnalyzer#testStandard().mjava","pathOld":"src/test/org/apache/lucene/analysis/TestStandardAnalyzer#testStandard().mjava","sourceNew":"  public void testStandard() throws Exception {\n    Analyzer a = new StandardAnalyzer();\n\n    // alphanumeric tokens\n    assertAnalyzesTo(a, \"B2B\", new String[]{\"b2b\"});\n    assertAnalyzesTo(a, \"2B\", new String[]{\"2b\"});\n\n    // underscores are delimiters, but not in email addresses (below)\n    assertAnalyzesTo(a, \"word_having_underscore\", new String[]{\"word\", \"having\", \"underscore\"});\n    assertAnalyzesTo(a, \"word_with_underscore_and_stopwords\", new String[]{\"word\", \"underscore\", \"stopwords\"});\n\n    // other delimiters: \"-\", \"/\", \",\"\n    assertAnalyzesTo(a, \"some-dashed-phrase\",   new String[]{\"some\", \"dashed\", \"phrase\" });\n    assertAnalyzesTo(a, \"dogs,chase,cats\", new String[]{\"dogs\", \"chase\", \"cats\"});\n    assertAnalyzesTo(a, \"ac/dc\", new String[]{\"ac\", \"dc\"});\n\n    // internal apostrophes: O'Reilly, you're, O'Reilly's\n    // possessives are actually removed by StardardFilter, not the tokenizer\n    assertAnalyzesTo(a, \"O'Reilly\", new String[]{\"o'reilly\"});\n    assertAnalyzesTo(a, \"you're\", new String[]{\"you're\"});\n    assertAnalyzesTo(a, \"she's\", new String[]{\"she\"});\n    assertAnalyzesTo(a, \"Jim's\", new String[]{\"jim\"});\n    assertAnalyzesTo(a, \"don't\", new String[]{\"don't\"});\n    assertAnalyzesTo(a, \"O'Reilly's\", new String[]{\"o'reilly\"});\n\n    // t and s had been stopwords in Lucene <= 2.0, which made it impossible\n    // to correctly search for these terms:\n    assertAnalyzesTo(a, \"s-class\", new String[]{\"s\", \"class\"});\n    assertAnalyzesTo(a, \"t-com\", new String[]{\"t\", \"com\"});\n    // 'a' is still a stopword:\n    assertAnalyzesTo(a, \"a-class\", new String[]{\"class\"});\n\n    // company names\n    assertAnalyzesTo(a, \"AT&T\", new String[]{\"at&t\"});\n    assertAnalyzesTo(a, \"Excite@Home\", new String[]{\"excite@home\"});\n\n    // domain names\n    assertAnalyzesTo(a, \"www.nutch.org\",   new String[]{\"www.nutch.org\" });\n\n    // email addresses, possibly with underscores, periods, etc\n    assertAnalyzesTo(a, \"test@example.com\", new String[]{\"test@example.com\"});\n    assertAnalyzesTo(a, \"first.lastname@example.com\", new String[]{\"first.lastname@example.com\"});\n    assertAnalyzesTo(a, \"first_lastname@example.com\", new String[]{\"first_lastname@example.com\"});\n\n    // floating point, serial, model numbers, ip addresses, etc.\n    // every other segment must have at least one digit\n    assertAnalyzesTo(a, \"21.35\", new String[]{\"21.35\"});\n    assertAnalyzesTo(a, \"R2D2 C3PO\", new String[]{\"r2d2\", \"c3po\"});\n    assertAnalyzesTo(a, \"216.239.63.104\",   new String[]{\"216.239.63.104\"});\n    assertAnalyzesTo(a, \"1-2-3\",   new String[]{\"1-2-3\"});\n    assertAnalyzesTo(a, \"a1-b2-c3\",   new String[]{\"a1-b2-c3\"});\n    assertAnalyzesTo(a, \"a1-b-c3\",   new String[]{\"a1-b-c3\"});\n\n    // numbers\n    assertAnalyzesTo(a, \"David has 5000 bones\", new String[]{\"david\", \"has\", \"5000\", \"bones\"});\n\n    // various\n    assertAnalyzesTo(a, \"C embedded developers wanted\", new String[]{\"c\", \"embedded\", \"developers\", \"wanted\" });\n    assertAnalyzesTo(a, \"foo bar FOO BAR\", new String[]{\"foo\", \"bar\", \"foo\", \"bar\"});\n    assertAnalyzesTo(a, \"foo      bar .  FOO <> BAR\", new String[]{\"foo\", \"bar\", \"foo\", \"bar\"});\n    assertAnalyzesTo(a, \"\\\"QUOTED\\\" word\", new String[]{\"quoted\", \"word\"});\n\n    // acronyms have their dots stripped\n    assertAnalyzesTo(a, \"U.S.A.\", new String[]{ \"usa\" });\n\n    // It would be nice to change the grammar in StandardTokenizer.jj to make \"C#\" and \"C++\" end up as tokens.\n    assertAnalyzesTo(a, \"C++\", new String[]{\"c\"});\n    assertAnalyzesTo(a, \"C#\", new String[]{\"c\"});\n\n    // Korean words\n    assertAnalyzesTo(a, \"안녕하세요 한글입니다\", new String[]{\"안녕하세요\", \"한글입니다\"});\n\n  }\n\n","sourceOld":"  public void testStandard() throws Exception {\n    Analyzer a = new StandardAnalyzer();\n\n    // alphanumeric tokens\n    assertAnalyzesTo(a, \"B2B\", new String[]{\"b2b\"});\n    assertAnalyzesTo(a, \"2B\", new String[]{\"2b\"});\n\n    // underscores are delimiters, but not in email addresses (below)\n    assertAnalyzesTo(a, \"word_having_underscore\", new String[]{\"word\", \"having\", \"underscore\"});\n    assertAnalyzesTo(a, \"word_with_underscore_and_stopwords\", new String[]{\"word\", \"underscore\", \"stopwords\"});\n\n    // other delimiters: \"-\", \"/\", \",\"\n    assertAnalyzesTo(a, \"some-dashed-phrase\",   new String[]{\"some\", \"dashed\", \"phrase\" });\n    assertAnalyzesTo(a, \"dogs,chase,cats\", new String[]{\"dogs\", \"chase\", \"cats\"});\n    assertAnalyzesTo(a, \"ac/dc\", new String[]{\"ac\", \"dc\"});\n\n    // internal apostrophes: O'Reilly, you're, O'Reilly's\n    // possessives are actually removed by StardardFilter, not the tokenizer\n    assertAnalyzesTo(a, \"O'Reilly\", new String[]{\"o'reilly\"});\n    assertAnalyzesTo(a, \"you're\", new String[]{\"you're\"});\n    assertAnalyzesTo(a, \"O'Reilly's\", new String[]{\"o'reilly\"});\n\n    // company names\n    assertAnalyzesTo(a, \"AT&T\", new String[]{\"at&t\"});\n    assertAnalyzesTo(a, \"Excite@Home\", new String[]{\"excite@home\"});\n\n    // domain names\n    assertAnalyzesTo(a, \"www.nutch.org\",   new String[]{\"www.nutch.org\" });\n\n    // email addresses, possibly with underscores, periods, etc\n    assertAnalyzesTo(a, \"test@example.com\", new String[]{\"test@example.com\"});\n    assertAnalyzesTo(a, \"first.lastname@example.com\", new String[]{\"first.lastname@example.com\"});\n    assertAnalyzesTo(a, \"first_lastname@example.com\", new String[]{\"first_lastname@example.com\"});\n\n    // floating point, serial, model numbers, ip addresses, etc.\n    // every other segment must have at least one digit\n    assertAnalyzesTo(a, \"21.35\", new String[]{\"21.35\"});\n    assertAnalyzesTo(a, \"R2D2 C3PO\", new String[]{\"r2d2\", \"c3po\"});\n    assertAnalyzesTo(a, \"216.239.63.104\",   new String[]{\"216.239.63.104\"});\n    assertAnalyzesTo(a, \"1-2-3\",   new String[]{\"1-2-3\"});\n    assertAnalyzesTo(a, \"a1-b2-c3\",   new String[]{\"a1-b2-c3\"});\n    assertAnalyzesTo(a, \"a1-b-c3\",   new String[]{\"a1-b-c3\"});\n\n    // numbers\n    assertAnalyzesTo(a, \"David has 5000 bones\", new String[]{\"david\", \"has\", \"5000\", \"bones\"});\n\n    // various\n    assertAnalyzesTo(a, \"C embedded developers wanted\", new String[]{\"c\", \"embedded\", \"developers\", \"wanted\" });\n    assertAnalyzesTo(a, \"foo bar FOO BAR\", new String[]{\"foo\", \"bar\", \"foo\", \"bar\"});\n    assertAnalyzesTo(a, \"foo      bar .  FOO <> BAR\", new String[]{\"foo\", \"bar\", \"foo\", \"bar\"});\n    assertAnalyzesTo(a, \"\\\"QUOTED\\\" word\", new String[]{\"quoted\", \"word\"});\n\n    // acronyms have their dots stripped\n    assertAnalyzesTo(a, \"U.S.A.\", new String[]{ \"usa\" });\n\n    // It would be nice to change the grammar in StandardTokenizer.jj to make \"C#\" and \"C++\" end up as tokens.\n    assertAnalyzesTo(a, \"C++\", new String[]{\"c\"});\n    assertAnalyzesTo(a, \"C#\", new String[]{\"c\"});\n\n    // Korean words\n    assertAnalyzesTo(a, \"안녕하세요 한글입니다\", new String[]{\"안녕하세요\", \"한글입니다\"});\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8e191901ff30c9f34f5e72d2e7b6f6c975eb9b4e","date":1186612004,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"src/test/org/apache/lucene/analysis/TestStandardAnalyzer#testStandard().mjava","sourceNew":null,"sourceOld":"  public void testStandard() throws Exception {\n    Analyzer a = new StandardAnalyzer();\n\n    // alphanumeric tokens\n    assertAnalyzesTo(a, \"B2B\", new String[]{\"b2b\"});\n    assertAnalyzesTo(a, \"2B\", new String[]{\"2b\"});\n\n    // underscores are delimiters, but not in email addresses (below)\n    assertAnalyzesTo(a, \"word_having_underscore\", new String[]{\"word\", \"having\", \"underscore\"});\n    assertAnalyzesTo(a, \"word_with_underscore_and_stopwords\", new String[]{\"word\", \"underscore\", \"stopwords\"});\n\n    // other delimiters: \"-\", \"/\", \",\"\n    assertAnalyzesTo(a, \"some-dashed-phrase\",   new String[]{\"some\", \"dashed\", \"phrase\" });\n    assertAnalyzesTo(a, \"dogs,chase,cats\", new String[]{\"dogs\", \"chase\", \"cats\"});\n    assertAnalyzesTo(a, \"ac/dc\", new String[]{\"ac\", \"dc\"});\n\n    // internal apostrophes: O'Reilly, you're, O'Reilly's\n    // possessives are actually removed by StardardFilter, not the tokenizer\n    assertAnalyzesTo(a, \"O'Reilly\", new String[]{\"o'reilly\"});\n    assertAnalyzesTo(a, \"you're\", new String[]{\"you're\"});\n    assertAnalyzesTo(a, \"she's\", new String[]{\"she\"});\n    assertAnalyzesTo(a, \"Jim's\", new String[]{\"jim\"});\n    assertAnalyzesTo(a, \"don't\", new String[]{\"don't\"});\n    assertAnalyzesTo(a, \"O'Reilly's\", new String[]{\"o'reilly\"});\n\n    // t and s had been stopwords in Lucene <= 2.0, which made it impossible\n    // to correctly search for these terms:\n    assertAnalyzesTo(a, \"s-class\", new String[]{\"s\", \"class\"});\n    assertAnalyzesTo(a, \"t-com\", new String[]{\"t\", \"com\"});\n    // 'a' is still a stopword:\n    assertAnalyzesTo(a, \"a-class\", new String[]{\"class\"});\n\n    // company names\n    assertAnalyzesTo(a, \"AT&T\", new String[]{\"at&t\"});\n    assertAnalyzesTo(a, \"Excite@Home\", new String[]{\"excite@home\"});\n\n    // domain names\n    assertAnalyzesTo(a, \"www.nutch.org\",   new String[]{\"www.nutch.org\" });\n\n    // email addresses, possibly with underscores, periods, etc\n    assertAnalyzesTo(a, \"test@example.com\", new String[]{\"test@example.com\"});\n    assertAnalyzesTo(a, \"first.lastname@example.com\", new String[]{\"first.lastname@example.com\"});\n    assertAnalyzesTo(a, \"first_lastname@example.com\", new String[]{\"first_lastname@example.com\"});\n\n    // floating point, serial, model numbers, ip addresses, etc.\n    // every other segment must have at least one digit\n    assertAnalyzesTo(a, \"21.35\", new String[]{\"21.35\"});\n    assertAnalyzesTo(a, \"R2D2 C3PO\", new String[]{\"r2d2\", \"c3po\"});\n    assertAnalyzesTo(a, \"216.239.63.104\",   new String[]{\"216.239.63.104\"});\n    assertAnalyzesTo(a, \"1-2-3\",   new String[]{\"1-2-3\"});\n    assertAnalyzesTo(a, \"a1-b2-c3\",   new String[]{\"a1-b2-c3\"});\n    assertAnalyzesTo(a, \"a1-b-c3\",   new String[]{\"a1-b-c3\"});\n\n    // numbers\n    assertAnalyzesTo(a, \"David has 5000 bones\", new String[]{\"david\", \"has\", \"5000\", \"bones\"});\n\n    // various\n    assertAnalyzesTo(a, \"C embedded developers wanted\", new String[]{\"c\", \"embedded\", \"developers\", \"wanted\" });\n    assertAnalyzesTo(a, \"foo bar FOO BAR\", new String[]{\"foo\", \"bar\", \"foo\", \"bar\"});\n    assertAnalyzesTo(a, \"foo      bar .  FOO <> BAR\", new String[]{\"foo\", \"bar\", \"foo\", \"bar\"});\n    assertAnalyzesTo(a, \"\\\"QUOTED\\\" word\", new String[]{\"quoted\", \"word\"});\n\n    // acronyms have their dots stripped\n    assertAnalyzesTo(a, \"U.S.A.\", new String[]{ \"usa\" });\n\n    // It would be nice to change the grammar in StandardTokenizer.jj to make \"C#\" and \"C++\" end up as tokens.\n    assertAnalyzesTo(a, \"C++\", new String[]{\"c\"});\n    assertAnalyzesTo(a, \"C#\", new String[]{\"c\"});\n\n    // Korean words\n    assertAnalyzesTo(a, \"안녕하세요 한글입니다\", new String[]{\"안녕하세요\", \"한글입니다\"});\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"5e067886e4ac03fc3c4df3d58054940a48ee6e85":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"8e191901ff30c9f34f5e72d2e7b6f6c975eb9b4e":["2fca70c0a9eadf498ff1ef998206337220b05b10"],"2fca70c0a9eadf498ff1ef998206337220b05b10":["71440939a99e296b53d3d64630c7355b914f55a2"],"71440939a99e296b53d3d64630c7355b914f55a2":["5e067886e4ac03fc3c4df3d58054940a48ee6e85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["8e191901ff30c9f34f5e72d2e7b6f6c975eb9b4e"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["5e067886e4ac03fc3c4df3d58054940a48ee6e85"],"5e067886e4ac03fc3c4df3d58054940a48ee6e85":["71440939a99e296b53d3d64630c7355b914f55a2"],"8e191901ff30c9f34f5e72d2e7b6f6c975eb9b4e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"2fca70c0a9eadf498ff1ef998206337220b05b10":["8e191901ff30c9f34f5e72d2e7b6f6c975eb9b4e"],"71440939a99e296b53d3d64630c7355b914f55a2":["2fca70c0a9eadf498ff1ef998206337220b05b10"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}