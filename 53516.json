{"path":"backwards/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","commits":[{"id":"480d01e5b0ef8efb136d51670fec297ae5ae2c9c","date":1268821447,"type":0,"author":"Uwe Schindler","isMerge":false,"pathNew":"backwards/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"/dev/null","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text);\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n        \n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        TermPositions[] tps = new TermPositions[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n            tps[i] = reader.termPositions(terms[i]);\n        }\n        \n        while (tps[0].next()) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].next();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    tps[j].getPayload(verifyPayloadData, offset);\n                    offset += tps[j].getPayloadLength();\n                }\n            }\n        }\n        \n        for (int i = 0; i < numTerms; i++) {\n            tps[i].close();\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        TermPositions tp = reader.termPositions(terms[0]);\n        tp.next();\n        tp.nextPosition();\n        // now we don't read this payload\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        byte[] payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[numTerms]);\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.skipTo(5);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp.seek(terms[1]);\n        tp.next();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayloadLength());\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        tp.getPayload(null, 0);\n        try {\n            // it is forbidden to call getPayload() more than once\n            // without calling nextPosition()\n            tp.getPayload(null, 0);\n            fail(\"Expected exception not thrown\");\n        } catch (Exception expected) {\n            // expected exception\n        }\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = reader.termPositions(new Term(fieldName, singleTerm));\n        tp.next();\n        tp.nextPosition();\n\n        verifyPayloadData = new byte[tp.getPayloadLength()];\n        tp.getPayload(verifyPayloadData, 0);\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, verifyPayloadData);\n        reader.close();\n        \n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/backwards/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"backwards/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text);\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n        \n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        TermPositions[] tps = new TermPositions[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n            tps[i] = reader.termPositions(terms[i]);\n        }\n        \n        while (tps[0].next()) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].next();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    tps[j].getPayload(verifyPayloadData, offset);\n                    offset += tps[j].getPayloadLength();\n                }\n            }\n        }\n        \n        for (int i = 0; i < numTerms; i++) {\n            tps[i].close();\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        TermPositions tp = reader.termPositions(terms[0]);\n        tp.next();\n        tp.nextPosition();\n        // now we don't read this payload\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        byte[] payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[numTerms]);\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.skipTo(5);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp.seek(terms[1]);\n        tp.next();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayloadLength());\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        tp.getPayload(null, 0);\n        try {\n            // it is forbidden to call getPayload() more than once\n            // without calling nextPosition()\n            tp.getPayload(null, 0);\n            fail(\"Expected exception not thrown\");\n        } catch (Exception expected) {\n            // expected exception\n        }\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = reader.termPositions(new Term(fieldName, singleTerm));\n        tp.next();\n        tp.nextPosition();\n\n        verifyPayloadData = new byte[tp.getPayloadLength()];\n        tp.getPayload(verifyPayloadData, 0);\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, verifyPayloadData);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text);\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n        \n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        TermPositions[] tps = new TermPositions[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n            tps[i] = reader.termPositions(terms[i]);\n        }\n        \n        while (tps[0].next()) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].next();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    tps[j].getPayload(verifyPayloadData, offset);\n                    offset += tps[j].getPayloadLength();\n                }\n            }\n        }\n        \n        for (int i = 0; i < numTerms; i++) {\n            tps[i].close();\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        TermPositions tp = reader.termPositions(terms[0]);\n        tp.next();\n        tp.nextPosition();\n        // now we don't read this payload\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        byte[] payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[numTerms]);\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.skipTo(5);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp.seek(terms[1]);\n        tp.next();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayloadLength());\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        tp.getPayload(null, 0);\n        try {\n            // it is forbidden to call getPayload() more than once\n            // without calling nextPosition()\n            tp.getPayload(null, 0);\n            fail(\"Expected exception not thrown\");\n        } catch (Exception expected) {\n            // expected exception\n        }\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = reader.termPositions(new Term(fieldName, singleTerm));\n        tp.next();\n        tp.nextPosition();\n\n        verifyPayloadData = new byte[tp.getPayloadLength()];\n        tp.getPayload(verifyPayloadData, 0);\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, verifyPayloadData);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"480d01e5b0ef8efb136d51670fec297ae5ae2c9c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["480d01e5b0ef8efb136d51670fec297ae5ae2c9c"]},"commit2Childs":{"480d01e5b0ef8efb136d51670fec297ae5ae2c9c":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["480d01e5b0ef8efb136d51670fec297ae5ae2c9c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}