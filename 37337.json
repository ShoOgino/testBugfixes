{"path":"lucene/contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map[String,Int],String).mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map[String,Int],String).mjava","pathOld":"contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map[String,Int],String).mjava","sourceNew":"\t/**\n\t * Adds term frequencies found by tokenizing text from reader into the Map words\n\t * @param r a source of text to be tokenized\n\t * @param termFreqMap a Map of terms and their frequencies\n\t * @param fieldName Used by analyzer for any special per-field analysis\n\t */\n\tprivate void addTermFrequencies(Reader r, Map<String,Int> termFreqMap, String fieldName)\n\t\tthrows IOException\n\t{\n\t\t   TokenStream ts = analyzer.tokenStream(fieldName, r);\n\t\t\tint tokenCount=0;\n\t\t\t// for every token\n\t\t\tTermAttribute termAtt = ts.addAttribute(TermAttribute.class);\n\t\t\t\n\t\t\twhile (ts.incrementToken()) {\n\t\t\t\tString word = termAtt.term();\n\t\t\t\ttokenCount++;\n\t\t\t\tif(tokenCount>maxNumTokensParsed)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif(isNoiseWord(word)){\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\t// increment frequency\n\t\t\t\tInt cnt = termFreqMap.get(word);\n\t\t\t\tif (cnt == null) {\n\t\t\t\t\ttermFreqMap.put(word, new Int());\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tcnt.x++;\n\t\t\t\t}\n\t\t\t}\n\t}\n\n","sourceOld":"\t/**\n\t * Adds term frequencies found by tokenizing text from reader into the Map words\n\t * @param r a source of text to be tokenized\n\t * @param termFreqMap a Map of terms and their frequencies\n\t * @param fieldName Used by analyzer for any special per-field analysis\n\t */\n\tprivate void addTermFrequencies(Reader r, Map<String,Int> termFreqMap, String fieldName)\n\t\tthrows IOException\n\t{\n\t\t   TokenStream ts = analyzer.tokenStream(fieldName, r);\n\t\t\tint tokenCount=0;\n\t\t\t// for every token\n\t\t\tTermAttribute termAtt = ts.addAttribute(TermAttribute.class);\n\t\t\t\n\t\t\twhile (ts.incrementToken()) {\n\t\t\t\tString word = termAtt.term();\n\t\t\t\ttokenCount++;\n\t\t\t\tif(tokenCount>maxNumTokensParsed)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif(isNoiseWord(word)){\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\t// increment frequency\n\t\t\t\tInt cnt = termFreqMap.get(word);\n\t\t\t\tif (cnt == null) {\n\t\t\t\t\ttermFreqMap.put(word, new Int());\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tcnt.x++;\n\t\t\t\t}\n\t\t\t}\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"790c3f61c9b891d66d919c5d10db9fa5216eb0f1","date":1274818604,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map[String,Int],String).mjava","pathOld":"lucene/contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map[String,Int],String).mjava","sourceNew":"\t/**\n\t * Adds term frequencies found by tokenizing text from reader into the Map words\n\t * @param r a source of text to be tokenized\n\t * @param termFreqMap a Map of terms and their frequencies\n\t * @param fieldName Used by analyzer for any special per-field analysis\n\t */\n\tprivate void addTermFrequencies(Reader r, Map<String,Int> termFreqMap, String fieldName)\n\t\tthrows IOException\n\t{\n\t  if (analyzer == null) {\n\t    throw new UnsupportedOperationException(\"To use MoreLikeThis without \" +\n\t    \t\t\"term vectors, you must provide an Analyzer\");\n\t  }\n\t\t   TokenStream ts = analyzer.tokenStream(fieldName, r);\n\t\t\tint tokenCount=0;\n\t\t\t// for every token\n\t\t\tTermAttribute termAtt = ts.addAttribute(TermAttribute.class);\n\t\t\t\n\t\t\twhile (ts.incrementToken()) {\n\t\t\t\tString word = termAtt.term();\n\t\t\t\ttokenCount++;\n\t\t\t\tif(tokenCount>maxNumTokensParsed)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif(isNoiseWord(word)){\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\t// increment frequency\n\t\t\t\tInt cnt = termFreqMap.get(word);\n\t\t\t\tif (cnt == null) {\n\t\t\t\t\ttermFreqMap.put(word, new Int());\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tcnt.x++;\n\t\t\t\t}\n\t\t\t}\n\t}\n\n","sourceOld":"\t/**\n\t * Adds term frequencies found by tokenizing text from reader into the Map words\n\t * @param r a source of text to be tokenized\n\t * @param termFreqMap a Map of terms and their frequencies\n\t * @param fieldName Used by analyzer for any special per-field analysis\n\t */\n\tprivate void addTermFrequencies(Reader r, Map<String,Int> termFreqMap, String fieldName)\n\t\tthrows IOException\n\t{\n\t\t   TokenStream ts = analyzer.tokenStream(fieldName, r);\n\t\t\tint tokenCount=0;\n\t\t\t// for every token\n\t\t\tTermAttribute termAtt = ts.addAttribute(TermAttribute.class);\n\t\t\t\n\t\t\twhile (ts.incrementToken()) {\n\t\t\t\tString word = termAtt.term();\n\t\t\t\ttokenCount++;\n\t\t\t\tif(tokenCount>maxNumTokensParsed)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif(isNoiseWord(word)){\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\t// increment frequency\n\t\t\t\tInt cnt = termFreqMap.get(word);\n\t\t\t\tif (cnt == null) {\n\t\t\t\t\ttermFreqMap.put(word, new Int());\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tcnt.x++;\n\t\t\t\t}\n\t\t\t}\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a7347509fad0711ac30cb15a746e9a3830a38ebd","date":1275388513,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map[String,Int],String).mjava","pathOld":"lucene/contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map[String,Int],String).mjava","sourceNew":"\t/**\n\t * Adds term frequencies found by tokenizing text from reader into the Map words\n\t * @param r a source of text to be tokenized\n\t * @param termFreqMap a Map of terms and their frequencies\n\t * @param fieldName Used by analyzer for any special per-field analysis\n\t */\n\tprivate void addTermFrequencies(Reader r, Map<String,Int> termFreqMap, String fieldName)\n\t\tthrows IOException\n\t{\n\t  if (analyzer == null) {\n\t    throw new UnsupportedOperationException(\"To use MoreLikeThis without \" +\n\t    \t\t\"term vectors, you must provide an Analyzer\");\n\t  }\n\t\t   TokenStream ts = analyzer.tokenStream(fieldName, r);\n\t\t\tint tokenCount=0;\n\t\t\t// for every token\n\t\t\tCharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\t\t\t\n\t\t\twhile (ts.incrementToken()) {\n\t\t\t\tString word = termAtt.toString();\n\t\t\t\ttokenCount++;\n\t\t\t\tif(tokenCount>maxNumTokensParsed)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif(isNoiseWord(word)){\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\t// increment frequency\n\t\t\t\tInt cnt = termFreqMap.get(word);\n\t\t\t\tif (cnt == null) {\n\t\t\t\t\ttermFreqMap.put(word, new Int());\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tcnt.x++;\n\t\t\t\t}\n\t\t\t}\n\t}\n\n","sourceOld":"\t/**\n\t * Adds term frequencies found by tokenizing text from reader into the Map words\n\t * @param r a source of text to be tokenized\n\t * @param termFreqMap a Map of terms and their frequencies\n\t * @param fieldName Used by analyzer for any special per-field analysis\n\t */\n\tprivate void addTermFrequencies(Reader r, Map<String,Int> termFreqMap, String fieldName)\n\t\tthrows IOException\n\t{\n\t  if (analyzer == null) {\n\t    throw new UnsupportedOperationException(\"To use MoreLikeThis without \" +\n\t    \t\t\"term vectors, you must provide an Analyzer\");\n\t  }\n\t\t   TokenStream ts = analyzer.tokenStream(fieldName, r);\n\t\t\tint tokenCount=0;\n\t\t\t// for every token\n\t\t\tTermAttribute termAtt = ts.addAttribute(TermAttribute.class);\n\t\t\t\n\t\t\twhile (ts.incrementToken()) {\n\t\t\t\tString word = termAtt.term();\n\t\t\t\ttokenCount++;\n\t\t\t\tif(tokenCount>maxNumTokensParsed)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif(isNoiseWord(word)){\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\t// increment frequency\n\t\t\t\tInt cnt = termFreqMap.get(word);\n\t\t\t\tif (cnt == null) {\n\t\t\t\t\ttermFreqMap.put(word, new Int());\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tcnt.x++;\n\t\t\t\t}\n\t\t\t}\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4","date":1305207152,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map[String,Int],String).mjava","pathOld":"lucene/contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map[String,Int],String).mjava","sourceNew":"\t/**\n\t * Adds term frequencies found by tokenizing text from reader into the Map words\n\t * @param r a source of text to be tokenized\n\t * @param termFreqMap a Map of terms and their frequencies\n\t * @param fieldName Used by analyzer for any special per-field analysis\n\t */\n\tprivate void addTermFrequencies(Reader r, Map<String,Int> termFreqMap, String fieldName)\n\t\tthrows IOException\n\t{\n\t  if (analyzer == null) {\n\t    throw new UnsupportedOperationException(\"To use MoreLikeThis without \" +\n\t    \t\t\"term vectors, you must provide an Analyzer\");\n\t  }\n\t\t   TokenStream ts = analyzer.tokenStream(fieldName, r);\n\t\t\tint tokenCount=0;\n\t\t\t// for every token\n\t\t\tCharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\t\t\tts.reset();\n\t\t\twhile (ts.incrementToken()) {\n\t\t\t\tString word = termAtt.toString();\n\t\t\t\ttokenCount++;\n\t\t\t\tif(tokenCount>maxNumTokensParsed)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif(isNoiseWord(word)){\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\t// increment frequency\n\t\t\t\tInt cnt = termFreqMap.get(word);\n\t\t\t\tif (cnt == null) {\n\t\t\t\t\ttermFreqMap.put(word, new Int());\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tcnt.x++;\n\t\t\t\t}\n\t\t\t}\n\t\t\tts.end();\n\t\t\tts.close();\n\t}\n\n","sourceOld":"\t/**\n\t * Adds term frequencies found by tokenizing text from reader into the Map words\n\t * @param r a source of text to be tokenized\n\t * @param termFreqMap a Map of terms and their frequencies\n\t * @param fieldName Used by analyzer for any special per-field analysis\n\t */\n\tprivate void addTermFrequencies(Reader r, Map<String,Int> termFreqMap, String fieldName)\n\t\tthrows IOException\n\t{\n\t  if (analyzer == null) {\n\t    throw new UnsupportedOperationException(\"To use MoreLikeThis without \" +\n\t    \t\t\"term vectors, you must provide an Analyzer\");\n\t  }\n\t\t   TokenStream ts = analyzer.tokenStream(fieldName, r);\n\t\t\tint tokenCount=0;\n\t\t\t// for every token\n\t\t\tCharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\t\t\t\n\t\t\twhile (ts.incrementToken()) {\n\t\t\t\tString word = termAtt.toString();\n\t\t\t\ttokenCount++;\n\t\t\t\tif(tokenCount>maxNumTokensParsed)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif(isNoiseWord(word)){\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\t// increment frequency\n\t\t\t\tInt cnt = termFreqMap.get(word);\n\t\t\t\tif (cnt == null) {\n\t\t\t\t\ttermFreqMap.put(word, new Int());\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tcnt.x++;\n\t\t\t\t}\n\t\t\t}\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c700f8d0842d3e52bb2bdfbfdc046a137e836edb","date":1305285499,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map[String,Int],String).mjava","pathOld":"lucene/contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map[String,Int],String).mjava","sourceNew":"\t/**\n\t * Adds term frequencies found by tokenizing text from reader into the Map words\n\t * @param r a source of text to be tokenized\n\t * @param termFreqMap a Map of terms and their frequencies\n\t * @param fieldName Used by analyzer for any special per-field analysis\n\t */\n\tprivate void addTermFrequencies(Reader r, Map<String,Int> termFreqMap, String fieldName)\n\t\tthrows IOException\n\t{\n\t  if (analyzer == null) {\n\t    throw new UnsupportedOperationException(\"To use MoreLikeThis without \" +\n\t    \t\t\"term vectors, you must provide an Analyzer\");\n\t  }\n\t\t   TokenStream ts = analyzer.tokenStream(fieldName, r);\n\t\t\tint tokenCount=0;\n\t\t\t// for every token\n\t\t\tCharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\t\t\tts.reset();\n\t\t\twhile (ts.incrementToken()) {\n\t\t\t\tString word = termAtt.toString();\n\t\t\t\ttokenCount++;\n\t\t\t\tif(tokenCount>maxNumTokensParsed)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif(isNoiseWord(word)){\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\t// increment frequency\n\t\t\t\tInt cnt = termFreqMap.get(word);\n\t\t\t\tif (cnt == null) {\n\t\t\t\t\ttermFreqMap.put(word, new Int());\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tcnt.x++;\n\t\t\t\t}\n\t\t\t}\n\t\t\tts.end();\n\t\t\tts.close();\n\t}\n\n","sourceOld":"\t/**\n\t * Adds term frequencies found by tokenizing text from reader into the Map words\n\t * @param r a source of text to be tokenized\n\t * @param termFreqMap a Map of terms and their frequencies\n\t * @param fieldName Used by analyzer for any special per-field analysis\n\t */\n\tprivate void addTermFrequencies(Reader r, Map<String,Int> termFreqMap, String fieldName)\n\t\tthrows IOException\n\t{\n\t  if (analyzer == null) {\n\t    throw new UnsupportedOperationException(\"To use MoreLikeThis without \" +\n\t    \t\t\"term vectors, you must provide an Analyzer\");\n\t  }\n\t\t   TokenStream ts = analyzer.tokenStream(fieldName, r);\n\t\t\tint tokenCount=0;\n\t\t\t// for every token\n\t\t\tCharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\t\t\t\n\t\t\twhile (ts.incrementToken()) {\n\t\t\t\tString word = termAtt.toString();\n\t\t\t\ttokenCount++;\n\t\t\t\tif(tokenCount>maxNumTokensParsed)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif(isNoiseWord(word)){\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\t// increment frequency\n\t\t\t\tInt cnt = termFreqMap.get(word);\n\t\t\t\tif (cnt == null) {\n\t\t\t\t\ttermFreqMap.put(word, new Int());\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tcnt.x++;\n\t\t\t\t}\n\t\t\t}\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"00746ad002a54281629e3b6f3eb39833a33f093e","date":1305306799,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map[String,Int],String).mjava","pathOld":"lucene/contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map[String,Int],String).mjava","sourceNew":"\t/**\n\t * Adds term frequencies found by tokenizing text from reader into the Map words\n\t * @param r a source of text to be tokenized\n\t * @param termFreqMap a Map of terms and their frequencies\n\t * @param fieldName Used by analyzer for any special per-field analysis\n\t */\n\tprivate void addTermFrequencies(Reader r, Map<String,Int> termFreqMap, String fieldName)\n\t\tthrows IOException\n\t{\n\t  if (analyzer == null) {\n\t    throw new UnsupportedOperationException(\"To use MoreLikeThis without \" +\n\t    \t\t\"term vectors, you must provide an Analyzer\");\n\t  }\n\t\t   TokenStream ts = analyzer.reusableTokenStream(fieldName, r);\n\t\t\tint tokenCount=0;\n\t\t\t// for every token\n\t\t\tCharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\t\t\tts.reset();\n\t\t\twhile (ts.incrementToken()) {\n\t\t\t\tString word = termAtt.toString();\n\t\t\t\ttokenCount++;\n\t\t\t\tif(tokenCount>maxNumTokensParsed)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif(isNoiseWord(word)){\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\t// increment frequency\n\t\t\t\tInt cnt = termFreqMap.get(word);\n\t\t\t\tif (cnt == null) {\n\t\t\t\t\ttermFreqMap.put(word, new Int());\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tcnt.x++;\n\t\t\t\t}\n\t\t\t}\n\t\t\tts.end();\n\t\t\tts.close();\n\t}\n\n","sourceOld":"\t/**\n\t * Adds term frequencies found by tokenizing text from reader into the Map words\n\t * @param r a source of text to be tokenized\n\t * @param termFreqMap a Map of terms and their frequencies\n\t * @param fieldName Used by analyzer for any special per-field analysis\n\t */\n\tprivate void addTermFrequencies(Reader r, Map<String,Int> termFreqMap, String fieldName)\n\t\tthrows IOException\n\t{\n\t  if (analyzer == null) {\n\t    throw new UnsupportedOperationException(\"To use MoreLikeThis without \" +\n\t    \t\t\"term vectors, you must provide an Analyzer\");\n\t  }\n\t\t   TokenStream ts = analyzer.tokenStream(fieldName, r);\n\t\t\tint tokenCount=0;\n\t\t\t// for every token\n\t\t\tCharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\t\t\tts.reset();\n\t\t\twhile (ts.incrementToken()) {\n\t\t\t\tString word = termAtt.toString();\n\t\t\t\ttokenCount++;\n\t\t\t\tif(tokenCount>maxNumTokensParsed)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif(isNoiseWord(word)){\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\t// increment frequency\n\t\t\t\tInt cnt = termFreqMap.get(word);\n\t\t\t\tif (cnt == null) {\n\t\t\t\t\ttermFreqMap.put(word, new Int());\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tcnt.x++;\n\t\t\t\t}\n\t\t\t}\n\t\t\tts.end();\n\t\t\tts.close();\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c3a8a449466c1ff7ce2274fe73dab487256964b4","date":1305735867,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map[String,Int],String).mjava","pathOld":"lucene/contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map[String,Int],String).mjava","sourceNew":"\t/**\n\t * Adds term frequencies found by tokenizing text from reader into the Map words\n\t * @param r a source of text to be tokenized\n\t * @param termFreqMap a Map of terms and their frequencies\n\t * @param fieldName Used by analyzer for any special per-field analysis\n\t */\n\tprivate void addTermFrequencies(Reader r, Map<String,Int> termFreqMap, String fieldName)\n\t\tthrows IOException\n\t{\n\t  if (analyzer == null) {\n\t    throw new UnsupportedOperationException(\"To use MoreLikeThis without \" +\n\t    \t\t\"term vectors, you must provide an Analyzer\");\n\t  }\n\t\t   TokenStream ts = analyzer.reusableTokenStream(fieldName, r);\n\t\t\tint tokenCount=0;\n\t\t\t// for every token\n\t\t\tCharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\t\t\tts.reset();\n\t\t\twhile (ts.incrementToken()) {\n\t\t\t\tString word = termAtt.toString();\n\t\t\t\ttokenCount++;\n\t\t\t\tif(tokenCount>maxNumTokensParsed)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif(isNoiseWord(word)){\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\t// increment frequency\n\t\t\t\tInt cnt = termFreqMap.get(word);\n\t\t\t\tif (cnt == null) {\n\t\t\t\t\ttermFreqMap.put(word, new Int());\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tcnt.x++;\n\t\t\t\t}\n\t\t\t}\n\t\t\tts.end();\n\t\t\tts.close();\n\t}\n\n","sourceOld":"\t/**\n\t * Adds term frequencies found by tokenizing text from reader into the Map words\n\t * @param r a source of text to be tokenized\n\t * @param termFreqMap a Map of terms and their frequencies\n\t * @param fieldName Used by analyzer for any special per-field analysis\n\t */\n\tprivate void addTermFrequencies(Reader r, Map<String,Int> termFreqMap, String fieldName)\n\t\tthrows IOException\n\t{\n\t  if (analyzer == null) {\n\t    throw new UnsupportedOperationException(\"To use MoreLikeThis without \" +\n\t    \t\t\"term vectors, you must provide an Analyzer\");\n\t  }\n\t\t   TokenStream ts = analyzer.tokenStream(fieldName, r);\n\t\t\tint tokenCount=0;\n\t\t\t// for every token\n\t\t\tCharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\t\t\tts.reset();\n\t\t\twhile (ts.incrementToken()) {\n\t\t\t\tString word = termAtt.toString();\n\t\t\t\ttokenCount++;\n\t\t\t\tif(tokenCount>maxNumTokensParsed)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif(isNoiseWord(word)){\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\t// increment frequency\n\t\t\t\tInt cnt = termFreqMap.get(word);\n\t\t\t\tif (cnt == null) {\n\t\t\t\t\ttermFreqMap.put(word, new Int());\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tcnt.x++;\n\t\t\t\t}\n\t\t\t}\n\t\t\tts.end();\n\t\t\tts.close();\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map[String,Int],String).mjava","pathOld":"lucene/contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map[String,Int],String).mjava","sourceNew":"\t/**\n\t * Adds term frequencies found by tokenizing text from reader into the Map words\n\t * @param r a source of text to be tokenized\n\t * @param termFreqMap a Map of terms and their frequencies\n\t * @param fieldName Used by analyzer for any special per-field analysis\n\t */\n\tprivate void addTermFrequencies(Reader r, Map<String,Int> termFreqMap, String fieldName)\n\t\tthrows IOException\n\t{\n\t  if (analyzer == null) {\n\t    throw new UnsupportedOperationException(\"To use MoreLikeThis without \" +\n\t    \t\t\"term vectors, you must provide an Analyzer\");\n\t  }\n\t\t   TokenStream ts = analyzer.reusableTokenStream(fieldName, r);\n\t\t\tint tokenCount=0;\n\t\t\t// for every token\n\t\t\tCharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\t\t\tts.reset();\n\t\t\twhile (ts.incrementToken()) {\n\t\t\t\tString word = termAtt.toString();\n\t\t\t\ttokenCount++;\n\t\t\t\tif(tokenCount>maxNumTokensParsed)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif(isNoiseWord(word)){\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\t// increment frequency\n\t\t\t\tInt cnt = termFreqMap.get(word);\n\t\t\t\tif (cnt == null) {\n\t\t\t\t\ttermFreqMap.put(word, new Int());\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tcnt.x++;\n\t\t\t\t}\n\t\t\t}\n\t\t\tts.end();\n\t\t\tts.close();\n\t}\n\n","sourceOld":"\t/**\n\t * Adds term frequencies found by tokenizing text from reader into the Map words\n\t * @param r a source of text to be tokenized\n\t * @param termFreqMap a Map of terms and their frequencies\n\t * @param fieldName Used by analyzer for any special per-field analysis\n\t */\n\tprivate void addTermFrequencies(Reader r, Map<String,Int> termFreqMap, String fieldName)\n\t\tthrows IOException\n\t{\n\t  if (analyzer == null) {\n\t    throw new UnsupportedOperationException(\"To use MoreLikeThis without \" +\n\t    \t\t\"term vectors, you must provide an Analyzer\");\n\t  }\n\t\t   TokenStream ts = analyzer.tokenStream(fieldName, r);\n\t\t\tint tokenCount=0;\n\t\t\t// for every token\n\t\t\tCharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\t\t\t\n\t\t\twhile (ts.incrementToken()) {\n\t\t\t\tString word = termAtt.toString();\n\t\t\t\ttokenCount++;\n\t\t\t\tif(tokenCount>maxNumTokensParsed)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif(isNoiseWord(word)){\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\t// increment frequency\n\t\t\t\tInt cnt = termFreqMap.get(word);\n\t\t\t\tif (cnt == null) {\n\t\t\t\t\ttermFreqMap.put(word, new Int());\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tcnt.x++;\n\t\t\t\t}\n\t\t\t}\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e141595402370bee958745de8b1c9de1fa182581","date":1310547892,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"lucene/contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map[String,Int],String).mjava","pathOld":"lucene/contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map[String,Int],String).mjava","sourceNew":"  /**\n   * Adds term frequencies found by tokenizing text from reader into the Map words\n   *\n   * @param r a source of text to be tokenized\n   * @param termFreqMap a Map of terms and their frequencies\n   * @param fieldName Used by analyzer for any special per-field analysis\n   */\n  private void addTermFrequencies(Reader r, Map<String, Int> termFreqMap, String fieldName)\n      throws IOException {\n    if (analyzer == null) {\n      throw new UnsupportedOperationException(\"To use MoreLikeThis without \" +\n          \"term vectors, you must provide an Analyzer\");\n    }\n    TokenStream ts = analyzer.reusableTokenStream(fieldName, r);\n    int tokenCount = 0;\n    // for every token\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n    ts.reset();\n    while (ts.incrementToken()) {\n      String word = termAtt.toString();\n      tokenCount++;\n      if (tokenCount > maxNumTokensParsed) {\n        break;\n      }\n      if (isNoiseWord(word)) {\n        continue;\n      }\n\n      // increment frequency\n      Int cnt = termFreqMap.get(word);\n      if (cnt == null) {\n        termFreqMap.put(word, new Int());\n      } else {\n        cnt.x++;\n      }\n    }\n    ts.end();\n    ts.close();\n  }\n\n","sourceOld":"\t/**\n\t * Adds term frequencies found by tokenizing text from reader into the Map words\n\t * @param r a source of text to be tokenized\n\t * @param termFreqMap a Map of terms and their frequencies\n\t * @param fieldName Used by analyzer for any special per-field analysis\n\t */\n\tprivate void addTermFrequencies(Reader r, Map<String,Int> termFreqMap, String fieldName)\n\t\tthrows IOException\n\t{\n\t  if (analyzer == null) {\n\t    throw new UnsupportedOperationException(\"To use MoreLikeThis without \" +\n\t    \t\t\"term vectors, you must provide an Analyzer\");\n\t  }\n\t\t   TokenStream ts = analyzer.reusableTokenStream(fieldName, r);\n\t\t\tint tokenCount=0;\n\t\t\t// for every token\n\t\t\tCharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\t\t\tts.reset();\n\t\t\twhile (ts.incrementToken()) {\n\t\t\t\tString word = termAtt.toString();\n\t\t\t\ttokenCount++;\n\t\t\t\tif(tokenCount>maxNumTokensParsed)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif(isNoiseWord(word)){\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\t// increment frequency\n\t\t\t\tInt cnt = termFreqMap.get(word);\n\t\t\t\tif (cnt == null) {\n\t\t\t\t\ttermFreqMap.put(word, new Int());\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tcnt.x++;\n\t\t\t\t}\n\t\t\t}\n\t\t\tts.end();\n\t\t\tts.close();\n\t}\n\n","bugFix":null,"bugIntro":["782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b7bae1b88906b69dec0d80b1a7afc3c98ec50fa1","date":1310609231,"type":5,"author":"Christopher John Male","isMerge":false,"pathNew":"modules/queries/src/java/org/apache/lucene/queries/mlt/MoreLikeThis#addTermFrequencies(Reader,Map[String,Int],String).mjava","pathOld":"lucene/contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map[String,Int],String).mjava","sourceNew":"  /**\n   * Adds term frequencies found by tokenizing text from reader into the Map words\n   *\n   * @param r a source of text to be tokenized\n   * @param termFreqMap a Map of terms and their frequencies\n   * @param fieldName Used by analyzer for any special per-field analysis\n   */\n  private void addTermFrequencies(Reader r, Map<String, Int> termFreqMap, String fieldName)\n      throws IOException {\n    if (analyzer == null) {\n      throw new UnsupportedOperationException(\"To use MoreLikeThis without \" +\n          \"term vectors, you must provide an Analyzer\");\n    }\n    TokenStream ts = analyzer.reusableTokenStream(fieldName, r);\n    int tokenCount = 0;\n    // for every token\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n    ts.reset();\n    while (ts.incrementToken()) {\n      String word = termAtt.toString();\n      tokenCount++;\n      if (tokenCount > maxNumTokensParsed) {\n        break;\n      }\n      if (isNoiseWord(word)) {\n        continue;\n      }\n\n      // increment frequency\n      Int cnt = termFreqMap.get(word);\n      if (cnt == null) {\n        termFreqMap.put(word, new Int());\n      } else {\n        cnt.x++;\n      }\n    }\n    ts.end();\n    ts.close();\n  }\n\n","sourceOld":"  /**\n   * Adds term frequencies found by tokenizing text from reader into the Map words\n   *\n   * @param r a source of text to be tokenized\n   * @param termFreqMap a Map of terms and their frequencies\n   * @param fieldName Used by analyzer for any special per-field analysis\n   */\n  private void addTermFrequencies(Reader r, Map<String, Int> termFreqMap, String fieldName)\n      throws IOException {\n    if (analyzer == null) {\n      throw new UnsupportedOperationException(\"To use MoreLikeThis without \" +\n          \"term vectors, you must provide an Analyzer\");\n    }\n    TokenStream ts = analyzer.reusableTokenStream(fieldName, r);\n    int tokenCount = 0;\n    // for every token\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n    ts.reset();\n    while (ts.incrementToken()) {\n      String word = termAtt.toString();\n      tokenCount++;\n      if (tokenCount > maxNumTokensParsed) {\n        break;\n      }\n      if (isNoiseWord(word)) {\n        continue;\n      }\n\n      // increment frequency\n      Int cnt = termFreqMap.get(word);\n      if (cnt == null) {\n        termFreqMap.put(word, new Int());\n      } else {\n        cnt.x++;\n      }\n    }\n    ts.end();\n    ts.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"c3a8a449466c1ff7ce2274fe73dab487256964b4":["c700f8d0842d3e52bb2bdfbfdc046a137e836edb","00746ad002a54281629e3b6f3eb39833a33f093e"],"e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4":["a7347509fad0711ac30cb15a746e9a3830a38ebd"],"a3776dccca01c11e7046323cfad46a3b4a471233":["a7347509fad0711ac30cb15a746e9a3830a38ebd","00746ad002a54281629e3b6f3eb39833a33f093e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":["a7347509fad0711ac30cb15a746e9a3830a38ebd","e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4"],"b7bae1b88906b69dec0d80b1a7afc3c98ec50fa1":["e141595402370bee958745de8b1c9de1fa182581"],"00746ad002a54281629e3b6f3eb39833a33f093e":["e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4"],"e141595402370bee958745de8b1c9de1fa182581":["00746ad002a54281629e3b6f3eb39833a33f093e"],"a7347509fad0711ac30cb15a746e9a3830a38ebd":["790c3f61c9b891d66d919c5d10db9fa5216eb0f1"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b7bae1b88906b69dec0d80b1a7afc3c98ec50fa1"],"790c3f61c9b891d66d919c5d10db9fa5216eb0f1":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"c3a8a449466c1ff7ce2274fe73dab487256964b4":[],"e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4":["c700f8d0842d3e52bb2bdfbfdc046a137e836edb","00746ad002a54281629e3b6f3eb39833a33f093e"],"a3776dccca01c11e7046323cfad46a3b4a471233":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":["c3a8a449466c1ff7ce2274fe73dab487256964b4"],"b7bae1b88906b69dec0d80b1a7afc3c98ec50fa1":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"00746ad002a54281629e3b6f3eb39833a33f093e":["c3a8a449466c1ff7ce2274fe73dab487256964b4","a3776dccca01c11e7046323cfad46a3b4a471233","e141595402370bee958745de8b1c9de1fa182581"],"a7347509fad0711ac30cb15a746e9a3830a38ebd":["e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4","a3776dccca01c11e7046323cfad46a3b4a471233","c700f8d0842d3e52bb2bdfbfdc046a137e836edb"],"e141595402370bee958745de8b1c9de1fa182581":["b7bae1b88906b69dec0d80b1a7afc3c98ec50fa1"],"790c3f61c9b891d66d919c5d10db9fa5216eb0f1":["a7347509fad0711ac30cb15a746e9a3830a38ebd"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["790c3f61c9b891d66d919c5d10db9fa5216eb0f1"]},"heads":["c3a8a449466c1ff7ce2274fe73dab487256964b4","a3776dccca01c11e7046323cfad46a3b4a471233","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}