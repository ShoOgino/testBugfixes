{"path":"lucene/src/test/org/apache/lucene/codecs/pulsing/TestPulsingReuse#testSophisticatedReuse().mjava","commits":[{"id":"a0ae5e3ed1232483b7b8a014f175a5fe43595982","date":1324062192,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/codecs/pulsing/TestPulsingReuse#testSophisticatedReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/codecs/pulsing/TestPulsingReuse#testSophisticatedReuse().mjava","sourceNew":"  // TODO: this is a basic test. this thing is complicated, add more\n  public void testSophisticatedReuse() throws Exception {\n    // we always run this test with pulsing codec.\n    Codec cp = _TestUtil.alwaysPostingsFormat(new Pulsing40PostingsFormat(1));\n    Directory dir = newDirectory();\n    RandomIndexWriter iw = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));\n    Document doc = new Document();\n    doc.add(new Field(\"foo\", \"a b b c c c d e f g g h i i j j k\", TextField.TYPE_UNSTORED));\n    iw.addDocument(doc);\n    IndexReader ir = iw.getReader();\n    iw.close();\n    \n    IndexReader segment = ir.getSequentialSubReaders()[0];\n    DocsEnum reuse = null;\n    Map<DocsEnum,Boolean> allEnums = new IdentityHashMap<DocsEnum,Boolean>();\n    TermsEnum te = segment.terms(\"foo\").iterator(null);\n    while (te.next() != null) {\n      reuse = te.docs(null, reuse, false);\n      allEnums.put(reuse, true);\n    }\n    \n    assertEquals(2, allEnums.size());\n    \n    allEnums.clear();\n    DocsAndPositionsEnum posReuse = null;\n    te = segment.terms(\"foo\").iterator(null);\n    while (te.next() != null) {\n      posReuse = te.docsAndPositions(null, posReuse);\n      allEnums.put(posReuse, true);\n    }\n    \n    assertEquals(2, allEnums.size());\n    \n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":"  // TODO: this is a basic test. this thing is complicated, add more\n  public void testSophisticatedReuse() throws Exception {\n    // we always run this test with pulsing codec.\n    Codec cp = _TestUtil.alwaysPostingsFormat(new Pulsing40PostingsFormat(1));\n    Directory dir = newDirectory();\n    RandomIndexWriter iw = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));\n    Document doc = new Document();\n    doc.add(new Field(\"foo\", \"a b b c c c d e f g g h i i j j k\", TextField.TYPE_UNSTORED));\n    iw.addDocument(doc);\n    IndexReader ir = iw.getReader();\n    iw.close();\n    \n    IndexReader segment = ir.getSequentialSubReaders()[0];\n    DocsEnum reuse = null;\n    Map<DocsEnum,Boolean> allEnums = new IdentityHashMap<DocsEnum,Boolean>();\n    TermsEnum te = segment.terms(\"foo\").iterator(null);\n    while (te.next() != null) {\n      reuse = te.docs(null, reuse, false);\n      allEnums.put(reuse, true);\n    }\n    \n    assertEquals(2, allEnums.size());\n    \n    allEnums.clear();\n    DocsAndPositionsEnum posReuse = null;\n    te = segment.terms(\"foo\").iterator(null);\n    while (te.next() != null) {\n      posReuse = te.docsAndPositions(null, posReuse);\n      allEnums.put(posReuse, true);\n    }\n    \n    assertEquals(2, allEnums.size());\n    \n    ir.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"31f025ae60076ae95274433f3fe8e6ace2857a87","date":1326669465,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/codecs/pulsing/TestPulsingReuse#testSophisticatedReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/codecs/pulsing/TestPulsingReuse#testSophisticatedReuse().mjava","sourceNew":"  // TODO: this is a basic test. this thing is complicated, add more\n  public void testSophisticatedReuse() throws Exception {\n    // we always run this test with pulsing codec.\n    Codec cp = _TestUtil.alwaysPostingsFormat(new Pulsing40PostingsFormat(1));\n    Directory dir = newDirectory();\n    RandomIndexWriter iw = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));\n    Document doc = new Document();\n    doc.add(new Field(\"foo\", \"a b b c c c d e f g g h i i j j k\", TextField.TYPE_UNSTORED));\n    iw.addDocument(doc);\n    IndexReader ir = iw.getReader();\n    iw.close();\n    \n    IndexReader segment = ir.getSequentialSubReaders()[0];\n    DocsEnum reuse = null;\n    Map<DocsEnum,Boolean> allEnums = new IdentityHashMap<DocsEnum,Boolean>();\n    TermsEnum te = segment.terms(\"foo\").iterator(null);\n    while (te.next() != null) {\n      reuse = te.docs(null, reuse, false);\n      allEnums.put(reuse, true);\n    }\n    \n    assertEquals(2, allEnums.size());\n    \n    allEnums.clear();\n    DocsAndPositionsEnum posReuse = null;\n    te = segment.terms(\"foo\").iterator(null);\n    while (te.next() != null) {\n      posReuse = te.docsAndPositions(null, posReuse, false);\n      allEnums.put(posReuse, true);\n    }\n    \n    assertEquals(2, allEnums.size());\n    \n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":"  // TODO: this is a basic test. this thing is complicated, add more\n  public void testSophisticatedReuse() throws Exception {\n    // we always run this test with pulsing codec.\n    Codec cp = _TestUtil.alwaysPostingsFormat(new Pulsing40PostingsFormat(1));\n    Directory dir = newDirectory();\n    RandomIndexWriter iw = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));\n    Document doc = new Document();\n    doc.add(new Field(\"foo\", \"a b b c c c d e f g g h i i j j k\", TextField.TYPE_UNSTORED));\n    iw.addDocument(doc);\n    IndexReader ir = iw.getReader();\n    iw.close();\n    \n    IndexReader segment = ir.getSequentialSubReaders()[0];\n    DocsEnum reuse = null;\n    Map<DocsEnum,Boolean> allEnums = new IdentityHashMap<DocsEnum,Boolean>();\n    TermsEnum te = segment.terms(\"foo\").iterator(null);\n    while (te.next() != null) {\n      reuse = te.docs(null, reuse, false);\n      allEnums.put(reuse, true);\n    }\n    \n    assertEquals(2, allEnums.size());\n    \n    allEnums.clear();\n    DocsAndPositionsEnum posReuse = null;\n    te = segment.terms(\"foo\").iterator(null);\n    while (te.next() != null) {\n      posReuse = te.docsAndPositions(null, posReuse);\n      allEnums.put(posReuse, true);\n    }\n    \n    assertEquals(2, allEnums.size());\n    \n    ir.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ae5fea5d5594e69cc0e1aa13b33fa8b6e78dd12b","date":1327849186,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/codecs/pulsing/TestPulsingReuse#testSophisticatedReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/codecs/pulsing/TestPulsingReuse#testSophisticatedReuse().mjava","sourceNew":"  // TODO: this is a basic test. this thing is complicated, add more\n  public void testSophisticatedReuse() throws Exception {\n    // we always run this test with pulsing codec.\n    Codec cp = _TestUtil.alwaysPostingsFormat(new Pulsing40PostingsFormat(1));\n    Directory dir = newDirectory();\n    RandomIndexWriter iw = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));\n    Document doc = new Document();\n    doc.add(new Field(\"foo\", \"a b b c c c d e f g g h i i j j k\", TextField.TYPE_UNSTORED));\n    iw.addDocument(doc);\n    DirectoryReader ir = iw.getReader();\n    iw.close();\n    \n    AtomicIndexReader segment = getOnlySegmentReader(ir);\n    DocsEnum reuse = null;\n    Map<DocsEnum,Boolean> allEnums = new IdentityHashMap<DocsEnum,Boolean>();\n    TermsEnum te = segment.terms(\"foo\").iterator(null);\n    while (te.next() != null) {\n      reuse = te.docs(null, reuse, false);\n      allEnums.put(reuse, true);\n    }\n    \n    assertEquals(2, allEnums.size());\n    \n    allEnums.clear();\n    DocsAndPositionsEnum posReuse = null;\n    te = segment.terms(\"foo\").iterator(null);\n    while (te.next() != null) {\n      posReuse = te.docsAndPositions(null, posReuse, false);\n      allEnums.put(posReuse, true);\n    }\n    \n    assertEquals(2, allEnums.size());\n    \n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":"  // TODO: this is a basic test. this thing is complicated, add more\n  public void testSophisticatedReuse() throws Exception {\n    // we always run this test with pulsing codec.\n    Codec cp = _TestUtil.alwaysPostingsFormat(new Pulsing40PostingsFormat(1));\n    Directory dir = newDirectory();\n    RandomIndexWriter iw = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));\n    Document doc = new Document();\n    doc.add(new Field(\"foo\", \"a b b c c c d e f g g h i i j j k\", TextField.TYPE_UNSTORED));\n    iw.addDocument(doc);\n    IndexReader ir = iw.getReader();\n    iw.close();\n    \n    IndexReader segment = ir.getSequentialSubReaders()[0];\n    DocsEnum reuse = null;\n    Map<DocsEnum,Boolean> allEnums = new IdentityHashMap<DocsEnum,Boolean>();\n    TermsEnum te = segment.terms(\"foo\").iterator(null);\n    while (te.next() != null) {\n      reuse = te.docs(null, reuse, false);\n      allEnums.put(reuse, true);\n    }\n    \n    assertEquals(2, allEnums.size());\n    \n    allEnums.clear();\n    DocsAndPositionsEnum posReuse = null;\n    te = segment.terms(\"foo\").iterator(null);\n    while (te.next() != null) {\n      posReuse = te.docsAndPositions(null, posReuse, false);\n      allEnums.put(posReuse, true);\n    }\n    \n    assertEquals(2, allEnums.size());\n    \n    ir.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"da6d5ac19a80d65b1e864251f155d30960353b7e","date":1327881054,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/codecs/pulsing/TestPulsingReuse#testSophisticatedReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/codecs/pulsing/TestPulsingReuse#testSophisticatedReuse().mjava","sourceNew":"  // TODO: this is a basic test. this thing is complicated, add more\n  public void testSophisticatedReuse() throws Exception {\n    // we always run this test with pulsing codec.\n    Codec cp = _TestUtil.alwaysPostingsFormat(new Pulsing40PostingsFormat(1));\n    Directory dir = newDirectory();\n    RandomIndexWriter iw = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));\n    Document doc = new Document();\n    doc.add(new Field(\"foo\", \"a b b c c c d e f g g h i i j j k\", TextField.TYPE_UNSTORED));\n    iw.addDocument(doc);\n    DirectoryReader ir = iw.getReader();\n    iw.close();\n    \n    AtomicReader segment = getOnlySegmentReader(ir);\n    DocsEnum reuse = null;\n    Map<DocsEnum,Boolean> allEnums = new IdentityHashMap<DocsEnum,Boolean>();\n    TermsEnum te = segment.terms(\"foo\").iterator(null);\n    while (te.next() != null) {\n      reuse = te.docs(null, reuse, false);\n      allEnums.put(reuse, true);\n    }\n    \n    assertEquals(2, allEnums.size());\n    \n    allEnums.clear();\n    DocsAndPositionsEnum posReuse = null;\n    te = segment.terms(\"foo\").iterator(null);\n    while (te.next() != null) {\n      posReuse = te.docsAndPositions(null, posReuse, false);\n      allEnums.put(posReuse, true);\n    }\n    \n    assertEquals(2, allEnums.size());\n    \n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":"  // TODO: this is a basic test. this thing is complicated, add more\n  public void testSophisticatedReuse() throws Exception {\n    // we always run this test with pulsing codec.\n    Codec cp = _TestUtil.alwaysPostingsFormat(new Pulsing40PostingsFormat(1));\n    Directory dir = newDirectory();\n    RandomIndexWriter iw = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));\n    Document doc = new Document();\n    doc.add(new Field(\"foo\", \"a b b c c c d e f g g h i i j j k\", TextField.TYPE_UNSTORED));\n    iw.addDocument(doc);\n    DirectoryReader ir = iw.getReader();\n    iw.close();\n    \n    AtomicIndexReader segment = getOnlySegmentReader(ir);\n    DocsEnum reuse = null;\n    Map<DocsEnum,Boolean> allEnums = new IdentityHashMap<DocsEnum,Boolean>();\n    TermsEnum te = segment.terms(\"foo\").iterator(null);\n    while (te.next() != null) {\n      reuse = te.docs(null, reuse, false);\n      allEnums.put(reuse, true);\n    }\n    \n    assertEquals(2, allEnums.size());\n    \n    allEnums.clear();\n    DocsAndPositionsEnum posReuse = null;\n    te = segment.terms(\"foo\").iterator(null);\n    while (te.next() != null) {\n      posReuse = te.docsAndPositions(null, posReuse, false);\n      allEnums.put(posReuse, true);\n    }\n    \n    assertEquals(2, allEnums.size());\n    \n    ir.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5cab9a86bd67202d20b6adc463008c8e982b070a","date":1327966443,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/codecs/pulsing/TestPulsingReuse#testSophisticatedReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/codecs/pulsing/TestPulsingReuse#testSophisticatedReuse().mjava","sourceNew":"  // TODO: this is a basic test. this thing is complicated, add more\n  public void testSophisticatedReuse() throws Exception {\n    // we always run this test with pulsing codec.\n    Codec cp = _TestUtil.alwaysPostingsFormat(new Pulsing40PostingsFormat(1));\n    Directory dir = newDirectory();\n    RandomIndexWriter iw = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));\n    Document doc = new Document();\n    doc.add(new Field(\"foo\", \"a b b c c c d e f g g h i i j j k\", TextField.TYPE_UNSTORED));\n    iw.addDocument(doc);\n    DirectoryReader ir = iw.getReader();\n    iw.close();\n    \n    AtomicReader segment = getOnlySegmentReader(ir);\n    DocsEnum reuse = null;\n    Map<DocsEnum,Boolean> allEnums = new IdentityHashMap<DocsEnum,Boolean>();\n    TermsEnum te = segment.terms(\"foo\").iterator(null);\n    while (te.next() != null) {\n      reuse = te.docs(null, reuse, false);\n      allEnums.put(reuse, true);\n    }\n    \n    assertEquals(2, allEnums.size());\n    \n    allEnums.clear();\n    DocsAndPositionsEnum posReuse = null;\n    te = segment.terms(\"foo\").iterator(null);\n    while (te.next() != null) {\n      posReuse = te.docsAndPositions(null, posReuse, false);\n      allEnums.put(posReuse, true);\n    }\n    \n    assertEquals(2, allEnums.size());\n    \n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":"  // TODO: this is a basic test. this thing is complicated, add more\n  public void testSophisticatedReuse() throws Exception {\n    // we always run this test with pulsing codec.\n    Codec cp = _TestUtil.alwaysPostingsFormat(new Pulsing40PostingsFormat(1));\n    Directory dir = newDirectory();\n    RandomIndexWriter iw = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));\n    Document doc = new Document();\n    doc.add(new Field(\"foo\", \"a b b c c c d e f g g h i i j j k\", TextField.TYPE_UNSTORED));\n    iw.addDocument(doc);\n    IndexReader ir = iw.getReader();\n    iw.close();\n    \n    IndexReader segment = ir.getSequentialSubReaders()[0];\n    DocsEnum reuse = null;\n    Map<DocsEnum,Boolean> allEnums = new IdentityHashMap<DocsEnum,Boolean>();\n    TermsEnum te = segment.terms(\"foo\").iterator(null);\n    while (te.next() != null) {\n      reuse = te.docs(null, reuse, false);\n      allEnums.put(reuse, true);\n    }\n    \n    assertEquals(2, allEnums.size());\n    \n    allEnums.clear();\n    DocsAndPositionsEnum posReuse = null;\n    te = segment.terms(\"foo\").iterator(null);\n    while (te.next() != null) {\n      posReuse = te.docsAndPositions(null, posReuse, false);\n      allEnums.put(posReuse, true);\n    }\n    \n    assertEquals(2, allEnums.size());\n    \n    ir.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/codecs/pulsing/TestPulsingReuse#testSophisticatedReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/codecs/pulsing/TestPulsingReuse#testSophisticatedReuse().mjava","sourceNew":"  // TODO: this is a basic test. this thing is complicated, add more\n  public void testSophisticatedReuse() throws Exception {\n    // we always run this test with pulsing codec.\n    Codec cp = _TestUtil.alwaysPostingsFormat(new Pulsing40PostingsFormat(1));\n    Directory dir = newDirectory();\n    RandomIndexWriter iw = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));\n    Document doc = new Document();\n    doc.add(new Field(\"foo\", \"a b b c c c d e f g g h i i j j k\", TextField.TYPE_UNSTORED));\n    iw.addDocument(doc);\n    DirectoryReader ir = iw.getReader();\n    iw.close();\n    \n    AtomicReader segment = getOnlySegmentReader(ir);\n    DocsEnum reuse = null;\n    Map<DocsEnum,Boolean> allEnums = new IdentityHashMap<DocsEnum,Boolean>();\n    TermsEnum te = segment.terms(\"foo\").iterator(null);\n    while (te.next() != null) {\n      reuse = te.docs(null, reuse, false);\n      allEnums.put(reuse, true);\n    }\n    \n    assertEquals(2, allEnums.size());\n    \n    allEnums.clear();\n    DocsAndPositionsEnum posReuse = null;\n    te = segment.terms(\"foo\").iterator(null);\n    while (te.next() != null) {\n      posReuse = te.docsAndPositions(null, posReuse, false);\n      allEnums.put(posReuse, true);\n    }\n    \n    assertEquals(2, allEnums.size());\n    \n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":"  // TODO: this is a basic test. this thing is complicated, add more\n  public void testSophisticatedReuse() throws Exception {\n    // we always run this test with pulsing codec.\n    Codec cp = _TestUtil.alwaysPostingsFormat(new Pulsing40PostingsFormat(1));\n    Directory dir = newDirectory();\n    RandomIndexWriter iw = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));\n    Document doc = new Document();\n    doc.add(new Field(\"foo\", \"a b b c c c d e f g g h i i j j k\", TextField.TYPE_UNSTORED));\n    iw.addDocument(doc);\n    DirectoryReader ir = iw.getReader();\n    iw.close();\n    \n    AtomicReader segment = getOnlySegmentReader(ir);\n    DocsEnum reuse = null;\n    Map<DocsEnum,Boolean> allEnums = new IdentityHashMap<DocsEnum,Boolean>();\n    TermsEnum te = segment.terms(\"foo\").iterator(null);\n    while (te.next() != null) {\n      reuse = te.docs(null, reuse, false);\n      allEnums.put(reuse, true);\n    }\n    \n    assertEquals(2, allEnums.size());\n    \n    allEnums.clear();\n    DocsAndPositionsEnum posReuse = null;\n    te = segment.terms(\"foo\").iterator(null);\n    while (te.next() != null) {\n      posReuse = te.docsAndPositions(null, posReuse, false);\n      allEnums.put(posReuse, true);\n    }\n    \n    assertEquals(2, allEnums.size());\n    \n    ir.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"da6d5ac19a80d65b1e864251f155d30960353b7e":["ae5fea5d5594e69cc0e1aa13b33fa8b6e78dd12b"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"ae5fea5d5594e69cc0e1aa13b33fa8b6e78dd12b":["31f025ae60076ae95274433f3fe8e6ace2857a87"],"a0ae5e3ed1232483b7b8a014f175a5fe43595982":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["31f025ae60076ae95274433f3fe8e6ace2857a87","da6d5ac19a80d65b1e864251f155d30960353b7e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"31f025ae60076ae95274433f3fe8e6ace2857a87":["a0ae5e3ed1232483b7b8a014f175a5fe43595982"]},"commit2Childs":{"da6d5ac19a80d65b1e864251f155d30960353b7e":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a0ae5e3ed1232483b7b8a014f175a5fe43595982"],"ae5fea5d5594e69cc0e1aa13b33fa8b6e78dd12b":["da6d5ac19a80d65b1e864251f155d30960353b7e"],"a0ae5e3ed1232483b7b8a014f175a5fe43595982":["31f025ae60076ae95274433f3fe8e6ace2857a87"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"31f025ae60076ae95274433f3fe8e6ace2857a87":["ae5fea5d5594e69cc0e1aa13b33fa8b6e78dd12b","5cab9a86bd67202d20b6adc463008c8e982b070a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}