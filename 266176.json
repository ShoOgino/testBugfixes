{"path":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","commits":[{"id":"953de31d76c9d58f1e3f4e41ff8a48a1529226de","date":1277371072,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"/dev/null","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = new MockRAMDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    IndexWriter w  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = new Field(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = newRandom();\n\n    int NUM_DOCS = 10*_TestUtil.getRandomMultiplier();\n    for(int i=0;i<NUM_DOCS;i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = new IndexSearcher(reader);\n    w.close();\n\n    // now search\n    for(int i=0;i<100*_TestUtil.getRandomMultiplier();i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    searcher.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["a78a90fc9701e511308346ea29f4f5e548bb39fe","782ed6a4b4ba50ec19734fc8db4e570ee193d627","71da933d30aea361ccc224d6544c451cbf49916d"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c084e47df29de3330311d69dabf515ceaa989512","date":1279030906,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = new MockRAMDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, \n        new IndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = new Field(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10*_TestUtil.getRandomMultiplier();\n    for(int i=0;i<NUM_DOCS;i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = new IndexSearcher(reader);\n    w.close();\n\n    // now search\n    for(int i=0;i<100*_TestUtil.getRandomMultiplier();i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = new MockRAMDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    IndexWriter w  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = new Field(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = newRandom();\n\n    int NUM_DOCS = 10*_TestUtil.getRandomMultiplier();\n    for(int i=0;i<NUM_DOCS;i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = new IndexSearcher(reader);\n    w.close();\n\n    // now search\n    for(int i=0;i<100*_TestUtil.getRandomMultiplier();i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    searcher.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5f4e87790277826a2aea119328600dfb07761f32","date":1279827275,"type":0,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"/dev/null","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = new MockRAMDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, \n        new IndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = new Field(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10*_TestUtil.getRandomMultiplier();\n    for(int i=0;i<NUM_DOCS;i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = new IndexSearcher(reader);\n    w.close();\n\n    // now search\n    for(int i=0;i<100*_TestUtil.getRandomMultiplier();i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"15bbd254c1506df5299c4df8c148262c7bd6301e","date":1279913113,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = new MockRAMDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, analyzer);\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = new Field(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10*_TestUtil.getRandomMultiplier();\n    for(int i=0;i<NUM_DOCS;i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = new IndexSearcher(reader);\n    w.close();\n\n    // now search\n    for(int i=0;i<100*_TestUtil.getRandomMultiplier();i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = new MockRAMDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, \n        new IndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = new Field(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10*_TestUtil.getRandomMultiplier();\n    for(int i=0;i<NUM_DOCS;i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = new IndexSearcher(reader);\n    w.close();\n\n    // now search\n    for(int i=0;i<100*_TestUtil.getRandomMultiplier();i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4b103252dee6afa1b6d7a622c773d178788eb85a","date":1280180143,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = new MockRAMDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, analyzer);\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = new Field(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10*_TestUtil.getRandomMultiplier();\n    for(int i=0;i<NUM_DOCS;i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = new IndexSearcher(reader);\n    w.close();\n\n    // now search\n    for(int i=0;i<100*_TestUtil.getRandomMultiplier();i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = new MockRAMDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, \n        new IndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = new Field(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10*_TestUtil.getRandomMultiplier();\n    for(int i=0;i<NUM_DOCS;i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = new IndexSearcher(reader);\n    w.close();\n\n    // now search\n    for(int i=0;i<100*_TestUtil.getRandomMultiplier();i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a0e45742e10e8e3b98e854babe6dbb07a4197b71","date":1280230285,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = new MockRAMDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, analyzer);\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = new Field(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = new IndexSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = new MockRAMDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, analyzer);\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = new Field(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10*_TestUtil.getRandomMultiplier();\n    for(int i=0;i<NUM_DOCS;i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = new IndexSearcher(reader);\n    w.close();\n\n    // now search\n    for(int i=0;i<100*_TestUtil.getRandomMultiplier();i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3242a09f703274d3b9283f2064a1a33064b53a1b","date":1280263474,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = new MockRAMDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, analyzer);\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = new Field(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = new IndexSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = new MockRAMDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, \n        new IndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = new Field(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10*_TestUtil.getRandomMultiplier();\n    for(int i=0;i<NUM_DOCS;i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = new IndexSearcher(reader);\n    w.close();\n\n    // now search\n    for(int i=0;i<100*_TestUtil.getRandomMultiplier();i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c","date":1281646583,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory(random);\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, analyzer);\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = new Field(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = new IndexSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = new MockRAMDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, analyzer);\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = new Field(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = new IndexSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1f653cfcf159baeaafe5d01682a911e95bba4012","date":1284122058,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, analyzer);\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = new Field(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = new IndexSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory(random);\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, analyzer);\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = new Field(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = new IndexSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"132903c28af3aa6f67284b78de91c0f0a99488c2","date":1284282129,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, analyzer);\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = new IndexSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, analyzer);\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = new Field(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = new IndexSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"266bff6c3b0f9600b79aae881dce6fab789942c2","date":1287335443,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    assumeFalse(\"test runs extremely slow (minutes) with SimpleText\", \n        CodecProvider.getDefaultCodec().equals(\"SimpleText\"));\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, analyzer);\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = new IndexSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, analyzer);\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = new IndexSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4ecea1664e8617d82eca3b8055a3c37cb4da8511","date":1287578668,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    assumeFalse(\"test runs extremely slow (minutes) with SimpleText\", \n        CodecProvider.getDefaultCodec().equals(\"SimpleText\"));\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, analyzer);\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = new IndexSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, analyzer);\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = new IndexSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5e8b3bb7fbafc318987f7e1b1336223ad1f6fe62","date":1289218965,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, analyzer);\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = new IndexSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    assumeFalse(\"test runs extremely slow (minutes) with SimpleText\", \n        CodecProvider.getDefaultCodec().equals(\"SimpleText\"));\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, analyzer);\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = new IndexSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"85a883878c0af761245ab048babc63d099f835f3","date":1289553330,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, analyzer);\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = new IndexSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    assumeFalse(\"test runs extremely slow (minutes) with SimpleText\", \n        CodecProvider.getDefaultCodec().equals(\"SimpleText\"));\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, analyzer);\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = new IndexSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, analyzer);\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = new IndexSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = new MockRAMDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, analyzer);\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = new Field(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = new IndexSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c19f985e36a65cc969e8e564fe337a0d41512075","date":1296330536,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newInOrderLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = new IndexSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, analyzer);\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = new IndexSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"790e1fde4caa765b3faaad3fbcd25c6973450336","date":1296689245,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newInOrderLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newInOrderLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = new IndexSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newInOrderLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, analyzer);\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = new IndexSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bde51b089eb7f86171eb3406e38a274743f9b7ac","date":1298336439,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newInOrderLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, analyzer);\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = new IndexSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"01e5948db9a07144112d2f08f28ca2e3cd880348","date":1301759232,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newInOrderLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"45669a651c970812a680841b97a77cce06af559f","date":1301922222,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newInOrderLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f2c5f0cb44df114db4228c8f77861714b5cabaea","date":1302542431,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random);\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"962d04139994fce5193143ef35615499a9a96d78","date":1302693744,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random);\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random);\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newInOrderLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4","date":1305207152,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random);\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          ts.reset();\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n          ts.end();\n          ts.close();\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random);\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c700f8d0842d3e52bb2bdfbfdc046a137e836edb","date":1305285499,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random);\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          ts.reset();\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n          ts.end();\n          ts.close();\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random);\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random);\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          ts.reset();\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n          ts.end();\n          ts.close();\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer();\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newInOrderLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0762b640e0d0d12b6edb96db68986e13145c3484","date":1307575932,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random);\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = atLeast(5000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          ts.reset();\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n          ts.end();\n          ts.close();\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random);\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          ts.reset();\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n          ts.end();\n          ts.close();\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["71da933d30aea361ccc224d6544c451cbf49916d"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"77cf4379b2824f6ea34b091c495d6e95c38ff9e2","date":1307610475,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random);\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = atLeast(5000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          ts.reset();\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n          ts.end();\n          ts.close();\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random);\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          ts.reset();\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n          ts.end();\n          ts.close();\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","date":1307729864,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random);\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = atLeast(5000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          ts.reset();\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n          ts.end();\n          ts.close();\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random);\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = 10 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(r, 10000, 30000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          ts.reset();\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n          ts.end();\n          ts.close();\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = 100 * RANDOM_MULTIPLIER;\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"dc8c5a273eaba3b6d644c0421799a519aaa8c8de","date":1308535576,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random);\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(random, 4097, 8200);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          ts.reset();\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n          ts.end();\n          ts.close();\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random);\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = atLeast(5000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          ts.reset();\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n          ts.end();\n          ts.close();\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f902dca0fec763317e17fa91ff6543fc8120c609","date":1308553979,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random);\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(random, 4097, 8200);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          ts.reset();\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n          ts.end();\n          ts.close();\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random);\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = atLeast(5000);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          ts.reset();\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n          ts.end();\n          ts.close();\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1509f151d7692d84fae414b2b799ac06ba60fcb4","date":1314451621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random);\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", TextField.TYPE_UNSTORED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(random, 4097, 8200);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          ts.reset();\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n          ts.end();\n          ts.close();\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random);\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(random, 4097, 8200);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          ts.reset();\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n          ts.end();\n          ts.close();\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"69e043c521d4e8db770cc140c63f5ef51f03426a","date":1317187614,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random);\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", TextField.TYPE_UNSTORED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(random, 4097, 8200);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.tokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          ts.reset();\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n          ts.end();\n          ts.close();\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random);\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", TextField.TYPE_UNSTORED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(random, 4097, 8200);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.reusableTokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          ts.reset();\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n          ts.end();\n          ts.close();\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["c83d6c4335f31cae14f625a222bc842f20073dcd"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0e7c2454a6a8237bfd0e953f5b940838408c9055","date":1323649300,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random);\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", TextField.TYPE_UNSTORED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(random, 4097, 8200);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.tokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          ts.reset();\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n          ts.end();\n          ts.close();\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random);\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", TextField.TYPE_UNSTORED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(random, 4097, 8200);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.tokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          ts.reset();\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n          ts.end();\n          ts.close();\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"93ccd971aca7fb61b7f1b946e44714cfc80bfc7c","date":1323720782,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random);\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", TextField.TYPE_UNSTORED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(random, 4097, 8200);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.tokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          ts.reset();\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n          ts.end();\n          ts.close();\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random);\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", TextField.TYPE_UNSTORED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(random, 4097, 8200);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.tokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          ts.reset();\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n          ts.end();\n          ts.close();\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    s.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random);\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", TextField.TYPE_UNSTORED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(random, 4097, 8200);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.tokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          ts.reset();\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n          ts.end();\n          ts.close();\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random);\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", TextField.TYPE_UNSTORED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(random, 4097, 8200);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.tokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          ts.reset();\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n          ts.end();\n          ts.close();\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e45742e10e8e3b98e854babe6dbb07a4197b71":["4b103252dee6afa1b6d7a622c773d178788eb85a"],"0762b640e0d0d12b6edb96db68986e13145c3484":["e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["0e7c2454a6a8237bfd0e953f5b940838408c9055"],"93ccd971aca7fb61b7f1b946e44714cfc80bfc7c":["69e043c521d4e8db770cc140c63f5ef51f03426a","0e7c2454a6a8237bfd0e953f5b940838408c9055"],"15bbd254c1506df5299c4df8c148262c7bd6301e":["c084e47df29de3330311d69dabf515ceaa989512"],"132903c28af3aa6f67284b78de91c0f0a99488c2":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["3242a09f703274d3b9283f2064a1a33064b53a1b","5e8b3bb7fbafc318987f7e1b1336223ad1f6fe62"],"c19f985e36a65cc969e8e564fe337a0d41512075":["5e8b3bb7fbafc318987f7e1b1336223ad1f6fe62"],"01e5948db9a07144112d2f08f28ca2e3cd880348":["790e1fde4caa765b3faaad3fbcd25c6973450336"],"c084e47df29de3330311d69dabf515ceaa989512":["953de31d76c9d58f1e3f4e41ff8a48a1529226de"],"953de31d76c9d58f1e3f4e41ff8a48a1529226de":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["01e5948db9a07144112d2f08f28ca2e3cd880348"],"0e7c2454a6a8237bfd0e953f5b940838408c9055":["69e043c521d4e8db770cc140c63f5ef51f03426a"],"1f653cfcf159baeaafe5d01682a911e95bba4012":["ab9633cb67e3c0aec3c066147a23a957d6e7ad8c"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["85a883878c0af761245ab048babc63d099f835f3","790e1fde4caa765b3faaad3fbcd25c6973450336"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":["135621f3a0670a9394eb563224a3b76cc4dddc0f","e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["dc8c5a273eaba3b6d644c0421799a519aaa8c8de"],"69e043c521d4e8db770cc140c63f5ef51f03426a":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","790e1fde4caa765b3faaad3fbcd25c6973450336"],"266bff6c3b0f9600b79aae881dce6fab789942c2":["132903c28af3aa6f67284b78de91c0f0a99488c2"],"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655":["a3776dccca01c11e7046323cfad46a3b4a471233","0762b640e0d0d12b6edb96db68986e13145c3484"],"3242a09f703274d3b9283f2064a1a33064b53a1b":["5f4e87790277826a2aea119328600dfb07761f32","a0e45742e10e8e3b98e854babe6dbb07a4197b71"],"4b103252dee6afa1b6d7a622c773d178788eb85a":["c084e47df29de3330311d69dabf515ceaa989512","15bbd254c1506df5299c4df8c148262c7bd6301e"],"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c":["a0e45742e10e8e3b98e854babe6dbb07a4197b71"],"f902dca0fec763317e17fa91ff6543fc8120c609":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","dc8c5a273eaba3b6d644c0421799a519aaa8c8de"],"85a883878c0af761245ab048babc63d099f835f3":["4ecea1664e8617d82eca3b8055a3c37cb4da8511","5e8b3bb7fbafc318987f7e1b1336223ad1f6fe62"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["29ef99d61cda9641b6250bf9567329a6e65f901d","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"5e8b3bb7fbafc318987f7e1b1336223ad1f6fe62":["266bff6c3b0f9600b79aae881dce6fab789942c2"],"5f4e87790277826a2aea119328600dfb07761f32":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","c084e47df29de3330311d69dabf515ceaa989512"],"962d04139994fce5193143ef35615499a9a96d78":["45669a651c970812a680841b97a77cce06af559f","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"dc8c5a273eaba3b6d644c0421799a519aaa8c8de":["0762b640e0d0d12b6edb96db68986e13145c3484"],"790e1fde4caa765b3faaad3fbcd25c6973450336":["c19f985e36a65cc969e8e564fe337a0d41512075"],"e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4":["f2c5f0cb44df114db4228c8f77861714b5cabaea"],"a3776dccca01c11e7046323cfad46a3b4a471233":["790e1fde4caa765b3faaad3fbcd25c6973450336","e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4"],"77cf4379b2824f6ea34b091c495d6e95c38ff9e2":["c700f8d0842d3e52bb2bdfbfdc046a137e836edb","0762b640e0d0d12b6edb96db68986e13145c3484"],"4ecea1664e8617d82eca3b8055a3c37cb4da8511":["132903c28af3aa6f67284b78de91c0f0a99488c2","266bff6c3b0f9600b79aae881dce6fab789942c2"],"45669a651c970812a680841b97a77cce06af559f":["bde51b089eb7f86171eb3406e38a274743f9b7ac","01e5948db9a07144112d2f08f28ca2e3cd880348"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"a0e45742e10e8e3b98e854babe6dbb07a4197b71":["3242a09f703274d3b9283f2064a1a33064b53a1b","ab9633cb67e3c0aec3c066147a23a957d6e7ad8c"],"0762b640e0d0d12b6edb96db68986e13145c3484":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","dc8c5a273eaba3b6d644c0421799a519aaa8c8de","77cf4379b2824f6ea34b091c495d6e95c38ff9e2"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"93ccd971aca7fb61b7f1b946e44714cfc80bfc7c":[],"15bbd254c1506df5299c4df8c148262c7bd6301e":["4b103252dee6afa1b6d7a622c773d178788eb85a"],"132903c28af3aa6f67284b78de91c0f0a99488c2":["266bff6c3b0f9600b79aae881dce6fab789942c2","4ecea1664e8617d82eca3b8055a3c37cb4da8511"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["bde51b089eb7f86171eb3406e38a274743f9b7ac"],"c19f985e36a65cc969e8e564fe337a0d41512075":["790e1fde4caa765b3faaad3fbcd25c6973450336"],"01e5948db9a07144112d2f08f28ca2e3cd880348":["f2c5f0cb44df114db4228c8f77861714b5cabaea","45669a651c970812a680841b97a77cce06af559f"],"c084e47df29de3330311d69dabf515ceaa989512":["15bbd254c1506df5299c4df8c148262c7bd6301e","4b103252dee6afa1b6d7a622c773d178788eb85a","5f4e87790277826a2aea119328600dfb07761f32"],"953de31d76c9d58f1e3f4e41ff8a48a1529226de":["c084e47df29de3330311d69dabf515ceaa989512"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["135621f3a0670a9394eb563224a3b76cc4dddc0f","962d04139994fce5193143ef35615499a9a96d78","e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4"],"0e7c2454a6a8237bfd0e953f5b940838408c9055":["3a119bbc8703c10faa329ec201c654b3a35a1e3e","93ccd971aca7fb61b7f1b946e44714cfc80bfc7c"],"1f653cfcf159baeaafe5d01682a911e95bba4012":["132903c28af3aa6f67284b78de91c0f0a99488c2"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["953de31d76c9d58f1e3f4e41ff8a48a1529226de","5f4e87790277826a2aea119328600dfb07761f32"],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":["77cf4379b2824f6ea34b091c495d6e95c38ff9e2"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["69e043c521d4e8db770cc140c63f5ef51f03426a"],"69e043c521d4e8db770cc140c63f5ef51f03426a":["93ccd971aca7fb61b7f1b946e44714cfc80bfc7c","0e7c2454a6a8237bfd0e953f5b940838408c9055"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["45669a651c970812a680841b97a77cce06af559f"],"266bff6c3b0f9600b79aae881dce6fab789942c2":["5e8b3bb7fbafc318987f7e1b1336223ad1f6fe62","4ecea1664e8617d82eca3b8055a3c37cb4da8511"],"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655":["f902dca0fec763317e17fa91ff6543fc8120c609"],"3242a09f703274d3b9283f2064a1a33064b53a1b":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"4b103252dee6afa1b6d7a622c773d178788eb85a":["a0e45742e10e8e3b98e854babe6dbb07a4197b71"],"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"f902dca0fec763317e17fa91ff6543fc8120c609":[],"85a883878c0af761245ab048babc63d099f835f3":["29ef99d61cda9641b6250bf9567329a6e65f901d"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["c700f8d0842d3e52bb2bdfbfdc046a137e836edb"],"5e8b3bb7fbafc318987f7e1b1336223ad1f6fe62":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","c19f985e36a65cc969e8e564fe337a0d41512075","85a883878c0af761245ab048babc63d099f835f3"],"5f4e87790277826a2aea119328600dfb07761f32":["3242a09f703274d3b9283f2064a1a33064b53a1b"],"962d04139994fce5193143ef35615499a9a96d78":[],"790e1fde4caa765b3faaad3fbcd25c6973450336":["01e5948db9a07144112d2f08f28ca2e3cd880348","29ef99d61cda9641b6250bf9567329a6e65f901d","bde51b089eb7f86171eb3406e38a274743f9b7ac","a3776dccca01c11e7046323cfad46a3b4a471233"],"dc8c5a273eaba3b6d644c0421799a519aaa8c8de":["1509f151d7692d84fae414b2b799ac06ba60fcb4","f902dca0fec763317e17fa91ff6543fc8120c609"],"e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4":["0762b640e0d0d12b6edb96db68986e13145c3484","c700f8d0842d3e52bb2bdfbfdc046a137e836edb","a3776dccca01c11e7046323cfad46a3b4a471233"],"a3776dccca01c11e7046323cfad46a3b4a471233":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655"],"77cf4379b2824f6ea34b091c495d6e95c38ff9e2":[],"4ecea1664e8617d82eca3b8055a3c37cb4da8511":["85a883878c0af761245ab048babc63d099f835f3"],"45669a651c970812a680841b97a77cce06af559f":["962d04139994fce5193143ef35615499a9a96d78"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["93ccd971aca7fb61b7f1b946e44714cfc80bfc7c","f902dca0fec763317e17fa91ff6543fc8120c609","962d04139994fce5193143ef35615499a9a96d78","77cf4379b2824f6ea34b091c495d6e95c38ff9e2","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}