{"path":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsWriter#writeField(FieldInfo,PointsReader).mjava","commits":[{"id":"4522ffca5a1f420c6a02198c9332d7c596a30ca5","date":1457270822,"type":1,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsWriter#writeField(FieldInfo,PointsReader).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointWriter#writeField(FieldInfo,PointReader).mjava","sourceNew":"  @Override\n  public void writeField(FieldInfo fieldInfo, PointsReader values) throws IOException {\n\n    // We use the normal BKDWriter, but subclass to customize how it writes the index and blocks to disk:\n    try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                          writeState.directory,\n                                          writeState.segmentInfo.name,\n                                          fieldInfo.getPointDimensionCount(),\n                                          fieldInfo.getPointNumBytes(),\n                                          BKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE,\n                                          BKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP) {\n\n        @Override\n        protected void writeIndex(IndexOutput out, long[] leafBlockFPs, byte[] splitPackedValues) throws IOException {\n          write(out, NUM_DIMS);\n          writeInt(out, numDims);\n          newline(out);\n\n          write(out, BYTES_PER_DIM);\n          writeInt(out, bytesPerDim);\n          newline(out);\n\n          write(out, MAX_LEAF_POINTS);\n          writeInt(out, maxPointsInLeafNode);\n          newline(out);\n\n          write(out, INDEX_COUNT);\n          writeInt(out, leafBlockFPs.length);\n          newline(out);\n\n          write(out, MIN_VALUE);\n          BytesRef br = new BytesRef(minPackedValue, 0, minPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, MAX_VALUE);\n          br = new BytesRef(maxPackedValue, 0, maxPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, POINT_COUNT);\n          writeLong(out, pointCount);\n          newline(out);\n\n          write(out, DOC_COUNT);\n          writeInt(out, docsSeen.cardinality());\n          newline(out);\n\n          for(int i=0;i<leafBlockFPs.length;i++) {\n            write(out, BLOCK_FP);\n            writeLong(out, leafBlockFPs[i]);\n            newline(out);\n          }\n\n          assert (splitPackedValues.length % (1 + fieldInfo.getPointNumBytes())) == 0;\n          int count = splitPackedValues.length / (1 + fieldInfo.getPointNumBytes());\n          assert count == leafBlockFPs.length;\n\n          write(out, SPLIT_COUNT);\n          writeInt(out, count);\n          newline(out);\n\n          for(int i=0;i<count;i++) {\n            write(out, SPLIT_DIM);\n            writeInt(out, splitPackedValues[i * (1 + fieldInfo.getPointNumBytes())] & 0xff);\n            newline(out);\n            write(out, SPLIT_VALUE);\n            br = new BytesRef(splitPackedValues, 1+(i * (1+fieldInfo.getPointNumBytes())), fieldInfo.getPointNumBytes());\n            write(out, br.toString());\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeLeafBlockDocs(IndexOutput out, int[] docIDs, int start, int count) throws IOException {\n          write(out, BLOCK_COUNT);\n          writeInt(out, count);\n          newline(out);\n          for(int i=0;i<count;i++) {\n            write(out, BLOCK_DOC_ID);\n            writeInt(out, docIDs[start+i]);\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeCommonPrefixes(IndexOutput out, int[] commonPrefixLengths, byte[] packedValue) {\n          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n        }\n\n        @Override\n        protected void writeLeafBlockPackedValue(IndexOutput out, int[] commonPrefixLengths, byte[] bytes) throws IOException {\n          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n          assert bytes.length == packedBytesLength;\n          write(out, BLOCK_VALUE);\n          write(out, new BytesRef(bytes, 0, bytes.length).toString());\n          newline(out);\n        }          \n      }) {\n\n      values.intersect(fieldInfo.name, new IntersectVisitor() {\n          @Override\n          public void visit(int docID) {\n            throw new IllegalStateException();\n          }\n\n          public void visit(int docID, byte[] packedValue) throws IOException {\n            writer.add(packedValue, docID);\n          }\n\n          @Override\n          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n            return Relation.CELL_CROSSES_QUERY;\n          }\n        });\n\n      // We could have 0 points on merge since all docs with points may be deleted:\n      if (writer.getPointCount() > 0) {\n        indexFPs.put(fieldInfo.name, writer.finish(dataOut));\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void writeField(FieldInfo fieldInfo, PointReader values) throws IOException {\n\n    // We use the normal BKDWriter, but subclass to customize how it writes the index and blocks to disk:\n    try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                          writeState.directory,\n                                          writeState.segmentInfo.name,\n                                          fieldInfo.getPointDimensionCount(),\n                                          fieldInfo.getPointNumBytes(),\n                                          BKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE,\n                                          BKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP) {\n\n        @Override\n        protected void writeIndex(IndexOutput out, long[] leafBlockFPs, byte[] splitPackedValues) throws IOException {\n          write(out, NUM_DIMS);\n          writeInt(out, numDims);\n          newline(out);\n\n          write(out, BYTES_PER_DIM);\n          writeInt(out, bytesPerDim);\n          newline(out);\n\n          write(out, MAX_LEAF_POINTS);\n          writeInt(out, maxPointsInLeafNode);\n          newline(out);\n\n          write(out, INDEX_COUNT);\n          writeInt(out, leafBlockFPs.length);\n          newline(out);\n\n          write(out, MIN_VALUE);\n          BytesRef br = new BytesRef(minPackedValue, 0, minPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, MAX_VALUE);\n          br = new BytesRef(maxPackedValue, 0, maxPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, POINT_COUNT);\n          writeLong(out, pointCount);\n          newline(out);\n\n          write(out, DOC_COUNT);\n          writeInt(out, docsSeen.cardinality());\n          newline(out);\n\n          for(int i=0;i<leafBlockFPs.length;i++) {\n            write(out, BLOCK_FP);\n            writeLong(out, leafBlockFPs[i]);\n            newline(out);\n          }\n\n          assert (splitPackedValues.length % (1 + fieldInfo.getPointNumBytes())) == 0;\n          int count = splitPackedValues.length / (1 + fieldInfo.getPointNumBytes());\n          assert count == leafBlockFPs.length;\n\n          write(out, SPLIT_COUNT);\n          writeInt(out, count);\n          newline(out);\n\n          for(int i=0;i<count;i++) {\n            write(out, SPLIT_DIM);\n            writeInt(out, splitPackedValues[i * (1 + fieldInfo.getPointNumBytes())] & 0xff);\n            newline(out);\n            write(out, SPLIT_VALUE);\n            br = new BytesRef(splitPackedValues, 1+(i * (1+fieldInfo.getPointNumBytes())), fieldInfo.getPointNumBytes());\n            write(out, br.toString());\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeLeafBlockDocs(IndexOutput out, int[] docIDs, int start, int count) throws IOException {\n          write(out, BLOCK_COUNT);\n          writeInt(out, count);\n          newline(out);\n          for(int i=0;i<count;i++) {\n            write(out, BLOCK_DOC_ID);\n            writeInt(out, docIDs[start+i]);\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeCommonPrefixes(IndexOutput out, int[] commonPrefixLengths, byte[] packedValue) {\n          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n        }\n\n        @Override\n        protected void writeLeafBlockPackedValue(IndexOutput out, int[] commonPrefixLengths, byte[] bytes) throws IOException {\n          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n          assert bytes.length == packedBytesLength;\n          write(out, BLOCK_VALUE);\n          write(out, new BytesRef(bytes, 0, bytes.length).toString());\n          newline(out);\n        }          \n      }) {\n\n      values.intersect(fieldInfo.name, new IntersectVisitor() {\n          @Override\n          public void visit(int docID) {\n            throw new IllegalStateException();\n          }\n\n          public void visit(int docID, byte[] packedValue) throws IOException {\n            writer.add(packedValue, docID);\n          }\n\n          @Override\n          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n            return Relation.CELL_CROSSES_QUERY;\n          }\n        });\n\n      // We could have 0 points on merge since all docs with points may be deleted:\n      if (writer.getPointCount() > 0) {\n        indexFPs.put(fieldInfo.name, writer.finish(dataOut));\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9a5a0f27d9486cd33de88627ed3d2ff8dc5074ca","date":1457777566,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsWriter#writeField(FieldInfo,PointsReader).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsWriter#writeField(FieldInfo,PointsReader).mjava","sourceNew":"  @Override\n  public void writeField(FieldInfo fieldInfo, PointsReader values) throws IOException {\n\n    // We use the normal BKDWriter, but subclass to customize how it writes the index and blocks to disk:\n    try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                          writeState.directory,\n                                          writeState.segmentInfo.name,\n                                          fieldInfo.getPointDimensionCount(),\n                                          fieldInfo.getPointNumBytes(),\n                                          BKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE,\n                                          BKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP,\n                                          values.size(fieldInfo.name)) {\n\n        @Override\n        protected void writeIndex(IndexOutput out, long[] leafBlockFPs, byte[] splitPackedValues) throws IOException {\n          write(out, NUM_DIMS);\n          writeInt(out, numDims);\n          newline(out);\n\n          write(out, BYTES_PER_DIM);\n          writeInt(out, bytesPerDim);\n          newline(out);\n\n          write(out, MAX_LEAF_POINTS);\n          writeInt(out, maxPointsInLeafNode);\n          newline(out);\n\n          write(out, INDEX_COUNT);\n          writeInt(out, leafBlockFPs.length);\n          newline(out);\n\n          write(out, MIN_VALUE);\n          BytesRef br = new BytesRef(minPackedValue, 0, minPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, MAX_VALUE);\n          br = new BytesRef(maxPackedValue, 0, maxPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, POINT_COUNT);\n          writeLong(out, pointCount);\n          newline(out);\n\n          write(out, DOC_COUNT);\n          writeInt(out, docsSeen.cardinality());\n          newline(out);\n\n          for(int i=0;i<leafBlockFPs.length;i++) {\n            write(out, BLOCK_FP);\n            writeLong(out, leafBlockFPs[i]);\n            newline(out);\n          }\n\n          assert (splitPackedValues.length % (1 + fieldInfo.getPointNumBytes())) == 0;\n          int count = splitPackedValues.length / (1 + fieldInfo.getPointNumBytes());\n          assert count == leafBlockFPs.length;\n\n          write(out, SPLIT_COUNT);\n          writeInt(out, count);\n          newline(out);\n\n          for(int i=0;i<count;i++) {\n            write(out, SPLIT_DIM);\n            writeInt(out, splitPackedValues[i * (1 + fieldInfo.getPointNumBytes())] & 0xff);\n            newline(out);\n            write(out, SPLIT_VALUE);\n            br = new BytesRef(splitPackedValues, 1+(i * (1+fieldInfo.getPointNumBytes())), fieldInfo.getPointNumBytes());\n            write(out, br.toString());\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeLeafBlockDocs(IndexOutput out, int[] docIDs, int start, int count) throws IOException {\n          write(out, BLOCK_COUNT);\n          writeInt(out, count);\n          newline(out);\n          for(int i=0;i<count;i++) {\n            write(out, BLOCK_DOC_ID);\n            writeInt(out, docIDs[start+i]);\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeCommonPrefixes(IndexOutput out, int[] commonPrefixLengths, byte[] packedValue) {\n          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n        }\n\n        @Override\n        protected void writeLeafBlockPackedValue(IndexOutput out, int[] commonPrefixLengths, byte[] bytes) throws IOException {\n          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n          assert bytes.length == packedBytesLength;\n          write(out, BLOCK_VALUE);\n          write(out, new BytesRef(bytes, 0, bytes.length).toString());\n          newline(out);\n        }          \n      }) {\n\n      values.intersect(fieldInfo.name, new IntersectVisitor() {\n          @Override\n          public void visit(int docID) {\n            throw new IllegalStateException();\n          }\n\n          public void visit(int docID, byte[] packedValue) throws IOException {\n            writer.add(packedValue, docID);\n          }\n\n          @Override\n          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n            return Relation.CELL_CROSSES_QUERY;\n          }\n        });\n\n      // We could have 0 points on merge since all docs with points may be deleted:\n      if (writer.getPointCount() > 0) {\n        indexFPs.put(fieldInfo.name, writer.finish(dataOut));\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void writeField(FieldInfo fieldInfo, PointsReader values) throws IOException {\n\n    // We use the normal BKDWriter, but subclass to customize how it writes the index and blocks to disk:\n    try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                          writeState.directory,\n                                          writeState.segmentInfo.name,\n                                          fieldInfo.getPointDimensionCount(),\n                                          fieldInfo.getPointNumBytes(),\n                                          BKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE,\n                                          BKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP) {\n\n        @Override\n        protected void writeIndex(IndexOutput out, long[] leafBlockFPs, byte[] splitPackedValues) throws IOException {\n          write(out, NUM_DIMS);\n          writeInt(out, numDims);\n          newline(out);\n\n          write(out, BYTES_PER_DIM);\n          writeInt(out, bytesPerDim);\n          newline(out);\n\n          write(out, MAX_LEAF_POINTS);\n          writeInt(out, maxPointsInLeafNode);\n          newline(out);\n\n          write(out, INDEX_COUNT);\n          writeInt(out, leafBlockFPs.length);\n          newline(out);\n\n          write(out, MIN_VALUE);\n          BytesRef br = new BytesRef(minPackedValue, 0, minPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, MAX_VALUE);\n          br = new BytesRef(maxPackedValue, 0, maxPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, POINT_COUNT);\n          writeLong(out, pointCount);\n          newline(out);\n\n          write(out, DOC_COUNT);\n          writeInt(out, docsSeen.cardinality());\n          newline(out);\n\n          for(int i=0;i<leafBlockFPs.length;i++) {\n            write(out, BLOCK_FP);\n            writeLong(out, leafBlockFPs[i]);\n            newline(out);\n          }\n\n          assert (splitPackedValues.length % (1 + fieldInfo.getPointNumBytes())) == 0;\n          int count = splitPackedValues.length / (1 + fieldInfo.getPointNumBytes());\n          assert count == leafBlockFPs.length;\n\n          write(out, SPLIT_COUNT);\n          writeInt(out, count);\n          newline(out);\n\n          for(int i=0;i<count;i++) {\n            write(out, SPLIT_DIM);\n            writeInt(out, splitPackedValues[i * (1 + fieldInfo.getPointNumBytes())] & 0xff);\n            newline(out);\n            write(out, SPLIT_VALUE);\n            br = new BytesRef(splitPackedValues, 1+(i * (1+fieldInfo.getPointNumBytes())), fieldInfo.getPointNumBytes());\n            write(out, br.toString());\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeLeafBlockDocs(IndexOutput out, int[] docIDs, int start, int count) throws IOException {\n          write(out, BLOCK_COUNT);\n          writeInt(out, count);\n          newline(out);\n          for(int i=0;i<count;i++) {\n            write(out, BLOCK_DOC_ID);\n            writeInt(out, docIDs[start+i]);\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeCommonPrefixes(IndexOutput out, int[] commonPrefixLengths, byte[] packedValue) {\n          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n        }\n\n        @Override\n        protected void writeLeafBlockPackedValue(IndexOutput out, int[] commonPrefixLengths, byte[] bytes) throws IOException {\n          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n          assert bytes.length == packedBytesLength;\n          write(out, BLOCK_VALUE);\n          write(out, new BytesRef(bytes, 0, bytes.length).toString());\n          newline(out);\n        }          \n      }) {\n\n      values.intersect(fieldInfo.name, new IntersectVisitor() {\n          @Override\n          public void visit(int docID) {\n            throw new IllegalStateException();\n          }\n\n          public void visit(int docID, byte[] packedValue) throws IOException {\n            writer.add(packedValue, docID);\n          }\n\n          @Override\n          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n            return Relation.CELL_CROSSES_QUERY;\n          }\n        });\n\n      // We could have 0 points on merge since all docs with points may be deleted:\n      if (writer.getPointCount() > 0) {\n        indexFPs.put(fieldInfo.name, writer.finish(dataOut));\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"16ffb58ba57f805651a528311c104f104d9f4573","date":1457861471,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsWriter#writeField(FieldInfo,PointsReader).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsWriter#writeField(FieldInfo,PointsReader).mjava","sourceNew":"  @Override\n  public void writeField(FieldInfo fieldInfo, PointsReader values) throws IOException {\n\n    // We use the normal BKDWriter, but subclass to customize how it writes the index and blocks to disk:\n    try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                          writeState.directory,\n                                          writeState.segmentInfo.name,\n                                          fieldInfo.getPointDimensionCount(),\n                                          fieldInfo.getPointNumBytes(),\n                                          BKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE,\n                                          BKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP,\n                                          values.size(fieldInfo.name)) {\n\n        @Override\n        protected void writeIndex(IndexOutput out, long[] leafBlockFPs, byte[] splitPackedValues) throws IOException {\n          write(out, NUM_DIMS);\n          writeInt(out, numDims);\n          newline(out);\n\n          write(out, BYTES_PER_DIM);\n          writeInt(out, bytesPerDim);\n          newline(out);\n\n          write(out, MAX_LEAF_POINTS);\n          writeInt(out, maxPointsInLeafNode);\n          newline(out);\n\n          write(out, INDEX_COUNT);\n          writeInt(out, leafBlockFPs.length);\n          newline(out);\n\n          write(out, MIN_VALUE);\n          BytesRef br = new BytesRef(minPackedValue, 0, minPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, MAX_VALUE);\n          br = new BytesRef(maxPackedValue, 0, maxPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, POINT_COUNT);\n          writeLong(out, pointCount);\n          newline(out);\n\n          write(out, DOC_COUNT);\n          writeInt(out, docsSeen.cardinality());\n          newline(out);\n\n          for(int i=0;i<leafBlockFPs.length;i++) {\n            write(out, BLOCK_FP);\n            writeLong(out, leafBlockFPs[i]);\n            newline(out);\n          }\n\n          assert (splitPackedValues.length % (1 + fieldInfo.getPointNumBytes())) == 0;\n          int count = splitPackedValues.length / (1 + fieldInfo.getPointNumBytes());\n          assert count == leafBlockFPs.length;\n\n          write(out, SPLIT_COUNT);\n          writeInt(out, count);\n          newline(out);\n\n          for(int i=0;i<count;i++) {\n            write(out, SPLIT_DIM);\n            writeInt(out, splitPackedValues[i * (1 + fieldInfo.getPointNumBytes())] & 0xff);\n            newline(out);\n            write(out, SPLIT_VALUE);\n            br = new BytesRef(splitPackedValues, 1+(i * (1+fieldInfo.getPointNumBytes())), fieldInfo.getPointNumBytes());\n            write(out, br.toString());\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeLeafBlockDocs(IndexOutput out, int[] docIDs, int start, int count) throws IOException {\n          write(out, BLOCK_COUNT);\n          writeInt(out, count);\n          newline(out);\n          for(int i=0;i<count;i++) {\n            write(out, BLOCK_DOC_ID);\n            writeInt(out, docIDs[start+i]);\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeCommonPrefixes(IndexOutput out, int[] commonPrefixLengths, byte[] packedValue) {\n          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n        }\n\n        @Override\n        protected void writeLeafBlockPackedValue(IndexOutput out, int[] commonPrefixLengths, byte[] bytes, int bytesOffset) throws IOException {\n          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n          write(out, BLOCK_VALUE);\n          write(out, new BytesRef(bytes, bytesOffset, packedBytesLength).toString());\n          newline(out);\n        }          \n      }) {\n\n      values.intersect(fieldInfo.name, new IntersectVisitor() {\n          @Override\n          public void visit(int docID) {\n            throw new IllegalStateException();\n          }\n\n          public void visit(int docID, byte[] packedValue) throws IOException {\n            writer.add(packedValue, docID);\n          }\n\n          @Override\n          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n            return Relation.CELL_CROSSES_QUERY;\n          }\n        });\n\n      // We could have 0 points on merge since all docs with points may be deleted:\n      if (writer.getPointCount() > 0) {\n        indexFPs.put(fieldInfo.name, writer.finish(dataOut));\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void writeField(FieldInfo fieldInfo, PointsReader values) throws IOException {\n\n    // We use the normal BKDWriter, but subclass to customize how it writes the index and blocks to disk:\n    try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                          writeState.directory,\n                                          writeState.segmentInfo.name,\n                                          fieldInfo.getPointDimensionCount(),\n                                          fieldInfo.getPointNumBytes(),\n                                          BKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE,\n                                          BKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP,\n                                          values.size(fieldInfo.name)) {\n\n        @Override\n        protected void writeIndex(IndexOutput out, long[] leafBlockFPs, byte[] splitPackedValues) throws IOException {\n          write(out, NUM_DIMS);\n          writeInt(out, numDims);\n          newline(out);\n\n          write(out, BYTES_PER_DIM);\n          writeInt(out, bytesPerDim);\n          newline(out);\n\n          write(out, MAX_LEAF_POINTS);\n          writeInt(out, maxPointsInLeafNode);\n          newline(out);\n\n          write(out, INDEX_COUNT);\n          writeInt(out, leafBlockFPs.length);\n          newline(out);\n\n          write(out, MIN_VALUE);\n          BytesRef br = new BytesRef(minPackedValue, 0, minPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, MAX_VALUE);\n          br = new BytesRef(maxPackedValue, 0, maxPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, POINT_COUNT);\n          writeLong(out, pointCount);\n          newline(out);\n\n          write(out, DOC_COUNT);\n          writeInt(out, docsSeen.cardinality());\n          newline(out);\n\n          for(int i=0;i<leafBlockFPs.length;i++) {\n            write(out, BLOCK_FP);\n            writeLong(out, leafBlockFPs[i]);\n            newline(out);\n          }\n\n          assert (splitPackedValues.length % (1 + fieldInfo.getPointNumBytes())) == 0;\n          int count = splitPackedValues.length / (1 + fieldInfo.getPointNumBytes());\n          assert count == leafBlockFPs.length;\n\n          write(out, SPLIT_COUNT);\n          writeInt(out, count);\n          newline(out);\n\n          for(int i=0;i<count;i++) {\n            write(out, SPLIT_DIM);\n            writeInt(out, splitPackedValues[i * (1 + fieldInfo.getPointNumBytes())] & 0xff);\n            newline(out);\n            write(out, SPLIT_VALUE);\n            br = new BytesRef(splitPackedValues, 1+(i * (1+fieldInfo.getPointNumBytes())), fieldInfo.getPointNumBytes());\n            write(out, br.toString());\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeLeafBlockDocs(IndexOutput out, int[] docIDs, int start, int count) throws IOException {\n          write(out, BLOCK_COUNT);\n          writeInt(out, count);\n          newline(out);\n          for(int i=0;i<count;i++) {\n            write(out, BLOCK_DOC_ID);\n            writeInt(out, docIDs[start+i]);\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeCommonPrefixes(IndexOutput out, int[] commonPrefixLengths, byte[] packedValue) {\n          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n        }\n\n        @Override\n        protected void writeLeafBlockPackedValue(IndexOutput out, int[] commonPrefixLengths, byte[] bytes) throws IOException {\n          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n          assert bytes.length == packedBytesLength;\n          write(out, BLOCK_VALUE);\n          write(out, new BytesRef(bytes, 0, bytes.length).toString());\n          newline(out);\n        }          \n      }) {\n\n      values.intersect(fieldInfo.name, new IntersectVisitor() {\n          @Override\n          public void visit(int docID) {\n            throw new IllegalStateException();\n          }\n\n          public void visit(int docID, byte[] packedValue) throws IOException {\n            writer.add(packedValue, docID);\n          }\n\n          @Override\n          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n            return Relation.CELL_CROSSES_QUERY;\n          }\n        });\n\n      // We could have 0 points on merge since all docs with points may be deleted:\n      if (writer.getPointCount() > 0) {\n        indexFPs.put(fieldInfo.name, writer.finish(dataOut));\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b9028cf27fe30db95667505bb92ecaee8fa3aef7","date":1457861734,"type":3,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsWriter#writeField(FieldInfo,PointsReader).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsWriter#writeField(FieldInfo,PointsReader).mjava","sourceNew":"  @Override\n  public void writeField(FieldInfo fieldInfo, PointsReader values) throws IOException {\n\n    // We use the normal BKDWriter, but subclass to customize how it writes the index and blocks to disk:\n    try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                          writeState.directory,\n                                          writeState.segmentInfo.name,\n                                          fieldInfo.getPointDimensionCount(),\n                                          fieldInfo.getPointNumBytes(),\n                                          BKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE,\n                                          BKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP,\n                                          values.size(fieldInfo.name)) {\n\n        @Override\n        protected void writeIndex(IndexOutput out, long[] leafBlockFPs, byte[] splitPackedValues) throws IOException {\n          write(out, NUM_DIMS);\n          writeInt(out, numDims);\n          newline(out);\n\n          write(out, BYTES_PER_DIM);\n          writeInt(out, bytesPerDim);\n          newline(out);\n\n          write(out, MAX_LEAF_POINTS);\n          writeInt(out, maxPointsInLeafNode);\n          newline(out);\n\n          write(out, INDEX_COUNT);\n          writeInt(out, leafBlockFPs.length);\n          newline(out);\n\n          write(out, MIN_VALUE);\n          BytesRef br = new BytesRef(minPackedValue, 0, minPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, MAX_VALUE);\n          br = new BytesRef(maxPackedValue, 0, maxPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, POINT_COUNT);\n          writeLong(out, pointCount);\n          newline(out);\n\n          write(out, DOC_COUNT);\n          writeInt(out, docsSeen.cardinality());\n          newline(out);\n\n          for(int i=0;i<leafBlockFPs.length;i++) {\n            write(out, BLOCK_FP);\n            writeLong(out, leafBlockFPs[i]);\n            newline(out);\n          }\n\n          assert (splitPackedValues.length % (1 + fieldInfo.getPointNumBytes())) == 0;\n          int count = splitPackedValues.length / (1 + fieldInfo.getPointNumBytes());\n          assert count == leafBlockFPs.length;\n\n          write(out, SPLIT_COUNT);\n          writeInt(out, count);\n          newline(out);\n\n          for(int i=0;i<count;i++) {\n            write(out, SPLIT_DIM);\n            writeInt(out, splitPackedValues[i * (1 + fieldInfo.getPointNumBytes())] & 0xff);\n            newline(out);\n            write(out, SPLIT_VALUE);\n            br = new BytesRef(splitPackedValues, 1+(i * (1+fieldInfo.getPointNumBytes())), fieldInfo.getPointNumBytes());\n            write(out, br.toString());\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeLeafBlockDocs(IndexOutput out, int[] docIDs, int start, int count) throws IOException {\n          write(out, BLOCK_COUNT);\n          writeInt(out, count);\n          newline(out);\n          for(int i=0;i<count;i++) {\n            write(out, BLOCK_DOC_ID);\n            writeInt(out, docIDs[start+i]);\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeCommonPrefixes(IndexOutput out, int[] commonPrefixLengths, byte[] packedValue) {\n          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n        }\n\n        @Override\n        protected void writeLeafBlockPackedValue(IndexOutput out, int[] commonPrefixLengths, byte[] bytes, int bytesOffset) throws IOException {\n          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n          write(out, BLOCK_VALUE);\n          write(out, new BytesRef(bytes, bytesOffset, packedBytesLength).toString());\n          newline(out);\n        }          \n      }) {\n\n      values.intersect(fieldInfo.name, new IntersectVisitor() {\n          @Override\n          public void visit(int docID) {\n            throw new IllegalStateException();\n          }\n\n          public void visit(int docID, byte[] packedValue) throws IOException {\n            writer.add(packedValue, docID);\n          }\n\n          @Override\n          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n            return Relation.CELL_CROSSES_QUERY;\n          }\n        });\n\n      // We could have 0 points on merge since all docs with points may be deleted:\n      if (writer.getPointCount() > 0) {\n        indexFPs.put(fieldInfo.name, writer.finish(dataOut));\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void writeField(FieldInfo fieldInfo, PointsReader values) throws IOException {\n\n    // We use the normal BKDWriter, but subclass to customize how it writes the index and blocks to disk:\n    try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                          writeState.directory,\n                                          writeState.segmentInfo.name,\n                                          fieldInfo.getPointDimensionCount(),\n                                          fieldInfo.getPointNumBytes(),\n                                          BKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE,\n                                          BKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP,\n                                          values.size(fieldInfo.name)) {\n\n        @Override\n        protected void writeIndex(IndexOutput out, long[] leafBlockFPs, byte[] splitPackedValues) throws IOException {\n          write(out, NUM_DIMS);\n          writeInt(out, numDims);\n          newline(out);\n\n          write(out, BYTES_PER_DIM);\n          writeInt(out, bytesPerDim);\n          newline(out);\n\n          write(out, MAX_LEAF_POINTS);\n          writeInt(out, maxPointsInLeafNode);\n          newline(out);\n\n          write(out, INDEX_COUNT);\n          writeInt(out, leafBlockFPs.length);\n          newline(out);\n\n          write(out, MIN_VALUE);\n          BytesRef br = new BytesRef(minPackedValue, 0, minPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, MAX_VALUE);\n          br = new BytesRef(maxPackedValue, 0, maxPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, POINT_COUNT);\n          writeLong(out, pointCount);\n          newline(out);\n\n          write(out, DOC_COUNT);\n          writeInt(out, docsSeen.cardinality());\n          newline(out);\n\n          for(int i=0;i<leafBlockFPs.length;i++) {\n            write(out, BLOCK_FP);\n            writeLong(out, leafBlockFPs[i]);\n            newline(out);\n          }\n\n          assert (splitPackedValues.length % (1 + fieldInfo.getPointNumBytes())) == 0;\n          int count = splitPackedValues.length / (1 + fieldInfo.getPointNumBytes());\n          assert count == leafBlockFPs.length;\n\n          write(out, SPLIT_COUNT);\n          writeInt(out, count);\n          newline(out);\n\n          for(int i=0;i<count;i++) {\n            write(out, SPLIT_DIM);\n            writeInt(out, splitPackedValues[i * (1 + fieldInfo.getPointNumBytes())] & 0xff);\n            newline(out);\n            write(out, SPLIT_VALUE);\n            br = new BytesRef(splitPackedValues, 1+(i * (1+fieldInfo.getPointNumBytes())), fieldInfo.getPointNumBytes());\n            write(out, br.toString());\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeLeafBlockDocs(IndexOutput out, int[] docIDs, int start, int count) throws IOException {\n          write(out, BLOCK_COUNT);\n          writeInt(out, count);\n          newline(out);\n          for(int i=0;i<count;i++) {\n            write(out, BLOCK_DOC_ID);\n            writeInt(out, docIDs[start+i]);\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeCommonPrefixes(IndexOutput out, int[] commonPrefixLengths, byte[] packedValue) {\n          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n        }\n\n        @Override\n        protected void writeLeafBlockPackedValue(IndexOutput out, int[] commonPrefixLengths, byte[] bytes) throws IOException {\n          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n          assert bytes.length == packedBytesLength;\n          write(out, BLOCK_VALUE);\n          write(out, new BytesRef(bytes, 0, bytes.length).toString());\n          newline(out);\n        }          \n      }) {\n\n      values.intersect(fieldInfo.name, new IntersectVisitor() {\n          @Override\n          public void visit(int docID) {\n            throw new IllegalStateException();\n          }\n\n          public void visit(int docID, byte[] packedValue) throws IOException {\n            writer.add(packedValue, docID);\n          }\n\n          @Override\n          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n            return Relation.CELL_CROSSES_QUERY;\n          }\n        });\n\n      // We could have 0 points on merge since all docs with points may be deleted:\n      if (writer.getPointCount() > 0) {\n        indexFPs.put(fieldInfo.name, writer.finish(dataOut));\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"416f9e28900210be57b69bc12e2954fb98ed7ebe","date":1458479803,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsWriter#writeField(FieldInfo,PointsReader).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsWriter#writeField(FieldInfo,PointsReader).mjava","sourceNew":"  @Override\n  public void writeField(FieldInfo fieldInfo, PointsReader values) throws IOException {\n\n    boolean singleValuePerDoc = values.size(fieldInfo.name) == values.getDocCount(fieldInfo.name);\n\n    // We use the normal BKDWriter, but subclass to customize how it writes the index and blocks to disk:\n    try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                          writeState.directory,\n                                          writeState.segmentInfo.name,\n                                          fieldInfo.getPointDimensionCount(),\n                                          fieldInfo.getPointNumBytes(),\n                                          BKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE,\n                                          BKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP,\n                                          values.size(fieldInfo.name),\n                                          singleValuePerDoc) {\n\n        @Override\n        protected void writeIndex(IndexOutput out, long[] leafBlockFPs, byte[] splitPackedValues) throws IOException {\n          write(out, NUM_DIMS);\n          writeInt(out, numDims);\n          newline(out);\n\n          write(out, BYTES_PER_DIM);\n          writeInt(out, bytesPerDim);\n          newline(out);\n\n          write(out, MAX_LEAF_POINTS);\n          writeInt(out, maxPointsInLeafNode);\n          newline(out);\n\n          write(out, INDEX_COUNT);\n          writeInt(out, leafBlockFPs.length);\n          newline(out);\n\n          write(out, MIN_VALUE);\n          BytesRef br = new BytesRef(minPackedValue, 0, minPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, MAX_VALUE);\n          br = new BytesRef(maxPackedValue, 0, maxPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, POINT_COUNT);\n          writeLong(out, pointCount);\n          newline(out);\n\n          write(out, DOC_COUNT);\n          writeInt(out, docsSeen.cardinality());\n          newline(out);\n\n          for(int i=0;i<leafBlockFPs.length;i++) {\n            write(out, BLOCK_FP);\n            writeLong(out, leafBlockFPs[i]);\n            newline(out);\n          }\n\n          assert (splitPackedValues.length % (1 + fieldInfo.getPointNumBytes())) == 0;\n          int count = splitPackedValues.length / (1 + fieldInfo.getPointNumBytes());\n          assert count == leafBlockFPs.length;\n\n          write(out, SPLIT_COUNT);\n          writeInt(out, count);\n          newline(out);\n\n          for(int i=0;i<count;i++) {\n            write(out, SPLIT_DIM);\n            writeInt(out, splitPackedValues[i * (1 + fieldInfo.getPointNumBytes())] & 0xff);\n            newline(out);\n            write(out, SPLIT_VALUE);\n            br = new BytesRef(splitPackedValues, 1+(i * (1+fieldInfo.getPointNumBytes())), fieldInfo.getPointNumBytes());\n            write(out, br.toString());\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeLeafBlockDocs(IndexOutput out, int[] docIDs, int start, int count) throws IOException {\n          write(out, BLOCK_COUNT);\n          writeInt(out, count);\n          newline(out);\n          for(int i=0;i<count;i++) {\n            write(out, BLOCK_DOC_ID);\n            writeInt(out, docIDs[start+i]);\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeCommonPrefixes(IndexOutput out, int[] commonPrefixLengths, byte[] packedValue) {\n          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n        }\n\n        @Override\n        protected void writeLeafBlockPackedValue(IndexOutput out, int[] commonPrefixLengths, byte[] bytes, int bytesOffset) throws IOException {\n          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n          write(out, BLOCK_VALUE);\n          write(out, new BytesRef(bytes, bytesOffset, packedBytesLength).toString());\n          newline(out);\n        }          \n      }) {\n\n      values.intersect(fieldInfo.name, new IntersectVisitor() {\n          @Override\n          public void visit(int docID) {\n            throw new IllegalStateException();\n          }\n\n          public void visit(int docID, byte[] packedValue) throws IOException {\n            writer.add(packedValue, docID);\n          }\n\n          @Override\n          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n            return Relation.CELL_CROSSES_QUERY;\n          }\n        });\n\n      // We could have 0 points on merge since all docs with points may be deleted:\n      if (writer.getPointCount() > 0) {\n        indexFPs.put(fieldInfo.name, writer.finish(dataOut));\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void writeField(FieldInfo fieldInfo, PointsReader values) throws IOException {\n\n    // We use the normal BKDWriter, but subclass to customize how it writes the index and blocks to disk:\n    try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                          writeState.directory,\n                                          writeState.segmentInfo.name,\n                                          fieldInfo.getPointDimensionCount(),\n                                          fieldInfo.getPointNumBytes(),\n                                          BKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE,\n                                          BKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP,\n                                          values.size(fieldInfo.name)) {\n\n        @Override\n        protected void writeIndex(IndexOutput out, long[] leafBlockFPs, byte[] splitPackedValues) throws IOException {\n          write(out, NUM_DIMS);\n          writeInt(out, numDims);\n          newline(out);\n\n          write(out, BYTES_PER_DIM);\n          writeInt(out, bytesPerDim);\n          newline(out);\n\n          write(out, MAX_LEAF_POINTS);\n          writeInt(out, maxPointsInLeafNode);\n          newline(out);\n\n          write(out, INDEX_COUNT);\n          writeInt(out, leafBlockFPs.length);\n          newline(out);\n\n          write(out, MIN_VALUE);\n          BytesRef br = new BytesRef(minPackedValue, 0, minPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, MAX_VALUE);\n          br = new BytesRef(maxPackedValue, 0, maxPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, POINT_COUNT);\n          writeLong(out, pointCount);\n          newline(out);\n\n          write(out, DOC_COUNT);\n          writeInt(out, docsSeen.cardinality());\n          newline(out);\n\n          for(int i=0;i<leafBlockFPs.length;i++) {\n            write(out, BLOCK_FP);\n            writeLong(out, leafBlockFPs[i]);\n            newline(out);\n          }\n\n          assert (splitPackedValues.length % (1 + fieldInfo.getPointNumBytes())) == 0;\n          int count = splitPackedValues.length / (1 + fieldInfo.getPointNumBytes());\n          assert count == leafBlockFPs.length;\n\n          write(out, SPLIT_COUNT);\n          writeInt(out, count);\n          newline(out);\n\n          for(int i=0;i<count;i++) {\n            write(out, SPLIT_DIM);\n            writeInt(out, splitPackedValues[i * (1 + fieldInfo.getPointNumBytes())] & 0xff);\n            newline(out);\n            write(out, SPLIT_VALUE);\n            br = new BytesRef(splitPackedValues, 1+(i * (1+fieldInfo.getPointNumBytes())), fieldInfo.getPointNumBytes());\n            write(out, br.toString());\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeLeafBlockDocs(IndexOutput out, int[] docIDs, int start, int count) throws IOException {\n          write(out, BLOCK_COUNT);\n          writeInt(out, count);\n          newline(out);\n          for(int i=0;i<count;i++) {\n            write(out, BLOCK_DOC_ID);\n            writeInt(out, docIDs[start+i]);\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeCommonPrefixes(IndexOutput out, int[] commonPrefixLengths, byte[] packedValue) {\n          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n        }\n\n        @Override\n        protected void writeLeafBlockPackedValue(IndexOutput out, int[] commonPrefixLengths, byte[] bytes, int bytesOffset) throws IOException {\n          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n          write(out, BLOCK_VALUE);\n          write(out, new BytesRef(bytes, bytesOffset, packedBytesLength).toString());\n          newline(out);\n        }          \n      }) {\n\n      values.intersect(fieldInfo.name, new IntersectVisitor() {\n          @Override\n          public void visit(int docID) {\n            throw new IllegalStateException();\n          }\n\n          public void visit(int docID, byte[] packedValue) throws IOException {\n            writer.add(packedValue, docID);\n          }\n\n          @Override\n          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n            return Relation.CELL_CROSSES_QUERY;\n          }\n        });\n\n      // We could have 0 points on merge since all docs with points may be deleted:\n      if (writer.getPointCount() > 0) {\n        indexFPs.put(fieldInfo.name, writer.finish(dataOut));\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3ca40baa99f9578eb8408ee5b9177f7ffe6f65d6","date":1468339076,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsWriter#writeField(FieldInfo,PointsReader).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsWriter#writeField(FieldInfo,PointsReader).mjava","sourceNew":"  @Override\n  public void writeField(FieldInfo fieldInfo, PointsReader values) throws IOException {\n\n    boolean singleValuePerDoc = values.size(fieldInfo.name) == values.getDocCount(fieldInfo.name);\n\n    // We use the normal BKDWriter, but subclass to customize how it writes the index and blocks to disk:\n    try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                          writeState.directory,\n                                          writeState.segmentInfo.name,\n                                          fieldInfo.getPointDimensionCount(),\n                                          fieldInfo.getPointNumBytes(),\n                                          BKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE,\n                                          BKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP,\n                                          values.size(fieldInfo.name),\n                                          singleValuePerDoc) {\n\n        @Override\n        protected void writeIndex(IndexOutput out, long[] leafBlockFPs, byte[] splitPackedValues) throws IOException {\n          write(out, NUM_DIMS);\n          writeInt(out, numDims);\n          newline(out);\n\n          write(out, BYTES_PER_DIM);\n          writeInt(out, bytesPerDim);\n          newline(out);\n\n          write(out, MAX_LEAF_POINTS);\n          writeInt(out, maxPointsInLeafNode);\n          newline(out);\n\n          write(out, INDEX_COUNT);\n          writeInt(out, leafBlockFPs.length);\n          newline(out);\n\n          write(out, MIN_VALUE);\n          BytesRef br = new BytesRef(minPackedValue, 0, minPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, MAX_VALUE);\n          br = new BytesRef(maxPackedValue, 0, maxPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, POINT_COUNT);\n          writeLong(out, pointCount);\n          newline(out);\n\n          write(out, DOC_COUNT);\n          writeInt(out, docsSeen.cardinality());\n          newline(out);\n\n          for(int i=0;i<leafBlockFPs.length;i++) {\n            write(out, BLOCK_FP);\n            writeLong(out, leafBlockFPs[i]);\n            newline(out);\n          }\n\n          assert (splitPackedValues.length % (1 + fieldInfo.getPointNumBytes())) == 0;\n          int count = splitPackedValues.length / (1 + fieldInfo.getPointNumBytes());\n          assert count == leafBlockFPs.length;\n\n          write(out, SPLIT_COUNT);\n          writeInt(out, count);\n          newline(out);\n\n          for(int i=0;i<count;i++) {\n            write(out, SPLIT_DIM);\n            writeInt(out, splitPackedValues[i * (1 + fieldInfo.getPointNumBytes())] & 0xff);\n            newline(out);\n            write(out, SPLIT_VALUE);\n            br = new BytesRef(splitPackedValues, 1+(i * (1+fieldInfo.getPointNumBytes())), fieldInfo.getPointNumBytes());\n            write(out, br.toString());\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeLeafBlockDocs(IndexOutput out, int[] docIDs, int start, int count) throws IOException {\n          write(out, BLOCK_COUNT);\n          writeInt(out, count);\n          newline(out);\n          for(int i=0;i<count;i++) {\n            write(out, BLOCK_DOC_ID);\n            writeInt(out, docIDs[start+i]);\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeCommonPrefixes(IndexOutput out, int[] commonPrefixLengths, byte[] packedValue) {\n          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n        }\n\n        @Override\n        protected void writeLeafBlockPackedValues(IndexOutput out, int[] commonPrefixLengths, int count, int sortedDim, IntFunction<BytesRef> packedValues) throws IOException {\n          for (int i = 0; i < count; ++i) {\n            BytesRef packedValue = packedValues.apply(i);\n            // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n            write(out, BLOCK_VALUE);\n            write(out, packedValue.toString());\n            newline(out);\n          }\n        }\n      }) {\n\n      values.intersect(fieldInfo.name, new IntersectVisitor() {\n          @Override\n          public void visit(int docID) {\n            throw new IllegalStateException();\n          }\n\n          public void visit(int docID, byte[] packedValue) throws IOException {\n            writer.add(packedValue, docID);\n          }\n\n          @Override\n          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n            return Relation.CELL_CROSSES_QUERY;\n          }\n        });\n\n      // We could have 0 points on merge since all docs with points may be deleted:\n      if (writer.getPointCount() > 0) {\n        indexFPs.put(fieldInfo.name, writer.finish(dataOut));\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void writeField(FieldInfo fieldInfo, PointsReader values) throws IOException {\n\n    boolean singleValuePerDoc = values.size(fieldInfo.name) == values.getDocCount(fieldInfo.name);\n\n    // We use the normal BKDWriter, but subclass to customize how it writes the index and blocks to disk:\n    try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                          writeState.directory,\n                                          writeState.segmentInfo.name,\n                                          fieldInfo.getPointDimensionCount(),\n                                          fieldInfo.getPointNumBytes(),\n                                          BKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE,\n                                          BKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP,\n                                          values.size(fieldInfo.name),\n                                          singleValuePerDoc) {\n\n        @Override\n        protected void writeIndex(IndexOutput out, long[] leafBlockFPs, byte[] splitPackedValues) throws IOException {\n          write(out, NUM_DIMS);\n          writeInt(out, numDims);\n          newline(out);\n\n          write(out, BYTES_PER_DIM);\n          writeInt(out, bytesPerDim);\n          newline(out);\n\n          write(out, MAX_LEAF_POINTS);\n          writeInt(out, maxPointsInLeafNode);\n          newline(out);\n\n          write(out, INDEX_COUNT);\n          writeInt(out, leafBlockFPs.length);\n          newline(out);\n\n          write(out, MIN_VALUE);\n          BytesRef br = new BytesRef(minPackedValue, 0, minPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, MAX_VALUE);\n          br = new BytesRef(maxPackedValue, 0, maxPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, POINT_COUNT);\n          writeLong(out, pointCount);\n          newline(out);\n\n          write(out, DOC_COUNT);\n          writeInt(out, docsSeen.cardinality());\n          newline(out);\n\n          for(int i=0;i<leafBlockFPs.length;i++) {\n            write(out, BLOCK_FP);\n            writeLong(out, leafBlockFPs[i]);\n            newline(out);\n          }\n\n          assert (splitPackedValues.length % (1 + fieldInfo.getPointNumBytes())) == 0;\n          int count = splitPackedValues.length / (1 + fieldInfo.getPointNumBytes());\n          assert count == leafBlockFPs.length;\n\n          write(out, SPLIT_COUNT);\n          writeInt(out, count);\n          newline(out);\n\n          for(int i=0;i<count;i++) {\n            write(out, SPLIT_DIM);\n            writeInt(out, splitPackedValues[i * (1 + fieldInfo.getPointNumBytes())] & 0xff);\n            newline(out);\n            write(out, SPLIT_VALUE);\n            br = new BytesRef(splitPackedValues, 1+(i * (1+fieldInfo.getPointNumBytes())), fieldInfo.getPointNumBytes());\n            write(out, br.toString());\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeLeafBlockDocs(IndexOutput out, int[] docIDs, int start, int count) throws IOException {\n          write(out, BLOCK_COUNT);\n          writeInt(out, count);\n          newline(out);\n          for(int i=0;i<count;i++) {\n            write(out, BLOCK_DOC_ID);\n            writeInt(out, docIDs[start+i]);\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeCommonPrefixes(IndexOutput out, int[] commonPrefixLengths, byte[] packedValue) {\n          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n        }\n\n        @Override\n        protected void writeLeafBlockPackedValue(IndexOutput out, int[] commonPrefixLengths, byte[] bytes, int bytesOffset) throws IOException {\n          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n          write(out, BLOCK_VALUE);\n          write(out, new BytesRef(bytes, bytesOffset, packedBytesLength).toString());\n          newline(out);\n        }          \n      }) {\n\n      values.intersect(fieldInfo.name, new IntersectVisitor() {\n          @Override\n          public void visit(int docID) {\n            throw new IllegalStateException();\n          }\n\n          public void visit(int docID, byte[] packedValue) throws IOException {\n            writer.add(packedValue, docID);\n          }\n\n          @Override\n          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n            return Relation.CELL_CROSSES_QUERY;\n          }\n        });\n\n      // We could have 0 points on merge since all docs with points may be deleted:\n      if (writer.getPointCount() > 0) {\n        indexFPs.put(fieldInfo.name, writer.finish(dataOut));\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"10005c6013abbd1102f2463cf95604d4c8774c99","date":1469460814,"type":5,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsWriter#writeField(FieldInfo,PointsReader,double).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsWriter#writeField(FieldInfo,PointsReader).mjava","sourceNew":"  @Override\n  public void writeField(FieldInfo fieldInfo, PointsReader values, double maxMBSortInHeap) throws IOException {\n\n    boolean singleValuePerDoc = values.size(fieldInfo.name) == values.getDocCount(fieldInfo.name);\n\n    // We use the normal BKDWriter, but subclass to customize how it writes the index and blocks to disk:\n    try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                          writeState.directory,\n                                          writeState.segmentInfo.name,\n                                          fieldInfo.getPointDimensionCount(),\n                                          fieldInfo.getPointNumBytes(),\n                                          BKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE,\n                                          maxMBSortInHeap,\n                                          values.size(fieldInfo.name),\n                                          singleValuePerDoc) {\n\n        @Override\n        protected void writeIndex(IndexOutput out, long[] leafBlockFPs, byte[] splitPackedValues) throws IOException {\n          write(out, NUM_DIMS);\n          writeInt(out, numDims);\n          newline(out);\n\n          write(out, BYTES_PER_DIM);\n          writeInt(out, bytesPerDim);\n          newline(out);\n\n          write(out, MAX_LEAF_POINTS);\n          writeInt(out, maxPointsInLeafNode);\n          newline(out);\n\n          write(out, INDEX_COUNT);\n          writeInt(out, leafBlockFPs.length);\n          newline(out);\n\n          write(out, MIN_VALUE);\n          BytesRef br = new BytesRef(minPackedValue, 0, minPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, MAX_VALUE);\n          br = new BytesRef(maxPackedValue, 0, maxPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, POINT_COUNT);\n          writeLong(out, pointCount);\n          newline(out);\n\n          write(out, DOC_COUNT);\n          writeInt(out, docsSeen.cardinality());\n          newline(out);\n\n          for(int i=0;i<leafBlockFPs.length;i++) {\n            write(out, BLOCK_FP);\n            writeLong(out, leafBlockFPs[i]);\n            newline(out);\n          }\n\n          assert (splitPackedValues.length % (1 + fieldInfo.getPointNumBytes())) == 0;\n          int count = splitPackedValues.length / (1 + fieldInfo.getPointNumBytes());\n          assert count == leafBlockFPs.length;\n\n          write(out, SPLIT_COUNT);\n          writeInt(out, count);\n          newline(out);\n\n          for(int i=0;i<count;i++) {\n            write(out, SPLIT_DIM);\n            writeInt(out, splitPackedValues[i * (1 + fieldInfo.getPointNumBytes())] & 0xff);\n            newline(out);\n            write(out, SPLIT_VALUE);\n            br = new BytesRef(splitPackedValues, 1+(i * (1+fieldInfo.getPointNumBytes())), fieldInfo.getPointNumBytes());\n            write(out, br.toString());\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeLeafBlockDocs(IndexOutput out, int[] docIDs, int start, int count) throws IOException {\n          write(out, BLOCK_COUNT);\n          writeInt(out, count);\n          newline(out);\n          for(int i=0;i<count;i++) {\n            write(out, BLOCK_DOC_ID);\n            writeInt(out, docIDs[start+i]);\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeCommonPrefixes(IndexOutput out, int[] commonPrefixLengths, byte[] packedValue) {\n          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n        }\n\n        @Override\n        protected void writeLeafBlockPackedValues(IndexOutput out, int[] commonPrefixLengths, int count, int sortedDim, IntFunction<BytesRef> packedValues) throws IOException {\n          for (int i = 0; i < count; ++i) {\n            BytesRef packedValue = packedValues.apply(i);\n            // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n            write(out, BLOCK_VALUE);\n            write(out, packedValue.toString());\n            newline(out);\n          }\n        }\n      }) {\n\n      values.intersect(fieldInfo.name, new IntersectVisitor() {\n          @Override\n          public void visit(int docID) {\n            throw new IllegalStateException();\n          }\n\n          public void visit(int docID, byte[] packedValue) throws IOException {\n            writer.add(packedValue, docID);\n          }\n\n          @Override\n          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n            return Relation.CELL_CROSSES_QUERY;\n          }\n        });\n\n      // We could have 0 points on merge since all docs with points may be deleted:\n      if (writer.getPointCount() > 0) {\n        indexFPs.put(fieldInfo.name, writer.finish(dataOut));\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void writeField(FieldInfo fieldInfo, PointsReader values) throws IOException {\n\n    boolean singleValuePerDoc = values.size(fieldInfo.name) == values.getDocCount(fieldInfo.name);\n\n    // We use the normal BKDWriter, but subclass to customize how it writes the index and blocks to disk:\n    try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                          writeState.directory,\n                                          writeState.segmentInfo.name,\n                                          fieldInfo.getPointDimensionCount(),\n                                          fieldInfo.getPointNumBytes(),\n                                          BKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE,\n                                          BKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP,\n                                          values.size(fieldInfo.name),\n                                          singleValuePerDoc) {\n\n        @Override\n        protected void writeIndex(IndexOutput out, long[] leafBlockFPs, byte[] splitPackedValues) throws IOException {\n          write(out, NUM_DIMS);\n          writeInt(out, numDims);\n          newline(out);\n\n          write(out, BYTES_PER_DIM);\n          writeInt(out, bytesPerDim);\n          newline(out);\n\n          write(out, MAX_LEAF_POINTS);\n          writeInt(out, maxPointsInLeafNode);\n          newline(out);\n\n          write(out, INDEX_COUNT);\n          writeInt(out, leafBlockFPs.length);\n          newline(out);\n\n          write(out, MIN_VALUE);\n          BytesRef br = new BytesRef(minPackedValue, 0, minPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, MAX_VALUE);\n          br = new BytesRef(maxPackedValue, 0, maxPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, POINT_COUNT);\n          writeLong(out, pointCount);\n          newline(out);\n\n          write(out, DOC_COUNT);\n          writeInt(out, docsSeen.cardinality());\n          newline(out);\n\n          for(int i=0;i<leafBlockFPs.length;i++) {\n            write(out, BLOCK_FP);\n            writeLong(out, leafBlockFPs[i]);\n            newline(out);\n          }\n\n          assert (splitPackedValues.length % (1 + fieldInfo.getPointNumBytes())) == 0;\n          int count = splitPackedValues.length / (1 + fieldInfo.getPointNumBytes());\n          assert count == leafBlockFPs.length;\n\n          write(out, SPLIT_COUNT);\n          writeInt(out, count);\n          newline(out);\n\n          for(int i=0;i<count;i++) {\n            write(out, SPLIT_DIM);\n            writeInt(out, splitPackedValues[i * (1 + fieldInfo.getPointNumBytes())] & 0xff);\n            newline(out);\n            write(out, SPLIT_VALUE);\n            br = new BytesRef(splitPackedValues, 1+(i * (1+fieldInfo.getPointNumBytes())), fieldInfo.getPointNumBytes());\n            write(out, br.toString());\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeLeafBlockDocs(IndexOutput out, int[] docIDs, int start, int count) throws IOException {\n          write(out, BLOCK_COUNT);\n          writeInt(out, count);\n          newline(out);\n          for(int i=0;i<count;i++) {\n            write(out, BLOCK_DOC_ID);\n            writeInt(out, docIDs[start+i]);\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeCommonPrefixes(IndexOutput out, int[] commonPrefixLengths, byte[] packedValue) {\n          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n        }\n\n        @Override\n        protected void writeLeafBlockPackedValues(IndexOutput out, int[] commonPrefixLengths, int count, int sortedDim, IntFunction<BytesRef> packedValues) throws IOException {\n          for (int i = 0; i < count; ++i) {\n            BytesRef packedValue = packedValues.apply(i);\n            // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n            write(out, BLOCK_VALUE);\n            write(out, packedValue.toString());\n            newline(out);\n          }\n        }\n      }) {\n\n      values.intersect(fieldInfo.name, new IntersectVisitor() {\n          @Override\n          public void visit(int docID) {\n            throw new IllegalStateException();\n          }\n\n          public void visit(int docID, byte[] packedValue) throws IOException {\n            writer.add(packedValue, docID);\n          }\n\n          @Override\n          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n            return Relation.CELL_CROSSES_QUERY;\n          }\n        });\n\n      // We could have 0 points on merge since all docs with points may be deleted:\n      if (writer.getPointCount() > 0) {\n        indexFPs.put(fieldInfo.name, writer.finish(dataOut));\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d08973aa47f2cf98a588293a53af4e948952ccfb","date":1469518724,"type":5,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsWriter#writeField(FieldInfo,PointsReader,double).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsWriter#writeField(FieldInfo,PointsReader).mjava","sourceNew":"  @Override\n  public void writeField(FieldInfo fieldInfo, PointsReader values, double maxMBSortInHeap) throws IOException {\n\n    boolean singleValuePerDoc = values.size(fieldInfo.name) == values.getDocCount(fieldInfo.name);\n\n    // We use the normal BKDWriter, but subclass to customize how it writes the index and blocks to disk:\n    try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                          writeState.directory,\n                                          writeState.segmentInfo.name,\n                                          fieldInfo.getPointDimensionCount(),\n                                          fieldInfo.getPointNumBytes(),\n                                          BKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE,\n                                          maxMBSortInHeap,\n                                          values.size(fieldInfo.name),\n                                          singleValuePerDoc) {\n\n        @Override\n        protected void writeIndex(IndexOutput out, long[] leafBlockFPs, byte[] splitPackedValues) throws IOException {\n          write(out, NUM_DIMS);\n          writeInt(out, numDims);\n          newline(out);\n\n          write(out, BYTES_PER_DIM);\n          writeInt(out, bytesPerDim);\n          newline(out);\n\n          write(out, MAX_LEAF_POINTS);\n          writeInt(out, maxPointsInLeafNode);\n          newline(out);\n\n          write(out, INDEX_COUNT);\n          writeInt(out, leafBlockFPs.length);\n          newline(out);\n\n          write(out, MIN_VALUE);\n          BytesRef br = new BytesRef(minPackedValue, 0, minPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, MAX_VALUE);\n          br = new BytesRef(maxPackedValue, 0, maxPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, POINT_COUNT);\n          writeLong(out, pointCount);\n          newline(out);\n\n          write(out, DOC_COUNT);\n          writeInt(out, docsSeen.cardinality());\n          newline(out);\n\n          for(int i=0;i<leafBlockFPs.length;i++) {\n            write(out, BLOCK_FP);\n            writeLong(out, leafBlockFPs[i]);\n            newline(out);\n          }\n\n          assert (splitPackedValues.length % (1 + fieldInfo.getPointNumBytes())) == 0;\n          int count = splitPackedValues.length / (1 + fieldInfo.getPointNumBytes());\n          assert count == leafBlockFPs.length;\n\n          write(out, SPLIT_COUNT);\n          writeInt(out, count);\n          newline(out);\n\n          for(int i=0;i<count;i++) {\n            write(out, SPLIT_DIM);\n            writeInt(out, splitPackedValues[i * (1 + fieldInfo.getPointNumBytes())] & 0xff);\n            newline(out);\n            write(out, SPLIT_VALUE);\n            br = new BytesRef(splitPackedValues, 1+(i * (1+fieldInfo.getPointNumBytes())), fieldInfo.getPointNumBytes());\n            write(out, br.toString());\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeLeafBlockDocs(IndexOutput out, int[] docIDs, int start, int count) throws IOException {\n          write(out, BLOCK_COUNT);\n          writeInt(out, count);\n          newline(out);\n          for(int i=0;i<count;i++) {\n            write(out, BLOCK_DOC_ID);\n            writeInt(out, docIDs[start+i]);\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeCommonPrefixes(IndexOutput out, int[] commonPrefixLengths, byte[] packedValue) {\n          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n        }\n\n        @Override\n        protected void writeLeafBlockPackedValues(IndexOutput out, int[] commonPrefixLengths, int count, int sortedDim, IntFunction<BytesRef> packedValues) throws IOException {\n          for (int i = 0; i < count; ++i) {\n            BytesRef packedValue = packedValues.apply(i);\n            // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n            write(out, BLOCK_VALUE);\n            write(out, packedValue.toString());\n            newline(out);\n          }\n        }\n      }) {\n\n      values.intersect(fieldInfo.name, new IntersectVisitor() {\n          @Override\n          public void visit(int docID) {\n            throw new IllegalStateException();\n          }\n\n          public void visit(int docID, byte[] packedValue) throws IOException {\n            writer.add(packedValue, docID);\n          }\n\n          @Override\n          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n            return Relation.CELL_CROSSES_QUERY;\n          }\n        });\n\n      // We could have 0 points on merge since all docs with points may be deleted:\n      if (writer.getPointCount() > 0) {\n        indexFPs.put(fieldInfo.name, writer.finish(dataOut));\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void writeField(FieldInfo fieldInfo, PointsReader values) throws IOException {\n\n    boolean singleValuePerDoc = values.size(fieldInfo.name) == values.getDocCount(fieldInfo.name);\n\n    // We use the normal BKDWriter, but subclass to customize how it writes the index and blocks to disk:\n    try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                          writeState.directory,\n                                          writeState.segmentInfo.name,\n                                          fieldInfo.getPointDimensionCount(),\n                                          fieldInfo.getPointNumBytes(),\n                                          BKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE,\n                                          BKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP,\n                                          values.size(fieldInfo.name),\n                                          singleValuePerDoc) {\n\n        @Override\n        protected void writeIndex(IndexOutput out, long[] leafBlockFPs, byte[] splitPackedValues) throws IOException {\n          write(out, NUM_DIMS);\n          writeInt(out, numDims);\n          newline(out);\n\n          write(out, BYTES_PER_DIM);\n          writeInt(out, bytesPerDim);\n          newline(out);\n\n          write(out, MAX_LEAF_POINTS);\n          writeInt(out, maxPointsInLeafNode);\n          newline(out);\n\n          write(out, INDEX_COUNT);\n          writeInt(out, leafBlockFPs.length);\n          newline(out);\n\n          write(out, MIN_VALUE);\n          BytesRef br = new BytesRef(minPackedValue, 0, minPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, MAX_VALUE);\n          br = new BytesRef(maxPackedValue, 0, maxPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, POINT_COUNT);\n          writeLong(out, pointCount);\n          newline(out);\n\n          write(out, DOC_COUNT);\n          writeInt(out, docsSeen.cardinality());\n          newline(out);\n\n          for(int i=0;i<leafBlockFPs.length;i++) {\n            write(out, BLOCK_FP);\n            writeLong(out, leafBlockFPs[i]);\n            newline(out);\n          }\n\n          assert (splitPackedValues.length % (1 + fieldInfo.getPointNumBytes())) == 0;\n          int count = splitPackedValues.length / (1 + fieldInfo.getPointNumBytes());\n          assert count == leafBlockFPs.length;\n\n          write(out, SPLIT_COUNT);\n          writeInt(out, count);\n          newline(out);\n\n          for(int i=0;i<count;i++) {\n            write(out, SPLIT_DIM);\n            writeInt(out, splitPackedValues[i * (1 + fieldInfo.getPointNumBytes())] & 0xff);\n            newline(out);\n            write(out, SPLIT_VALUE);\n            br = new BytesRef(splitPackedValues, 1+(i * (1+fieldInfo.getPointNumBytes())), fieldInfo.getPointNumBytes());\n            write(out, br.toString());\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeLeafBlockDocs(IndexOutput out, int[] docIDs, int start, int count) throws IOException {\n          write(out, BLOCK_COUNT);\n          writeInt(out, count);\n          newline(out);\n          for(int i=0;i<count;i++) {\n            write(out, BLOCK_DOC_ID);\n            writeInt(out, docIDs[start+i]);\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeCommonPrefixes(IndexOutput out, int[] commonPrefixLengths, byte[] packedValue) {\n          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n        }\n\n        @Override\n        protected void writeLeafBlockPackedValues(IndexOutput out, int[] commonPrefixLengths, int count, int sortedDim, IntFunction<BytesRef> packedValues) throws IOException {\n          for (int i = 0; i < count; ++i) {\n            BytesRef packedValue = packedValues.apply(i);\n            // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n            write(out, BLOCK_VALUE);\n            write(out, packedValue.toString());\n            newline(out);\n          }\n        }\n      }) {\n\n      values.intersect(fieldInfo.name, new IntersectVisitor() {\n          @Override\n          public void visit(int docID) {\n            throw new IllegalStateException();\n          }\n\n          public void visit(int docID, byte[] packedValue) throws IOException {\n            writer.add(packedValue, docID);\n          }\n\n          @Override\n          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n            return Relation.CELL_CROSSES_QUERY;\n          }\n        });\n\n      // We could have 0 points on merge since all docs with points may be deleted:\n      if (writer.getPointCount() > 0) {\n        indexFPs.put(fieldInfo.name, writer.finish(dataOut));\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b0567940defa1ea6eb8a039d9d36e3682063f8a4","date":1469815320,"type":1,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsWriter#writeField(FieldInfo,PointsReader).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsWriter#writeField(FieldInfo,PointsReader,double).mjava","sourceNew":"  @Override\n  public void writeField(FieldInfo fieldInfo, PointsReader values) throws IOException {\n\n    boolean singleValuePerDoc = values.size(fieldInfo.name) == values.getDocCount(fieldInfo.name);\n\n    // We use the normal BKDWriter, but subclass to customize how it writes the index and blocks to disk:\n    try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                          writeState.directory,\n                                          writeState.segmentInfo.name,\n                                          fieldInfo.getPointDimensionCount(),\n                                          fieldInfo.getPointNumBytes(),\n                                          BKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE,\n                                          BKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP,\n                                          values.size(fieldInfo.name),\n                                          singleValuePerDoc) {\n\n        @Override\n        protected void writeIndex(IndexOutput out, long[] leafBlockFPs, byte[] splitPackedValues) throws IOException {\n          write(out, NUM_DIMS);\n          writeInt(out, numDims);\n          newline(out);\n\n          write(out, BYTES_PER_DIM);\n          writeInt(out, bytesPerDim);\n          newline(out);\n\n          write(out, MAX_LEAF_POINTS);\n          writeInt(out, maxPointsInLeafNode);\n          newline(out);\n\n          write(out, INDEX_COUNT);\n          writeInt(out, leafBlockFPs.length);\n          newline(out);\n\n          write(out, MIN_VALUE);\n          BytesRef br = new BytesRef(minPackedValue, 0, minPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, MAX_VALUE);\n          br = new BytesRef(maxPackedValue, 0, maxPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, POINT_COUNT);\n          writeLong(out, pointCount);\n          newline(out);\n\n          write(out, DOC_COUNT);\n          writeInt(out, docsSeen.cardinality());\n          newline(out);\n\n          for(int i=0;i<leafBlockFPs.length;i++) {\n            write(out, BLOCK_FP);\n            writeLong(out, leafBlockFPs[i]);\n            newline(out);\n          }\n\n          assert (splitPackedValues.length % (1 + fieldInfo.getPointNumBytes())) == 0;\n          int count = splitPackedValues.length / (1 + fieldInfo.getPointNumBytes());\n          assert count == leafBlockFPs.length;\n\n          write(out, SPLIT_COUNT);\n          writeInt(out, count);\n          newline(out);\n\n          for(int i=0;i<count;i++) {\n            write(out, SPLIT_DIM);\n            writeInt(out, splitPackedValues[i * (1 + fieldInfo.getPointNumBytes())] & 0xff);\n            newline(out);\n            write(out, SPLIT_VALUE);\n            br = new BytesRef(splitPackedValues, 1+(i * (1+fieldInfo.getPointNumBytes())), fieldInfo.getPointNumBytes());\n            write(out, br.toString());\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeLeafBlockDocs(IndexOutput out, int[] docIDs, int start, int count) throws IOException {\n          write(out, BLOCK_COUNT);\n          writeInt(out, count);\n          newline(out);\n          for(int i=0;i<count;i++) {\n            write(out, BLOCK_DOC_ID);\n            writeInt(out, docIDs[start+i]);\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeCommonPrefixes(IndexOutput out, int[] commonPrefixLengths, byte[] packedValue) {\n          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n        }\n\n        @Override\n        protected void writeLeafBlockPackedValues(IndexOutput out, int[] commonPrefixLengths, int count, int sortedDim, IntFunction<BytesRef> packedValues) throws IOException {\n          for (int i = 0; i < count; ++i) {\n            BytesRef packedValue = packedValues.apply(i);\n            // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n            write(out, BLOCK_VALUE);\n            write(out, packedValue.toString());\n            newline(out);\n          }\n        }\n      }) {\n\n      values.intersect(fieldInfo.name, new IntersectVisitor() {\n          @Override\n          public void visit(int docID) {\n            throw new IllegalStateException();\n          }\n\n          public void visit(int docID, byte[] packedValue) throws IOException {\n            writer.add(packedValue, docID);\n          }\n\n          @Override\n          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n            return Relation.CELL_CROSSES_QUERY;\n          }\n        });\n\n      // We could have 0 points on merge since all docs with points may be deleted:\n      if (writer.getPointCount() > 0) {\n        indexFPs.put(fieldInfo.name, writer.finish(dataOut));\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void writeField(FieldInfo fieldInfo, PointsReader values, double maxMBSortInHeap) throws IOException {\n\n    boolean singleValuePerDoc = values.size(fieldInfo.name) == values.getDocCount(fieldInfo.name);\n\n    // We use the normal BKDWriter, but subclass to customize how it writes the index and blocks to disk:\n    try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                          writeState.directory,\n                                          writeState.segmentInfo.name,\n                                          fieldInfo.getPointDimensionCount(),\n                                          fieldInfo.getPointNumBytes(),\n                                          BKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE,\n                                          maxMBSortInHeap,\n                                          values.size(fieldInfo.name),\n                                          singleValuePerDoc) {\n\n        @Override\n        protected void writeIndex(IndexOutput out, long[] leafBlockFPs, byte[] splitPackedValues) throws IOException {\n          write(out, NUM_DIMS);\n          writeInt(out, numDims);\n          newline(out);\n\n          write(out, BYTES_PER_DIM);\n          writeInt(out, bytesPerDim);\n          newline(out);\n\n          write(out, MAX_LEAF_POINTS);\n          writeInt(out, maxPointsInLeafNode);\n          newline(out);\n\n          write(out, INDEX_COUNT);\n          writeInt(out, leafBlockFPs.length);\n          newline(out);\n\n          write(out, MIN_VALUE);\n          BytesRef br = new BytesRef(minPackedValue, 0, minPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, MAX_VALUE);\n          br = new BytesRef(maxPackedValue, 0, maxPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, POINT_COUNT);\n          writeLong(out, pointCount);\n          newline(out);\n\n          write(out, DOC_COUNT);\n          writeInt(out, docsSeen.cardinality());\n          newline(out);\n\n          for(int i=0;i<leafBlockFPs.length;i++) {\n            write(out, BLOCK_FP);\n            writeLong(out, leafBlockFPs[i]);\n            newline(out);\n          }\n\n          assert (splitPackedValues.length % (1 + fieldInfo.getPointNumBytes())) == 0;\n          int count = splitPackedValues.length / (1 + fieldInfo.getPointNumBytes());\n          assert count == leafBlockFPs.length;\n\n          write(out, SPLIT_COUNT);\n          writeInt(out, count);\n          newline(out);\n\n          for(int i=0;i<count;i++) {\n            write(out, SPLIT_DIM);\n            writeInt(out, splitPackedValues[i * (1 + fieldInfo.getPointNumBytes())] & 0xff);\n            newline(out);\n            write(out, SPLIT_VALUE);\n            br = new BytesRef(splitPackedValues, 1+(i * (1+fieldInfo.getPointNumBytes())), fieldInfo.getPointNumBytes());\n            write(out, br.toString());\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeLeafBlockDocs(IndexOutput out, int[] docIDs, int start, int count) throws IOException {\n          write(out, BLOCK_COUNT);\n          writeInt(out, count);\n          newline(out);\n          for(int i=0;i<count;i++) {\n            write(out, BLOCK_DOC_ID);\n            writeInt(out, docIDs[start+i]);\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeCommonPrefixes(IndexOutput out, int[] commonPrefixLengths, byte[] packedValue) {\n          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n        }\n\n        @Override\n        protected void writeLeafBlockPackedValues(IndexOutput out, int[] commonPrefixLengths, int count, int sortedDim, IntFunction<BytesRef> packedValues) throws IOException {\n          for (int i = 0; i < count; ++i) {\n            BytesRef packedValue = packedValues.apply(i);\n            // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n            write(out, BLOCK_VALUE);\n            write(out, packedValue.toString());\n            newline(out);\n          }\n        }\n      }) {\n\n      values.intersect(fieldInfo.name, new IntersectVisitor() {\n          @Override\n          public void visit(int docID) {\n            throw new IllegalStateException();\n          }\n\n          public void visit(int docID, byte[] packedValue) throws IOException {\n            writer.add(packedValue, docID);\n          }\n\n          @Override\n          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n            return Relation.CELL_CROSSES_QUERY;\n          }\n        });\n\n      // We could have 0 points on merge since all docs with points may be deleted:\n      if (writer.getPointCount() > 0) {\n        indexFPs.put(fieldInfo.name, writer.finish(dataOut));\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3b013574eedcdbac35dc7e35b0ee616ffc38895d","date":1470897818,"type":0,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsWriter#writeField(FieldInfo,PointsReader).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public void writeField(FieldInfo fieldInfo, PointsReader values) throws IOException {\n\n    boolean singleValuePerDoc = values.size(fieldInfo.name) == values.getDocCount(fieldInfo.name);\n\n    // We use the normal BKDWriter, but subclass to customize how it writes the index and blocks to disk:\n    try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                          writeState.directory,\n                                          writeState.segmentInfo.name,\n                                          fieldInfo.getPointDimensionCount(),\n                                          fieldInfo.getPointNumBytes(),\n                                          BKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE,\n                                          BKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP,\n                                          values.size(fieldInfo.name),\n                                          singleValuePerDoc) {\n\n        @Override\n        protected void writeIndex(IndexOutput out, long[] leafBlockFPs, byte[] splitPackedValues) throws IOException {\n          write(out, NUM_DIMS);\n          writeInt(out, numDims);\n          newline(out);\n\n          write(out, BYTES_PER_DIM);\n          writeInt(out, bytesPerDim);\n          newline(out);\n\n          write(out, MAX_LEAF_POINTS);\n          writeInt(out, maxPointsInLeafNode);\n          newline(out);\n\n          write(out, INDEX_COUNT);\n          writeInt(out, leafBlockFPs.length);\n          newline(out);\n\n          write(out, MIN_VALUE);\n          BytesRef br = new BytesRef(minPackedValue, 0, minPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, MAX_VALUE);\n          br = new BytesRef(maxPackedValue, 0, maxPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, POINT_COUNT);\n          writeLong(out, pointCount);\n          newline(out);\n\n          write(out, DOC_COUNT);\n          writeInt(out, docsSeen.cardinality());\n          newline(out);\n\n          for(int i=0;i<leafBlockFPs.length;i++) {\n            write(out, BLOCK_FP);\n            writeLong(out, leafBlockFPs[i]);\n            newline(out);\n          }\n\n          assert (splitPackedValues.length % (1 + fieldInfo.getPointNumBytes())) == 0;\n          int count = splitPackedValues.length / (1 + fieldInfo.getPointNumBytes());\n          assert count == leafBlockFPs.length;\n\n          write(out, SPLIT_COUNT);\n          writeInt(out, count);\n          newline(out);\n\n          for(int i=0;i<count;i++) {\n            write(out, SPLIT_DIM);\n            writeInt(out, splitPackedValues[i * (1 + fieldInfo.getPointNumBytes())] & 0xff);\n            newline(out);\n            write(out, SPLIT_VALUE);\n            br = new BytesRef(splitPackedValues, 1+(i * (1+fieldInfo.getPointNumBytes())), fieldInfo.getPointNumBytes());\n            write(out, br.toString());\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeLeafBlockDocs(IndexOutput out, int[] docIDs, int start, int count) throws IOException {\n          write(out, BLOCK_COUNT);\n          writeInt(out, count);\n          newline(out);\n          for(int i=0;i<count;i++) {\n            write(out, BLOCK_DOC_ID);\n            writeInt(out, docIDs[start+i]);\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeCommonPrefixes(IndexOutput out, int[] commonPrefixLengths, byte[] packedValue) {\n          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n        }\n\n        @Override\n        protected void writeLeafBlockPackedValues(IndexOutput out, int[] commonPrefixLengths, int count, int sortedDim, IntFunction<BytesRef> packedValues) throws IOException {\n          for (int i = 0; i < count; ++i) {\n            BytesRef packedValue = packedValues.apply(i);\n            // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n            write(out, BLOCK_VALUE);\n            write(out, packedValue.toString());\n            newline(out);\n          }\n        }\n      }) {\n\n      values.intersect(fieldInfo.name, new IntersectVisitor() {\n          @Override\n          public void visit(int docID) {\n            throw new IllegalStateException();\n          }\n\n          public void visit(int docID, byte[] packedValue) throws IOException {\n            writer.add(packedValue, docID);\n          }\n\n          @Override\n          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n            return Relation.CELL_CROSSES_QUERY;\n          }\n        });\n\n      // We could have 0 points on merge since all docs with points may be deleted:\n      if (writer.getPointCount() > 0) {\n        indexFPs.put(fieldInfo.name, writer.finish(dataOut));\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"367f57e2ee85b7f7e28cfe73370a22cf67624f65","date":1476778467,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsWriter#writeField(FieldInfo,PointsReader).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsWriter#writeField(FieldInfo,PointsReader).mjava","sourceNew":"  @Override\n  public void writeField(FieldInfo fieldInfo, PointsReader reader) throws IOException {\n\n    PointValues values = reader.getValues(fieldInfo.name);\n    boolean singleValuePerDoc = values.size() == values.getDocCount();\n\n    // We use the normal BKDWriter, but subclass to customize how it writes the index and blocks to disk:\n    try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                          writeState.directory,\n                                          writeState.segmentInfo.name,\n                                          fieldInfo.getPointDimensionCount(),\n                                          fieldInfo.getPointNumBytes(),\n                                          BKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE,\n                                          BKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP,\n                                          values.size(),\n                                          singleValuePerDoc) {\n\n        @Override\n        protected void writeIndex(IndexOutput out, long[] leafBlockFPs, byte[] splitPackedValues) throws IOException {\n          write(out, NUM_DIMS);\n          writeInt(out, numDims);\n          newline(out);\n\n          write(out, BYTES_PER_DIM);\n          writeInt(out, bytesPerDim);\n          newline(out);\n\n          write(out, MAX_LEAF_POINTS);\n          writeInt(out, maxPointsInLeafNode);\n          newline(out);\n\n          write(out, INDEX_COUNT);\n          writeInt(out, leafBlockFPs.length);\n          newline(out);\n\n          write(out, MIN_VALUE);\n          BytesRef br = new BytesRef(minPackedValue, 0, minPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, MAX_VALUE);\n          br = new BytesRef(maxPackedValue, 0, maxPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, POINT_COUNT);\n          writeLong(out, pointCount);\n          newline(out);\n\n          write(out, DOC_COUNT);\n          writeInt(out, docsSeen.cardinality());\n          newline(out);\n\n          for(int i=0;i<leafBlockFPs.length;i++) {\n            write(out, BLOCK_FP);\n            writeLong(out, leafBlockFPs[i]);\n            newline(out);\n          }\n\n          assert (splitPackedValues.length % (1 + fieldInfo.getPointNumBytes())) == 0;\n          int count = splitPackedValues.length / (1 + fieldInfo.getPointNumBytes());\n          assert count == leafBlockFPs.length;\n\n          write(out, SPLIT_COUNT);\n          writeInt(out, count);\n          newline(out);\n\n          for(int i=0;i<count;i++) {\n            write(out, SPLIT_DIM);\n            writeInt(out, splitPackedValues[i * (1 + fieldInfo.getPointNumBytes())] & 0xff);\n            newline(out);\n            write(out, SPLIT_VALUE);\n            br = new BytesRef(splitPackedValues, 1+(i * (1+fieldInfo.getPointNumBytes())), fieldInfo.getPointNumBytes());\n            write(out, br.toString());\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeLeafBlockDocs(IndexOutput out, int[] docIDs, int start, int count) throws IOException {\n          write(out, BLOCK_COUNT);\n          writeInt(out, count);\n          newline(out);\n          for(int i=0;i<count;i++) {\n            write(out, BLOCK_DOC_ID);\n            writeInt(out, docIDs[start+i]);\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeCommonPrefixes(IndexOutput out, int[] commonPrefixLengths, byte[] packedValue) {\n          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n        }\n\n        @Override\n        protected void writeLeafBlockPackedValues(IndexOutput out, int[] commonPrefixLengths, int count, int sortedDim, IntFunction<BytesRef> packedValues) throws IOException {\n          for (int i = 0; i < count; ++i) {\n            BytesRef packedValue = packedValues.apply(i);\n            // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n            write(out, BLOCK_VALUE);\n            write(out, packedValue.toString());\n            newline(out);\n          }\n        }\n      }) {\n\n      values.intersect(new IntersectVisitor() {\n          @Override\n          public void visit(int docID) {\n            throw new IllegalStateException();\n          }\n\n          public void visit(int docID, byte[] packedValue) throws IOException {\n            writer.add(packedValue, docID);\n          }\n\n          @Override\n          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n            return Relation.CELL_CROSSES_QUERY;\n          }\n        });\n\n      // We could have 0 points on merge since all docs with points may be deleted:\n      if (writer.getPointCount() > 0) {\n        indexFPs.put(fieldInfo.name, writer.finish(dataOut));\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void writeField(FieldInfo fieldInfo, PointsReader values) throws IOException {\n\n    boolean singleValuePerDoc = values.size(fieldInfo.name) == values.getDocCount(fieldInfo.name);\n\n    // We use the normal BKDWriter, but subclass to customize how it writes the index and blocks to disk:\n    try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                          writeState.directory,\n                                          writeState.segmentInfo.name,\n                                          fieldInfo.getPointDimensionCount(),\n                                          fieldInfo.getPointNumBytes(),\n                                          BKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE,\n                                          BKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP,\n                                          values.size(fieldInfo.name),\n                                          singleValuePerDoc) {\n\n        @Override\n        protected void writeIndex(IndexOutput out, long[] leafBlockFPs, byte[] splitPackedValues) throws IOException {\n          write(out, NUM_DIMS);\n          writeInt(out, numDims);\n          newline(out);\n\n          write(out, BYTES_PER_DIM);\n          writeInt(out, bytesPerDim);\n          newline(out);\n\n          write(out, MAX_LEAF_POINTS);\n          writeInt(out, maxPointsInLeafNode);\n          newline(out);\n\n          write(out, INDEX_COUNT);\n          writeInt(out, leafBlockFPs.length);\n          newline(out);\n\n          write(out, MIN_VALUE);\n          BytesRef br = new BytesRef(minPackedValue, 0, minPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, MAX_VALUE);\n          br = new BytesRef(maxPackedValue, 0, maxPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, POINT_COUNT);\n          writeLong(out, pointCount);\n          newline(out);\n\n          write(out, DOC_COUNT);\n          writeInt(out, docsSeen.cardinality());\n          newline(out);\n\n          for(int i=0;i<leafBlockFPs.length;i++) {\n            write(out, BLOCK_FP);\n            writeLong(out, leafBlockFPs[i]);\n            newline(out);\n          }\n\n          assert (splitPackedValues.length % (1 + fieldInfo.getPointNumBytes())) == 0;\n          int count = splitPackedValues.length / (1 + fieldInfo.getPointNumBytes());\n          assert count == leafBlockFPs.length;\n\n          write(out, SPLIT_COUNT);\n          writeInt(out, count);\n          newline(out);\n\n          for(int i=0;i<count;i++) {\n            write(out, SPLIT_DIM);\n            writeInt(out, splitPackedValues[i * (1 + fieldInfo.getPointNumBytes())] & 0xff);\n            newline(out);\n            write(out, SPLIT_VALUE);\n            br = new BytesRef(splitPackedValues, 1+(i * (1+fieldInfo.getPointNumBytes())), fieldInfo.getPointNumBytes());\n            write(out, br.toString());\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeLeafBlockDocs(IndexOutput out, int[] docIDs, int start, int count) throws IOException {\n          write(out, BLOCK_COUNT);\n          writeInt(out, count);\n          newline(out);\n          for(int i=0;i<count;i++) {\n            write(out, BLOCK_DOC_ID);\n            writeInt(out, docIDs[start+i]);\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeCommonPrefixes(IndexOutput out, int[] commonPrefixLengths, byte[] packedValue) {\n          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n        }\n\n        @Override\n        protected void writeLeafBlockPackedValues(IndexOutput out, int[] commonPrefixLengths, int count, int sortedDim, IntFunction<BytesRef> packedValues) throws IOException {\n          for (int i = 0; i < count; ++i) {\n            BytesRef packedValue = packedValues.apply(i);\n            // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n            write(out, BLOCK_VALUE);\n            write(out, packedValue.toString());\n            newline(out);\n          }\n        }\n      }) {\n\n      values.intersect(fieldInfo.name, new IntersectVisitor() {\n          @Override\n          public void visit(int docID) {\n            throw new IllegalStateException();\n          }\n\n          public void visit(int docID, byte[] packedValue) throws IOException {\n            writer.add(packedValue, docID);\n          }\n\n          @Override\n          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n            return Relation.CELL_CROSSES_QUERY;\n          }\n        });\n\n      // We could have 0 points on merge since all docs with points may be deleted:\n      if (writer.getPointCount() > 0) {\n        indexFPs.put(fieldInfo.name, writer.finish(dataOut));\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsWriter#writeField(FieldInfo,PointsReader).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsWriter#writeField(FieldInfo,PointsReader).mjava","sourceNew":"  @Override\n  public void writeField(FieldInfo fieldInfo, PointsReader reader) throws IOException {\n\n    PointValues values = reader.getValues(fieldInfo.name);\n    boolean singleValuePerDoc = values.size() == values.getDocCount();\n\n    // We use the normal BKDWriter, but subclass to customize how it writes the index and blocks to disk:\n    try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                          writeState.directory,\n                                          writeState.segmentInfo.name,\n                                          fieldInfo.getPointDimensionCount(),\n                                          fieldInfo.getPointNumBytes(),\n                                          BKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE,\n                                          BKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP,\n                                          values.size(),\n                                          singleValuePerDoc) {\n\n        @Override\n        protected void writeIndex(IndexOutput out, long[] leafBlockFPs, byte[] splitPackedValues) throws IOException {\n          write(out, NUM_DIMS);\n          writeInt(out, numDims);\n          newline(out);\n\n          write(out, BYTES_PER_DIM);\n          writeInt(out, bytesPerDim);\n          newline(out);\n\n          write(out, MAX_LEAF_POINTS);\n          writeInt(out, maxPointsInLeafNode);\n          newline(out);\n\n          write(out, INDEX_COUNT);\n          writeInt(out, leafBlockFPs.length);\n          newline(out);\n\n          write(out, MIN_VALUE);\n          BytesRef br = new BytesRef(minPackedValue, 0, minPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, MAX_VALUE);\n          br = new BytesRef(maxPackedValue, 0, maxPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, POINT_COUNT);\n          writeLong(out, pointCount);\n          newline(out);\n\n          write(out, DOC_COUNT);\n          writeInt(out, docsSeen.cardinality());\n          newline(out);\n\n          for(int i=0;i<leafBlockFPs.length;i++) {\n            write(out, BLOCK_FP);\n            writeLong(out, leafBlockFPs[i]);\n            newline(out);\n          }\n\n          assert (splitPackedValues.length % (1 + fieldInfo.getPointNumBytes())) == 0;\n          int count = splitPackedValues.length / (1 + fieldInfo.getPointNumBytes());\n          assert count == leafBlockFPs.length;\n\n          write(out, SPLIT_COUNT);\n          writeInt(out, count);\n          newline(out);\n\n          for(int i=0;i<count;i++) {\n            write(out, SPLIT_DIM);\n            writeInt(out, splitPackedValues[i * (1 + fieldInfo.getPointNumBytes())] & 0xff);\n            newline(out);\n            write(out, SPLIT_VALUE);\n            br = new BytesRef(splitPackedValues, 1+(i * (1+fieldInfo.getPointNumBytes())), fieldInfo.getPointNumBytes());\n            write(out, br.toString());\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeLeafBlockDocs(IndexOutput out, int[] docIDs, int start, int count) throws IOException {\n          write(out, BLOCK_COUNT);\n          writeInt(out, count);\n          newline(out);\n          for(int i=0;i<count;i++) {\n            write(out, BLOCK_DOC_ID);\n            writeInt(out, docIDs[start+i]);\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeCommonPrefixes(IndexOutput out, int[] commonPrefixLengths, byte[] packedValue) {\n          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n        }\n\n        @Override\n        protected void writeLeafBlockPackedValues(IndexOutput out, int[] commonPrefixLengths, int count, int sortedDim, IntFunction<BytesRef> packedValues) throws IOException {\n          for (int i = 0; i < count; ++i) {\n            BytesRef packedValue = packedValues.apply(i);\n            // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n            write(out, BLOCK_VALUE);\n            write(out, packedValue.toString());\n            newline(out);\n          }\n        }\n      }) {\n\n      values.intersect(new IntersectVisitor() {\n          @Override\n          public void visit(int docID) {\n            throw new IllegalStateException();\n          }\n\n          public void visit(int docID, byte[] packedValue) throws IOException {\n            writer.add(packedValue, docID);\n          }\n\n          @Override\n          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n            return Relation.CELL_CROSSES_QUERY;\n          }\n        });\n\n      // We could have 0 points on merge since all docs with points may be deleted:\n      if (writer.getPointCount() > 0) {\n        indexFPs.put(fieldInfo.name, writer.finish(dataOut));\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void writeField(FieldInfo fieldInfo, PointsReader values) throws IOException {\n\n    boolean singleValuePerDoc = values.size(fieldInfo.name) == values.getDocCount(fieldInfo.name);\n\n    // We use the normal BKDWriter, but subclass to customize how it writes the index and blocks to disk:\n    try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                          writeState.directory,\n                                          writeState.segmentInfo.name,\n                                          fieldInfo.getPointDimensionCount(),\n                                          fieldInfo.getPointNumBytes(),\n                                          BKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE,\n                                          BKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP,\n                                          values.size(fieldInfo.name),\n                                          singleValuePerDoc) {\n\n        @Override\n        protected void writeIndex(IndexOutput out, long[] leafBlockFPs, byte[] splitPackedValues) throws IOException {\n          write(out, NUM_DIMS);\n          writeInt(out, numDims);\n          newline(out);\n\n          write(out, BYTES_PER_DIM);\n          writeInt(out, bytesPerDim);\n          newline(out);\n\n          write(out, MAX_LEAF_POINTS);\n          writeInt(out, maxPointsInLeafNode);\n          newline(out);\n\n          write(out, INDEX_COUNT);\n          writeInt(out, leafBlockFPs.length);\n          newline(out);\n\n          write(out, MIN_VALUE);\n          BytesRef br = new BytesRef(minPackedValue, 0, minPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, MAX_VALUE);\n          br = new BytesRef(maxPackedValue, 0, maxPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, POINT_COUNT);\n          writeLong(out, pointCount);\n          newline(out);\n\n          write(out, DOC_COUNT);\n          writeInt(out, docsSeen.cardinality());\n          newline(out);\n\n          for(int i=0;i<leafBlockFPs.length;i++) {\n            write(out, BLOCK_FP);\n            writeLong(out, leafBlockFPs[i]);\n            newline(out);\n          }\n\n          assert (splitPackedValues.length % (1 + fieldInfo.getPointNumBytes())) == 0;\n          int count = splitPackedValues.length / (1 + fieldInfo.getPointNumBytes());\n          assert count == leafBlockFPs.length;\n\n          write(out, SPLIT_COUNT);\n          writeInt(out, count);\n          newline(out);\n\n          for(int i=0;i<count;i++) {\n            write(out, SPLIT_DIM);\n            writeInt(out, splitPackedValues[i * (1 + fieldInfo.getPointNumBytes())] & 0xff);\n            newline(out);\n            write(out, SPLIT_VALUE);\n            br = new BytesRef(splitPackedValues, 1+(i * (1+fieldInfo.getPointNumBytes())), fieldInfo.getPointNumBytes());\n            write(out, br.toString());\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeLeafBlockDocs(IndexOutput out, int[] docIDs, int start, int count) throws IOException {\n          write(out, BLOCK_COUNT);\n          writeInt(out, count);\n          newline(out);\n          for(int i=0;i<count;i++) {\n            write(out, BLOCK_DOC_ID);\n            writeInt(out, docIDs[start+i]);\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeCommonPrefixes(IndexOutput out, int[] commonPrefixLengths, byte[] packedValue) {\n          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n        }\n\n        @Override\n        protected void writeLeafBlockPackedValue(IndexOutput out, int[] commonPrefixLengths, byte[] bytes, int bytesOffset) throws IOException {\n          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n          write(out, BLOCK_VALUE);\n          write(out, new BytesRef(bytes, bytesOffset, packedBytesLength).toString());\n          newline(out);\n        }          \n      }) {\n\n      values.intersect(fieldInfo.name, new IntersectVisitor() {\n          @Override\n          public void visit(int docID) {\n            throw new IllegalStateException();\n          }\n\n          public void visit(int docID, byte[] packedValue) throws IOException {\n            writer.add(packedValue, docID);\n          }\n\n          @Override\n          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n            return Relation.CELL_CROSSES_QUERY;\n          }\n        });\n\n      // We could have 0 points on merge since all docs with points may be deleted:\n      if (writer.getPointCount() > 0) {\n        indexFPs.put(fieldInfo.name, writer.finish(dataOut));\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9fc0d60683b47b5d922124c31f57c8b34734f9e6","date":1480846684,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsWriter#writeField(FieldInfo,PointsReader).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsWriter#writeField(FieldInfo,PointsReader).mjava","sourceNew":"  @Override\n  public void writeField(FieldInfo fieldInfo, PointsReader reader) throws IOException {\n\n    PointValues values = reader.getValues(fieldInfo.name);\n    boolean singleValuePerDoc = values.size() == values.getDocCount();\n\n    // We use the normal BKDWriter, but subclass to customize how it writes the index and blocks to disk:\n    try (SimpleTextBKDWriter writer = new SimpleTextBKDWriter(writeState.segmentInfo.maxDoc(),\n                                                              writeState.directory,\n                                                              writeState.segmentInfo.name,\n                                                              fieldInfo.getPointDimensionCount(),\n                                                              fieldInfo.getPointNumBytes(),\n                                                              SimpleTextBKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE,\n                                                              SimpleTextBKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP,\n                                                              values.size(),\n                                                              singleValuePerDoc)) {\n\n      values.intersect(new IntersectVisitor() {\n          @Override\n          public void visit(int docID) {\n            throw new IllegalStateException();\n          }\n\n          public void visit(int docID, byte[] packedValue) throws IOException {\n            writer.add(packedValue, docID);\n          }\n\n          @Override\n          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n            return Relation.CELL_CROSSES_QUERY;\n          }\n        });\n\n      // We could have 0 points on merge since all docs with points may be deleted:\n      if (writer.getPointCount() > 0) {\n        indexFPs.put(fieldInfo.name, writer.finish(dataOut));\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void writeField(FieldInfo fieldInfo, PointsReader reader) throws IOException {\n\n    PointValues values = reader.getValues(fieldInfo.name);\n    boolean singleValuePerDoc = values.size() == values.getDocCount();\n\n    // We use the normal BKDWriter, but subclass to customize how it writes the index and blocks to disk:\n    try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                          writeState.directory,\n                                          writeState.segmentInfo.name,\n                                          fieldInfo.getPointDimensionCount(),\n                                          fieldInfo.getPointNumBytes(),\n                                          BKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE,\n                                          BKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP,\n                                          values.size(),\n                                          singleValuePerDoc) {\n\n        @Override\n        protected void writeIndex(IndexOutput out, long[] leafBlockFPs, byte[] splitPackedValues) throws IOException {\n          write(out, NUM_DIMS);\n          writeInt(out, numDims);\n          newline(out);\n\n          write(out, BYTES_PER_DIM);\n          writeInt(out, bytesPerDim);\n          newline(out);\n\n          write(out, MAX_LEAF_POINTS);\n          writeInt(out, maxPointsInLeafNode);\n          newline(out);\n\n          write(out, INDEX_COUNT);\n          writeInt(out, leafBlockFPs.length);\n          newline(out);\n\n          write(out, MIN_VALUE);\n          BytesRef br = new BytesRef(minPackedValue, 0, minPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, MAX_VALUE);\n          br = new BytesRef(maxPackedValue, 0, maxPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, POINT_COUNT);\n          writeLong(out, pointCount);\n          newline(out);\n\n          write(out, DOC_COUNT);\n          writeInt(out, docsSeen.cardinality());\n          newline(out);\n\n          for(int i=0;i<leafBlockFPs.length;i++) {\n            write(out, BLOCK_FP);\n            writeLong(out, leafBlockFPs[i]);\n            newline(out);\n          }\n\n          assert (splitPackedValues.length % (1 + fieldInfo.getPointNumBytes())) == 0;\n          int count = splitPackedValues.length / (1 + fieldInfo.getPointNumBytes());\n          assert count == leafBlockFPs.length;\n\n          write(out, SPLIT_COUNT);\n          writeInt(out, count);\n          newline(out);\n\n          for(int i=0;i<count;i++) {\n            write(out, SPLIT_DIM);\n            writeInt(out, splitPackedValues[i * (1 + fieldInfo.getPointNumBytes())] & 0xff);\n            newline(out);\n            write(out, SPLIT_VALUE);\n            br = new BytesRef(splitPackedValues, 1+(i * (1+fieldInfo.getPointNumBytes())), fieldInfo.getPointNumBytes());\n            write(out, br.toString());\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeLeafBlockDocs(IndexOutput out, int[] docIDs, int start, int count) throws IOException {\n          write(out, BLOCK_COUNT);\n          writeInt(out, count);\n          newline(out);\n          for(int i=0;i<count;i++) {\n            write(out, BLOCK_DOC_ID);\n            writeInt(out, docIDs[start+i]);\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeCommonPrefixes(IndexOutput out, int[] commonPrefixLengths, byte[] packedValue) {\n          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n        }\n\n        @Override\n        protected void writeLeafBlockPackedValues(IndexOutput out, int[] commonPrefixLengths, int count, int sortedDim, IntFunction<BytesRef> packedValues) throws IOException {\n          for (int i = 0; i < count; ++i) {\n            BytesRef packedValue = packedValues.apply(i);\n            // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n            write(out, BLOCK_VALUE);\n            write(out, packedValue.toString());\n            newline(out);\n          }\n        }\n      }) {\n\n      values.intersect(new IntersectVisitor() {\n          @Override\n          public void visit(int docID) {\n            throw new IllegalStateException();\n          }\n\n          public void visit(int docID, byte[] packedValue) throws IOException {\n            writer.add(packedValue, docID);\n          }\n\n          @Override\n          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n            return Relation.CELL_CROSSES_QUERY;\n          }\n        });\n\n      // We could have 0 points on merge since all docs with points may be deleted:\n      if (writer.getPointCount() > 0) {\n        indexFPs.put(fieldInfo.name, writer.finish(dataOut));\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fb6b97dab515a9a2d4832b6b2a46bcee1f18f627","date":1481239045,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsWriter#writeField(FieldInfo,PointsReader).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsWriter#writeField(FieldInfo,PointsReader).mjava","sourceNew":"  @Override\n  public void writeField(FieldInfo fieldInfo, PointsReader reader) throws IOException {\n\n    PointValues values = reader.getValues(fieldInfo.name);\n    boolean singleValuePerDoc = values.size() == values.getDocCount();\n\n    // We use our own fork of the BKDWriter to customize how it writes the index and blocks to disk:\n    try (SimpleTextBKDWriter writer = new SimpleTextBKDWriter(writeState.segmentInfo.maxDoc(),\n                                                              writeState.directory,\n                                                              writeState.segmentInfo.name,\n                                                              fieldInfo.getPointDimensionCount(),\n                                                              fieldInfo.getPointNumBytes(),\n                                                              SimpleTextBKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE,\n                                                              SimpleTextBKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP,\n                                                              values.size(),\n                                                              singleValuePerDoc)) {\n\n      values.intersect(new IntersectVisitor() {\n          @Override\n          public void visit(int docID) {\n            throw new IllegalStateException();\n          }\n\n          public void visit(int docID, byte[] packedValue) throws IOException {\n            writer.add(packedValue, docID);\n          }\n\n          @Override\n          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n            return Relation.CELL_CROSSES_QUERY;\n          }\n        });\n\n      // We could have 0 points on merge since all docs with points may be deleted:\n      if (writer.getPointCount() > 0) {\n        indexFPs.put(fieldInfo.name, writer.finish(dataOut));\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void writeField(FieldInfo fieldInfo, PointsReader reader) throws IOException {\n\n    PointValues values = reader.getValues(fieldInfo.name);\n    boolean singleValuePerDoc = values.size() == values.getDocCount();\n\n    // We use the normal BKDWriter, but subclass to customize how it writes the index and blocks to disk:\n    try (SimpleTextBKDWriter writer = new SimpleTextBKDWriter(writeState.segmentInfo.maxDoc(),\n                                                              writeState.directory,\n                                                              writeState.segmentInfo.name,\n                                                              fieldInfo.getPointDimensionCount(),\n                                                              fieldInfo.getPointNumBytes(),\n                                                              SimpleTextBKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE,\n                                                              SimpleTextBKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP,\n                                                              values.size(),\n                                                              singleValuePerDoc)) {\n\n      values.intersect(new IntersectVisitor() {\n          @Override\n          public void visit(int docID) {\n            throw new IllegalStateException();\n          }\n\n          public void visit(int docID, byte[] packedValue) throws IOException {\n            writer.add(packedValue, docID);\n          }\n\n          @Override\n          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n            return Relation.CELL_CROSSES_QUERY;\n          }\n        });\n\n      // We could have 0 points on merge since all docs with points may be deleted:\n      if (writer.getPointCount() > 0) {\n        indexFPs.put(fieldInfo.name, writer.finish(dataOut));\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9856095f7afb5a607bf5e65077615ed91273508c","date":1481837697,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsWriter#writeField(FieldInfo,PointsReader).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsWriter#writeField(FieldInfo,PointsReader).mjava","sourceNew":"  @Override\n  public void writeField(FieldInfo fieldInfo, PointsReader reader) throws IOException {\n\n    PointValues values = reader.getValues(fieldInfo.name);\n    boolean singleValuePerDoc = values.size() == values.getDocCount();\n\n    // We use our own fork of the BKDWriter to customize how it writes the index and blocks to disk:\n    try (SimpleTextBKDWriter writer = new SimpleTextBKDWriter(writeState.segmentInfo.maxDoc(),\n                                                              writeState.directory,\n                                                              writeState.segmentInfo.name,\n                                                              fieldInfo.getPointDimensionCount(),\n                                                              fieldInfo.getPointNumBytes(),\n                                                              SimpleTextBKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE,\n                                                              SimpleTextBKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP,\n                                                              values.size(),\n                                                              singleValuePerDoc)) {\n\n      values.intersect(new IntersectVisitor() {\n          @Override\n          public void visit(int docID) {\n            throw new IllegalStateException();\n          }\n\n          public void visit(int docID, byte[] packedValue) throws IOException {\n            writer.add(packedValue, docID);\n          }\n\n          @Override\n          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n            return Relation.CELL_CROSSES_QUERY;\n          }\n        });\n\n      // We could have 0 points on merge since all docs with points may be deleted:\n      if (writer.getPointCount() > 0) {\n        indexFPs.put(fieldInfo.name, writer.finish(dataOut));\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void writeField(FieldInfo fieldInfo, PointsReader reader) throws IOException {\n\n    PointValues values = reader.getValues(fieldInfo.name);\n    boolean singleValuePerDoc = values.size() == values.getDocCount();\n\n    // We use the normal BKDWriter, but subclass to customize how it writes the index and blocks to disk:\n    try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                          writeState.directory,\n                                          writeState.segmentInfo.name,\n                                          fieldInfo.getPointDimensionCount(),\n                                          fieldInfo.getPointNumBytes(),\n                                          BKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE,\n                                          BKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP,\n                                          values.size(),\n                                          singleValuePerDoc) {\n\n        @Override\n        protected void writeIndex(IndexOutput out, long[] leafBlockFPs, byte[] splitPackedValues) throws IOException {\n          write(out, NUM_DIMS);\n          writeInt(out, numDims);\n          newline(out);\n\n          write(out, BYTES_PER_DIM);\n          writeInt(out, bytesPerDim);\n          newline(out);\n\n          write(out, MAX_LEAF_POINTS);\n          writeInt(out, maxPointsInLeafNode);\n          newline(out);\n\n          write(out, INDEX_COUNT);\n          writeInt(out, leafBlockFPs.length);\n          newline(out);\n\n          write(out, MIN_VALUE);\n          BytesRef br = new BytesRef(minPackedValue, 0, minPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, MAX_VALUE);\n          br = new BytesRef(maxPackedValue, 0, maxPackedValue.length);\n          write(out, br.toString());\n          newline(out);\n\n          write(out, POINT_COUNT);\n          writeLong(out, pointCount);\n          newline(out);\n\n          write(out, DOC_COUNT);\n          writeInt(out, docsSeen.cardinality());\n          newline(out);\n\n          for(int i=0;i<leafBlockFPs.length;i++) {\n            write(out, BLOCK_FP);\n            writeLong(out, leafBlockFPs[i]);\n            newline(out);\n          }\n\n          assert (splitPackedValues.length % (1 + fieldInfo.getPointNumBytes())) == 0;\n          int count = splitPackedValues.length / (1 + fieldInfo.getPointNumBytes());\n          assert count == leafBlockFPs.length;\n\n          write(out, SPLIT_COUNT);\n          writeInt(out, count);\n          newline(out);\n\n          for(int i=0;i<count;i++) {\n            write(out, SPLIT_DIM);\n            writeInt(out, splitPackedValues[i * (1 + fieldInfo.getPointNumBytes())] & 0xff);\n            newline(out);\n            write(out, SPLIT_VALUE);\n            br = new BytesRef(splitPackedValues, 1+(i * (1+fieldInfo.getPointNumBytes())), fieldInfo.getPointNumBytes());\n            write(out, br.toString());\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeLeafBlockDocs(IndexOutput out, int[] docIDs, int start, int count) throws IOException {\n          write(out, BLOCK_COUNT);\n          writeInt(out, count);\n          newline(out);\n          for(int i=0;i<count;i++) {\n            write(out, BLOCK_DOC_ID);\n            writeInt(out, docIDs[start+i]);\n            newline(out);\n          }\n        }\n\n        @Override\n        protected void writeCommonPrefixes(IndexOutput out, int[] commonPrefixLengths, byte[] packedValue) {\n          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n        }\n\n        @Override\n        protected void writeLeafBlockPackedValues(IndexOutput out, int[] commonPrefixLengths, int count, int sortedDim, IntFunction<BytesRef> packedValues) throws IOException {\n          for (int i = 0; i < count; ++i) {\n            BytesRef packedValue = packedValues.apply(i);\n            // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths\n            write(out, BLOCK_VALUE);\n            write(out, packedValue.toString());\n            newline(out);\n          }\n        }\n      }) {\n\n      values.intersect(new IntersectVisitor() {\n          @Override\n          public void visit(int docID) {\n            throw new IllegalStateException();\n          }\n\n          public void visit(int docID, byte[] packedValue) throws IOException {\n            writer.add(packedValue, docID);\n          }\n\n          @Override\n          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n            return Relation.CELL_CROSSES_QUERY;\n          }\n        });\n\n      // We could have 0 points on merge since all docs with points may be deleted:\n      if (writer.getPointCount() > 0) {\n        indexFPs.put(fieldInfo.name, writer.finish(dataOut));\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f6652c943595e92c187ee904c382863013eae28f","date":1539042663,"type":3,"author":"Nicholas Knize","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsWriter#writeField(FieldInfo,PointsReader).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsWriter#writeField(FieldInfo,PointsReader).mjava","sourceNew":"  @Override\n  public void writeField(FieldInfo fieldInfo, PointsReader reader) throws IOException {\n\n    PointValues values = reader.getValues(fieldInfo.name);\n    boolean singleValuePerDoc = values.size() == values.getDocCount();\n\n    // We use our own fork of the BKDWriter to customize how it writes the index and blocks to disk:\n    try (SimpleTextBKDWriter writer = new SimpleTextBKDWriter(writeState.segmentInfo.maxDoc(),\n                                                              writeState.directory,\n                                                              writeState.segmentInfo.name,\n                                                              fieldInfo.getPointDataDimensionCount(),\n                                                              fieldInfo.getPointIndexDimensionCount(),\n                                                              fieldInfo.getPointNumBytes(),\n                                                              SimpleTextBKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE,\n                                                              SimpleTextBKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP,\n                                                              values.size(),\n                                                              singleValuePerDoc)) {\n\n      values.intersect(new IntersectVisitor() {\n          @Override\n          public void visit(int docID) {\n            throw new IllegalStateException();\n          }\n\n          public void visit(int docID, byte[] packedValue) throws IOException {\n            writer.add(packedValue, docID);\n          }\n\n          @Override\n          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n            return Relation.CELL_CROSSES_QUERY;\n          }\n        });\n\n      // We could have 0 points on merge since all docs with points may be deleted:\n      if (writer.getPointCount() > 0) {\n        indexFPs.put(fieldInfo.name, writer.finish(dataOut));\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void writeField(FieldInfo fieldInfo, PointsReader reader) throws IOException {\n\n    PointValues values = reader.getValues(fieldInfo.name);\n    boolean singleValuePerDoc = values.size() == values.getDocCount();\n\n    // We use our own fork of the BKDWriter to customize how it writes the index and blocks to disk:\n    try (SimpleTextBKDWriter writer = new SimpleTextBKDWriter(writeState.segmentInfo.maxDoc(),\n                                                              writeState.directory,\n                                                              writeState.segmentInfo.name,\n                                                              fieldInfo.getPointDimensionCount(),\n                                                              fieldInfo.getPointNumBytes(),\n                                                              SimpleTextBKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE,\n                                                              SimpleTextBKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP,\n                                                              values.size(),\n                                                              singleValuePerDoc)) {\n\n      values.intersect(new IntersectVisitor() {\n          @Override\n          public void visit(int docID) {\n            throw new IllegalStateException();\n          }\n\n          public void visit(int docID, byte[] packedValue) throws IOException {\n            writer.add(packedValue, docID);\n          }\n\n          @Override\n          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n            return Relation.CELL_CROSSES_QUERY;\n          }\n        });\n\n      // We could have 0 points on merge since all docs with points may be deleted:\n      if (writer.getPointCount() > 0) {\n        indexFPs.put(fieldInfo.name, writer.finish(dataOut));\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"78bdc7d6906146edb12a1a6c1f765ba680ed5124","date":1549523533,"type":3,"author":"iverase","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsWriter#writeField(FieldInfo,PointsReader).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsWriter#writeField(FieldInfo,PointsReader).mjava","sourceNew":"  @Override\n  public void writeField(FieldInfo fieldInfo, PointsReader reader) throws IOException {\n\n    PointValues values = reader.getValues(fieldInfo.name);\n\n    // We use our own fork of the BKDWriter to customize how it writes the index and blocks to disk:\n    try (SimpleTextBKDWriter writer = new SimpleTextBKDWriter(writeState.segmentInfo.maxDoc(),\n                                                              writeState.directory,\n                                                              writeState.segmentInfo.name,\n                                                              fieldInfo.getPointDataDimensionCount(),\n                                                              fieldInfo.getPointIndexDimensionCount(),\n                                                              fieldInfo.getPointNumBytes(),\n                                                              SimpleTextBKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE,\n                                                              SimpleTextBKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP,\n                                                              values.size())) {\n\n      values.intersect(new IntersectVisitor() {\n          @Override\n          public void visit(int docID) {\n            throw new IllegalStateException();\n          }\n\n          public void visit(int docID, byte[] packedValue) throws IOException {\n            writer.add(packedValue, docID);\n          }\n\n          @Override\n          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n            return Relation.CELL_CROSSES_QUERY;\n          }\n        });\n\n      // We could have 0 points on merge since all docs with points may be deleted:\n      if (writer.getPointCount() > 0) {\n        indexFPs.put(fieldInfo.name, writer.finish(dataOut));\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void writeField(FieldInfo fieldInfo, PointsReader reader) throws IOException {\n\n    PointValues values = reader.getValues(fieldInfo.name);\n    boolean singleValuePerDoc = values.size() == values.getDocCount();\n\n    // We use our own fork of the BKDWriter to customize how it writes the index and blocks to disk:\n    try (SimpleTextBKDWriter writer = new SimpleTextBKDWriter(writeState.segmentInfo.maxDoc(),\n                                                              writeState.directory,\n                                                              writeState.segmentInfo.name,\n                                                              fieldInfo.getPointDataDimensionCount(),\n                                                              fieldInfo.getPointIndexDimensionCount(),\n                                                              fieldInfo.getPointNumBytes(),\n                                                              SimpleTextBKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE,\n                                                              SimpleTextBKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP,\n                                                              values.size(),\n                                                              singleValuePerDoc)) {\n\n      values.intersect(new IntersectVisitor() {\n          @Override\n          public void visit(int docID) {\n            throw new IllegalStateException();\n          }\n\n          public void visit(int docID, byte[] packedValue) throws IOException {\n            writer.add(packedValue, docID);\n          }\n\n          @Override\n          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n            return Relation.CELL_CROSSES_QUERY;\n          }\n        });\n\n      // We could have 0 points on merge since all docs with points may be deleted:\n      if (writer.getPointCount() > 0) {\n        indexFPs.put(fieldInfo.name, writer.finish(dataOut));\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"59ed8c026ba85e3c42fb89605b2032dc6f9cc241","date":1581113294,"type":3,"author":"Nicholas Knize","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsWriter#writeField(FieldInfo,PointsReader).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsWriter#writeField(FieldInfo,PointsReader).mjava","sourceNew":"  @Override\n  public void writeField(FieldInfo fieldInfo, PointsReader reader) throws IOException {\n\n    PointValues values = reader.getValues(fieldInfo.name);\n\n    // We use our own fork of the BKDWriter to customize how it writes the index and blocks to disk:\n    try (SimpleTextBKDWriter writer = new SimpleTextBKDWriter(writeState.segmentInfo.maxDoc(),\n                                                              writeState.directory,\n                                                              writeState.segmentInfo.name,\n                                                              fieldInfo.getPointDimensionCount(),\n                                                              fieldInfo.getPointIndexDimensionCount(),\n                                                              fieldInfo.getPointNumBytes(),\n                                                              SimpleTextBKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE,\n                                                              SimpleTextBKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP,\n                                                              values.size())) {\n\n      values.intersect(new IntersectVisitor() {\n          @Override\n          public void visit(int docID) {\n            throw new IllegalStateException();\n          }\n\n          public void visit(int docID, byte[] packedValue) throws IOException {\n            writer.add(packedValue, docID);\n          }\n\n          @Override\n          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n            return Relation.CELL_CROSSES_QUERY;\n          }\n        });\n\n      // We could have 0 points on merge since all docs with points may be deleted:\n      if (writer.getPointCount() > 0) {\n        indexFPs.put(fieldInfo.name, writer.finish(dataOut));\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void writeField(FieldInfo fieldInfo, PointsReader reader) throws IOException {\n\n    PointValues values = reader.getValues(fieldInfo.name);\n\n    // We use our own fork of the BKDWriter to customize how it writes the index and blocks to disk:\n    try (SimpleTextBKDWriter writer = new SimpleTextBKDWriter(writeState.segmentInfo.maxDoc(),\n                                                              writeState.directory,\n                                                              writeState.segmentInfo.name,\n                                                              fieldInfo.getPointDataDimensionCount(),\n                                                              fieldInfo.getPointIndexDimensionCount(),\n                                                              fieldInfo.getPointNumBytes(),\n                                                              SimpleTextBKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE,\n                                                              SimpleTextBKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP,\n                                                              values.size())) {\n\n      values.intersect(new IntersectVisitor() {\n          @Override\n          public void visit(int docID) {\n            throw new IllegalStateException();\n          }\n\n          public void visit(int docID, byte[] packedValue) throws IOException {\n            writer.add(packedValue, docID);\n          }\n\n          @Override\n          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n            return Relation.CELL_CROSSES_QUERY;\n          }\n        });\n\n      // We could have 0 points on merge since all docs with points may be deleted:\n      if (writer.getPointCount() > 0) {\n        indexFPs.put(fieldInfo.name, writer.finish(dataOut));\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bb94bf667d51f9c390c99d97afb36b7caab6b6e9","date":1599548621,"type":3,"author":"Ignacio Vera","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsWriter#writeField(FieldInfo,PointsReader).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsWriter#writeField(FieldInfo,PointsReader).mjava","sourceNew":"  @Override\n  public void writeField(FieldInfo fieldInfo, PointsReader reader) throws IOException {\n\n    PointValues values = reader.getValues(fieldInfo.name);\n\n\n    BKDConfig config = new BKDConfig(fieldInfo.getPointDimensionCount(),\n        fieldInfo.getPointIndexDimensionCount(),\n        fieldInfo.getPointNumBytes(),\n        BKDConfig.DEFAULT_MAX_POINTS_IN_LEAF_NODE);\n\n    // We use our own fork of the BKDWriter to customize how it writes the index and blocks to disk:\n    try (SimpleTextBKDWriter writer = new SimpleTextBKDWriter(writeState.segmentInfo.maxDoc(),\n                                                              writeState.directory,\n                                                              writeState.segmentInfo.name,\n                                                              config,\n                                                              SimpleTextBKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP,\n                                                              values.size())) {\n\n      values.intersect(new IntersectVisitor() {\n          @Override\n          public void visit(int docID) {\n            throw new IllegalStateException();\n          }\n\n          public void visit(int docID, byte[] packedValue) throws IOException {\n            writer.add(packedValue, docID);\n          }\n\n          @Override\n          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n            return Relation.CELL_CROSSES_QUERY;\n          }\n        });\n\n      // We could have 0 points on merge since all docs with points may be deleted:\n      if (writer.getPointCount() > 0) {\n        indexFPs.put(fieldInfo.name, writer.finish(dataOut));\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void writeField(FieldInfo fieldInfo, PointsReader reader) throws IOException {\n\n    PointValues values = reader.getValues(fieldInfo.name);\n\n    // We use our own fork of the BKDWriter to customize how it writes the index and blocks to disk:\n    try (SimpleTextBKDWriter writer = new SimpleTextBKDWriter(writeState.segmentInfo.maxDoc(),\n                                                              writeState.directory,\n                                                              writeState.segmentInfo.name,\n                                                              fieldInfo.getPointDimensionCount(),\n                                                              fieldInfo.getPointIndexDimensionCount(),\n                                                              fieldInfo.getPointNumBytes(),\n                                                              SimpleTextBKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE,\n                                                              SimpleTextBKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP,\n                                                              values.size())) {\n\n      values.intersect(new IntersectVisitor() {\n          @Override\n          public void visit(int docID) {\n            throw new IllegalStateException();\n          }\n\n          public void visit(int docID, byte[] packedValue) throws IOException {\n            writer.add(packedValue, docID);\n          }\n\n          @Override\n          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n            return Relation.CELL_CROSSES_QUERY;\n          }\n        });\n\n      // We could have 0 points on merge since all docs with points may be deleted:\n      if (writer.getPointCount() > 0) {\n        indexFPs.put(fieldInfo.name, writer.finish(dataOut));\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b0567940defa1ea6eb8a039d9d36e3682063f8a4":["d08973aa47f2cf98a588293a53af4e948952ccfb"],"bb94bf667d51f9c390c99d97afb36b7caab6b6e9":["59ed8c026ba85e3c42fb89605b2032dc6f9cc241"],"59ed8c026ba85e3c42fb89605b2032dc6f9cc241":["78bdc7d6906146edb12a1a6c1f765ba680ed5124"],"f6652c943595e92c187ee904c382863013eae28f":["fb6b97dab515a9a2d4832b6b2a46bcee1f18f627"],"fb6b97dab515a9a2d4832b6b2a46bcee1f18f627":["9fc0d60683b47b5d922124c31f57c8b34734f9e6"],"78bdc7d6906146edb12a1a6c1f765ba680ed5124":["f6652c943595e92c187ee904c382863013eae28f"],"367f57e2ee85b7f7e28cfe73370a22cf67624f65":["b0567940defa1ea6eb8a039d9d36e3682063f8a4"],"10005c6013abbd1102f2463cf95604d4c8774c99":["3ca40baa99f9578eb8408ee5b9177f7ffe6f65d6"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["416f9e28900210be57b69bc12e2954fb98ed7ebe","367f57e2ee85b7f7e28cfe73370a22cf67624f65"],"9fc0d60683b47b5d922124c31f57c8b34734f9e6":["367f57e2ee85b7f7e28cfe73370a22cf67624f65"],"16ffb58ba57f805651a528311c104f104d9f4573":["9a5a0f27d9486cd33de88627ed3d2ff8dc5074ca"],"4522ffca5a1f420c6a02198c9332d7c596a30ca5":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"b9028cf27fe30db95667505bb92ecaee8fa3aef7":["9a5a0f27d9486cd33de88627ed3d2ff8dc5074ca","16ffb58ba57f805651a528311c104f104d9f4573"],"9a5a0f27d9486cd33de88627ed3d2ff8dc5074ca":["4522ffca5a1f420c6a02198c9332d7c596a30ca5"],"3ca40baa99f9578eb8408ee5b9177f7ffe6f65d6":["416f9e28900210be57b69bc12e2954fb98ed7ebe"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"416f9e28900210be57b69bc12e2954fb98ed7ebe":["b9028cf27fe30db95667505bb92ecaee8fa3aef7"],"9856095f7afb5a607bf5e65077615ed91273508c":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","fb6b97dab515a9a2d4832b6b2a46bcee1f18f627"],"d08973aa47f2cf98a588293a53af4e948952ccfb":["3ca40baa99f9578eb8408ee5b9177f7ffe6f65d6","10005c6013abbd1102f2463cf95604d4c8774c99"],"3b013574eedcdbac35dc7e35b0ee616ffc38895d":["d08973aa47f2cf98a588293a53af4e948952ccfb","b0567940defa1ea6eb8a039d9d36e3682063f8a4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["bb94bf667d51f9c390c99d97afb36b7caab6b6e9"]},"commit2Childs":{"b0567940defa1ea6eb8a039d9d36e3682063f8a4":["367f57e2ee85b7f7e28cfe73370a22cf67624f65","3b013574eedcdbac35dc7e35b0ee616ffc38895d"],"bb94bf667d51f9c390c99d97afb36b7caab6b6e9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"59ed8c026ba85e3c42fb89605b2032dc6f9cc241":["bb94bf667d51f9c390c99d97afb36b7caab6b6e9"],"f6652c943595e92c187ee904c382863013eae28f":["78bdc7d6906146edb12a1a6c1f765ba680ed5124"],"fb6b97dab515a9a2d4832b6b2a46bcee1f18f627":["f6652c943595e92c187ee904c382863013eae28f","9856095f7afb5a607bf5e65077615ed91273508c"],"78bdc7d6906146edb12a1a6c1f765ba680ed5124":["59ed8c026ba85e3c42fb89605b2032dc6f9cc241"],"367f57e2ee85b7f7e28cfe73370a22cf67624f65":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","9fc0d60683b47b5d922124c31f57c8b34734f9e6"],"10005c6013abbd1102f2463cf95604d4c8774c99":["d08973aa47f2cf98a588293a53af4e948952ccfb"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["9856095f7afb5a607bf5e65077615ed91273508c"],"9fc0d60683b47b5d922124c31f57c8b34734f9e6":["fb6b97dab515a9a2d4832b6b2a46bcee1f18f627"],"16ffb58ba57f805651a528311c104f104d9f4573":["b9028cf27fe30db95667505bb92ecaee8fa3aef7"],"4522ffca5a1f420c6a02198c9332d7c596a30ca5":["9a5a0f27d9486cd33de88627ed3d2ff8dc5074ca"],"b9028cf27fe30db95667505bb92ecaee8fa3aef7":["416f9e28900210be57b69bc12e2954fb98ed7ebe"],"9a5a0f27d9486cd33de88627ed3d2ff8dc5074ca":["16ffb58ba57f805651a528311c104f104d9f4573","b9028cf27fe30db95667505bb92ecaee8fa3aef7"],"3ca40baa99f9578eb8408ee5b9177f7ffe6f65d6":["10005c6013abbd1102f2463cf95604d4c8774c99","d08973aa47f2cf98a588293a53af4e948952ccfb"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["4522ffca5a1f420c6a02198c9332d7c596a30ca5"],"416f9e28900210be57b69bc12e2954fb98ed7ebe":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","3ca40baa99f9578eb8408ee5b9177f7ffe6f65d6"],"9856095f7afb5a607bf5e65077615ed91273508c":[],"d08973aa47f2cf98a588293a53af4e948952ccfb":["b0567940defa1ea6eb8a039d9d36e3682063f8a4","3b013574eedcdbac35dc7e35b0ee616ffc38895d"],"3b013574eedcdbac35dc7e35b0ee616ffc38895d":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["9856095f7afb5a607bf5e65077615ed91273508c","3b013574eedcdbac35dc7e35b0ee616ffc38895d","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}