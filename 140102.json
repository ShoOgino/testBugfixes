{"path":"lucene/facet/src/java/org/apache/lucene/facet/sampling/RepeatableSampler#sample2(ScoredDocIDs,int,int[],long[]).mjava","commits":[{"id":"607428da722dcb3e86bbd11c63de8986e6275c36","date":1360334150,"type":1,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/sampling/RepeatableSampler#sample2(ScoredDocIDs,int,int[],long[]).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/search/sampling/RepeatableSampler#sample2(ScoredDocIDs,int,int[],long[]).mjava","sourceNew":"  /**\n   * Returns <code>sample</code>.length values chosen from the first <code>collectionSize</code>\n   * locations of <code>collection</code>, using the HASHING algorithm. Performance measurements\n   * are returned in <code>times</code>, which must be an array of at least three longs. The first\n   * will be set when the algorithm starts; the second, when a hash key has been calculated and\n   * inserted into the priority queue for every element in the collection; and the third when the\n   * original elements associated with the keys remaining in the PQ have been stored in the sample\n   * array for return.\n   * <P>\n   * This algorithm slows as the sample size becomes a significant fraction of the collection\n   * size, because the PQ is as large as the sample set, and will not do early rejection of values\n   * below the minimum until it fills up, and a larger PQ contains more small values to be purged,\n   * resulting in less early rejection and more logN insertions.\n   * \n   * @param collection The set to be sampled.\n   * @param collectionSize The number of values to use (starting from first).\n   * @param sample The array in which to return the sample.\n   * @param times The times of three events, for measuring performance.\n   */\n  private static void sample2(ScoredDocIDs collection, int collectionSize, int[] sample, long[] times) \n  throws IOException {\n    if (returnTimings) {\n      times[0] = System.currentTimeMillis();\n    }\n    int sampleSize = sample.length;\n    IntPriorityQueue pq = new IntPriorityQueue(sampleSize);\n    /*\n     * Convert every value in the collection to a hashed \"weight\" value, and insert\n     * into a bounded PQ (retains only sampleSize highest weights).\n     */\n    ScoredDocIDsIterator it = collection.iterator();\n    MI mi = null;\n    while (it.next()) {\n      if (mi == null) {\n        mi = new MI();\n      }\n      mi.value = (int) (it.getDocID() * PHI_32) & 0x7FFFFFFF;\n      mi = pq.insertWithOverflow(mi);\n    }\n    if (returnTimings) {\n      times[1] = System.currentTimeMillis();\n    }\n    /*\n     * Extract heap, convert weights back to original values, and return as integers.\n     */\n    Object[] heap = pq.getHeap();\n    for (int si = 0; si < sampleSize; si++) {\n      sample[si] = (int)(((MI) heap[si+1]).value * PHI_32I) & 0x7FFFFFFF;\n    }\n    if (returnTimings) {\n      times[2] = System.currentTimeMillis();\n    }\n  }\n\n","sourceOld":"  /**\n   * Returns <code>sample</code>.length values chosen from the first <code>collectionSize</code>\n   * locations of <code>collection</code>, using the HASHING algorithm. Performance measurements\n   * are returned in <code>times</code>, which must be an array of at least three longs. The first\n   * will be set when the algorithm starts; the second, when a hash key has been calculated and\n   * inserted into the priority queue for every element in the collection; and the third when the\n   * original elements associated with the keys remaining in the PQ have been stored in the sample\n   * array for return.\n   * <P>\n   * This algorithm slows as the sample size becomes a significant fraction of the collection\n   * size, because the PQ is as large as the sample set, and will not do early rejection of values\n   * below the minimum until it fills up, and a larger PQ contains more small values to be purged,\n   * resulting in less early rejection and more logN insertions.\n   * \n   * @param collection The set to be sampled.\n   * @param collectionSize The number of values to use (starting from first).\n   * @param sample The array in which to return the sample.\n   * @param times The times of three events, for measuring performance.\n   */\n  private static void sample2(ScoredDocIDs collection, int collectionSize, int[] sample, long[] times) \n  throws IOException {\n    if (returnTimings) {\n      times[0] = System.currentTimeMillis();\n    }\n    int sampleSize = sample.length;\n    IntPriorityQueue pq = new IntPriorityQueue(sampleSize);\n    /*\n     * Convert every value in the collection to a hashed \"weight\" value, and insert\n     * into a bounded PQ (retains only sampleSize highest weights).\n     */\n    ScoredDocIDsIterator it = collection.iterator();\n    MI mi = null;\n    while (it.next()) {\n      if (mi == null) {\n        mi = new MI();\n      }\n      mi.value = (int) (it.getDocID() * PHI_32) & 0x7FFFFFFF;\n      mi = pq.insertWithOverflow(mi);\n    }\n    if (returnTimings) {\n      times[1] = System.currentTimeMillis();\n    }\n    /*\n     * Extract heap, convert weights back to original values, and return as integers.\n     */\n    Object[] heap = pq.getHeap();\n    for (int si = 0; si < sampleSize; si++) {\n      sample[si] = (int)(((MI) heap[si+1]).value * PHI_32I) & 0x7FFFFFFF;\n    }\n    if (returnTimings) {\n      times[2] = System.currentTimeMillis();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d33e19a97046248623a7591aeaa6547233fd15e2","date":1385424777,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/sampling/RepeatableSampler#sample2(ScoredDocIDs,int,int[],long[]).mjava","sourceNew":null,"sourceOld":"  /**\n   * Returns <code>sample</code>.length values chosen from the first <code>collectionSize</code>\n   * locations of <code>collection</code>, using the HASHING algorithm. Performance measurements\n   * are returned in <code>times</code>, which must be an array of at least three longs. The first\n   * will be set when the algorithm starts; the second, when a hash key has been calculated and\n   * inserted into the priority queue for every element in the collection; and the third when the\n   * original elements associated with the keys remaining in the PQ have been stored in the sample\n   * array for return.\n   * <P>\n   * This algorithm slows as the sample size becomes a significant fraction of the collection\n   * size, because the PQ is as large as the sample set, and will not do early rejection of values\n   * below the minimum until it fills up, and a larger PQ contains more small values to be purged,\n   * resulting in less early rejection and more logN insertions.\n   * \n   * @param collection The set to be sampled.\n   * @param collectionSize The number of values to use (starting from first).\n   * @param sample The array in which to return the sample.\n   * @param times The times of three events, for measuring performance.\n   */\n  private static void sample2(ScoredDocIDs collection, int collectionSize, int[] sample, long[] times) \n  throws IOException {\n    if (returnTimings) {\n      times[0] = System.currentTimeMillis();\n    }\n    int sampleSize = sample.length;\n    IntPriorityQueue pq = new IntPriorityQueue(sampleSize);\n    /*\n     * Convert every value in the collection to a hashed \"weight\" value, and insert\n     * into a bounded PQ (retains only sampleSize highest weights).\n     */\n    ScoredDocIDsIterator it = collection.iterator();\n    MI mi = null;\n    while (it.next()) {\n      if (mi == null) {\n        mi = new MI();\n      }\n      mi.value = (int) (it.getDocID() * PHI_32) & 0x7FFFFFFF;\n      mi = pq.insertWithOverflow(mi);\n    }\n    if (returnTimings) {\n      times[1] = System.currentTimeMillis();\n    }\n    /*\n     * Extract heap, convert weights back to original values, and return as integers.\n     */\n    Object[] heap = pq.getHeap();\n    for (int si = 0; si < sampleSize; si++) {\n      sample[si] = (int)(((MI) heap[si+1]).value * PHI_32I) & 0x7FFFFFFF;\n    }\n    if (returnTimings) {\n      times[2] = System.currentTimeMillis();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3cc728b07df73b197e6d940d27f9b08b63918f13","date":1388834348,"type":4,"author":"Michael McCandless","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/sampling/RepeatableSampler#sample2(ScoredDocIDs,int,int[],long[]).mjava","sourceNew":null,"sourceOld":"  /**\n   * Returns <code>sample</code>.length values chosen from the first <code>collectionSize</code>\n   * locations of <code>collection</code>, using the HASHING algorithm. Performance measurements\n   * are returned in <code>times</code>, which must be an array of at least three longs. The first\n   * will be set when the algorithm starts; the second, when a hash key has been calculated and\n   * inserted into the priority queue for every element in the collection; and the third when the\n   * original elements associated with the keys remaining in the PQ have been stored in the sample\n   * array for return.\n   * <P>\n   * This algorithm slows as the sample size becomes a significant fraction of the collection\n   * size, because the PQ is as large as the sample set, and will not do early rejection of values\n   * below the minimum until it fills up, and a larger PQ contains more small values to be purged,\n   * resulting in less early rejection and more logN insertions.\n   * \n   * @param collection The set to be sampled.\n   * @param collectionSize The number of values to use (starting from first).\n   * @param sample The array in which to return the sample.\n   * @param times The times of three events, for measuring performance.\n   */\n  private static void sample2(ScoredDocIDs collection, int collectionSize, int[] sample, long[] times) \n  throws IOException {\n    if (returnTimings) {\n      times[0] = System.currentTimeMillis();\n    }\n    int sampleSize = sample.length;\n    IntPriorityQueue pq = new IntPriorityQueue(sampleSize);\n    /*\n     * Convert every value in the collection to a hashed \"weight\" value, and insert\n     * into a bounded PQ (retains only sampleSize highest weights).\n     */\n    ScoredDocIDsIterator it = collection.iterator();\n    MI mi = null;\n    while (it.next()) {\n      if (mi == null) {\n        mi = new MI();\n      }\n      mi.value = (int) (it.getDocID() * PHI_32) & 0x7FFFFFFF;\n      mi = pq.insertWithOverflow(mi);\n    }\n    if (returnTimings) {\n      times[1] = System.currentTimeMillis();\n    }\n    /*\n     * Extract heap, convert weights back to original values, and return as integers.\n     */\n    Object[] heap = pq.getHeap();\n    for (int si = 0; si < sampleSize; si++) {\n      sample[si] = (int)(((MI) heap[si+1]).value * PHI_32I) & 0x7FFFFFFF;\n    }\n    if (returnTimings) {\n      times[2] = System.currentTimeMillis();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"607428da722dcb3e86bbd11c63de8986e6275c36":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"d33e19a97046248623a7591aeaa6547233fd15e2":["607428da722dcb3e86bbd11c63de8986e6275c36"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3cc728b07df73b197e6d940d27f9b08b63918f13":["607428da722dcb3e86bbd11c63de8986e6275c36","d33e19a97046248623a7591aeaa6547233fd15e2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3cc728b07df73b197e6d940d27f9b08b63918f13"]},"commit2Childs":{"607428da722dcb3e86bbd11c63de8986e6275c36":["d33e19a97046248623a7591aeaa6547233fd15e2","3cc728b07df73b197e6d940d27f9b08b63918f13"],"d33e19a97046248623a7591aeaa6547233fd15e2":["3cc728b07df73b197e6d940d27f9b08b63918f13"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["607428da722dcb3e86bbd11c63de8986e6275c36"],"3cc728b07df73b197e6d940d27f9b08b63918f13":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}