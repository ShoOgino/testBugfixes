{"path":"lucene/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum#testReuseDocsEnumDifferentReader().mjava","commits":[{"id":"a0ae5e3ed1232483b7b8a014f175a5fe43595982","date":1324062192,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum#testReuseDocsEnumDifferentReader().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/codecs/lucene40/TestReuseDocsEnum#testReuseDocsEnumDifferentReader().mjava","sourceNew":"  // make sure we never reuse from another reader even if it is the same field & codec etc\n  public void testReuseDocsEnumDifferentReader() throws IOException {\n    Directory dir = newDirectory();\n    Codec cp = _TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat());\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));\n    int numdocs = atLeast(20);\n    createRandomIndex(numdocs, writer, random);\n    writer.commit();\n\n    IndexReader firstReader = IndexReader.open(dir);\n    IndexReader secondReader = IndexReader.open(dir);\n    IndexReader[] sequentialSubReaders = firstReader.getSequentialSubReaders();\n    IndexReader[] sequentialSubReaders2 = secondReader.getSequentialSubReaders();\n    \n    for (IndexReader indexReader : sequentialSubReaders) {\n      Terms terms = indexReader.terms(\"body\");\n      TermsEnum iterator = terms.iterator(null);\n      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<DocsEnum, Boolean>();\n      MatchNoBits bits = new Bits.MatchNoBits(firstReader.maxDoc());\n      iterator = terms.iterator(null);\n      DocsEnum docs = null;\n      BytesRef term = null;\n      while ((term = iterator.next()) != null) {\n        docs = iterator.docs(null, randomDocsEnum(\"body\", term, sequentialSubReaders2, bits), random.nextBoolean());\n        enums.put(docs, true);\n      }\n      assertEquals(terms.getUniqueTermCount(), enums.size());  \n      \n      iterator = terms.iterator(null);\n      enums.clear();\n      docs = null;\n      while ((term = iterator.next()) != null) {\n        docs = iterator.docs(bits, randomDocsEnum(\"body\", term, sequentialSubReaders2, bits), random.nextBoolean());\n        enums.put(docs, true);\n      }\n      assertEquals(terms.getUniqueTermCount(), enums.size());  \n    }\n    IOUtils.close(writer, firstReader, secondReader, dir);\n  }\n\n","sourceOld":"  // make sure we never reuse from another reader even if it is the same field & codec etc\n  public void testReuseDocsEnumDifferentReader() throws IOException {\n    Directory dir = newDirectory();\n    Codec cp = _TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat());\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));\n    int numdocs = atLeast(20);\n    createRandomIndex(numdocs, writer, random);\n    writer.commit();\n\n    IndexReader firstReader = IndexReader.open(dir);\n    IndexReader secondReader = IndexReader.open(dir);\n    IndexReader[] sequentialSubReaders = firstReader.getSequentialSubReaders();\n    IndexReader[] sequentialSubReaders2 = secondReader.getSequentialSubReaders();\n    \n    for (IndexReader indexReader : sequentialSubReaders) {\n      Terms terms = indexReader.terms(\"body\");\n      TermsEnum iterator = terms.iterator(null);\n      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<DocsEnum, Boolean>();\n      MatchNoBits bits = new Bits.MatchNoBits(firstReader.maxDoc());\n      iterator = terms.iterator(null);\n      DocsEnum docs = null;\n      BytesRef term = null;\n      while ((term = iterator.next()) != null) {\n        docs = iterator.docs(null, randomDocsEnum(\"body\", term, sequentialSubReaders2, bits), random.nextBoolean());\n        enums.put(docs, true);\n      }\n      assertEquals(terms.getUniqueTermCount(), enums.size());  \n      \n      iterator = terms.iterator(null);\n      enums.clear();\n      docs = null;\n      while ((term = iterator.next()) != null) {\n        docs = iterator.docs(bits, randomDocsEnum(\"body\", term, sequentialSubReaders2, bits), random.nextBoolean());\n        enums.put(docs, true);\n      }\n      assertEquals(terms.getUniqueTermCount(), enums.size());  \n    }\n    IOUtils.close(writer, firstReader, secondReader, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d40b62adb64d8f7b2f85ee849349cfb0bef03f45","date":1327855938,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum#testReuseDocsEnumDifferentReader().mjava","pathOld":"lucene/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum#testReuseDocsEnumDifferentReader().mjava","sourceNew":"  // make sure we never reuse from another reader even if it is the same field & codec etc\n  public void testReuseDocsEnumDifferentReader() throws IOException {\n    Directory dir = newDirectory();\n    Codec cp = _TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat());\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));\n    int numdocs = atLeast(20);\n    createRandomIndex(numdocs, writer, random);\n    writer.commit();\n\n    DirectoryReader firstReader = DirectoryReader.open(dir);\n    DirectoryReader secondReader = DirectoryReader.open(dir);\n    IndexReader[] sequentialSubReaders = firstReader.getSequentialSubReaders();\n    IndexReader[] sequentialSubReaders2 = secondReader.getSequentialSubReaders();\n    \n    for (IndexReader indexReader : sequentialSubReaders) {\n      Terms terms = ((AtomicIndexReader) indexReader).terms(\"body\");\n      TermsEnum iterator = terms.iterator(null);\n      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<DocsEnum, Boolean>();\n      MatchNoBits bits = new Bits.MatchNoBits(firstReader.maxDoc());\n      iterator = terms.iterator(null);\n      DocsEnum docs = null;\n      BytesRef term = null;\n      while ((term = iterator.next()) != null) {\n        docs = iterator.docs(null, randomDocsEnum(\"body\", term, sequentialSubReaders2, bits), random.nextBoolean());\n        enums.put(docs, true);\n      }\n      assertEquals(terms.getUniqueTermCount(), enums.size());  \n      \n      iterator = terms.iterator(null);\n      enums.clear();\n      docs = null;\n      while ((term = iterator.next()) != null) {\n        docs = iterator.docs(bits, randomDocsEnum(\"body\", term, sequentialSubReaders2, bits), random.nextBoolean());\n        enums.put(docs, true);\n      }\n      assertEquals(terms.getUniqueTermCount(), enums.size());  \n    }\n    IOUtils.close(writer, firstReader, secondReader, dir);\n  }\n\n","sourceOld":"  // make sure we never reuse from another reader even if it is the same field & codec etc\n  public void testReuseDocsEnumDifferentReader() throws IOException {\n    Directory dir = newDirectory();\n    Codec cp = _TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat());\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));\n    int numdocs = atLeast(20);\n    createRandomIndex(numdocs, writer, random);\n    writer.commit();\n\n    IndexReader firstReader = IndexReader.open(dir);\n    IndexReader secondReader = IndexReader.open(dir);\n    IndexReader[] sequentialSubReaders = firstReader.getSequentialSubReaders();\n    IndexReader[] sequentialSubReaders2 = secondReader.getSequentialSubReaders();\n    \n    for (IndexReader indexReader : sequentialSubReaders) {\n      Terms terms = indexReader.terms(\"body\");\n      TermsEnum iterator = terms.iterator(null);\n      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<DocsEnum, Boolean>();\n      MatchNoBits bits = new Bits.MatchNoBits(firstReader.maxDoc());\n      iterator = terms.iterator(null);\n      DocsEnum docs = null;\n      BytesRef term = null;\n      while ((term = iterator.next()) != null) {\n        docs = iterator.docs(null, randomDocsEnum(\"body\", term, sequentialSubReaders2, bits), random.nextBoolean());\n        enums.put(docs, true);\n      }\n      assertEquals(terms.getUniqueTermCount(), enums.size());  \n      \n      iterator = terms.iterator(null);\n      enums.clear();\n      docs = null;\n      while ((term = iterator.next()) != null) {\n        docs = iterator.docs(bits, randomDocsEnum(\"body\", term, sequentialSubReaders2, bits), random.nextBoolean());\n        enums.put(docs, true);\n      }\n      assertEquals(terms.getUniqueTermCount(), enums.size());  \n    }\n    IOUtils.close(writer, firstReader, secondReader, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"da6d5ac19a80d65b1e864251f155d30960353b7e","date":1327881054,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum#testReuseDocsEnumDifferentReader().mjava","pathOld":"lucene/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum#testReuseDocsEnumDifferentReader().mjava","sourceNew":"  // make sure we never reuse from another reader even if it is the same field & codec etc\n  public void testReuseDocsEnumDifferentReader() throws IOException {\n    Directory dir = newDirectory();\n    Codec cp = _TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat());\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));\n    int numdocs = atLeast(20);\n    createRandomIndex(numdocs, writer, random);\n    writer.commit();\n\n    DirectoryReader firstReader = DirectoryReader.open(dir);\n    DirectoryReader secondReader = DirectoryReader.open(dir);\n    IndexReader[] sequentialSubReaders = firstReader.getSequentialSubReaders();\n    IndexReader[] sequentialSubReaders2 = secondReader.getSequentialSubReaders();\n    \n    for (IndexReader indexReader : sequentialSubReaders) {\n      Terms terms = ((AtomicReader) indexReader).terms(\"body\");\n      TermsEnum iterator = terms.iterator(null);\n      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<DocsEnum, Boolean>();\n      MatchNoBits bits = new Bits.MatchNoBits(firstReader.maxDoc());\n      iterator = terms.iterator(null);\n      DocsEnum docs = null;\n      BytesRef term = null;\n      while ((term = iterator.next()) != null) {\n        docs = iterator.docs(null, randomDocsEnum(\"body\", term, sequentialSubReaders2, bits), random.nextBoolean());\n        enums.put(docs, true);\n      }\n      assertEquals(terms.getUniqueTermCount(), enums.size());  \n      \n      iterator = terms.iterator(null);\n      enums.clear();\n      docs = null;\n      while ((term = iterator.next()) != null) {\n        docs = iterator.docs(bits, randomDocsEnum(\"body\", term, sequentialSubReaders2, bits), random.nextBoolean());\n        enums.put(docs, true);\n      }\n      assertEquals(terms.getUniqueTermCount(), enums.size());  \n    }\n    IOUtils.close(writer, firstReader, secondReader, dir);\n  }\n\n","sourceOld":"  // make sure we never reuse from another reader even if it is the same field & codec etc\n  public void testReuseDocsEnumDifferentReader() throws IOException {\n    Directory dir = newDirectory();\n    Codec cp = _TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat());\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));\n    int numdocs = atLeast(20);\n    createRandomIndex(numdocs, writer, random);\n    writer.commit();\n\n    DirectoryReader firstReader = DirectoryReader.open(dir);\n    DirectoryReader secondReader = DirectoryReader.open(dir);\n    IndexReader[] sequentialSubReaders = firstReader.getSequentialSubReaders();\n    IndexReader[] sequentialSubReaders2 = secondReader.getSequentialSubReaders();\n    \n    for (IndexReader indexReader : sequentialSubReaders) {\n      Terms terms = ((AtomicIndexReader) indexReader).terms(\"body\");\n      TermsEnum iterator = terms.iterator(null);\n      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<DocsEnum, Boolean>();\n      MatchNoBits bits = new Bits.MatchNoBits(firstReader.maxDoc());\n      iterator = terms.iterator(null);\n      DocsEnum docs = null;\n      BytesRef term = null;\n      while ((term = iterator.next()) != null) {\n        docs = iterator.docs(null, randomDocsEnum(\"body\", term, sequentialSubReaders2, bits), random.nextBoolean());\n        enums.put(docs, true);\n      }\n      assertEquals(terms.getUniqueTermCount(), enums.size());  \n      \n      iterator = terms.iterator(null);\n      enums.clear();\n      docs = null;\n      while ((term = iterator.next()) != null) {\n        docs = iterator.docs(bits, randomDocsEnum(\"body\", term, sequentialSubReaders2, bits), random.nextBoolean());\n        enums.put(docs, true);\n      }\n      assertEquals(terms.getUniqueTermCount(), enums.size());  \n    }\n    IOUtils.close(writer, firstReader, secondReader, dir);\n  }\n\n","bugFix":null,"bugIntro":["ced66195b26fdb1f77ee00e2a77ec6918dedd766"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5cab9a86bd67202d20b6adc463008c8e982b070a","date":1327966443,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum#testReuseDocsEnumDifferentReader().mjava","pathOld":"lucene/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum#testReuseDocsEnumDifferentReader().mjava","sourceNew":"  // make sure we never reuse from another reader even if it is the same field & codec etc\n  public void testReuseDocsEnumDifferentReader() throws IOException {\n    Directory dir = newDirectory();\n    Codec cp = _TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat());\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));\n    int numdocs = atLeast(20);\n    createRandomIndex(numdocs, writer, random);\n    writer.commit();\n\n    DirectoryReader firstReader = DirectoryReader.open(dir);\n    DirectoryReader secondReader = DirectoryReader.open(dir);\n    IndexReader[] sequentialSubReaders = firstReader.getSequentialSubReaders();\n    IndexReader[] sequentialSubReaders2 = secondReader.getSequentialSubReaders();\n    \n    for (IndexReader indexReader : sequentialSubReaders) {\n      Terms terms = ((AtomicReader) indexReader).terms(\"body\");\n      TermsEnum iterator = terms.iterator(null);\n      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<DocsEnum, Boolean>();\n      MatchNoBits bits = new Bits.MatchNoBits(firstReader.maxDoc());\n      iterator = terms.iterator(null);\n      DocsEnum docs = null;\n      BytesRef term = null;\n      while ((term = iterator.next()) != null) {\n        docs = iterator.docs(null, randomDocsEnum(\"body\", term, sequentialSubReaders2, bits), random.nextBoolean());\n        enums.put(docs, true);\n      }\n      assertEquals(terms.getUniqueTermCount(), enums.size());  \n      \n      iterator = terms.iterator(null);\n      enums.clear();\n      docs = null;\n      while ((term = iterator.next()) != null) {\n        docs = iterator.docs(bits, randomDocsEnum(\"body\", term, sequentialSubReaders2, bits), random.nextBoolean());\n        enums.put(docs, true);\n      }\n      assertEquals(terms.getUniqueTermCount(), enums.size());  \n    }\n    IOUtils.close(writer, firstReader, secondReader, dir);\n  }\n\n","sourceOld":"  // make sure we never reuse from another reader even if it is the same field & codec etc\n  public void testReuseDocsEnumDifferentReader() throws IOException {\n    Directory dir = newDirectory();\n    Codec cp = _TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat());\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));\n    int numdocs = atLeast(20);\n    createRandomIndex(numdocs, writer, random);\n    writer.commit();\n\n    IndexReader firstReader = IndexReader.open(dir);\n    IndexReader secondReader = IndexReader.open(dir);\n    IndexReader[] sequentialSubReaders = firstReader.getSequentialSubReaders();\n    IndexReader[] sequentialSubReaders2 = secondReader.getSequentialSubReaders();\n    \n    for (IndexReader indexReader : sequentialSubReaders) {\n      Terms terms = indexReader.terms(\"body\");\n      TermsEnum iterator = terms.iterator(null);\n      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<DocsEnum, Boolean>();\n      MatchNoBits bits = new Bits.MatchNoBits(firstReader.maxDoc());\n      iterator = terms.iterator(null);\n      DocsEnum docs = null;\n      BytesRef term = null;\n      while ((term = iterator.next()) != null) {\n        docs = iterator.docs(null, randomDocsEnum(\"body\", term, sequentialSubReaders2, bits), random.nextBoolean());\n        enums.put(docs, true);\n      }\n      assertEquals(terms.getUniqueTermCount(), enums.size());  \n      \n      iterator = terms.iterator(null);\n      enums.clear();\n      docs = null;\n      while ((term = iterator.next()) != null) {\n        docs = iterator.docs(bits, randomDocsEnum(\"body\", term, sequentialSubReaders2, bits), random.nextBoolean());\n        enums.put(docs, true);\n      }\n      assertEquals(terms.getUniqueTermCount(), enums.size());  \n    }\n    IOUtils.close(writer, firstReader, secondReader, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum#testReuseDocsEnumDifferentReader().mjava","pathOld":"lucene/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum#testReuseDocsEnumDifferentReader().mjava","sourceNew":"  // make sure we never reuse from another reader even if it is the same field & codec etc\n  public void testReuseDocsEnumDifferentReader() throws IOException {\n    Directory dir = newDirectory();\n    Codec cp = _TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat());\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));\n    int numdocs = atLeast(20);\n    createRandomIndex(numdocs, writer, random);\n    writer.commit();\n\n    DirectoryReader firstReader = DirectoryReader.open(dir);\n    DirectoryReader secondReader = DirectoryReader.open(dir);\n    IndexReader[] sequentialSubReaders = firstReader.getSequentialSubReaders();\n    IndexReader[] sequentialSubReaders2 = secondReader.getSequentialSubReaders();\n    \n    for (IndexReader indexReader : sequentialSubReaders) {\n      Terms terms = ((AtomicReader) indexReader).terms(\"body\");\n      TermsEnum iterator = terms.iterator(null);\n      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<DocsEnum, Boolean>();\n      MatchNoBits bits = new Bits.MatchNoBits(firstReader.maxDoc());\n      iterator = terms.iterator(null);\n      DocsEnum docs = null;\n      BytesRef term = null;\n      while ((term = iterator.next()) != null) {\n        docs = iterator.docs(null, randomDocsEnum(\"body\", term, sequentialSubReaders2, bits), random.nextBoolean());\n        enums.put(docs, true);\n      }\n      assertEquals(terms.getUniqueTermCount(), enums.size());  \n      \n      iterator = terms.iterator(null);\n      enums.clear();\n      docs = null;\n      while ((term = iterator.next()) != null) {\n        docs = iterator.docs(bits, randomDocsEnum(\"body\", term, sequentialSubReaders2, bits), random.nextBoolean());\n        enums.put(docs, true);\n      }\n      assertEquals(terms.getUniqueTermCount(), enums.size());  \n    }\n    IOUtils.close(writer, firstReader, secondReader, dir);\n  }\n\n","sourceOld":"  // make sure we never reuse from another reader even if it is the same field & codec etc\n  public void testReuseDocsEnumDifferentReader() throws IOException {\n    Directory dir = newDirectory();\n    Codec cp = _TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat());\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));\n    int numdocs = atLeast(20);\n    createRandomIndex(numdocs, writer, random);\n    writer.commit();\n\n    DirectoryReader firstReader = DirectoryReader.open(dir);\n    DirectoryReader secondReader = DirectoryReader.open(dir);\n    IndexReader[] sequentialSubReaders = firstReader.getSequentialSubReaders();\n    IndexReader[] sequentialSubReaders2 = secondReader.getSequentialSubReaders();\n    \n    for (IndexReader indexReader : sequentialSubReaders) {\n      Terms terms = ((AtomicReader) indexReader).terms(\"body\");\n      TermsEnum iterator = terms.iterator(null);\n      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<DocsEnum, Boolean>();\n      MatchNoBits bits = new Bits.MatchNoBits(firstReader.maxDoc());\n      iterator = terms.iterator(null);\n      DocsEnum docs = null;\n      BytesRef term = null;\n      while ((term = iterator.next()) != null) {\n        docs = iterator.docs(null, randomDocsEnum(\"body\", term, sequentialSubReaders2, bits), random.nextBoolean());\n        enums.put(docs, true);\n      }\n      assertEquals(terms.getUniqueTermCount(), enums.size());  \n      \n      iterator = terms.iterator(null);\n      enums.clear();\n      docs = null;\n      while ((term = iterator.next()) != null) {\n        docs = iterator.docs(bits, randomDocsEnum(\"body\", term, sequentialSubReaders2, bits), random.nextBoolean());\n        enums.put(docs, true);\n      }\n      assertEquals(terms.getUniqueTermCount(), enums.size());  \n    }\n    IOUtils.close(writer, firstReader, secondReader, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"da6d5ac19a80d65b1e864251f155d30960353b7e":["d40b62adb64d8f7b2f85ee849349cfb0bef03f45"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d40b62adb64d8f7b2f85ee849349cfb0bef03f45":["a0ae5e3ed1232483b7b8a014f175a5fe43595982"],"a0ae5e3ed1232483b7b8a014f175a5fe43595982":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["a0ae5e3ed1232483b7b8a014f175a5fe43595982","da6d5ac19a80d65b1e864251f155d30960353b7e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"da6d5ac19a80d65b1e864251f155d30960353b7e":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a0ae5e3ed1232483b7b8a014f175a5fe43595982"],"d40b62adb64d8f7b2f85ee849349cfb0bef03f45":["da6d5ac19a80d65b1e864251f155d30960353b7e"],"a0ae5e3ed1232483b7b8a014f175a5fe43595982":["d40b62adb64d8f7b2f85ee849349cfb0bef03f45","5cab9a86bd67202d20b6adc463008c8e982b070a"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}