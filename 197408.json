{"path":"solr/core/src/java/org/apache/solr/util/SimplePostTool#webCrawl(int,OutputStream).mjava","commits":[{"id":"c2c3a504730329ae644b009dee43024116605d47","date":1345253449,"type":0,"author":"Jan HÃ¸ydahl","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/util/SimplePostTool#webCrawl(int,OutputStream).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * A very simple crawler, pulling URLs to fetch from a backlog and then\n   * recurses N levels deep if recursive>0. Links are parsed from HTML\n   * through first getting an XHTML version using SolrCell with extractOnly,\n   * and followed if they are local. The crawler pauses for a default delay\n   * of 10 seconds between each fetch, this can be configured in the delay\n   * variable. This is only meant for test purposes, as it does not respect\n   * robots or anything else fancy :)\n   * @param level which level to crawl\n   * @param out output stream to write to\n   * @return number of pages crawled on this level and below\n   */\n  protected int webCrawl(int level, OutputStream out) {\n    int numPages = 0;\n    LinkedHashSet<URL> stack = backlog.get(level);\n    int rawStackSize = stack.size();\n    stack.removeAll(visited);\n    int stackSize = stack.size();\n    LinkedHashSet<URL> subStack = new LinkedHashSet<URL>();\n    info(\"Entering crawl at level \"+level+\" (\"+rawStackSize+\" links total, \"+stackSize+\" new)\");\n    for(URL u : stack) {\n      try {\n        visited.add(u);\n        PageFetcherResult result = pageFetcher.readPageFromUrl(u);\n        if(result.httpStatus == 200) {\n          u = (result.redirectUrl != null) ? result.redirectUrl : u;\n          URL postUrl = new URL(appendParam(solrUrl.toString(), \n              \"literal.id=\"+URLEncoder.encode(u.toString(),\"UTF-8\") +\n              \"&literal.url=\"+URLEncoder.encode(u.toString(),\"UTF-8\")));\n          boolean success = postData(new ByteArrayInputStream(result.content), null, out, result.contentType, postUrl);\n          if (success) {\n            info(\"POSTed web resource \"+u+\" (depth: \"+level+\")\");\n            Thread.sleep(delay * 1000);\n            numPages++;\n            // Pull links from HTML pages only\n            if(recursive > level && result.contentType.equals(\"text/html\")) {\n              Set<URL> children = pageFetcher.getLinksFromWebPage(u, new ByteArrayInputStream(result.content), result.contentType, postUrl);\n              subStack.addAll(children);\n            }\n          } else {\n            warn(\"An error occurred while posting \"+u);\n          }\n        } else {\n          warn(\"The URL \"+u+\" returned a HTTP result status of \"+result.httpStatus);\n        }\n      } catch (IOException e) {\n        warn(\"Caught exception when trying to open connection to \"+u+\": \"+e.getMessage());\n      } catch (InterruptedException e) {\n        throw new RuntimeException();\n      }\n    }\n    if(!subStack.isEmpty()) {\n      backlog.add(subStack);\n      numPages += webCrawl(level+1, out);\n    }\n    return numPages;    \n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["437e65c578cab603d9201916b0e285f3d68aff45","1e210ae1e604402eb4eeff2a52e56d189cd4f2f1"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"70fa1c0f4d75735ff2e1485e059d9bc5efa50598","date":1345296911,"type":0,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/util/SimplePostTool#webCrawl(int,OutputStream).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * A very simple crawler, pulling URLs to fetch from a backlog and then\n   * recurses N levels deep if recursive>0. Links are parsed from HTML\n   * through first getting an XHTML version using SolrCell with extractOnly,\n   * and followed if they are local. The crawler pauses for a default delay\n   * of 10 seconds between each fetch, this can be configured in the delay\n   * variable. This is only meant for test purposes, as it does not respect\n   * robots or anything else fancy :)\n   * @param level which level to crawl\n   * @param out output stream to write to\n   * @return number of pages crawled on this level and below\n   */\n  protected int webCrawl(int level, OutputStream out) {\n    int numPages = 0;\n    LinkedHashSet<URL> stack = backlog.get(level);\n    int rawStackSize = stack.size();\n    stack.removeAll(visited);\n    int stackSize = stack.size();\n    LinkedHashSet<URL> subStack = new LinkedHashSet<URL>();\n    info(\"Entering crawl at level \"+level+\" (\"+rawStackSize+\" links total, \"+stackSize+\" new)\");\n    for(URL u : stack) {\n      try {\n        visited.add(u);\n        PageFetcherResult result = pageFetcher.readPageFromUrl(u);\n        if(result.httpStatus == 200) {\n          u = (result.redirectUrl != null) ? result.redirectUrl : u;\n          URL postUrl = new URL(appendParam(solrUrl.toString(), \n              \"literal.id=\"+URLEncoder.encode(u.toString(),\"UTF-8\") +\n              \"&literal.url=\"+URLEncoder.encode(u.toString(),\"UTF-8\")));\n          boolean success = postData(new ByteArrayInputStream(result.content), null, out, result.contentType, postUrl);\n          if (success) {\n            info(\"POSTed web resource \"+u+\" (depth: \"+level+\")\");\n            Thread.sleep(delay * 1000);\n            numPages++;\n            // Pull links from HTML pages only\n            if(recursive > level && result.contentType.equals(\"text/html\")) {\n              Set<URL> children = pageFetcher.getLinksFromWebPage(u, new ByteArrayInputStream(result.content), result.contentType, postUrl);\n              subStack.addAll(children);\n            }\n          } else {\n            warn(\"An error occurred while posting \"+u);\n          }\n        } else {\n          warn(\"The URL \"+u+\" returned a HTTP result status of \"+result.httpStatus);\n        }\n      } catch (IOException e) {\n        warn(\"Caught exception when trying to open connection to \"+u+\": \"+e.getMessage());\n      } catch (InterruptedException e) {\n        throw new RuntimeException();\n      }\n    }\n    if(!subStack.isEmpty()) {\n      backlog.add(subStack);\n      numPages += webCrawl(level+1, out);\n    }\n    return numPages;    \n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"616c1830142ff5c1ddedec1ed898733b73c8e23b","date":1345368925,"type":0,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/util/SimplePostTool#webCrawl(int,OutputStream).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * A very simple crawler, pulling URLs to fetch from a backlog and then\n   * recurses N levels deep if recursive>0. Links are parsed from HTML\n   * through first getting an XHTML version using SolrCell with extractOnly,\n   * and followed if they are local. The crawler pauses for a default delay\n   * of 10 seconds between each fetch, this can be configured in the delay\n   * variable. This is only meant for test purposes, as it does not respect\n   * robots or anything else fancy :)\n   * @param level which level to crawl\n   * @param out output stream to write to\n   * @return number of pages crawled on this level and below\n   */\n  protected int webCrawl(int level, OutputStream out) {\n    int numPages = 0;\n    LinkedHashSet<URL> stack = backlog.get(level);\n    int rawStackSize = stack.size();\n    stack.removeAll(visited);\n    int stackSize = stack.size();\n    LinkedHashSet<URL> subStack = new LinkedHashSet<URL>();\n    info(\"Entering crawl at level \"+level+\" (\"+rawStackSize+\" links total, \"+stackSize+\" new)\");\n    for(URL u : stack) {\n      try {\n        visited.add(u);\n        PageFetcherResult result = pageFetcher.readPageFromUrl(u);\n        if(result.httpStatus == 200) {\n          u = (result.redirectUrl != null) ? result.redirectUrl : u;\n          URL postUrl = new URL(appendParam(solrUrl.toString(), \n              \"literal.id=\"+URLEncoder.encode(u.toString(),\"UTF-8\") +\n              \"&literal.url=\"+URLEncoder.encode(u.toString(),\"UTF-8\")));\n          boolean success = postData(new ByteArrayInputStream(result.content), null, out, result.contentType, postUrl);\n          if (success) {\n            info(\"POSTed web resource \"+u+\" (depth: \"+level+\")\");\n            Thread.sleep(delay * 1000);\n            numPages++;\n            // Pull links from HTML pages only\n            if(recursive > level && result.contentType.equals(\"text/html\")) {\n              Set<URL> children = pageFetcher.getLinksFromWebPage(u, new ByteArrayInputStream(result.content), result.contentType, postUrl);\n              subStack.addAll(children);\n            }\n          } else {\n            warn(\"An error occurred while posting \"+u);\n          }\n        } else {\n          warn(\"The URL \"+u+\" returned a HTTP result status of \"+result.httpStatus);\n        }\n      } catch (IOException e) {\n        warn(\"Caught exception when trying to open connection to \"+u+\": \"+e.getMessage());\n      } catch (InterruptedException e) {\n        throw new RuntimeException();\n      }\n    }\n    if(!subStack.isEmpty()) {\n      backlog.add(subStack);\n      numPages += webCrawl(level+1, out);\n    }\n    return numPages;    \n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/util/SimplePostTool#webCrawl(int,OutputStream).mjava","pathOld":"solr/core/src/java/org/apache/solr/util/SimplePostTool#webCrawl(int,OutputStream).mjava","sourceNew":"  /**\n   * A very simple crawler, pulling URLs to fetch from a backlog and then\n   * recurses N levels deep if recursive>0. Links are parsed from HTML\n   * through first getting an XHTML version using SolrCell with extractOnly,\n   * and followed if they are local. The crawler pauses for a default delay\n   * of 10 seconds between each fetch, this can be configured in the delay\n   * variable. This is only meant for test purposes, as it does not respect\n   * robots or anything else fancy :)\n   * @param level which level to crawl\n   * @param out output stream to write to\n   * @return number of pages crawled on this level and below\n   */\n  protected int webCrawl(int level, OutputStream out) {\n    int numPages = 0;\n    LinkedHashSet<URL> stack = backlog.get(level);\n    int rawStackSize = stack.size();\n    stack.removeAll(visited);\n    int stackSize = stack.size();\n    LinkedHashSet<URL> subStack = new LinkedHashSet<>();\n    info(\"Entering crawl at level \"+level+\" (\"+rawStackSize+\" links total, \"+stackSize+\" new)\");\n    for(URL u : stack) {\n      try {\n        visited.add(u);\n        PageFetcherResult result = pageFetcher.readPageFromUrl(u);\n        if(result.httpStatus == 200) {\n          u = (result.redirectUrl != null) ? result.redirectUrl : u;\n          URL postUrl = new URL(appendParam(solrUrl.toString(), \n              \"literal.id=\"+URLEncoder.encode(u.toString(),\"UTF-8\") +\n              \"&literal.url=\"+URLEncoder.encode(u.toString(),\"UTF-8\")));\n          boolean success = postData(new ByteArrayInputStream(result.content), null, out, result.contentType, postUrl);\n          if (success) {\n            info(\"POSTed web resource \"+u+\" (depth: \"+level+\")\");\n            Thread.sleep(delay * 1000);\n            numPages++;\n            // Pull links from HTML pages only\n            if(recursive > level && result.contentType.equals(\"text/html\")) {\n              Set<URL> children = pageFetcher.getLinksFromWebPage(u, new ByteArrayInputStream(result.content), result.contentType, postUrl);\n              subStack.addAll(children);\n            }\n          } else {\n            warn(\"An error occurred while posting \"+u);\n          }\n        } else {\n          warn(\"The URL \"+u+\" returned a HTTP result status of \"+result.httpStatus);\n        }\n      } catch (IOException e) {\n        warn(\"Caught exception when trying to open connection to \"+u+\": \"+e.getMessage());\n      } catch (InterruptedException e) {\n        throw new RuntimeException();\n      }\n    }\n    if(!subStack.isEmpty()) {\n      backlog.add(subStack);\n      numPages += webCrawl(level+1, out);\n    }\n    return numPages;    \n  }\n\n","sourceOld":"  /**\n   * A very simple crawler, pulling URLs to fetch from a backlog and then\n   * recurses N levels deep if recursive>0. Links are parsed from HTML\n   * through first getting an XHTML version using SolrCell with extractOnly,\n   * and followed if they are local. The crawler pauses for a default delay\n   * of 10 seconds between each fetch, this can be configured in the delay\n   * variable. This is only meant for test purposes, as it does not respect\n   * robots or anything else fancy :)\n   * @param level which level to crawl\n   * @param out output stream to write to\n   * @return number of pages crawled on this level and below\n   */\n  protected int webCrawl(int level, OutputStream out) {\n    int numPages = 0;\n    LinkedHashSet<URL> stack = backlog.get(level);\n    int rawStackSize = stack.size();\n    stack.removeAll(visited);\n    int stackSize = stack.size();\n    LinkedHashSet<URL> subStack = new LinkedHashSet<URL>();\n    info(\"Entering crawl at level \"+level+\" (\"+rawStackSize+\" links total, \"+stackSize+\" new)\");\n    for(URL u : stack) {\n      try {\n        visited.add(u);\n        PageFetcherResult result = pageFetcher.readPageFromUrl(u);\n        if(result.httpStatus == 200) {\n          u = (result.redirectUrl != null) ? result.redirectUrl : u;\n          URL postUrl = new URL(appendParam(solrUrl.toString(), \n              \"literal.id=\"+URLEncoder.encode(u.toString(),\"UTF-8\") +\n              \"&literal.url=\"+URLEncoder.encode(u.toString(),\"UTF-8\")));\n          boolean success = postData(new ByteArrayInputStream(result.content), null, out, result.contentType, postUrl);\n          if (success) {\n            info(\"POSTed web resource \"+u+\" (depth: \"+level+\")\");\n            Thread.sleep(delay * 1000);\n            numPages++;\n            // Pull links from HTML pages only\n            if(recursive > level && result.contentType.equals(\"text/html\")) {\n              Set<URL> children = pageFetcher.getLinksFromWebPage(u, new ByteArrayInputStream(result.content), result.contentType, postUrl);\n              subStack.addAll(children);\n            }\n          } else {\n            warn(\"An error occurred while posting \"+u);\n          }\n        } else {\n          warn(\"The URL \"+u+\" returned a HTTP result status of \"+result.httpStatus);\n        }\n      } catch (IOException e) {\n        warn(\"Caught exception when trying to open connection to \"+u+\": \"+e.getMessage());\n      } catch (InterruptedException e) {\n        throw new RuntimeException();\n      }\n    }\n    if(!subStack.isEmpty()) {\n      backlog.add(subStack);\n      numPages += webCrawl(level+1, out);\n    }\n    return numPages;    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a0d196318cb8ce39ae32035f290cd1397833e472","date":1418140712,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/util/SimplePostTool#webCrawl(int,OutputStream).mjava","pathOld":"solr/core/src/java/org/apache/solr/util/SimplePostTool#webCrawl(int,OutputStream).mjava","sourceNew":"  /**\n   * A very simple crawler, pulling URLs to fetch from a backlog and then\n   * recurses N levels deep if recursive>0. Links are parsed from HTML\n   * through first getting an XHTML version using SolrCell with extractOnly,\n   * and followed if they are local. The crawler pauses for a default delay\n   * of 10 seconds between each fetch, this can be configured in the delay\n   * variable. This is only meant for test purposes, as it does not respect\n   * robots or anything else fancy :)\n   * @param level which level to crawl\n   * @param out output stream to write to\n   * @return number of pages crawled on this level and below\n   */\n  protected int webCrawl(int level, OutputStream out) {\n    int numPages = 0;\n    LinkedHashSet<URL> stack = backlog.get(level);\n    int rawStackSize = stack.size();\n    stack.removeAll(visited);\n    int stackSize = stack.size();\n    LinkedHashSet<URL> subStack = new LinkedHashSet<>();\n    info(\"Entering crawl at level \"+level+\" (\"+rawStackSize+\" links total, \"+stackSize+\" new)\");\n    for(URL u : stack) {\n      try {\n        visited.add(u);\n        PageFetcherResult result = pageFetcher.readPageFromUrl(u);\n        if(result.httpStatus == 200) {\n          u = (result.redirectUrl != null) ? result.redirectUrl : u;\n          URL postUrl = new URL(appendParam(solrUrl.toString(), \n              \"literal.id=\"+URLEncoder.encode(u.toString(),\"UTF-8\") +\n              \"&literal.url=\"+URLEncoder.encode(u.toString(),\"UTF-8\")));\n          boolean success = postData(new ByteArrayInputStream(result.content.array(), result.content.arrayOffset(),result.content.limit() ), null, out, result.contentType, postUrl);\n          if (success) {\n            info(\"POSTed web resource \"+u+\" (depth: \"+level+\")\");\n            Thread.sleep(delay * 1000);\n            numPages++;\n            // Pull links from HTML pages only\n            if(recursive > level && result.contentType.equals(\"text/html\")) {\n              Set<URL> children = pageFetcher.getLinksFromWebPage(u, new ByteBufferInputStream(result.content), result.contentType, postUrl);\n              subStack.addAll(children);\n            }\n          } else {\n            warn(\"An error occurred while posting \"+u);\n          }\n        } else {\n          warn(\"The URL \"+u+\" returned a HTTP result status of \"+result.httpStatus);\n        }\n      } catch (IOException e) {\n        warn(\"Caught exception when trying to open connection to \"+u+\": \"+e.getMessage());\n      } catch (InterruptedException e) {\n        throw new RuntimeException();\n      }\n    }\n    if(!subStack.isEmpty()) {\n      backlog.add(subStack);\n      numPages += webCrawl(level+1, out);\n    }\n    return numPages;    \n  }\n\n","sourceOld":"  /**\n   * A very simple crawler, pulling URLs to fetch from a backlog and then\n   * recurses N levels deep if recursive>0. Links are parsed from HTML\n   * through first getting an XHTML version using SolrCell with extractOnly,\n   * and followed if they are local. The crawler pauses for a default delay\n   * of 10 seconds between each fetch, this can be configured in the delay\n   * variable. This is only meant for test purposes, as it does not respect\n   * robots or anything else fancy :)\n   * @param level which level to crawl\n   * @param out output stream to write to\n   * @return number of pages crawled on this level and below\n   */\n  protected int webCrawl(int level, OutputStream out) {\n    int numPages = 0;\n    LinkedHashSet<URL> stack = backlog.get(level);\n    int rawStackSize = stack.size();\n    stack.removeAll(visited);\n    int stackSize = stack.size();\n    LinkedHashSet<URL> subStack = new LinkedHashSet<>();\n    info(\"Entering crawl at level \"+level+\" (\"+rawStackSize+\" links total, \"+stackSize+\" new)\");\n    for(URL u : stack) {\n      try {\n        visited.add(u);\n        PageFetcherResult result = pageFetcher.readPageFromUrl(u);\n        if(result.httpStatus == 200) {\n          u = (result.redirectUrl != null) ? result.redirectUrl : u;\n          URL postUrl = new URL(appendParam(solrUrl.toString(), \n              \"literal.id=\"+URLEncoder.encode(u.toString(),\"UTF-8\") +\n              \"&literal.url=\"+URLEncoder.encode(u.toString(),\"UTF-8\")));\n          boolean success = postData(new ByteArrayInputStream(result.content), null, out, result.contentType, postUrl);\n          if (success) {\n            info(\"POSTed web resource \"+u+\" (depth: \"+level+\")\");\n            Thread.sleep(delay * 1000);\n            numPages++;\n            // Pull links from HTML pages only\n            if(recursive > level && result.contentType.equals(\"text/html\")) {\n              Set<URL> children = pageFetcher.getLinksFromWebPage(u, new ByteArrayInputStream(result.content), result.contentType, postUrl);\n              subStack.addAll(children);\n            }\n          } else {\n            warn(\"An error occurred while posting \"+u);\n          }\n        } else {\n          warn(\"The URL \"+u+\" returned a HTTP result status of \"+result.httpStatus);\n        }\n      } catch (IOException e) {\n        warn(\"Caught exception when trying to open connection to \"+u+\": \"+e.getMessage());\n      } catch (InterruptedException e) {\n        throw new RuntimeException();\n      }\n    }\n    if(!subStack.isEmpty()) {\n      backlog.add(subStack);\n      numPages += webCrawl(level+1, out);\n    }\n    return numPages;    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"45a46a55a71202d6d2e3aec8777a73c8934108f0","date":1418232745,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/util/SimplePostTool#webCrawl(int,OutputStream).mjava","pathOld":"solr/core/src/java/org/apache/solr/util/SimplePostTool#webCrawl(int,OutputStream).mjava","sourceNew":"  /**\n   * A very simple crawler, pulling URLs to fetch from a backlog and then\n   * recurses N levels deep if recursive>0. Links are parsed from HTML\n   * through first getting an XHTML version using SolrCell with extractOnly,\n   * and followed if they are local. The crawler pauses for a default delay\n   * of 10 seconds between each fetch, this can be configured in the delay\n   * variable. This is only meant for test purposes, as it does not respect\n   * robots or anything else fancy :)\n   * @param level which level to crawl\n   * @param out output stream to write to\n   * @return number of pages crawled on this level and below\n   */\n  protected int webCrawl(int level, OutputStream out) {\n    int numPages = 0;\n    LinkedHashSet<URL> stack = backlog.get(level);\n    int rawStackSize = stack.size();\n    stack.removeAll(visited);\n    int stackSize = stack.size();\n    LinkedHashSet<URL> subStack = new LinkedHashSet<>();\n    info(\"Entering crawl at level \"+level+\" (\"+rawStackSize+\" links total, \"+stackSize+\" new)\");\n    for(URL u : stack) {\n      try {\n        visited.add(u);\n        PageFetcherResult result = pageFetcher.readPageFromUrl(u);\n        if(result.httpStatus == 200) {\n          u = (result.redirectUrl != null) ? result.redirectUrl : u;\n          URL postUrl = new URL(appendParam(solrUrl.toString(), \n              \"literal.id=\"+URLEncoder.encode(u.toString(),\"UTF-8\") +\n              \"&literal.url=\"+URLEncoder.encode(u.toString(),\"UTF-8\")));\n          boolean success = postData(new ByteArrayInputStream(result.content.array(), result.content.arrayOffset(),result.content.limit() ), null, out, result.contentType, postUrl);\n          if (success) {\n            info(\"POSTed web resource \"+u+\" (depth: \"+level+\")\");\n            Thread.sleep(delay * 1000);\n            numPages++;\n            // Pull links from HTML pages only\n            if(recursive > level && result.contentType.equals(\"text/html\")) {\n              Set<URL> children = pageFetcher.getLinksFromWebPage(u, new ByteArrayInputStream(result.content.array(), result.content.arrayOffset(), result.content.limit()), result.contentType, postUrl);\n              subStack.addAll(children);\n            }\n          } else {\n            warn(\"An error occurred while posting \"+u);\n          }\n        } else {\n          warn(\"The URL \"+u+\" returned a HTTP result status of \"+result.httpStatus);\n        }\n      } catch (IOException e) {\n        warn(\"Caught exception when trying to open connection to \"+u+\": \"+e.getMessage());\n      } catch (InterruptedException e) {\n        throw new RuntimeException();\n      }\n    }\n    if(!subStack.isEmpty()) {\n      backlog.add(subStack);\n      numPages += webCrawl(level+1, out);\n    }\n    return numPages;    \n  }\n\n","sourceOld":"  /**\n   * A very simple crawler, pulling URLs to fetch from a backlog and then\n   * recurses N levels deep if recursive>0. Links are parsed from HTML\n   * through first getting an XHTML version using SolrCell with extractOnly,\n   * and followed if they are local. The crawler pauses for a default delay\n   * of 10 seconds between each fetch, this can be configured in the delay\n   * variable. This is only meant for test purposes, as it does not respect\n   * robots or anything else fancy :)\n   * @param level which level to crawl\n   * @param out output stream to write to\n   * @return number of pages crawled on this level and below\n   */\n  protected int webCrawl(int level, OutputStream out) {\n    int numPages = 0;\n    LinkedHashSet<URL> stack = backlog.get(level);\n    int rawStackSize = stack.size();\n    stack.removeAll(visited);\n    int stackSize = stack.size();\n    LinkedHashSet<URL> subStack = new LinkedHashSet<>();\n    info(\"Entering crawl at level \"+level+\" (\"+rawStackSize+\" links total, \"+stackSize+\" new)\");\n    for(URL u : stack) {\n      try {\n        visited.add(u);\n        PageFetcherResult result = pageFetcher.readPageFromUrl(u);\n        if(result.httpStatus == 200) {\n          u = (result.redirectUrl != null) ? result.redirectUrl : u;\n          URL postUrl = new URL(appendParam(solrUrl.toString(), \n              \"literal.id=\"+URLEncoder.encode(u.toString(),\"UTF-8\") +\n              \"&literal.url=\"+URLEncoder.encode(u.toString(),\"UTF-8\")));\n          boolean success = postData(new ByteArrayInputStream(result.content.array(), result.content.arrayOffset(),result.content.limit() ), null, out, result.contentType, postUrl);\n          if (success) {\n            info(\"POSTed web resource \"+u+\" (depth: \"+level+\")\");\n            Thread.sleep(delay * 1000);\n            numPages++;\n            // Pull links from HTML pages only\n            if(recursive > level && result.contentType.equals(\"text/html\")) {\n              Set<URL> children = pageFetcher.getLinksFromWebPage(u, new ByteBufferInputStream(result.content), result.contentType, postUrl);\n              subStack.addAll(children);\n            }\n          } else {\n            warn(\"An error occurred while posting \"+u);\n          }\n        } else {\n          warn(\"The URL \"+u+\" returned a HTTP result status of \"+result.httpStatus);\n        }\n      } catch (IOException e) {\n        warn(\"Caught exception when trying to open connection to \"+u+\": \"+e.getMessage());\n      } catch (InterruptedException e) {\n        throw new RuntimeException();\n      }\n    }\n    if(!subStack.isEmpty()) {\n      backlog.add(subStack);\n      numPages += webCrawl(level+1, out);\n    }\n    return numPages;    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1e210ae1e604402eb4eeff2a52e56d189cd4f2f1","date":1423508552,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/util/SimplePostTool#webCrawl(int,OutputStream).mjava","pathOld":"solr/core/src/java/org/apache/solr/util/SimplePostTool#webCrawl(int,OutputStream).mjava","sourceNew":"  /**\n   * A very simple crawler, pulling URLs to fetch from a backlog and then\n   * recurses N levels deep if recursive&gt;0. Links are parsed from HTML\n   * through first getting an XHTML version using SolrCell with extractOnly,\n   * and followed if they are local. The crawler pauses for a default delay\n   * of 10 seconds between each fetch, this can be configured in the delay\n   * variable. This is only meant for test purposes, as it does not respect\n   * robots or anything else fancy :)\n   * @param level which level to crawl\n   * @param out output stream to write to\n   * @return number of pages crawled on this level and below\n   */\n  protected int webCrawl(int level, OutputStream out) {\n    int numPages = 0;\n    LinkedHashSet<URL> stack = backlog.get(level);\n    int rawStackSize = stack.size();\n    stack.removeAll(visited);\n    int stackSize = stack.size();\n    LinkedHashSet<URL> subStack = new LinkedHashSet<>();\n    info(\"Entering crawl at level \"+level+\" (\"+rawStackSize+\" links total, \"+stackSize+\" new)\");\n    for(URL u : stack) {\n      try {\n        visited.add(u);\n        PageFetcherResult result = pageFetcher.readPageFromUrl(u);\n        if(result.httpStatus == 200) {\n          u = (result.redirectUrl != null) ? result.redirectUrl : u;\n          URL postUrl = new URL(appendParam(solrUrl.toString(), \n              \"literal.id=\"+URLEncoder.encode(u.toString(),\"UTF-8\") +\n              \"&literal.url=\"+URLEncoder.encode(u.toString(),\"UTF-8\")));\n          boolean success = postData(new ByteArrayInputStream(result.content.array(), result.content.arrayOffset(),result.content.limit() ), null, out, result.contentType, postUrl);\n          if (success) {\n            info(\"POSTed web resource \"+u+\" (depth: \"+level+\")\");\n            Thread.sleep(delay * 1000);\n            numPages++;\n            // Pull links from HTML pages only\n            if(recursive > level && result.contentType.equals(\"text/html\")) {\n              Set<URL> children = pageFetcher.getLinksFromWebPage(u, new ByteArrayInputStream(result.content.array(), result.content.arrayOffset(), result.content.limit()), result.contentType, postUrl);\n              subStack.addAll(children);\n            }\n          } else {\n            warn(\"An error occurred while posting \"+u);\n          }\n        } else {\n          warn(\"The URL \"+u+\" returned a HTTP result status of \"+result.httpStatus);\n        }\n      } catch (IOException e) {\n        warn(\"Caught exception when trying to open connection to \"+u+\": \"+e.getMessage());\n      } catch (InterruptedException e) {\n        throw new RuntimeException();\n      }\n    }\n    if(!subStack.isEmpty()) {\n      backlog.add(subStack);\n      numPages += webCrawl(level+1, out);\n    }\n    return numPages;    \n  }\n\n","sourceOld":"  /**\n   * A very simple crawler, pulling URLs to fetch from a backlog and then\n   * recurses N levels deep if recursive>0. Links are parsed from HTML\n   * through first getting an XHTML version using SolrCell with extractOnly,\n   * and followed if they are local. The crawler pauses for a default delay\n   * of 10 seconds between each fetch, this can be configured in the delay\n   * variable. This is only meant for test purposes, as it does not respect\n   * robots or anything else fancy :)\n   * @param level which level to crawl\n   * @param out output stream to write to\n   * @return number of pages crawled on this level and below\n   */\n  protected int webCrawl(int level, OutputStream out) {\n    int numPages = 0;\n    LinkedHashSet<URL> stack = backlog.get(level);\n    int rawStackSize = stack.size();\n    stack.removeAll(visited);\n    int stackSize = stack.size();\n    LinkedHashSet<URL> subStack = new LinkedHashSet<>();\n    info(\"Entering crawl at level \"+level+\" (\"+rawStackSize+\" links total, \"+stackSize+\" new)\");\n    for(URL u : stack) {\n      try {\n        visited.add(u);\n        PageFetcherResult result = pageFetcher.readPageFromUrl(u);\n        if(result.httpStatus == 200) {\n          u = (result.redirectUrl != null) ? result.redirectUrl : u;\n          URL postUrl = new URL(appendParam(solrUrl.toString(), \n              \"literal.id=\"+URLEncoder.encode(u.toString(),\"UTF-8\") +\n              \"&literal.url=\"+URLEncoder.encode(u.toString(),\"UTF-8\")));\n          boolean success = postData(new ByteArrayInputStream(result.content.array(), result.content.arrayOffset(),result.content.limit() ), null, out, result.contentType, postUrl);\n          if (success) {\n            info(\"POSTed web resource \"+u+\" (depth: \"+level+\")\");\n            Thread.sleep(delay * 1000);\n            numPages++;\n            // Pull links from HTML pages only\n            if(recursive > level && result.contentType.equals(\"text/html\")) {\n              Set<URL> children = pageFetcher.getLinksFromWebPage(u, new ByteArrayInputStream(result.content.array(), result.content.arrayOffset(), result.content.limit()), result.contentType, postUrl);\n              subStack.addAll(children);\n            }\n          } else {\n            warn(\"An error occurred while posting \"+u);\n          }\n        } else {\n          warn(\"The URL \"+u+\" returned a HTTP result status of \"+result.httpStatus);\n        }\n      } catch (IOException e) {\n        warn(\"Caught exception when trying to open connection to \"+u+\": \"+e.getMessage());\n      } catch (InterruptedException e) {\n        throw new RuntimeException();\n      }\n    }\n    if(!subStack.isEmpty()) {\n      backlog.add(subStack);\n      numPages += webCrawl(level+1, out);\n    }\n    return numPages;    \n  }\n\n","bugFix":["c2c3a504730329ae644b009dee43024116605d47"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"437e65c578cab603d9201916b0e285f3d68aff45","date":1427831677,"type":3,"author":"Varun Thacker","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/util/SimplePostTool#webCrawl(int,OutputStream).mjava","pathOld":"solr/core/src/java/org/apache/solr/util/SimplePostTool#webCrawl(int,OutputStream).mjava","sourceNew":"  /**\n   * A very simple crawler, pulling URLs to fetch from a backlog and then\n   * recurses N levels deep if recursive&gt;0. Links are parsed from HTML\n   * through first getting an XHTML version using SolrCell with extractOnly,\n   * and followed if they are local. The crawler pauses for a default delay\n   * of 10 seconds between each fetch, this can be configured in the delay\n   * variable. This is only meant for test purposes, as it does not respect\n   * robots or anything else fancy :)\n   * @param level which level to crawl\n   * @param out output stream to write to\n   * @return number of pages crawled on this level and below\n   */\n  protected int webCrawl(int level, OutputStream out) {\n    int numPages = 0;\n    LinkedHashSet<URL> stack = backlog.get(level);\n    int rawStackSize = stack.size();\n    stack.removeAll(visited);\n    int stackSize = stack.size();\n    LinkedHashSet<URL> subStack = new LinkedHashSet<>();\n    info(\"Entering crawl at level \"+level+\" (\"+rawStackSize+\" links total, \"+stackSize+\" new)\");\n    for(URL u : stack) {\n      try {\n        visited.add(u);\n        PageFetcherResult result = pageFetcher.readPageFromUrl(u);\n        if(result.httpStatus == 200) {\n          u = (result.redirectUrl != null) ? result.redirectUrl : u;\n          URL postUrl = new URL(appendParam(solrUrl.toString(), \n              \"literal.id=\"+URLEncoder.encode(u.toString(),\"UTF-8\") +\n              \"&literal.url=\"+URLEncoder.encode(u.toString(),\"UTF-8\")));\n          boolean success = postData(new ByteArrayInputStream(result.content.array(), result.content.arrayOffset(),result.content.limit() ), null, out, result.contentType, postUrl);\n          if (success) {\n            info(\"POSTed web resource \"+u+\" (depth: \"+level+\")\");\n            Thread.sleep(delay * 1000);\n            numPages++;\n            // Pull links from HTML pages only\n            if(recursive > level && result.contentType.equals(\"text/html\")) {\n              Set<URL> children = pageFetcher.getLinksFromWebPage(u, new ByteArrayInputStream(result.content.array(), result.content.arrayOffset(), result.content.limit()), result.contentType, postUrl);\n              subStack.addAll(children);\n            }\n          } else {\n            warn(\"An error occurred while posting \"+u);\n          }\n        } else {\n          warn(\"The URL \"+u+\" returned a HTTP result status of \"+result.httpStatus);\n        }\n      } catch (IOException e) {\n        warn(\"Caught exception when trying to open connection to \"+u+\": \"+e.getMessage());\n      } catch (InterruptedException e) {\n        throw new RuntimeException(e);\n      }\n    }\n    if(!subStack.isEmpty()) {\n      backlog.add(subStack);\n      numPages += webCrawl(level+1, out);\n    }\n    return numPages;    \n  }\n\n","sourceOld":"  /**\n   * A very simple crawler, pulling URLs to fetch from a backlog and then\n   * recurses N levels deep if recursive&gt;0. Links are parsed from HTML\n   * through first getting an XHTML version using SolrCell with extractOnly,\n   * and followed if they are local. The crawler pauses for a default delay\n   * of 10 seconds between each fetch, this can be configured in the delay\n   * variable. This is only meant for test purposes, as it does not respect\n   * robots or anything else fancy :)\n   * @param level which level to crawl\n   * @param out output stream to write to\n   * @return number of pages crawled on this level and below\n   */\n  protected int webCrawl(int level, OutputStream out) {\n    int numPages = 0;\n    LinkedHashSet<URL> stack = backlog.get(level);\n    int rawStackSize = stack.size();\n    stack.removeAll(visited);\n    int stackSize = stack.size();\n    LinkedHashSet<URL> subStack = new LinkedHashSet<>();\n    info(\"Entering crawl at level \"+level+\" (\"+rawStackSize+\" links total, \"+stackSize+\" new)\");\n    for(URL u : stack) {\n      try {\n        visited.add(u);\n        PageFetcherResult result = pageFetcher.readPageFromUrl(u);\n        if(result.httpStatus == 200) {\n          u = (result.redirectUrl != null) ? result.redirectUrl : u;\n          URL postUrl = new URL(appendParam(solrUrl.toString(), \n              \"literal.id=\"+URLEncoder.encode(u.toString(),\"UTF-8\") +\n              \"&literal.url=\"+URLEncoder.encode(u.toString(),\"UTF-8\")));\n          boolean success = postData(new ByteArrayInputStream(result.content.array(), result.content.arrayOffset(),result.content.limit() ), null, out, result.contentType, postUrl);\n          if (success) {\n            info(\"POSTed web resource \"+u+\" (depth: \"+level+\")\");\n            Thread.sleep(delay * 1000);\n            numPages++;\n            // Pull links from HTML pages only\n            if(recursive > level && result.contentType.equals(\"text/html\")) {\n              Set<URL> children = pageFetcher.getLinksFromWebPage(u, new ByteArrayInputStream(result.content.array(), result.content.arrayOffset(), result.content.limit()), result.contentType, postUrl);\n              subStack.addAll(children);\n            }\n          } else {\n            warn(\"An error occurred while posting \"+u);\n          }\n        } else {\n          warn(\"The URL \"+u+\" returned a HTTP result status of \"+result.httpStatus);\n        }\n      } catch (IOException e) {\n        warn(\"Caught exception when trying to open connection to \"+u+\": \"+e.getMessage());\n      } catch (InterruptedException e) {\n        throw new RuntimeException();\n      }\n    }\n    if(!subStack.isEmpty()) {\n      backlog.add(subStack);\n      numPages += webCrawl(level+1, out);\n    }\n    return numPages;    \n  }\n\n","bugFix":["c2c3a504730329ae644b009dee43024116605d47"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fab172655716b96f7e42376116235017a922de3a","date":1427850611,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/util/SimplePostTool#webCrawl(int,OutputStream).mjava","pathOld":"solr/core/src/java/org/apache/solr/util/SimplePostTool#webCrawl(int,OutputStream).mjava","sourceNew":"  /**\n   * A very simple crawler, pulling URLs to fetch from a backlog and then\n   * recurses N levels deep if recursive&gt;0. Links are parsed from HTML\n   * through first getting an XHTML version using SolrCell with extractOnly,\n   * and followed if they are local. The crawler pauses for a default delay\n   * of 10 seconds between each fetch, this can be configured in the delay\n   * variable. This is only meant for test purposes, as it does not respect\n   * robots or anything else fancy :)\n   * @param level which level to crawl\n   * @param out output stream to write to\n   * @return number of pages crawled on this level and below\n   */\n  protected int webCrawl(int level, OutputStream out) {\n    int numPages = 0;\n    LinkedHashSet<URL> stack = backlog.get(level);\n    int rawStackSize = stack.size();\n    stack.removeAll(visited);\n    int stackSize = stack.size();\n    LinkedHashSet<URL> subStack = new LinkedHashSet<>();\n    info(\"Entering crawl at level \"+level+\" (\"+rawStackSize+\" links total, \"+stackSize+\" new)\");\n    for(URL u : stack) {\n      try {\n        visited.add(u);\n        PageFetcherResult result = pageFetcher.readPageFromUrl(u);\n        if(result.httpStatus == 200) {\n          u = (result.redirectUrl != null) ? result.redirectUrl : u;\n          URL postUrl = new URL(appendParam(solrUrl.toString(), \n              \"literal.id=\"+URLEncoder.encode(u.toString(),\"UTF-8\") +\n              \"&literal.url=\"+URLEncoder.encode(u.toString(),\"UTF-8\")));\n          boolean success = postData(new ByteArrayInputStream(result.content.array(), result.content.arrayOffset(),result.content.limit() ), null, out, result.contentType, postUrl);\n          if (success) {\n            info(\"POSTed web resource \"+u+\" (depth: \"+level+\")\");\n            Thread.sleep(delay * 1000);\n            numPages++;\n            // Pull links from HTML pages only\n            if(recursive > level && result.contentType.equals(\"text/html\")) {\n              Set<URL> children = pageFetcher.getLinksFromWebPage(u, new ByteArrayInputStream(result.content.array(), result.content.arrayOffset(), result.content.limit()), result.contentType, postUrl);\n              subStack.addAll(children);\n            }\n          } else {\n            warn(\"An error occurred while posting \"+u);\n          }\n        } else {\n          warn(\"The URL \"+u+\" returned a HTTP result status of \"+result.httpStatus);\n        }\n      } catch (IOException e) {\n        warn(\"Caught exception when trying to open connection to \"+u+\": \"+e.getMessage());\n      } catch (InterruptedException e) {\n        throw new RuntimeException(e);\n      }\n    }\n    if(!subStack.isEmpty()) {\n      backlog.add(subStack);\n      numPages += webCrawl(level+1, out);\n    }\n    return numPages;    \n  }\n\n","sourceOld":"  /**\n   * A very simple crawler, pulling URLs to fetch from a backlog and then\n   * recurses N levels deep if recursive&gt;0. Links are parsed from HTML\n   * through first getting an XHTML version using SolrCell with extractOnly,\n   * and followed if they are local. The crawler pauses for a default delay\n   * of 10 seconds between each fetch, this can be configured in the delay\n   * variable. This is only meant for test purposes, as it does not respect\n   * robots or anything else fancy :)\n   * @param level which level to crawl\n   * @param out output stream to write to\n   * @return number of pages crawled on this level and below\n   */\n  protected int webCrawl(int level, OutputStream out) {\n    int numPages = 0;\n    LinkedHashSet<URL> stack = backlog.get(level);\n    int rawStackSize = stack.size();\n    stack.removeAll(visited);\n    int stackSize = stack.size();\n    LinkedHashSet<URL> subStack = new LinkedHashSet<>();\n    info(\"Entering crawl at level \"+level+\" (\"+rawStackSize+\" links total, \"+stackSize+\" new)\");\n    for(URL u : stack) {\n      try {\n        visited.add(u);\n        PageFetcherResult result = pageFetcher.readPageFromUrl(u);\n        if(result.httpStatus == 200) {\n          u = (result.redirectUrl != null) ? result.redirectUrl : u;\n          URL postUrl = new URL(appendParam(solrUrl.toString(), \n              \"literal.id=\"+URLEncoder.encode(u.toString(),\"UTF-8\") +\n              \"&literal.url=\"+URLEncoder.encode(u.toString(),\"UTF-8\")));\n          boolean success = postData(new ByteArrayInputStream(result.content.array(), result.content.arrayOffset(),result.content.limit() ), null, out, result.contentType, postUrl);\n          if (success) {\n            info(\"POSTed web resource \"+u+\" (depth: \"+level+\")\");\n            Thread.sleep(delay * 1000);\n            numPages++;\n            // Pull links from HTML pages only\n            if(recursive > level && result.contentType.equals(\"text/html\")) {\n              Set<URL> children = pageFetcher.getLinksFromWebPage(u, new ByteArrayInputStream(result.content.array(), result.content.arrayOffset(), result.content.limit()), result.contentType, postUrl);\n              subStack.addAll(children);\n            }\n          } else {\n            warn(\"An error occurred while posting \"+u);\n          }\n        } else {\n          warn(\"The URL \"+u+\" returned a HTTP result status of \"+result.httpStatus);\n        }\n      } catch (IOException e) {\n        warn(\"Caught exception when trying to open connection to \"+u+\": \"+e.getMessage());\n      } catch (InterruptedException e) {\n        throw new RuntimeException();\n      }\n    }\n    if(!subStack.isEmpty()) {\n      backlog.add(subStack);\n      numPages += webCrawl(level+1, out);\n    }\n    return numPages;    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0db83f1bb855a4ac824c9a2a8e1ee9b29a039c15","date":1554259533,"type":3,"author":"Gus Heck","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/util/SimplePostTool#webCrawl(int,OutputStream).mjava","pathOld":"solr/core/src/java/org/apache/solr/util/SimplePostTool#webCrawl(int,OutputStream).mjava","sourceNew":"  /**\n   * A very simple crawler, pulling URLs to fetch from a backlog and then\n   * recurses N levels deep if recursive&gt;0. Links are parsed from HTML\n   * through first getting an XHTML version using SolrCell with extractOnly,\n   * and followed if they are local. The crawler pauses for a default delay\n   * of 10 seconds between each fetch, this can be configured in the delay\n   * variable. This is only meant for test purposes, as it does not respect\n   * robots or anything else fancy :)\n   * @param level which level to crawl\n   * @param out output stream to write to\n   * @return number of pages crawled on this level and below\n   */\n  protected int webCrawl(int level, OutputStream out) {\n    int numPages = 0;\n    LinkedHashSet<URL> stack = backlog.get(level);\n    int rawStackSize = stack.size();\n    stack.removeAll(visited);\n    int stackSize = stack.size();\n    LinkedHashSet<URL> subStack = new LinkedHashSet<>();\n    info(\"Entering crawl at level \"+level+\" (\"+rawStackSize+\" links total, \"+stackSize+\" new)\");\n    for(URL u : stack) {\n      try {\n        visited.add(u);\n        PageFetcherResult result = pageFetcher.readPageFromUrl(u);\n        if(result.httpStatus == 200) {\n          u = (result.redirectUrl != null) ? result.redirectUrl : u;\n          URL postUrl = new URL(appendParam(solrUrl.toString(),\n              \"literal.id=\"+URLEncoder.encode(u.toString(),\"UTF-8\") +\n              \"&literal.url=\"+URLEncoder.encode(u.toString(),\"UTF-8\")));\n          boolean success = postData(new ByteArrayInputStream(result.content.array(), result.content.arrayOffset(),result.content.limit() ), null, out, result.contentType, postUrl);\n          if (success) {\n            info(\"POSTed web resource \"+u+\" (depth: \"+level+\")\");\n            Thread.sleep(delay * 1000);\n            numPages++;\n            // Pull links from HTML pages only\n            if(recursive > level && result.contentType.equals(\"text/html\")) {\n              Set<URL> children = pageFetcher.getLinksFromWebPage(u, new ByteArrayInputStream(result.content.array(), result.content.arrayOffset(), result.content.limit()), result.contentType, postUrl);\n              subStack.addAll(children);\n            }\n          } else {\n            warn(\"An error occurred while posting \"+u);\n          }\n        } else {\n          warn(\"The URL \"+u+\" returned a HTTP result status of \"+result.httpStatus);\n        }\n      } catch (IOException e) {\n        warn(\"Caught exception when trying to open connection to \"+u+\": \"+e.getMessage());\n      } catch (InterruptedException e) {\n        throw new RuntimeException(e);\n      }\n    }\n    if(!subStack.isEmpty()) {\n      backlog.add(subStack);\n      numPages += webCrawl(level+1, out);\n    }\n    return numPages;\n  }\n\n","sourceOld":"  /**\n   * A very simple crawler, pulling URLs to fetch from a backlog and then\n   * recurses N levels deep if recursive&gt;0. Links are parsed from HTML\n   * through first getting an XHTML version using SolrCell with extractOnly,\n   * and followed if they are local. The crawler pauses for a default delay\n   * of 10 seconds between each fetch, this can be configured in the delay\n   * variable. This is only meant for test purposes, as it does not respect\n   * robots or anything else fancy :)\n   * @param level which level to crawl\n   * @param out output stream to write to\n   * @return number of pages crawled on this level and below\n   */\n  protected int webCrawl(int level, OutputStream out) {\n    int numPages = 0;\n    LinkedHashSet<URL> stack = backlog.get(level);\n    int rawStackSize = stack.size();\n    stack.removeAll(visited);\n    int stackSize = stack.size();\n    LinkedHashSet<URL> subStack = new LinkedHashSet<>();\n    info(\"Entering crawl at level \"+level+\" (\"+rawStackSize+\" links total, \"+stackSize+\" new)\");\n    for(URL u : stack) {\n      try {\n        visited.add(u);\n        PageFetcherResult result = pageFetcher.readPageFromUrl(u);\n        if(result.httpStatus == 200) {\n          u = (result.redirectUrl != null) ? result.redirectUrl : u;\n          URL postUrl = new URL(appendParam(solrUrl.toString(), \n              \"literal.id=\"+URLEncoder.encode(u.toString(),\"UTF-8\") +\n              \"&literal.url=\"+URLEncoder.encode(u.toString(),\"UTF-8\")));\n          boolean success = postData(new ByteArrayInputStream(result.content.array(), result.content.arrayOffset(),result.content.limit() ), null, out, result.contentType, postUrl);\n          if (success) {\n            info(\"POSTed web resource \"+u+\" (depth: \"+level+\")\");\n            Thread.sleep(delay * 1000);\n            numPages++;\n            // Pull links from HTML pages only\n            if(recursive > level && result.contentType.equals(\"text/html\")) {\n              Set<URL> children = pageFetcher.getLinksFromWebPage(u, new ByteArrayInputStream(result.content.array(), result.content.arrayOffset(), result.content.limit()), result.contentType, postUrl);\n              subStack.addAll(children);\n            }\n          } else {\n            warn(\"An error occurred while posting \"+u);\n          }\n        } else {\n          warn(\"The URL \"+u+\" returned a HTTP result status of \"+result.httpStatus);\n        }\n      } catch (IOException e) {\n        warn(\"Caught exception when trying to open connection to \"+u+\": \"+e.getMessage());\n      } catch (InterruptedException e) {\n        throw new RuntimeException(e);\n      }\n    }\n    if(!subStack.isEmpty()) {\n      backlog.add(subStack);\n      numPages += webCrawl(level+1, out);\n    }\n    return numPages;    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["c2c3a504730329ae644b009dee43024116605d47"],"616c1830142ff5c1ddedec1ed898733b73c8e23b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","c2c3a504730329ae644b009dee43024116605d47"],"70fa1c0f4d75735ff2e1485e059d9bc5efa50598":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","c2c3a504730329ae644b009dee43024116605d47"],"a0d196318cb8ce39ae32035f290cd1397833e472":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"c2c3a504730329ae644b009dee43024116605d47":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"45a46a55a71202d6d2e3aec8777a73c8934108f0":["a0d196318cb8ce39ae32035f290cd1397833e472"],"437e65c578cab603d9201916b0e285f3d68aff45":["1e210ae1e604402eb4eeff2a52e56d189cd4f2f1"],"0db83f1bb855a4ac824c9a2a8e1ee9b29a039c15":["437e65c578cab603d9201916b0e285f3d68aff45"],"fab172655716b96f7e42376116235017a922de3a":["1e210ae1e604402eb4eeff2a52e56d189cd4f2f1","437e65c578cab603d9201916b0e285f3d68aff45"],"1e210ae1e604402eb4eeff2a52e56d189cd4f2f1":["45a46a55a71202d6d2e3aec8777a73c8934108f0"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["0db83f1bb855a4ac824c9a2a8e1ee9b29a039c15"]},"commit2Childs":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["a0d196318cb8ce39ae32035f290cd1397833e472"],"616c1830142ff5c1ddedec1ed898733b73c8e23b":[],"70fa1c0f4d75735ff2e1485e059d9bc5efa50598":[],"c2c3a504730329ae644b009dee43024116605d47":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","616c1830142ff5c1ddedec1ed898733b73c8e23b","70fa1c0f4d75735ff2e1485e059d9bc5efa50598"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["616c1830142ff5c1ddedec1ed898733b73c8e23b","70fa1c0f4d75735ff2e1485e059d9bc5efa50598","c2c3a504730329ae644b009dee43024116605d47"],"a0d196318cb8ce39ae32035f290cd1397833e472":["45a46a55a71202d6d2e3aec8777a73c8934108f0"],"45a46a55a71202d6d2e3aec8777a73c8934108f0":["1e210ae1e604402eb4eeff2a52e56d189cd4f2f1"],"437e65c578cab603d9201916b0e285f3d68aff45":["0db83f1bb855a4ac824c9a2a8e1ee9b29a039c15","fab172655716b96f7e42376116235017a922de3a"],"0db83f1bb855a4ac824c9a2a8e1ee9b29a039c15":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"fab172655716b96f7e42376116235017a922de3a":[],"1e210ae1e604402eb4eeff2a52e56d189cd4f2f1":["437e65c578cab603d9201916b0e285f3d68aff45","fab172655716b96f7e42376116235017a922de3a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["616c1830142ff5c1ddedec1ed898733b73c8e23b","70fa1c0f4d75735ff2e1485e059d9bc5efa50598","fab172655716b96f7e42376116235017a922de3a","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}