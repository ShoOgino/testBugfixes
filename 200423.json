{"path":"contrib/similarity/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map,String).mjava","commits":[{"id":"aa8dcd4adc562c6e8da67c42872ef9194f8598fa","date":1108145465,"type":1,"author":"Erik Hatcher","isMerge":false,"pathNew":"contrib/similarity/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map,String).mjava","pathOld":"sandbox/contributions/similarity/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map,String).mjava","sourceNew":"\t/**\n\t * Adds term frequencies found by tokenizing text from reader into the Map words\n\t * @param r a source of text to be tokenized\n\t * @param termFreqMap a Map of terms and their frequencies\n\t * @param fieldName Used by analyzer for any special per-field analysis\n\t */\n\tprivate void addTermFrequencies(Reader r, Map termFreqMap, String fieldName)\n\t\tthrows IOException\n\t{\n\t\t   TokenStream ts = analyzer.tokenStream(fieldName, r);\n\t\t\torg.apache.lucene.analysis.Token token;\n\t\t\tint tokenCount=0;\n\t\t\twhile ((token = ts.next()) != null) { // for every token\n\t\t\t\tString word = token.termText();\n\t\t\t\ttokenCount++;\n\t\t\t\tif(tokenCount>maxNumTokensParsed)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif(isNoiseWord(word)){\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\t// increment frequency\n\t\t\t\tInt cnt = (Int) termFreqMap.get(word);\n\t\t\t\tif (cnt == null) {\n\t\t\t\t\ttermFreqMap.put(word, new Int());\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tcnt.x++;\n\t\t\t\t}\n\t\t\t}\n\t}\n\n","sourceOld":"\t/**\n\t * Adds term frequencies found by tokenizing text from reader into the Map words\n\t * @param r a source of text to be tokenized\n\t * @param termFreqMap a Map of terms and their frequencies\n\t * @param fieldName Used by analyzer for any special per-field analysis\n\t */\n\tprivate void addTermFrequencies(Reader r, Map termFreqMap, String fieldName)\n\t\tthrows IOException\n\t{\n\t\t   TokenStream ts = analyzer.tokenStream(fieldName, r);\n\t\t\torg.apache.lucene.analysis.Token token;\n\t\t\tint tokenCount=0;\n\t\t\twhile ((token = ts.next()) != null) { // for every token\n\t\t\t\tString word = token.termText();\n\t\t\t\ttokenCount++;\n\t\t\t\tif(tokenCount>maxNumTokensParsed)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif(isNoiseWord(word)){\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\t// increment frequency\n\t\t\t\tInt cnt = (Int) termFreqMap.get(word);\n\t\t\t\tif (cnt == null) {\n\t\t\t\t\ttermFreqMap.put(word, new Int());\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tcnt.x++;\n\t\t\t\t}\n\t\t\t}\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f49ac1854a94b947e36bbdaffa355cb7707aa768","date":1164746802,"type":4,"author":"Otis Gospodnetic","isMerge":false,"pathNew":"/dev/null","pathOld":"contrib/similarity/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map,String).mjava","sourceNew":null,"sourceOld":"\t/**\n\t * Adds term frequencies found by tokenizing text from reader into the Map words\n\t * @param r a source of text to be tokenized\n\t * @param termFreqMap a Map of terms and their frequencies\n\t * @param fieldName Used by analyzer for any special per-field analysis\n\t */\n\tprivate void addTermFrequencies(Reader r, Map termFreqMap, String fieldName)\n\t\tthrows IOException\n\t{\n\t\t   TokenStream ts = analyzer.tokenStream(fieldName, r);\n\t\t\torg.apache.lucene.analysis.Token token;\n\t\t\tint tokenCount=0;\n\t\t\twhile ((token = ts.next()) != null) { // for every token\n\t\t\t\tString word = token.termText();\n\t\t\t\ttokenCount++;\n\t\t\t\tif(tokenCount>maxNumTokensParsed)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif(isNoiseWord(word)){\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\t// increment frequency\n\t\t\t\tInt cnt = (Int) termFreqMap.get(word);\n\t\t\t\tif (cnt == null) {\n\t\t\t\t\ttermFreqMap.put(word, new Int());\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tcnt.x++;\n\t\t\t\t}\n\t\t\t}\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"f49ac1854a94b947e36bbdaffa355cb7707aa768":["aa8dcd4adc562c6e8da67c42872ef9194f8598fa"],"aa8dcd4adc562c6e8da67c42872ef9194f8598fa":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["f49ac1854a94b947e36bbdaffa355cb7707aa768"]},"commit2Childs":{"f49ac1854a94b947e36bbdaffa355cb7707aa768":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"aa8dcd4adc562c6e8da67c42872ef9194f8598fa":["f49ac1854a94b947e36bbdaffa355cb7707aa768"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["aa8dcd4adc562c6e8da67c42872ef9194f8598fa"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}