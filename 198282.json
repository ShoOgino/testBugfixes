{"path":"lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[Document],Analyzer,Term).mjava","commits":[{"id":"2c6dc1a64ac36088ccb8d5e20b74c48c8d3bba27","date":1306166545,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[Document],Analyzer,Term).mjava","pathOld":"/dev/null","sourceNew":"  public int updateDocuments(Iterable<Document> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert writer.testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segment == null) {\n      // this call is synchronized on IndexWriter.segmentInfos\n      segment = writer.newSegmentName();\n      assert numDocsInRAM == 0;\n    }\n\n    int docCount = 0;\n    try {\n      for(Document doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n\n            if (!aborting) {\n              // One of the documents hit a non-aborting\n              // exception (eg something happened during\n              // analysis).  We now go and mark any docs\n              // from this batch that we had already indexed\n              // as deleted:\n              int docID = docState.docID;\n              final int endDocID = docID - docCount;\n              while (docID > endDocID) {\n                deleteDocID(docID);\n                docID--;\n              }\n\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n              fieldInfos.revertUncommitted();\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["c00afe74a80796ed1f30a9509b150ff104746a1f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","date":1306767085,"type":0,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[Document],Analyzer,Term).mjava","pathOld":"/dev/null","sourceNew":"  public int updateDocuments(Iterable<Document> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert writer.testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segment == null) {\n      // this call is synchronized on IndexWriter.segmentInfos\n      segment = writer.newSegmentName();\n      assert numDocsInRAM == 0;\n    }\n\n    int docCount = 0;\n    try {\n      for(Document doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n\n            if (!aborting) {\n              // One of the documents hit a non-aborting\n              // exception (eg something happened during\n              // analysis).  We now go and mark any docs\n              // from this batch that we had already indexed\n              // as deleted:\n              int docID = docState.docID;\n              final int endDocID = docID - docCount;\n              while (docID > endDocID) {\n                deleteDocID(docID);\n                docID--;\n              }\n\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n              fieldInfos.revertUncommitted();\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2e10cb22a8bdb44339e282925a29182bb2f3174d","date":1306841137,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[Document],Analyzer,Term).mjava","pathOld":"/dev/null","sourceNew":"  public int updateDocuments(Iterable<Document> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert writer.testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segment == null) {\n      // this call is synchronized on IndexWriter.segmentInfos\n      segment = writer.newSegmentName();\n      assert numDocsInRAM == 0;\n    }\n\n    int docCount = 0;\n    try {\n      for(Document doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n\n            if (!aborting) {\n              // One of the documents hit a non-aborting\n              // exception (eg something happened during\n              // analysis).  We now go and mark any docs\n              // from this batch that we had already indexed\n              // as deleted:\n              int docID = docState.docID;\n              final int endDocID = docID - docCount;\n              while (docID > endDocID) {\n                deleteDocID(docID);\n                docID--;\n              }\n\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n              fieldInfos.revertUncommitted();\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c00afe74a80796ed1f30a9509b150ff104746a1f","date":1312881735,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[Document],Analyzer,Term).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[Document],Analyzer,Term).mjava","sourceNew":"  public int updateDocuments(Iterable<Document> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert writer.testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segment == null) {\n      // this call is synchronized on IndexWriter.segmentInfos\n      segment = writer.newSegmentName();\n      assert numDocsInRAM == 0;\n      if (INFO_VERBOSE) {\n        message(Thread.currentThread().getName() + \" init seg=\" + segment + \" delQueue=\" + deleteQueue);  \n      }\n    }\n    if (INFO_VERBOSE) {\n      message(Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segment);\n    }\n    int docCount = 0;\n    try {\n      for(Document doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n\n            if (!aborting) {\n              // One of the documents hit a non-aborting\n              // exception (eg something happened during\n              // analysis).  We now go and mark any docs\n              // from this batch that we had already indexed\n              // as deleted:\n              int docID = docState.docID;\n              final int endDocID = docID - docCount;\n              while (docID > endDocID) {\n                deleteDocID(docID);\n                docID--;\n              }\n\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n              fieldInfos.revertUncommitted();\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","sourceOld":"  public int updateDocuments(Iterable<Document> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert writer.testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segment == null) {\n      // this call is synchronized on IndexWriter.segmentInfos\n      segment = writer.newSegmentName();\n      assert numDocsInRAM == 0;\n    }\n\n    int docCount = 0;\n    try {\n      for(Document doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n\n            if (!aborting) {\n              // One of the documents hit a non-aborting\n              // exception (eg something happened during\n              // analysis).  We now go and mark any docs\n              // from this batch that we had already indexed\n              // as deleted:\n              int docID = docState.docID;\n              final int endDocID = docID - docCount;\n              while (docID > endDocID) {\n                deleteDocID(docID);\n                docID--;\n              }\n\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n              fieldInfos.revertUncommitted();\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","bugFix":["2c6dc1a64ac36088ccb8d5e20b74c48c8d3bba27"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1509f151d7692d84fae414b2b799ac06ba60fcb4","date":1314451621,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[Document],Analyzer,Term).mjava","sourceNew":"  public int updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert writer.testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segment == null) {\n      // this call is synchronized on IndexWriter.segmentInfos\n      segment = writer.newSegmentName();\n      assert numDocsInRAM == 0;\n      if (INFO_VERBOSE) {\n        message(Thread.currentThread().getName() + \" init seg=\" + segment + \" delQueue=\" + deleteQueue);  \n      }\n    }\n    if (INFO_VERBOSE) {\n      message(Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segment);\n    }\n    int docCount = 0;\n    try {\n      for(Iterable<? extends IndexableField> doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n\n            if (!aborting) {\n              // One of the documents hit a non-aborting\n              // exception (eg something happened during\n              // analysis).  We now go and mark any docs\n              // from this batch that we had already indexed\n              // as deleted:\n              int docID = docState.docID;\n              final int endDocID = docID - docCount;\n              while (docID > endDocID) {\n                deleteDocID(docID);\n                docID--;\n              }\n\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n              fieldInfos.revertUncommitted();\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","sourceOld":"  public int updateDocuments(Iterable<Document> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert writer.testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segment == null) {\n      // this call is synchronized on IndexWriter.segmentInfos\n      segment = writer.newSegmentName();\n      assert numDocsInRAM == 0;\n      if (INFO_VERBOSE) {\n        message(Thread.currentThread().getName() + \" init seg=\" + segment + \" delQueue=\" + deleteQueue);  \n      }\n    }\n    if (INFO_VERBOSE) {\n      message(Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segment);\n    }\n    int docCount = 0;\n    try {\n      for(Document doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n\n            if (!aborting) {\n              // One of the documents hit a non-aborting\n              // exception (eg something happened during\n              // analysis).  We now go and mark any docs\n              // from this batch that we had already indexed\n              // as deleted:\n              int docID = docState.docID;\n              final int endDocID = docID - docCount;\n              while (docID > endDocID) {\n                deleteDocID(docID);\n                docID--;\n              }\n\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n              fieldInfos.revertUncommitted();\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":["ac7fa87956e618e2e88572544ae87078647f6351","ac7fa87956e618e2e88572544ae87078647f6351"],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"2c6dc1a64ac36088ccb8d5e20b74c48c8d3bba27":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","2c6dc1a64ac36088ccb8d5e20b74c48c8d3bba27"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["c00afe74a80796ed1f30a9509b150ff104746a1f"],"c00afe74a80796ed1f30a9509b150ff104746a1f":["2c6dc1a64ac36088ccb8d5e20b74c48c8d3bba27"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"2e10cb22a8bdb44339e282925a29182bb2f3174d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","2c6dc1a64ac36088ccb8d5e20b74c48c8d3bba27"]},"commit2Childs":{"2c6dc1a64ac36088ccb8d5e20b74c48c8d3bba27":["5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","c00afe74a80796ed1f30a9509b150ff104746a1f","2e10cb22a8bdb44339e282925a29182bb2f3174d"],"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["2c6dc1a64ac36088ccb8d5e20b74c48c8d3bba27","5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","2e10cb22a8bdb44339e282925a29182bb2f3174d"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"c00afe74a80796ed1f30a9509b150ff104746a1f":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"2e10cb22a8bdb44339e282925a29182bb2f3174d":[]},"heads":["5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","cd5edd1f2b162a5cfa08efd17851a07373a96817","2e10cb22a8bdb44339e282925a29182bb2f3174d"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}