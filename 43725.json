{"path":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","commits":[{"id":"c903c3d15906a3da96b8c0c2fb704491005fdbdb","date":1453508227,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = MultiFields.getLiveDocs(fromSearcher.getIndexReader());\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : MultiFields.getLiveDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !term.startsWith(prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = MultiFields.getLiveDocs(fromSearcher.getIndexReader());\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : MultiFields.getLiveDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !term.startsWith(prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a258fbb26824fd104ed795e5d9033d2d040049ee","date":1453508252,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = MultiFields.getLiveDocs(fromSearcher.getIndexReader());\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : MultiFields.getLiveDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !term.startsWith(prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = MultiFields.getLiveDocs(fromSearcher.getIndexReader());\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : MultiFields.getLiveDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !term.startsWith(prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c26f00b574427b55127e869b935845554afde1fa","date":1310252513,"type":1,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = MultiFields.getLiveDocs(fromSearcher.getIndexReader());\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : MultiFields.getLiveDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !term.startsWith(prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = MultiFields.getLiveDocs(fromSearcher.getIndexReader());\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : MultiFields.getLiveDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !term.startsWith(prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3cc749c053615f5871f3b95715fe292f34e70a53","date":1321470575,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = MultiFields.getLiveDocs(fromSearcher.getIndexReader());\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : MultiFields.getLiveDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !term.startsWith(prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = MultiFields.getLiveDocs(fromSearcher.getIndexReader());\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : MultiFields.getLiveDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !term.startsWith(prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"61f30939a6ca0891c7b0c0f34aa43800bd4c9a15","date":1322511317,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = MultiFields.getLiveDocs(fromSearcher.getIndexReader());\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : MultiFields.getLiveDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = MultiFields.getLiveDocs(fromSearcher.getIndexReader());\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : MultiFields.getLiveDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !term.startsWith(prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":["487de3f55283f58d7e02a16993f8be55bbe32061"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0445bcd8433e331f296f5502fc089b336cbac3a6","date":1322630375,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = MultiFields.getLiveDocs(fromSearcher.getIndexReader());\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : MultiFields.getLiveDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.fastSet(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.fastSet(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = MultiFields.getLiveDocs(fromSearcher.getIndexReader());\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : MultiFields.getLiveDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":["487de3f55283f58d7e02a16993f8be55bbe32061"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"872cff1d3a554e0cd64014cd97f88d3002b0f491","date":1323024658,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = MultiFields.getLiveDocs(fromSearcher.getIndexReader());\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : MultiFields.getLiveDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum, false);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum, false);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.fastSet(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.fastSet(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = MultiFields.getLiveDocs(fromSearcher.getIndexReader());\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : MultiFields.getLiveDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.fastSet(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.fastSet(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":["02331260bb246364779cb6f04919ca47900d01bb"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b65b350ca9588f9fc76ce7d6804160d06c45ff42","date":1323026297,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = MultiFields.getLiveDocs(fromSearcher.getIndexReader());\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : MultiFields.getLiveDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum, false);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum, false);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.fastSet(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.fastSet(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = MultiFields.getLiveDocs(fromSearcher.getIndexReader());\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : MultiFields.getLiveDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.fastSet(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.fastSet(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"96d207426bd26fa5c1014e26d21d87603aea68b7","date":1327944562,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = fromSearcher.getAtomicReader().fields();\n      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getAtomicReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getAtomicReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getAtomicReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum, false);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum, false);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.fastSet(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.fastSet(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = MultiFields.getLiveDocs(fromSearcher.getIndexReader());\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : MultiFields.getLiveDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum, false);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum, false);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.fastSet(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.fastSet(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5cab9a86bd67202d20b6adc463008c8e982b070a","date":1327966443,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = fromSearcher.getAtomicReader().fields();\n      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getAtomicReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getAtomicReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getAtomicReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum, false);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum, false);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.fastSet(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.fastSet(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = MultiFields.getLiveDocs(fromSearcher.getIndexReader());\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : MultiFields.getLiveDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum, false);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum, false);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.fastSet(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.fastSet(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d08eba3d52b63561ebf936481ce73e6b6a14aa03","date":1333879759,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      InvertedFields fromFields = fromSearcher.getAtomicReader().fields();\n      InvertedFields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getAtomicReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getAtomicReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getAtomicReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum, false);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum, false);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.fastSet(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.fastSet(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = fromSearcher.getAtomicReader().fields();\n      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getAtomicReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getAtomicReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getAtomicReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum, false);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum, false);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.fastSet(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.fastSet(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf","date":1333892281,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = fromSearcher.getAtomicReader().fields();\n      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getAtomicReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getAtomicReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getAtomicReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum, false);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum, false);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.fastSet(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.fastSet(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      InvertedFields fromFields = fromSearcher.getAtomicReader().fields();\n      InvertedFields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getAtomicReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getAtomicReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getAtomicReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum, false);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum, false);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.fastSet(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.fastSet(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"02331260bb246364779cb6f04919ca47900d01bb","date":1343749884,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = fromSearcher.getAtomicReader().fields();\n      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getAtomicReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getAtomicReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getAtomicReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum, 0);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum, 0);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.fastSet(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.fastSet(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = fromSearcher.getAtomicReader().fields();\n      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getAtomicReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getAtomicReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getAtomicReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum, false);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum, false);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.fastSet(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.fastSet(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":["872cff1d3a554e0cd64014cd97f88d3002b0f491"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","date":1343768312,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = fromSearcher.getAtomicReader().fields();\n      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getAtomicReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getAtomicReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getAtomicReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum, 0);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum, 0);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.fastSet(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.fastSet(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = fromSearcher.getAtomicReader().fields();\n      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getAtomicReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getAtomicReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getAtomicReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum, false);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum, false);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.fastSet(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.fastSet(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d6f074e73200c07d54f242d3880a8da5a35ff97b","date":1344507653,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = fromSearcher.getAtomicReader().fields();\n      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getAtomicReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getAtomicReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getAtomicReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum, 0);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum, 0);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.fastSet(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.fastSet(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = fromSearcher.getAtomicReader().fields();\n      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getAtomicReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getAtomicReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getAtomicReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum, false);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum, false);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.fastSet(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.fastSet(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"15250ca94ba8ab3bcdd476daf6bf3f3febb92640","date":1355200097,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = fromSearcher.getAtomicReader().fields();\n      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getAtomicReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getAtomicReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getAtomicReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum, DocsEnum.FLAG_NONE);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum, DocsEnum.FLAG_NONE);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.fastSet(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.fastSet(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = fromSearcher.getAtomicReader().fields();\n      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getAtomicReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getAtomicReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getAtomicReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum, 0);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum, 0);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.fastSet(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.fastSet(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d3fcb70cf561547c7bb1506e0cf32ca7b1287064","date":1357616416,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = fromSearcher.getAtomicReader().fields();\n      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getAtomicReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getAtomicReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getAtomicReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum, DocsEnum.FLAG_NONE);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum, DocsEnum.FLAG_NONE);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.fastSet(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.fastSet(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = fromSearcher.getAtomicReader().fields();\n      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getAtomicReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getAtomicReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getAtomicReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum, 0);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum, 0);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.fastSet(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.fastSet(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"eee5f2a24465d2c9a5f86ab84b7c35041a30fda8","date":1373996650,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = fromSearcher.getAtomicReader().fields();\n      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getAtomicReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getAtomicReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getAtomicReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum, DocsEnum.FLAG_NONE);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum, DocsEnum.FLAG_NONE);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.fastSet(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.fastSet(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = fromSearcher.getAtomicReader().fields();\n      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getAtomicReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getAtomicReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getAtomicReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum, DocsEnum.FLAG_NONE);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum, DocsEnum.FLAG_NONE);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.fastSet(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.fastSet(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":["487de3f55283f58d7e02a16993f8be55bbe32061"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"716d18f3a9b0993bc679d7fa7abdc9bfb03411ec","date":1376375609,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = fromSearcher.getAtomicReader().fields();\n      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getAtomicReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getAtomicReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getAtomicReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum, DocsEnum.FLAG_NONE);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum, DocsEnum.FLAG_NONE);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.fastSet(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.fastSet(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = fromSearcher.getAtomicReader().fields();\n      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getAtomicReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getAtomicReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getAtomicReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum, DocsEnum.FLAG_NONE);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum, DocsEnum.FLAG_NONE);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.fastSet(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.fastSet(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a69cf7f1b4cac5d5b1363402b565cd535f13e6a1","date":1392536197,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      FixedBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = fromSearcher.getAtomicReader().fields();\n      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getAtomicReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getAtomicReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getAtomicReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum, DocsEnum.FLAG_NONE);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new FixedBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.addAllTo(new BitDocSet(resultBits));\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = ((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum, DocsEnum.FLAG_NONE);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.set(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.set(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        BitDocSet bitSet = new BitDocSet(resultBits);\n        for (DocSet set : resultList) {\n          set.addAllTo(bitSet);\n        }\n        return bitSet;\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = fromSearcher.getAtomicReader().fields();\n      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getAtomicReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getAtomicReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getAtomicReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum, DocsEnum.FLAG_NONE);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum, DocsEnum.FLAG_NONE);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.fastSet(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.fastSet(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":["487de3f55283f58d7e02a16993f8be55bbe32061"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      FixedBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = fromSearcher.getAtomicReader().fields();\n      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getAtomicReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getAtomicReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getAtomicReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum, DocsEnum.FLAG_NONE);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new FixedBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.addAllTo(new BitDocSet(resultBits));\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = ((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum, DocsEnum.FLAG_NONE);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.set(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.set(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        BitDocSet bitSet = new BitDocSet(resultBits);\n        for (DocSet set : resultList) {\n          set.addAllTo(bitSet);\n        }\n        return bitSet;\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      FixedBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = fromSearcher.getAtomicReader().fields();\n      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getAtomicReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getAtomicReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getAtomicReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum, DocsEnum.FLAG_NONE);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new FixedBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.addAllTo(new BitDocSet(resultBits));\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = ((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum, DocsEnum.FLAG_NONE);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.set(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.set(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        BitDocSet bitSet = new BitDocSet(resultBits);\n        for (DocSet set : resultList) {\n          set.addAllTo(bitSet);\n        }\n        return bitSet;\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":["487de3f55283f58d7e02a16993f8be55bbe32061"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      FixedBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = fromSearcher.getLeafReader().fields();\n      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getLeafReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getLeafReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getLeafReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum, DocsEnum.FLAG_NONE);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new FixedBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.addAllTo(new BitDocSet(resultBits));\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = ((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum, DocsEnum.FLAG_NONE);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.set(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.set(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        BitDocSet bitSet = new BitDocSet(resultBits);\n        for (DocSet set : resultList) {\n          set.addAllTo(bitSet);\n        }\n        return bitSet;\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      FixedBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = fromSearcher.getAtomicReader().fields();\n      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getAtomicReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getAtomicReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getAtomicReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum, DocsEnum.FLAG_NONE);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new FixedBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.addAllTo(new BitDocSet(resultBits));\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = ((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum, DocsEnum.FLAG_NONE);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.set(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.set(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        BitDocSet bitSet = new BitDocSet(resultBits);\n        for (DocSet set : resultList) {\n          set.addAllTo(bitSet);\n        }\n        return bitSet;\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"51f5280f31484820499077f41fcdfe92d527d9dc","date":1423229122,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      FixedBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = fromSearcher.getLeafReader().fields();\n      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getLeafReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getLeafReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getLeafReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.postingsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.postingsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.postingsEnum = fromDeState.termsEnum.postings(null, fromDeState.postingsEnum, PostingsEnum.FLAG_NONE);\n          PostingsEnum postingsEnum = fromDeState.postingsEnum;\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new FixedBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.addAllTo(new BitDocSet(resultBits));\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = ((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.postingsEnum = toDeState.termsEnum.postings(toDeState.liveDocs, toDeState.postingsEnum, PostingsEnum.FLAG_NONE);\n              PostingsEnum postingsEnum = toDeState.postingsEnum;\n\n              if (postingsEnum instanceof MultiPostingsEnum) {\n                MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n                int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.postingsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.set(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.set(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        BitDocSet bitSet = new BitDocSet(resultBits);\n        for (DocSet set : resultList) {\n          set.addAllTo(bitSet);\n        }\n        return bitSet;\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      FixedBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = fromSearcher.getLeafReader().fields();\n      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getLeafReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getLeafReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getLeafReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum, DocsEnum.FLAG_NONE);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new FixedBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.addAllTo(new BitDocSet(resultBits));\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = ((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum, DocsEnum.FLAG_NONE);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.set(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.set(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        BitDocSet bitSet = new BitDocSet(resultBits);\n        for (DocSet set : resultList) {\n          set.addAllTo(bitSet);\n        }\n        return bitSet;\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":["487de3f55283f58d7e02a16993f8be55bbe32061"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e73063b92d958076ef4ae8beb5f493e8ccdcecb4","date":1424177215,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      FixedBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = fromSearcher.getLeafReader().fields();\n      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getLeafReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getLeafReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getLeafReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.postingsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.postingsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.postingsEnum = fromDeState.termsEnum.postings(null, fromDeState.postingsEnum, PostingsEnum.NONE);\n          PostingsEnum postingsEnum = fromDeState.postingsEnum;\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new FixedBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.addAllTo(new BitDocSet(resultBits));\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = ((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.postingsEnum = toDeState.termsEnum.postings(toDeState.liveDocs, toDeState.postingsEnum, PostingsEnum.NONE);\n              PostingsEnum postingsEnum = toDeState.postingsEnum;\n\n              if (postingsEnum instanceof MultiPostingsEnum) {\n                MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n                int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.postingsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.set(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.set(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        BitDocSet bitSet = new BitDocSet(resultBits);\n        for (DocSet set : resultList) {\n          set.addAllTo(bitSet);\n        }\n        return bitSet;\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      FixedBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = fromSearcher.getLeafReader().fields();\n      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getLeafReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getLeafReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getLeafReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.postingsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.postingsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.postingsEnum = fromDeState.termsEnum.postings(null, fromDeState.postingsEnum, PostingsEnum.FLAG_NONE);\n          PostingsEnum postingsEnum = fromDeState.postingsEnum;\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new FixedBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.addAllTo(new BitDocSet(resultBits));\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = ((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.postingsEnum = toDeState.termsEnum.postings(toDeState.liveDocs, toDeState.postingsEnum, PostingsEnum.FLAG_NONE);\n              PostingsEnum postingsEnum = toDeState.postingsEnum;\n\n              if (postingsEnum instanceof MultiPostingsEnum) {\n                MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n                int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.postingsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.set(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.set(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        BitDocSet bitSet = new BitDocSet(resultBits);\n        for (DocSet set : resultList) {\n          set.addAllTo(bitSet);\n        }\n        return bitSet;\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82","date":1428522487,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      FixedBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = fromSearcher.getLeafReader().fields();\n      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getLeafReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getLeafReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getLeafReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.postingsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.postingsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.postingsEnum = fromDeState.termsEnum.postings(null, fromDeState.postingsEnum, PostingsEnum.NONE);\n          PostingsEnum postingsEnum = fromDeState.postingsEnum;\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new FixedBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.addAllTo(new BitDocSet(resultBits));\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = ((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.postingsEnum = toDeState.termsEnum.postings(toDeState.liveDocs, toDeState.postingsEnum, PostingsEnum.NONE);\n              PostingsEnum postingsEnum = toDeState.postingsEnum;\n\n              if (postingsEnum instanceof MultiPostingsEnum) {\n                MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n                int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.postingsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.set(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.set(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        BitDocSet bitSet = new BitDocSet(resultBits);\n        for (DocSet set : resultList) {\n          set.addAllTo(bitSet);\n        }\n        return bitSet;\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      FixedBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = fromSearcher.getLeafReader().fields();\n      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getLeafReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator(null);\n      TermsEnum  toTermsEnum = toTerms.iterator(null);\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getLeafReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getLeafReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.postingsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.postingsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.postingsEnum = fromDeState.termsEnum.postings(null, fromDeState.postingsEnum, PostingsEnum.NONE);\n          PostingsEnum postingsEnum = fromDeState.postingsEnum;\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new FixedBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.addAllTo(new BitDocSet(resultBits));\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = ((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.postingsEnum = toDeState.termsEnum.postings(toDeState.liveDocs, toDeState.postingsEnum, PostingsEnum.NONE);\n              PostingsEnum postingsEnum = toDeState.postingsEnum;\n\n              if (postingsEnum instanceof MultiPostingsEnum) {\n                MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n                int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.postingsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.set(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.set(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        BitDocSet bitSet = new BitDocSet(resultBits);\n        for (DocSet set : resultList) {\n          set.addAllTo(bitSet);\n        }\n        return bitSet;\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":["487de3f55283f58d7e02a16993f8be55bbe32061"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0f4464508ee83288c8c4585b533f9faaa93aa314","date":1435240759,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      FixedBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = fromSearcher.getLeafReader().fields();\n      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getLeafReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getLeafReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getLeafReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.postingsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.postingsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.postingsEnum = fromDeState.termsEnum.postings(fromDeState.postingsEnum, PostingsEnum.NONE);\n          PostingsEnum postingsEnum = fromDeState.postingsEnum;\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new FixedBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.addAllTo(new BitDocSet(resultBits));\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = ((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.postingsEnum = toDeState.termsEnum.postings(toDeState.postingsEnum, PostingsEnum.NONE);\n              toDeState.postingsEnum = BitsFilteredPostingsEnum.wrap(toDeState.postingsEnum, toDeState.liveDocs);\n              PostingsEnum postingsEnum = toDeState.postingsEnum;\n\n              if (postingsEnum instanceof MultiPostingsEnum) {\n                MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n                int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.postingsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.set(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.set(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        BitDocSet bitSet = new BitDocSet(resultBits);\n        for (DocSet set : resultList) {\n          set.addAllTo(bitSet);\n        }\n        return bitSet;\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      FixedBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = fromSearcher.getLeafReader().fields();\n      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getLeafReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getLeafReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getLeafReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.postingsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.postingsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.postingsEnum = fromDeState.termsEnum.postings(null, fromDeState.postingsEnum, PostingsEnum.NONE);\n          PostingsEnum postingsEnum = fromDeState.postingsEnum;\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new FixedBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.addAllTo(new BitDocSet(resultBits));\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = ((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.postingsEnum = toDeState.termsEnum.postings(toDeState.liveDocs, toDeState.postingsEnum, PostingsEnum.NONE);\n              PostingsEnum postingsEnum = toDeState.postingsEnum;\n\n              if (postingsEnum instanceof MultiPostingsEnum) {\n                MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n                int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.postingsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.set(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.set(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        BitDocSet bitSet = new BitDocSet(resultBits);\n        for (DocSet set : resultList) {\n          set.addAllTo(bitSet);\n        }\n        return bitSet;\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":["487de3f55283f58d7e02a16993f8be55bbe32061"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e07c409cff8701e4dc3d45934b021a949a5a8822","date":1475694629,"type":3,"author":"yonik","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      FixedBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = fromSearcher.getSlowAtomicReader().fields();\n      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getSlowAtomicReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getSlowAtomicReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getSlowAtomicReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.postingsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.postingsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.postingsEnum = fromDeState.termsEnum.postings(fromDeState.postingsEnum, PostingsEnum.NONE);\n          PostingsEnum postingsEnum = fromDeState.postingsEnum;\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new FixedBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.addAllTo(new BitDocSet(resultBits));\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = ((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.postingsEnum = toDeState.termsEnum.postings(toDeState.postingsEnum, PostingsEnum.NONE);\n              toDeState.postingsEnum = BitsFilteredPostingsEnum.wrap(toDeState.postingsEnum, toDeState.liveDocs);\n              PostingsEnum postingsEnum = toDeState.postingsEnum;\n\n              if (postingsEnum instanceof MultiPostingsEnum) {\n                MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n                int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.postingsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.set(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.set(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        BitDocSet bitSet = new BitDocSet(resultBits);\n        for (DocSet set : resultList) {\n          set.addAllTo(bitSet);\n        }\n        return bitSet;\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      FixedBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = fromSearcher.getLeafReader().fields();\n      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getLeafReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getLeafReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getLeafReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.postingsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.postingsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.postingsEnum = fromDeState.termsEnum.postings(fromDeState.postingsEnum, PostingsEnum.NONE);\n          PostingsEnum postingsEnum = fromDeState.postingsEnum;\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new FixedBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.addAllTo(new BitDocSet(resultBits));\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = ((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.postingsEnum = toDeState.termsEnum.postings(toDeState.postingsEnum, PostingsEnum.NONE);\n              toDeState.postingsEnum = BitsFilteredPostingsEnum.wrap(toDeState.postingsEnum, toDeState.liveDocs);\n              PostingsEnum postingsEnum = toDeState.postingsEnum;\n\n              if (postingsEnum instanceof MultiPostingsEnum) {\n                MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n                int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.postingsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.set(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.set(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        BitDocSet bitSet = new BitDocSet(resultBits);\n        for (DocSet set : resultList) {\n          set.addAllTo(bitSet);\n        }\n        return bitSet;\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":["487de3f55283f58d7e02a16993f8be55bbe32061"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      FixedBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = fromSearcher.getSlowAtomicReader().fields();\n      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getSlowAtomicReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getSlowAtomicReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getSlowAtomicReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.postingsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.postingsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.postingsEnum = fromDeState.termsEnum.postings(fromDeState.postingsEnum, PostingsEnum.NONE);\n          PostingsEnum postingsEnum = fromDeState.postingsEnum;\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new FixedBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.addAllTo(new BitDocSet(resultBits));\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = ((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.postingsEnum = toDeState.termsEnum.postings(toDeState.postingsEnum, PostingsEnum.NONE);\n              toDeState.postingsEnum = BitsFilteredPostingsEnum.wrap(toDeState.postingsEnum, toDeState.liveDocs);\n              PostingsEnum postingsEnum = toDeState.postingsEnum;\n\n              if (postingsEnum instanceof MultiPostingsEnum) {\n                MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n                int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.postingsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.set(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.set(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        BitDocSet bitSet = new BitDocSet(resultBits);\n        for (DocSet set : resultList) {\n          set.addAllTo(bitSet);\n        }\n        return bitSet;\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      FixedBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = fromSearcher.getLeafReader().fields();\n      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getLeafReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getLeafReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getLeafReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.postingsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.postingsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.postingsEnum = fromDeState.termsEnum.postings(fromDeState.postingsEnum, PostingsEnum.NONE);\n          PostingsEnum postingsEnum = fromDeState.postingsEnum;\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new FixedBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.addAllTo(new BitDocSet(resultBits));\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = ((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.postingsEnum = toDeState.termsEnum.postings(toDeState.postingsEnum, PostingsEnum.NONE);\n              toDeState.postingsEnum = BitsFilteredPostingsEnum.wrap(toDeState.postingsEnum, toDeState.liveDocs);\n              PostingsEnum postingsEnum = toDeState.postingsEnum;\n\n              if (postingsEnum instanceof MultiPostingsEnum) {\n                MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n                int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.postingsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.set(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.set(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        BitDocSet bitSet = new BitDocSet(resultBits);\n        for (DocSet set : resultList) {\n          set.addAllTo(bitSet);\n        }\n        return bitSet;\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b","date":1497408244,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      FixedBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n\n      LeafReader fromReader = fromSearcher.getSlowAtomicReader();\n      LeafReader toReader = fromSearcher==toSearcher ? fromReader : toSearcher.getSlowAtomicReader();\n      Terms terms = fromReader.terms(fromField);\n      Terms toTerms = toReader.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getSlowAtomicReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getSlowAtomicReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.postingsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.postingsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.postingsEnum = fromDeState.termsEnum.postings(fromDeState.postingsEnum, PostingsEnum.NONE);\n          PostingsEnum postingsEnum = fromDeState.postingsEnum;\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new FixedBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.addAllTo(new BitDocSet(resultBits));\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = ((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.postingsEnum = toDeState.termsEnum.postings(toDeState.postingsEnum, PostingsEnum.NONE);\n              toDeState.postingsEnum = BitsFilteredPostingsEnum.wrap(toDeState.postingsEnum, toDeState.liveDocs);\n              PostingsEnum postingsEnum = toDeState.postingsEnum;\n\n              if (postingsEnum instanceof MultiPostingsEnum) {\n                MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n                int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.postingsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.set(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.set(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        BitDocSet bitSet = new BitDocSet(resultBits);\n        for (DocSet set : resultList) {\n          set.addAllTo(bitSet);\n        }\n        return bitSet;\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      FixedBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = fromSearcher.getSlowAtomicReader().fields();\n      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getSlowAtomicReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getSlowAtomicReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getSlowAtomicReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.postingsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.postingsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.postingsEnum = fromDeState.termsEnum.postings(fromDeState.postingsEnum, PostingsEnum.NONE);\n          PostingsEnum postingsEnum = fromDeState.postingsEnum;\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new FixedBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.addAllTo(new BitDocSet(resultBits));\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = ((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.postingsEnum = toDeState.termsEnum.postings(toDeState.postingsEnum, PostingsEnum.NONE);\n              toDeState.postingsEnum = BitsFilteredPostingsEnum.wrap(toDeState.postingsEnum, toDeState.liveDocs);\n              PostingsEnum postingsEnum = toDeState.postingsEnum;\n\n              if (postingsEnum instanceof MultiPostingsEnum) {\n                MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n                int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.postingsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.set(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.set(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        BitDocSet bitSet = new BitDocSet(resultBits);\n        for (DocSet set : resultList) {\n          set.addAllTo(bitSet);\n        }\n        return bitSet;\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":["487de3f55283f58d7e02a16993f8be55bbe32061"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9","date":1498028748,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      FixedBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n\n      LeafReader fromReader = fromSearcher.getSlowAtomicReader();\n      LeafReader toReader = fromSearcher==toSearcher ? fromReader : toSearcher.getSlowAtomicReader();\n      Terms terms = fromReader.terms(fromField);\n      Terms toTerms = toReader.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getSlowAtomicReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getSlowAtomicReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.postingsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.postingsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.postingsEnum = fromDeState.termsEnum.postings(fromDeState.postingsEnum, PostingsEnum.NONE);\n          PostingsEnum postingsEnum = fromDeState.postingsEnum;\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new FixedBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.addAllTo(new BitDocSet(resultBits));\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = ((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.postingsEnum = toDeState.termsEnum.postings(toDeState.postingsEnum, PostingsEnum.NONE);\n              toDeState.postingsEnum = BitsFilteredPostingsEnum.wrap(toDeState.postingsEnum, toDeState.liveDocs);\n              PostingsEnum postingsEnum = toDeState.postingsEnum;\n\n              if (postingsEnum instanceof MultiPostingsEnum) {\n                MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n                int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.postingsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.set(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.set(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        BitDocSet bitSet = new BitDocSet(resultBits);\n        for (DocSet set : resultList) {\n          set.addAllTo(bitSet);\n        }\n        return bitSet;\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      FixedBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = fromSearcher.getSlowAtomicReader().fields();\n      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getSlowAtomicReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getSlowAtomicReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getSlowAtomicReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.postingsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.postingsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.postingsEnum = fromDeState.termsEnum.postings(fromDeState.postingsEnum, PostingsEnum.NONE);\n          PostingsEnum postingsEnum = fromDeState.postingsEnum;\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new FixedBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.addAllTo(new BitDocSet(resultBits));\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = ((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.postingsEnum = toDeState.termsEnum.postings(toDeState.postingsEnum, PostingsEnum.NONE);\n              toDeState.postingsEnum = BitsFilteredPostingsEnum.wrap(toDeState.postingsEnum, toDeState.liveDocs);\n              PostingsEnum postingsEnum = toDeState.postingsEnum;\n\n              if (postingsEnum instanceof MultiPostingsEnum) {\n                MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n                int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.postingsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.set(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.set(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        BitDocSet bitSet = new BitDocSet(resultBits);\n        for (DocSet set : resultList) {\n          set.addAllTo(bitSet);\n        }\n        return bitSet;\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      FixedBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n\n      LeafReader fromReader = fromSearcher.getSlowAtomicReader();\n      LeafReader toReader = fromSearcher==toSearcher ? fromReader : toSearcher.getSlowAtomicReader();\n      Terms terms = fromReader.terms(fromField);\n      Terms toTerms = toReader.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getSlowAtomicReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getSlowAtomicReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.postingsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.postingsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.postingsEnum = fromDeState.termsEnum.postings(fromDeState.postingsEnum, PostingsEnum.NONE);\n          PostingsEnum postingsEnum = fromDeState.postingsEnum;\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new FixedBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.addAllTo(new BitDocSet(resultBits));\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = ((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.postingsEnum = toDeState.termsEnum.postings(toDeState.postingsEnum, PostingsEnum.NONE);\n              toDeState.postingsEnum = BitsFilteredPostingsEnum.wrap(toDeState.postingsEnum, toDeState.liveDocs);\n              PostingsEnum postingsEnum = toDeState.postingsEnum;\n\n              if (postingsEnum instanceof MultiPostingsEnum) {\n                MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n                int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.postingsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.set(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.set(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        BitDocSet bitSet = new BitDocSet(resultBits);\n        for (DocSet set : resultList) {\n          set.addAllTo(bitSet);\n        }\n        return bitSet;\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      FixedBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = fromSearcher.getSlowAtomicReader().fields();\n      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getSlowAtomicReader().fields();\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getSlowAtomicReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getSlowAtomicReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.postingsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.postingsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.postingsEnum = fromDeState.termsEnum.postings(fromDeState.postingsEnum, PostingsEnum.NONE);\n          PostingsEnum postingsEnum = fromDeState.postingsEnum;\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new FixedBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.addAllTo(new BitDocSet(resultBits));\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = ((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.postingsEnum = toDeState.termsEnum.postings(toDeState.postingsEnum, PostingsEnum.NONE);\n              toDeState.postingsEnum = BitsFilteredPostingsEnum.wrap(toDeState.postingsEnum, toDeState.liveDocs);\n              PostingsEnum postingsEnum = toDeState.postingsEnum;\n\n              if (postingsEnum instanceof MultiPostingsEnum) {\n                MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n                int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.postingsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.set(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.set(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        BitDocSet bitSet = new BitDocSet(resultBits);\n        for (DocSet set : resultList) {\n          set.addAllTo(bitSet);\n        }\n        return bitSet;\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"487de3f55283f58d7e02a16993f8be55bbe32061","date":1502123368,"type":3,"author":"yonik","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      SchemaField fromSchemaField = fromSearcher.getSchema().getField(fromField);\n      SchemaField toSchemaField = toSearcher.getSchema().getField(toField);\n\n      boolean usePoints = false;\n      if (toSchemaField.getType().isPointField()) {\n        if (!fromSchemaField.hasDocValues()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"join from field \" + fromSchemaField + \" should have docValues to join with points field \" + toSchemaField);\n        }\n        usePoints = true;\n      }\n\n      if (!usePoints) {\n        return getDocSetEnumerate();\n      }\n\n      // point fields\n      GraphPointsCollector collector = new GraphPointsCollector(fromSchemaField, null, null);\n      fromSearcher.search(q, collector);\n      Query resultQ = collector.getResultQuery(toSchemaField, false);\n      // don't cache the resulting docSet... the query may be very large.  Better to cache the results of the join query itself\n      DocSet result = resultQ==null ? DocSet.EMPTY : toSearcher.getDocSetNC(resultQ, null);\n      return result;\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      FixedBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n\n      LeafReader fromReader = fromSearcher.getSlowAtomicReader();\n      LeafReader toReader = fromSearcher==toSearcher ? fromReader : toSearcher.getSlowAtomicReader();\n      Terms terms = fromReader.terms(fromField);\n      Terms toTerms = toReader.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getSlowAtomicReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getSlowAtomicReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.postingsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.postingsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.postingsEnum = fromDeState.termsEnum.postings(fromDeState.postingsEnum, PostingsEnum.NONE);\n          PostingsEnum postingsEnum = fromDeState.postingsEnum;\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new FixedBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.addAllTo(new BitDocSet(resultBits));\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = ((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.postingsEnum = toDeState.termsEnum.postings(toDeState.postingsEnum, PostingsEnum.NONE);\n              toDeState.postingsEnum = BitsFilteredPostingsEnum.wrap(toDeState.postingsEnum, toDeState.liveDocs);\n              PostingsEnum postingsEnum = toDeState.postingsEnum;\n\n              if (postingsEnum instanceof MultiPostingsEnum) {\n                MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n                int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.postingsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.set(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.set(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        BitDocSet bitSet = new BitDocSet(resultBits);\n        for (DocSet set : resultList) {\n          set.addAllTo(bitSet);\n        }\n        return bitSet;\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":["0f4464508ee83288c8c4585b533f9faaa93aa314","0445bcd8433e331f296f5502fc089b336cbac3a6","634f330c54fd3f9f491d52036dc3f40b4f4d8934","e07c409cff8701e4dc3d45934b021a949a5a8822","51f5280f31484820499077f41fcdfe92d527d9dc","843b845d397272dbafe8b80ebb8f9336d94568ef","e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b","eee5f2a24465d2c9a5f86ab84b7c35041a30fda8","0a773283ef5eab2e9c7136eeb66574a4b7a2dc82","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","fd9cc9d77712aba3662f24632df7539ab75e3667","f8f944ac3fe3f9d40d825177507fb381d2b106b3","fafef7c83fe8e0b3ca9298d5d75d6b943dc28153","61f30939a6ca0891c7b0c0f34aa43800bd4c9a15","a69cf7f1b4cac5d5b1363402b565cd535f13e6a1"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7a23cf16c8fa265dc0a564adcabb55e3f054e0ac","date":1502192746,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      SchemaField fromSchemaField = fromSearcher.getSchema().getField(fromField);\n      SchemaField toSchemaField = toSearcher.getSchema().getField(toField);\n\n      boolean usePoints = false;\n      if (toSchemaField.getType().isPointField()) {\n        if (!fromSchemaField.hasDocValues()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"join from field \" + fromSchemaField + \" should have docValues to join with points field \" + toSchemaField);\n        }\n        usePoints = true;\n      }\n\n      if (!usePoints) {\n        return getDocSetEnumerate();\n      }\n\n      // point fields\n      GraphPointsCollector collector = new GraphPointsCollector(fromSchemaField, null, null);\n      fromSearcher.search(q, collector);\n      Query resultQ = collector.getResultQuery(toSchemaField, false);\n      // don't cache the resulting docSet... the query may be very large.  Better to cache the results of the join query itself\n      DocSet result = resultQ==null ? DocSet.EMPTY : toSearcher.getDocSetNC(resultQ, null);\n      return result;\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      FixedBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n\n      LeafReader fromReader = fromSearcher.getSlowAtomicReader();\n      LeafReader toReader = fromSearcher==toSearcher ? fromReader : toSearcher.getSlowAtomicReader();\n      Terms terms = fromReader.terms(fromField);\n      Terms toTerms = toReader.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getSlowAtomicReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getSlowAtomicReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.postingsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.postingsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.postingsEnum = fromDeState.termsEnum.postings(fromDeState.postingsEnum, PostingsEnum.NONE);\n          PostingsEnum postingsEnum = fromDeState.postingsEnum;\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new FixedBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.addAllTo(new BitDocSet(resultBits));\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = ((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.postingsEnum = toDeState.termsEnum.postings(toDeState.postingsEnum, PostingsEnum.NONE);\n              toDeState.postingsEnum = BitsFilteredPostingsEnum.wrap(toDeState.postingsEnum, toDeState.liveDocs);\n              PostingsEnum postingsEnum = toDeState.postingsEnum;\n\n              if (postingsEnum instanceof MultiPostingsEnum) {\n                MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n                int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.postingsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.set(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.set(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        BitDocSet bitSet = new BitDocSet(resultBits);\n        for (DocSet set : resultList) {\n          set.addAllTo(bitSet);\n        }\n        return bitSet;\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"58884af1f68e9d61c217c753fbd6266d86a63b14","date":1502363401,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      SchemaField fromSchemaField = fromSearcher.getSchema().getField(fromField);\n      SchemaField toSchemaField = toSearcher.getSchema().getField(toField);\n\n      boolean usePoints = false;\n      if (toSchemaField.getType().isPointField()) {\n        if (!fromSchemaField.hasDocValues()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"join from field \" + fromSchemaField + \" should have docValues to join with points field \" + toSchemaField);\n        }\n        usePoints = true;\n      }\n\n      if (!usePoints) {\n        return getDocSetEnumerate();\n      }\n\n      // point fields\n      GraphPointsCollector collector = new GraphPointsCollector(fromSchemaField, null, null);\n      fromSearcher.search(q, collector);\n      Query resultQ = collector.getResultQuery(toSchemaField, false);\n      // don't cache the resulting docSet... the query may be very large.  Better to cache the results of the join query itself\n      DocSet result = resultQ==null ? DocSet.EMPTY : toSearcher.getDocSetNC(resultQ, null);\n      return result;\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      FixedBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n\n      LeafReader fromReader = fromSearcher.getSlowAtomicReader();\n      LeafReader toReader = fromSearcher==toSearcher ? fromReader : toSearcher.getSlowAtomicReader();\n      Terms terms = fromReader.terms(fromField);\n      Terms toTerms = toReader.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = fromSearcher.getSlowAtomicReader().getLiveDocs();\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getSlowAtomicReader().getLiveDocs();\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.postingsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.postingsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !StringHelper.startsWith(term, prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.postingsEnum = fromDeState.termsEnum.postings(fromDeState.postingsEnum, PostingsEnum.NONE);\n          PostingsEnum postingsEnum = fromDeState.postingsEnum;\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n              while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid+base)) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          } else {\n            int docid;\n            while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (fastForRandomSet.exists(docid)) {\n                intersects = true;\n                break;\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new FixedBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.addAllTo(new BitDocSet(resultBits));\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = ((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.postingsEnum = toDeState.termsEnum.postings(toDeState.postingsEnum, PostingsEnum.NONE);\n              toDeState.postingsEnum = BitsFilteredPostingsEnum.wrap(toDeState.postingsEnum, toDeState.liveDocs);\n              PostingsEnum postingsEnum = toDeState.postingsEnum;\n\n              if (postingsEnum instanceof MultiPostingsEnum) {\n                MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n                int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.postingsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    resultListDocs++;\n                    resultBits.set(docid + base);\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  resultListDocs++;\n                  resultBits.set(docid);\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        BitDocSet bitSet = new BitDocSet(resultBits);\n        for (DocSet set : resultList) {\n          set.addAllTo(bitSet);\n        }\n        return bitSet;\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d67f43c293f81a92c10131e75761f4e4e968b06c","date":1584534689,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      SchemaField fromSchemaField = fromSearcher.getSchema().getField(fromField);\n      SchemaField toSchemaField = toSearcher.getSchema().getField(toField);\n\n      boolean usePoints = false;\n      if (toSchemaField.getType().isPointField()) {\n        if (!fromSchemaField.hasDocValues()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"join from field \" + fromSchemaField + \" should have docValues to join with points field \" + toSchemaField);\n        }\n        usePoints = true;\n      }\n\n      if (!usePoints) {\n        return getDocSetEnumerate();\n      }\n\n      // point fields\n      GraphPointsCollector collector = new GraphPointsCollector(fromSchemaField, null, null);\n      fromSearcher.search(q, collector);\n      Query resultQ = collector.getResultQuery(toSchemaField, false);\n      // don't cache the resulting docSet... the query may be very large.  Better to cache the results of the join query itself\n      DocSet result = resultQ==null ? DocSet.empty() : toSearcher.getDocSetNC(resultQ, null);\n      return result;\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      SchemaField fromSchemaField = fromSearcher.getSchema().getField(fromField);\n      SchemaField toSchemaField = toSearcher.getSchema().getField(toField);\n\n      boolean usePoints = false;\n      if (toSchemaField.getType().isPointField()) {\n        if (!fromSchemaField.hasDocValues()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"join from field \" + fromSchemaField + \" should have docValues to join with points field \" + toSchemaField);\n        }\n        usePoints = true;\n      }\n\n      if (!usePoints) {\n        return getDocSetEnumerate();\n      }\n\n      // point fields\n      GraphPointsCollector collector = new GraphPointsCollector(fromSchemaField, null, null);\n      fromSearcher.search(q, collector);\n      Query resultQ = collector.getResultQuery(toSchemaField, false);\n      // don't cache the resulting docSet... the query may be very large.  Better to cache the results of the join query itself\n      DocSet result = resultQ==null ? DocSet.EMPTY : toSearcher.getDocSetNC(resultQ, null);\n      return result;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1e1491db4de13536b70146fc5a8f03101f0f84de","date":1593014806,"type":5,"author":"Atri Sharma","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/JoinQuery.JoinQueryWeight#getDocSet().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      SchemaField fromSchemaField = fromSearcher.getSchema().getField(fromField);\n      SchemaField toSchemaField = toSearcher.getSchema().getField(toField);\n\n      boolean usePoints = false;\n      if (toSchemaField.getType().isPointField()) {\n        if (!fromSchemaField.hasDocValues()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"join from field \" + fromSchemaField + \" should have docValues to join with points field \" + toSchemaField);\n        }\n        usePoints = true;\n      }\n\n      if (!usePoints) {\n        return getDocSetEnumerate();\n      }\n\n      // point fields\n      GraphPointsCollector collector = new GraphPointsCollector(fromSchemaField, null, null);\n      fromSearcher.search(q, collector);\n      Query resultQ = collector.getResultQuery(toSchemaField, false);\n      // don't cache the resulting docSet... the query may be very large.  Better to cache the results of the join query itself\n      DocSet result = resultQ==null ? DocSet.empty() : toSearcher.getDocSetNC(resultQ, null);\n      return result;\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      SchemaField fromSchemaField = fromSearcher.getSchema().getField(fromField);\n      SchemaField toSchemaField = toSearcher.getSchema().getField(toField);\n\n      boolean usePoints = false;\n      if (toSchemaField.getType().isPointField()) {\n        if (!fromSchemaField.hasDocValues()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"join from field \" + fromSchemaField + \" should have docValues to join with points field \" + toSchemaField);\n        }\n        usePoints = true;\n      }\n\n      if (!usePoints) {\n        return getDocSetEnumerate();\n      }\n\n      // point fields\n      GraphPointsCollector collector = new GraphPointsCollector(fromSchemaField, null, null);\n      fromSearcher.search(q, collector);\n      Query resultQ = collector.getResultQuery(toSchemaField, false);\n      // don't cache the resulting docSet... the query may be very large.  Better to cache the results of the join query itself\n      DocSet result = resultQ==null ? DocSet.empty() : toSearcher.getDocSetNC(resultQ, null);\n      return result;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4"],"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":["02331260bb246364779cb6f04919ca47900d01bb","15250ca94ba8ab3bcdd476daf6bf3f3febb92640"],"e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b":["e07c409cff8701e4dc3d45934b021a949a5a8822"],"1e1491db4de13536b70146fc5a8f03101f0f84de":["d67f43c293f81a92c10131e75761f4e4e968b06c"],"c26f00b574427b55127e869b935845554afde1fa":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"61f30939a6ca0891c7b0c0f34aa43800bd4c9a15":["3cc749c053615f5871f3b95715fe292f34e70a53"],"96d207426bd26fa5c1014e26d21d87603aea68b7":["872cff1d3a554e0cd64014cd97f88d3002b0f491"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"58884af1f68e9d61c217c753fbd6266d86a63b14":["28288370235ed02234a64753cdbf0c6ec096304a","487de3f55283f58d7e02a16993f8be55bbe32061"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf":["d08eba3d52b63561ebf936481ce73e6b6a14aa03"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["a69cf7f1b4cac5d5b1363402b565cd535f13e6a1"],"716d18f3a9b0993bc679d7fa7abdc9bfb03411ec":["15250ca94ba8ab3bcdd476daf6bf3f3febb92640","eee5f2a24465d2c9a5f86ab84b7c35041a30fda8"],"0445bcd8433e331f296f5502fc089b336cbac3a6":["61f30939a6ca0891c7b0c0f34aa43800bd4c9a15"],"487de3f55283f58d7e02a16993f8be55bbe32061":["28288370235ed02234a64753cdbf0c6ec096304a"],"3cc749c053615f5871f3b95715fe292f34e70a53":["c26f00b574427b55127e869b935845554afde1fa"],"d08eba3d52b63561ebf936481ce73e6b6a14aa03":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"872cff1d3a554e0cd64014cd97f88d3002b0f491":["0445bcd8433e331f296f5502fc089b336cbac3a6"],"e07c409cff8701e4dc3d45934b021a949a5a8822":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"51f5280f31484820499077f41fcdfe92d527d9dc":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["872cff1d3a554e0cd64014cd97f88d3002b0f491","96d207426bd26fa5c1014e26d21d87603aea68b7"],"28288370235ed02234a64753cdbf0c6ec096304a":["e07c409cff8701e4dc3d45934b021a949a5a8822","e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["0f4464508ee83288c8c4585b533f9faaa93aa314","e07c409cff8701e4dc3d45934b021a949a5a8822"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"e73063b92d958076ef4ae8beb5f493e8ccdcecb4":["51f5280f31484820499077f41fcdfe92d527d9dc"],"7a23cf16c8fa265dc0a564adcabb55e3f054e0ac":["2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9","487de3f55283f58d7e02a16993f8be55bbe32061"],"a69cf7f1b4cac5d5b1363402b565cd535f13e6a1":["eee5f2a24465d2c9a5f86ab84b7c35041a30fda8"],"15250ca94ba8ab3bcdd476daf6bf3f3febb92640":["02331260bb246364779cb6f04919ca47900d01bb"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"b65b350ca9588f9fc76ce7d6804160d06c45ff42":["0445bcd8433e331f296f5502fc089b336cbac3a6","872cff1d3a554e0cd64014cd97f88d3002b0f491"],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":["e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf","02331260bb246364779cb6f04919ca47900d01bb"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf","02331260bb246364779cb6f04919ca47900d01bb"],"a258fbb26824fd104ed795e5d9033d2d040049ee":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"eee5f2a24465d2c9a5f86ab84b7c35041a30fda8":["15250ca94ba8ab3bcdd476daf6bf3f3febb92640"],"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9":["e07c409cff8701e4dc3d45934b021a949a5a8822","e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["1e1491db4de13536b70146fc5a8f03101f0f84de"],"d67f43c293f81a92c10131e75761f4e4e968b06c":["487de3f55283f58d7e02a16993f8be55bbe32061"],"02331260bb246364779cb6f04919ca47900d01bb":["e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf"]},"commit2Childs":{"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":[],"e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b":["28288370235ed02234a64753cdbf0c6ec096304a","2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9"],"1e1491db4de13536b70146fc5a8f03101f0f84de":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"c26f00b574427b55127e869b935845554afde1fa":["3cc749c053615f5871f3b95715fe292f34e70a53"],"61f30939a6ca0891c7b0c0f34aa43800bd4c9a15":["0445bcd8433e331f296f5502fc089b336cbac3a6"],"96d207426bd26fa5c1014e26d21d87603aea68b7":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["51f5280f31484820499077f41fcdfe92d527d9dc"],"58884af1f68e9d61c217c753fbd6266d86a63b14":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c26f00b574427b55127e869b935845554afde1fa","c903c3d15906a3da96b8c0c2fb704491005fdbdb","a258fbb26824fd104ed795e5d9033d2d040049ee"],"e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf":["b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b","02331260bb246364779cb6f04919ca47900d01bb"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"716d18f3a9b0993bc679d7fa7abdc9bfb03411ec":[],"0445bcd8433e331f296f5502fc089b336cbac3a6":["872cff1d3a554e0cd64014cd97f88d3002b0f491","b65b350ca9588f9fc76ce7d6804160d06c45ff42"],"3cc749c053615f5871f3b95715fe292f34e70a53":["61f30939a6ca0891c7b0c0f34aa43800bd4c9a15"],"487de3f55283f58d7e02a16993f8be55bbe32061":["58884af1f68e9d61c217c753fbd6266d86a63b14","7a23cf16c8fa265dc0a564adcabb55e3f054e0ac","d67f43c293f81a92c10131e75761f4e4e968b06c"],"872cff1d3a554e0cd64014cd97f88d3002b0f491":["96d207426bd26fa5c1014e26d21d87603aea68b7","5cab9a86bd67202d20b6adc463008c8e982b070a","b65b350ca9588f9fc76ce7d6804160d06c45ff42"],"d08eba3d52b63561ebf936481ce73e6b6a14aa03":["e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf"],"e07c409cff8701e4dc3d45934b021a949a5a8822":["e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b","28288370235ed02234a64753cdbf0c6ec096304a","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9"],"51f5280f31484820499077f41fcdfe92d527d9dc":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4"],"28288370235ed02234a64753cdbf0c6ec096304a":["58884af1f68e9d61c217c753fbd6266d86a63b14","487de3f55283f58d7e02a16993f8be55bbe32061"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["d08eba3d52b63561ebf936481ce73e6b6a14aa03"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"e73063b92d958076ef4ae8beb5f493e8ccdcecb4":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["e07c409cff8701e4dc3d45934b021a949a5a8822","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"7a23cf16c8fa265dc0a564adcabb55e3f054e0ac":[],"15250ca94ba8ab3bcdd476daf6bf3f3febb92640":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","716d18f3a9b0993bc679d7fa7abdc9bfb03411ec","eee5f2a24465d2c9a5f86ab84b7c35041a30fda8"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["c26f00b574427b55127e869b935845554afde1fa"],"a69cf7f1b4cac5d5b1363402b565cd535f13e6a1":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"b65b350ca9588f9fc76ce7d6804160d06c45ff42":[],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":[],"d6f074e73200c07d54f242d3880a8da5a35ff97b":[],"a258fbb26824fd104ed795e5d9033d2d040049ee":[],"eee5f2a24465d2c9a5f86ab84b7c35041a30fda8":["716d18f3a9b0993bc679d7fa7abdc9bfb03411ec","a69cf7f1b4cac5d5b1363402b565cd535f13e6a1"],"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9":["7a23cf16c8fa265dc0a564adcabb55e3f054e0ac"],"02331260bb246364779cb6f04919ca47900d01bb":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","15250ca94ba8ab3bcdd476daf6bf3f3febb92640","b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b"],"d67f43c293f81a92c10131e75761f4e4e968b06c":["1e1491db4de13536b70146fc5a8f03101f0f84de"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","58884af1f68e9d61c217c753fbd6266d86a63b14","716d18f3a9b0993bc679d7fa7abdc9bfb03411ec","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","7a23cf16c8fa265dc0a564adcabb55e3f054e0ac","b65b350ca9588f9fc76ce7d6804160d06c45ff42","b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b","a258fbb26824fd104ed795e5d9033d2d040049ee","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}