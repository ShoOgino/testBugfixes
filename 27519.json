{"path":"lucene/src/java/org/apache/lucene/index/LogMergePolicy#findMergesForOptimizeMaxNumSegments(SegmentInfos,int,int).mjava","commits":[{"id":"8723a3379c08ae0b4ba0cf4f246306f86ad8362d","date":1287582680,"type":0,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/LogMergePolicy#findMergesForOptimizeMaxNumSegments(SegmentInfos,int,int).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Returns the merges necessary to optimize the index. This method constraints\n   * the returned merges only by the {@code maxNumSegments} parameter, and\n   * guaranteed that exactly that number of segments will remain in the index.\n   */\n  private MergeSpecification findMergesForOptimizeMaxNumSegments(SegmentInfos infos, int maxNumSegments, int last) throws IOException {\n    MergeSpecification spec = new MergeSpecification();\n    \n    // First, enroll all \"full\" merges (size\n    // mergeFactor) to potentially be run concurrently:\n    while (last - maxNumSegments + 1 >= mergeFactor) {\n      spec.add(new OneMerge(infos.range(last-mergeFactor, last), useCompoundFile));\n      last -= mergeFactor;\n    }\n\n    // Only if there are no full merges pending do we\n    // add a final partial (< mergeFactor segments) merge:\n    if (0 == spec.merges.size()) {\n      if (maxNumSegments == 1) {\n\n        // Since we must optimize down to 1 segment, the\n        // choice is simple:\n        if (last > 1 || !isOptimized(infos.info(0))) {\n          spec.add(new OneMerge(infos.range(0, last), useCompoundFile));\n        }\n      } else if (last > maxNumSegments) {\n\n        // Take care to pick a partial merge that is\n        // least cost, but does not make the index too\n        // lopsided.  If we always just picked the\n        // partial tail then we could produce a highly\n        // lopsided index over time:\n\n        // We must merge this many segments to leave\n        // maxNumSegments in the index (from when\n        // optimize was first kicked off):\n        final int finalMergeSize = last - maxNumSegments + 1;\n\n        // Consider all possible starting points:\n        long bestSize = 0;\n        int bestStart = 0;\n\n        for(int i=0;i<last-finalMergeSize+1;i++) {\n          long sumSize = 0;\n          for(int j=0;j<finalMergeSize;j++)\n            sumSize += size(infos.info(j+i));\n          if (i == 0 || (sumSize < 2*size(infos.info(i-1)) && sumSize < bestSize)) {\n            bestStart = i;\n            bestSize = sumSize;\n          }\n        }\n\n        spec.add(new OneMerge(infos.range(bestStart, bestStart+finalMergeSize), useCompoundFile));\n      }\n    }\n    return spec.merges.size() == 0 ? null : spec;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8017ab6544f30f93b106e419e7298173bad77f69","date":1287608126,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/LogMergePolicy#findMergesForOptimizeMaxNumSegments(SegmentInfos,int,int).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Returns the merges necessary to optimize the index. This method constraints\n   * the returned merges only by the {@code maxNumSegments} parameter, and\n   * guaranteed that exactly that number of segments will remain in the index.\n   */\n  private MergeSpecification findMergesForOptimizeMaxNumSegments(SegmentInfos infos, int maxNumSegments, int last) throws IOException {\n    MergeSpecification spec = new MergeSpecification();\n    \n    // First, enroll all \"full\" merges (size\n    // mergeFactor) to potentially be run concurrently:\n    while (last - maxNumSegments + 1 >= mergeFactor) {\n      spec.add(new OneMerge(infos.range(last-mergeFactor, last), useCompoundFile));\n      last -= mergeFactor;\n    }\n\n    // Only if there are no full merges pending do we\n    // add a final partial (< mergeFactor segments) merge:\n    if (0 == spec.merges.size()) {\n      if (maxNumSegments == 1) {\n\n        // Since we must optimize down to 1 segment, the\n        // choice is simple:\n        if (last > 1 || !isOptimized(infos.info(0))) {\n          spec.add(new OneMerge(infos.range(0, last), useCompoundFile));\n        }\n      } else if (last > maxNumSegments) {\n\n        // Take care to pick a partial merge that is\n        // least cost, but does not make the index too\n        // lopsided.  If we always just picked the\n        // partial tail then we could produce a highly\n        // lopsided index over time:\n\n        // We must merge this many segments to leave\n        // maxNumSegments in the index (from when\n        // optimize was first kicked off):\n        final int finalMergeSize = last - maxNumSegments + 1;\n\n        // Consider all possible starting points:\n        long bestSize = 0;\n        int bestStart = 0;\n\n        for(int i=0;i<last-finalMergeSize+1;i++) {\n          long sumSize = 0;\n          for(int j=0;j<finalMergeSize;j++)\n            sumSize += size(infos.info(j+i));\n          if (i == 0 || (sumSize < 2*size(infos.info(i-1)) && sumSize < bestSize)) {\n            bestStart = i;\n            bestSize = sumSize;\n          }\n        }\n\n        spec.add(new OneMerge(infos.range(bestStart, bestStart+finalMergeSize), useCompoundFile));\n      }\n    }\n    return spec.merges.size() == 0 ? null : spec;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5390d5f5bc8bf5d65eff4c1d596cf9547ead0c56","date":1290598569,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/LogMergePolicy#findMergesForOptimizeMaxNumSegments(SegmentInfos,int,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/LogMergePolicy#findMergesForOptimizeMaxNumSegments(SegmentInfos,int,int).mjava","sourceNew":"  /**\n   * Returns the merges necessary to optimize the index. This method constraints\n   * the returned merges only by the {@code maxNumSegments} parameter, and\n   * guaranteed that exactly that number of segments will remain in the index.\n   */\n  private MergeSpecification findMergesForOptimizeMaxNumSegments(SegmentInfos infos, int maxNumSegments, int last) throws IOException {\n    MergeSpecification spec = new MergeSpecification();\n    \n    // First, enroll all \"full\" merges (size\n    // mergeFactor) to potentially be run concurrently:\n    while (last - maxNumSegments + 1 >= mergeFactor) {\n      spec.add(makeOneMerge(infos, infos.range(last-mergeFactor, last)));\n      last -= mergeFactor;\n    }\n\n    // Only if there are no full merges pending do we\n    // add a final partial (< mergeFactor segments) merge:\n    if (0 == spec.merges.size()) {\n      if (maxNumSegments == 1) {\n\n        // Since we must optimize down to 1 segment, the\n        // choice is simple:\n        if (last > 1 || !isOptimized(infos.info(0))) {\n          spec.add(makeOneMerge(infos, infos.range(0, last)));\n        }\n      } else if (last > maxNumSegments) {\n\n        // Take care to pick a partial merge that is\n        // least cost, but does not make the index too\n        // lopsided.  If we always just picked the\n        // partial tail then we could produce a highly\n        // lopsided index over time:\n\n        // We must merge this many segments to leave\n        // maxNumSegments in the index (from when\n        // optimize was first kicked off):\n        final int finalMergeSize = last - maxNumSegments + 1;\n\n        // Consider all possible starting points:\n        long bestSize = 0;\n        int bestStart = 0;\n\n        for(int i=0;i<last-finalMergeSize+1;i++) {\n          long sumSize = 0;\n          for(int j=0;j<finalMergeSize;j++)\n            sumSize += size(infos.info(j+i));\n          if (i == 0 || (sumSize < 2*size(infos.info(i-1)) && sumSize < bestSize)) {\n            bestStart = i;\n            bestSize = sumSize;\n          }\n        }\n\n        spec.add(makeOneMerge(infos, infos.range(bestStart, bestStart+finalMergeSize)));\n      }\n    }\n    return spec.merges.size() == 0 ? null : spec;\n  }\n\n","sourceOld":"  /**\n   * Returns the merges necessary to optimize the index. This method constraints\n   * the returned merges only by the {@code maxNumSegments} parameter, and\n   * guaranteed that exactly that number of segments will remain in the index.\n   */\n  private MergeSpecification findMergesForOptimizeMaxNumSegments(SegmentInfos infos, int maxNumSegments, int last) throws IOException {\n    MergeSpecification spec = new MergeSpecification();\n    \n    // First, enroll all \"full\" merges (size\n    // mergeFactor) to potentially be run concurrently:\n    while (last - maxNumSegments + 1 >= mergeFactor) {\n      spec.add(new OneMerge(infos.range(last-mergeFactor, last), useCompoundFile));\n      last -= mergeFactor;\n    }\n\n    // Only if there are no full merges pending do we\n    // add a final partial (< mergeFactor segments) merge:\n    if (0 == spec.merges.size()) {\n      if (maxNumSegments == 1) {\n\n        // Since we must optimize down to 1 segment, the\n        // choice is simple:\n        if (last > 1 || !isOptimized(infos.info(0))) {\n          spec.add(new OneMerge(infos.range(0, last), useCompoundFile));\n        }\n      } else if (last > maxNumSegments) {\n\n        // Take care to pick a partial merge that is\n        // least cost, but does not make the index too\n        // lopsided.  If we always just picked the\n        // partial tail then we could produce a highly\n        // lopsided index over time:\n\n        // We must merge this many segments to leave\n        // maxNumSegments in the index (from when\n        // optimize was first kicked off):\n        final int finalMergeSize = last - maxNumSegments + 1;\n\n        // Consider all possible starting points:\n        long bestSize = 0;\n        int bestStart = 0;\n\n        for(int i=0;i<last-finalMergeSize+1;i++) {\n          long sumSize = 0;\n          for(int j=0;j<finalMergeSize;j++)\n            sumSize += size(infos.info(j+i));\n          if (i == 0 || (sumSize < 2*size(infos.info(i-1)) && sumSize < bestSize)) {\n            bestStart = i;\n            bestSize = sumSize;\n          }\n        }\n\n        spec.add(new OneMerge(infos.range(bestStart, bestStart+finalMergeSize), useCompoundFile));\n      }\n    }\n    return spec.merges.size() == 0 ? null : spec;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"38a62612cfa4e104080d89d7751a8f1a258ac335","date":1291442315,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/LogMergePolicy#findMergesForOptimizeMaxNumSegments(SegmentInfos,int,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/LogMergePolicy#findMergesForOptimizeMaxNumSegments(SegmentInfos,int,int).mjava","sourceNew":"  /**\n   * Returns the merges necessary to optimize the index. This method constraints\n   * the returned merges only by the {@code maxNumSegments} parameter, and\n   * guaranteed that exactly that number of segments will remain in the index.\n   */\n  private MergeSpecification findMergesForOptimizeMaxNumSegments(SegmentInfos infos, int maxNumSegments, int last) throws IOException {\n    MergeSpecification spec = new MergeSpecification();\n    \n    // First, enroll all \"full\" merges (size\n    // mergeFactor) to potentially be run concurrently:\n    while (last - maxNumSegments + 1 >= mergeFactor) {\n      spec.add(new OneMerge(infos.range(last - mergeFactor, last)));\n      last -= mergeFactor;\n    }\n\n    // Only if there are no full merges pending do we\n    // add a final partial (< mergeFactor segments) merge:\n    if (0 == spec.merges.size()) {\n      if (maxNumSegments == 1) {\n\n        // Since we must optimize down to 1 segment, the\n        // choice is simple:\n        if (last > 1 || !isOptimized(infos.info(0))) {\n          spec.add(new OneMerge(infos.range(0, last)));\n        }\n      } else if (last > maxNumSegments) {\n\n        // Take care to pick a partial merge that is\n        // least cost, but does not make the index too\n        // lopsided.  If we always just picked the\n        // partial tail then we could produce a highly\n        // lopsided index over time:\n\n        // We must merge this many segments to leave\n        // maxNumSegments in the index (from when\n        // optimize was first kicked off):\n        final int finalMergeSize = last - maxNumSegments + 1;\n\n        // Consider all possible starting points:\n        long bestSize = 0;\n        int bestStart = 0;\n\n        for(int i=0;i<last-finalMergeSize+1;i++) {\n          long sumSize = 0;\n          for(int j=0;j<finalMergeSize;j++)\n            sumSize += size(infos.info(j+i));\n          if (i == 0 || (sumSize < 2*size(infos.info(i-1)) && sumSize < bestSize)) {\n            bestStart = i;\n            bestSize = sumSize;\n          }\n        }\n\n        spec.add(new OneMerge(infos.range(bestStart, bestStart + finalMergeSize)));\n      }\n    }\n    return spec.merges.size() == 0 ? null : spec;\n  }\n\n","sourceOld":"  /**\n   * Returns the merges necessary to optimize the index. This method constraints\n   * the returned merges only by the {@code maxNumSegments} parameter, and\n   * guaranteed that exactly that number of segments will remain in the index.\n   */\n  private MergeSpecification findMergesForOptimizeMaxNumSegments(SegmentInfos infos, int maxNumSegments, int last) throws IOException {\n    MergeSpecification spec = new MergeSpecification();\n    \n    // First, enroll all \"full\" merges (size\n    // mergeFactor) to potentially be run concurrently:\n    while (last - maxNumSegments + 1 >= mergeFactor) {\n      spec.add(makeOneMerge(infos, infos.range(last-mergeFactor, last)));\n      last -= mergeFactor;\n    }\n\n    // Only if there are no full merges pending do we\n    // add a final partial (< mergeFactor segments) merge:\n    if (0 == spec.merges.size()) {\n      if (maxNumSegments == 1) {\n\n        // Since we must optimize down to 1 segment, the\n        // choice is simple:\n        if (last > 1 || !isOptimized(infos.info(0))) {\n          spec.add(makeOneMerge(infos, infos.range(0, last)));\n        }\n      } else if (last > maxNumSegments) {\n\n        // Take care to pick a partial merge that is\n        // least cost, but does not make the index too\n        // lopsided.  If we always just picked the\n        // partial tail then we could produce a highly\n        // lopsided index over time:\n\n        // We must merge this many segments to leave\n        // maxNumSegments in the index (from when\n        // optimize was first kicked off):\n        final int finalMergeSize = last - maxNumSegments + 1;\n\n        // Consider all possible starting points:\n        long bestSize = 0;\n        int bestStart = 0;\n\n        for(int i=0;i<last-finalMergeSize+1;i++) {\n          long sumSize = 0;\n          for(int j=0;j<finalMergeSize;j++)\n            sumSize += size(infos.info(j+i));\n          if (i == 0 || (sumSize < 2*size(infos.info(i-1)) && sumSize < bestSize)) {\n            bestStart = i;\n            bestSize = sumSize;\n          }\n        }\n\n        spec.add(makeOneMerge(infos, infos.range(bestStart, bestStart+finalMergeSize)));\n      }\n    }\n    return spec.merges.size() == 0 ? null : spec;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3bb13258feba31ab676502787ab2e1779f129b7a","date":1291596436,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/LogMergePolicy#findMergesForOptimizeMaxNumSegments(SegmentInfos,int,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/LogMergePolicy#findMergesForOptimizeMaxNumSegments(SegmentInfos,int,int).mjava","sourceNew":"  /**\n   * Returns the merges necessary to optimize the index. This method constraints\n   * the returned merges only by the {@code maxNumSegments} parameter, and\n   * guaranteed that exactly that number of segments will remain in the index.\n   */\n  private MergeSpecification findMergesForOptimizeMaxNumSegments(SegmentInfos infos, int maxNumSegments, int last) throws IOException {\n    MergeSpecification spec = new MergeSpecification();\n    \n    // First, enroll all \"full\" merges (size\n    // mergeFactor) to potentially be run concurrently:\n    while (last - maxNumSegments + 1 >= mergeFactor) {\n      spec.add(makeOneMerge(infos, infos.range(last-mergeFactor, last)));\n      last -= mergeFactor;\n    }\n\n    // Only if there are no full merges pending do we\n    // add a final partial (< mergeFactor segments) merge:\n    if (0 == spec.merges.size()) {\n      if (maxNumSegments == 1) {\n\n        // Since we must optimize down to 1 segment, the\n        // choice is simple:\n        if (last > 1 || !isOptimized(infos.info(0))) {\n          spec.add(makeOneMerge(infos, infos.range(0, last)));\n        }\n      } else if (last > maxNumSegments) {\n\n        // Take care to pick a partial merge that is\n        // least cost, but does not make the index too\n        // lopsided.  If we always just picked the\n        // partial tail then we could produce a highly\n        // lopsided index over time:\n\n        // We must merge this many segments to leave\n        // maxNumSegments in the index (from when\n        // optimize was first kicked off):\n        final int finalMergeSize = last - maxNumSegments + 1;\n\n        // Consider all possible starting points:\n        long bestSize = 0;\n        int bestStart = 0;\n\n        for(int i=0;i<last-finalMergeSize+1;i++) {\n          long sumSize = 0;\n          for(int j=0;j<finalMergeSize;j++)\n            sumSize += size(infos.info(j+i));\n          if (i == 0 || (sumSize < 2*size(infos.info(i-1)) && sumSize < bestSize)) {\n            bestStart = i;\n            bestSize = sumSize;\n          }\n        }\n\n        spec.add(makeOneMerge(infos, infos.range(bestStart, bestStart+finalMergeSize)));\n      }\n    }\n    return spec.merges.size() == 0 ? null : spec;\n  }\n\n","sourceOld":"  /**\n   * Returns the merges necessary to optimize the index. This method constraints\n   * the returned merges only by the {@code maxNumSegments} parameter, and\n   * guaranteed that exactly that number of segments will remain in the index.\n   */\n  private MergeSpecification findMergesForOptimizeMaxNumSegments(SegmentInfos infos, int maxNumSegments, int last) throws IOException {\n    MergeSpecification spec = new MergeSpecification();\n    \n    // First, enroll all \"full\" merges (size\n    // mergeFactor) to potentially be run concurrently:\n    while (last - maxNumSegments + 1 >= mergeFactor) {\n      spec.add(new OneMerge(infos.range(last-mergeFactor, last), useCompoundFile));\n      last -= mergeFactor;\n    }\n\n    // Only if there are no full merges pending do we\n    // add a final partial (< mergeFactor segments) merge:\n    if (0 == spec.merges.size()) {\n      if (maxNumSegments == 1) {\n\n        // Since we must optimize down to 1 segment, the\n        // choice is simple:\n        if (last > 1 || !isOptimized(infos.info(0))) {\n          spec.add(new OneMerge(infos.range(0, last), useCompoundFile));\n        }\n      } else if (last > maxNumSegments) {\n\n        // Take care to pick a partial merge that is\n        // least cost, but does not make the index too\n        // lopsided.  If we always just picked the\n        // partial tail then we could produce a highly\n        // lopsided index over time:\n\n        // We must merge this many segments to leave\n        // maxNumSegments in the index (from when\n        // optimize was first kicked off):\n        final int finalMergeSize = last - maxNumSegments + 1;\n\n        // Consider all possible starting points:\n        long bestSize = 0;\n        int bestStart = 0;\n\n        for(int i=0;i<last-finalMergeSize+1;i++) {\n          long sumSize = 0;\n          for(int j=0;j<finalMergeSize;j++)\n            sumSize += size(infos.info(j+i));\n          if (i == 0 || (sumSize < 2*size(infos.info(i-1)) && sumSize < bestSize)) {\n            bestStart = i;\n            bestSize = sumSize;\n          }\n        }\n\n        spec.add(new OneMerge(infos.range(bestStart, bestStart+finalMergeSize), useCompoundFile));\n      }\n    }\n    return spec.merges.size() == 0 ? null : spec;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4a69e5860d014751cc9329dfeb441a6d8fd1ed8e","date":1291833341,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/LogMergePolicy#findMergesForOptimizeMaxNumSegments(SegmentInfos,int,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/LogMergePolicy#findMergesForOptimizeMaxNumSegments(SegmentInfos,int,int).mjava","sourceNew":"  /**\n   * Returns the merges necessary to optimize the index. This method constraints\n   * the returned merges only by the {@code maxNumSegments} parameter, and\n   * guaranteed that exactly that number of segments will remain in the index.\n   */\n  private MergeSpecification findMergesForOptimizeMaxNumSegments(SegmentInfos infos, int maxNumSegments, int last) throws IOException {\n    MergeSpecification spec = new MergeSpecification();\n    \n    // First, enroll all \"full\" merges (size\n    // mergeFactor) to potentially be run concurrently:\n    while (last - maxNumSegments + 1 >= mergeFactor) {\n      spec.add(new OneMerge(infos.range(last - mergeFactor, last)));\n      last -= mergeFactor;\n    }\n\n    // Only if there are no full merges pending do we\n    // add a final partial (< mergeFactor segments) merge:\n    if (0 == spec.merges.size()) {\n      if (maxNumSegments == 1) {\n\n        // Since we must optimize down to 1 segment, the\n        // choice is simple:\n        if (last > 1 || !isOptimized(infos.info(0))) {\n          spec.add(new OneMerge(infos.range(0, last)));\n        }\n      } else if (last > maxNumSegments) {\n\n        // Take care to pick a partial merge that is\n        // least cost, but does not make the index too\n        // lopsided.  If we always just picked the\n        // partial tail then we could produce a highly\n        // lopsided index over time:\n\n        // We must merge this many segments to leave\n        // maxNumSegments in the index (from when\n        // optimize was first kicked off):\n        final int finalMergeSize = last - maxNumSegments + 1;\n\n        // Consider all possible starting points:\n        long bestSize = 0;\n        int bestStart = 0;\n\n        for(int i=0;i<last-finalMergeSize+1;i++) {\n          long sumSize = 0;\n          for(int j=0;j<finalMergeSize;j++)\n            sumSize += size(infos.info(j+i));\n          if (i == 0 || (sumSize < 2*size(infos.info(i-1)) && sumSize < bestSize)) {\n            bestStart = i;\n            bestSize = sumSize;\n          }\n        }\n\n        spec.add(new OneMerge(infos.range(bestStart, bestStart + finalMergeSize)));\n      }\n    }\n    return spec.merges.size() == 0 ? null : spec;\n  }\n\n","sourceOld":"  /**\n   * Returns the merges necessary to optimize the index. This method constraints\n   * the returned merges only by the {@code maxNumSegments} parameter, and\n   * guaranteed that exactly that number of segments will remain in the index.\n   */\n  private MergeSpecification findMergesForOptimizeMaxNumSegments(SegmentInfos infos, int maxNumSegments, int last) throws IOException {\n    MergeSpecification spec = new MergeSpecification();\n    \n    // First, enroll all \"full\" merges (size\n    // mergeFactor) to potentially be run concurrently:\n    while (last - maxNumSegments + 1 >= mergeFactor) {\n      spec.add(makeOneMerge(infos, infos.range(last-mergeFactor, last)));\n      last -= mergeFactor;\n    }\n\n    // Only if there are no full merges pending do we\n    // add a final partial (< mergeFactor segments) merge:\n    if (0 == spec.merges.size()) {\n      if (maxNumSegments == 1) {\n\n        // Since we must optimize down to 1 segment, the\n        // choice is simple:\n        if (last > 1 || !isOptimized(infos.info(0))) {\n          spec.add(makeOneMerge(infos, infos.range(0, last)));\n        }\n      } else if (last > maxNumSegments) {\n\n        // Take care to pick a partial merge that is\n        // least cost, but does not make the index too\n        // lopsided.  If we always just picked the\n        // partial tail then we could produce a highly\n        // lopsided index over time:\n\n        // We must merge this many segments to leave\n        // maxNumSegments in the index (from when\n        // optimize was first kicked off):\n        final int finalMergeSize = last - maxNumSegments + 1;\n\n        // Consider all possible starting points:\n        long bestSize = 0;\n        int bestStart = 0;\n\n        for(int i=0;i<last-finalMergeSize+1;i++) {\n          long sumSize = 0;\n          for(int j=0;j<finalMergeSize;j++)\n            sumSize += size(infos.info(j+i));\n          if (i == 0 || (sumSize < 2*size(infos.info(i-1)) && sumSize < bestSize)) {\n            bestStart = i;\n            bestSize = sumSize;\n          }\n        }\n\n        spec.add(makeOneMerge(infos, infos.range(bestStart, bestStart+finalMergeSize)));\n      }\n    }\n    return spec.merges.size() == 0 ? null : spec;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":0,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/LogMergePolicy#findMergesForOptimizeMaxNumSegments(SegmentInfos,int,int).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Returns the merges necessary to optimize the index. This method constraints\n   * the returned merges only by the {@code maxNumSegments} parameter, and\n   * guaranteed that exactly that number of segments will remain in the index.\n   */\n  private MergeSpecification findMergesForOptimizeMaxNumSegments(SegmentInfos infos, int maxNumSegments, int last) throws IOException {\n    MergeSpecification spec = new MergeSpecification();\n\n    // First, enroll all \"full\" merges (size\n    // mergeFactor) to potentially be run concurrently:\n    while (last - maxNumSegments + 1 >= mergeFactor) {\n      spec.add(new OneMerge(infos.range(last - mergeFactor, last)));\n      last -= mergeFactor;\n    }\n\n    // Only if there are no full merges pending do we\n    // add a final partial (< mergeFactor segments) merge:\n    if (0 == spec.merges.size()) {\n      if (maxNumSegments == 1) {\n\n        // Since we must optimize down to 1 segment, the\n        // choice is simple:\n        if (last > 1 || !isOptimized(infos.info(0))) {\n          spec.add(new OneMerge(infos.range(0, last)));\n        }\n      } else if (last > maxNumSegments) {\n\n        // Take care to pick a partial merge that is\n        // least cost, but does not make the index too\n        // lopsided.  If we always just picked the\n        // partial tail then we could produce a highly\n        // lopsided index over time:\n\n        // We must merge this many segments to leave\n        // maxNumSegments in the index (from when\n        // optimize was first kicked off):\n        final int finalMergeSize = last - maxNumSegments + 1;\n\n        // Consider all possible starting points:\n        long bestSize = 0;\n        int bestStart = 0;\n\n        for(int i=0;i<last-finalMergeSize+1;i++) {\n          long sumSize = 0;\n          for(int j=0;j<finalMergeSize;j++)\n            sumSize += size(infos.info(j+i));\n          if (i == 0 || (sumSize < 2*size(infos.info(i-1)) && sumSize < bestSize)) {\n            bestStart = i;\n            bestSize = sumSize;\n          }\n        }\n\n        spec.add(new OneMerge(infos.range(bestStart, bestStart + finalMergeSize)));\n      }\n    }\n    return spec.merges.size() == 0 ? null : spec;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"786a4d25ca958a1f315a9d6a74f0441fdafcd522","date":1305734256,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/LogMergePolicy#findMergesForOptimizeMaxNumSegments(SegmentInfos,int,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/LogMergePolicy#findMergesForOptimizeMaxNumSegments(SegmentInfos,int,int).mjava","sourceNew":"  /**\n   * Returns the merges necessary to optimize the index. This method constraints\n   * the returned merges only by the {@code maxNumSegments} parameter, and\n   * guaranteed that exactly that number of segments will remain in the index.\n   */\n  private MergeSpecification findMergesForOptimizeMaxNumSegments(SegmentInfos infos, int maxNumSegments, int last) throws IOException {\n    MergeSpecification spec = new MergeSpecification();\n    final List<SegmentInfo> segments = infos.asList();\n    \n    // First, enroll all \"full\" merges (size\n    // mergeFactor) to potentially be run concurrently:\n    while (last - maxNumSegments + 1 >= mergeFactor) {\n      spec.add(new OneMerge(segments.subList(last - mergeFactor, last)));\n      last -= mergeFactor;\n    }\n\n    // Only if there are no full merges pending do we\n    // add a final partial (< mergeFactor segments) merge:\n    if (0 == spec.merges.size()) {\n      if (maxNumSegments == 1) {\n\n        // Since we must optimize down to 1 segment, the\n        // choice is simple:\n        if (last > 1 || !isOptimized(infos.info(0))) {\n          spec.add(new OneMerge(segments.subList(0, last)));\n        }\n      } else if (last > maxNumSegments) {\n\n        // Take care to pick a partial merge that is\n        // least cost, but does not make the index too\n        // lopsided.  If we always just picked the\n        // partial tail then we could produce a highly\n        // lopsided index over time:\n\n        // We must merge this many segments to leave\n        // maxNumSegments in the index (from when\n        // optimize was first kicked off):\n        final int finalMergeSize = last - maxNumSegments + 1;\n\n        // Consider all possible starting points:\n        long bestSize = 0;\n        int bestStart = 0;\n\n        for(int i=0;i<last-finalMergeSize+1;i++) {\n          long sumSize = 0;\n          for(int j=0;j<finalMergeSize;j++)\n            sumSize += size(infos.info(j+i));\n          if (i == 0 || (sumSize < 2*size(infos.info(i-1)) && sumSize < bestSize)) {\n            bestStart = i;\n            bestSize = sumSize;\n          }\n        }\n\n        spec.add(new OneMerge(segments.subList(bestStart, bestStart + finalMergeSize)));\n      }\n    }\n    return spec.merges.size() == 0 ? null : spec;\n  }\n\n","sourceOld":"  /**\n   * Returns the merges necessary to optimize the index. This method constraints\n   * the returned merges only by the {@code maxNumSegments} parameter, and\n   * guaranteed that exactly that number of segments will remain in the index.\n   */\n  private MergeSpecification findMergesForOptimizeMaxNumSegments(SegmentInfos infos, int maxNumSegments, int last) throws IOException {\n    MergeSpecification spec = new MergeSpecification();\n    \n    // First, enroll all \"full\" merges (size\n    // mergeFactor) to potentially be run concurrently:\n    while (last - maxNumSegments + 1 >= mergeFactor) {\n      spec.add(new OneMerge(infos.range(last - mergeFactor, last)));\n      last -= mergeFactor;\n    }\n\n    // Only if there are no full merges pending do we\n    // add a final partial (< mergeFactor segments) merge:\n    if (0 == spec.merges.size()) {\n      if (maxNumSegments == 1) {\n\n        // Since we must optimize down to 1 segment, the\n        // choice is simple:\n        if (last > 1 || !isOptimized(infos.info(0))) {\n          spec.add(new OneMerge(infos.range(0, last)));\n        }\n      } else if (last > maxNumSegments) {\n\n        // Take care to pick a partial merge that is\n        // least cost, but does not make the index too\n        // lopsided.  If we always just picked the\n        // partial tail then we could produce a highly\n        // lopsided index over time:\n\n        // We must merge this many segments to leave\n        // maxNumSegments in the index (from when\n        // optimize was first kicked off):\n        final int finalMergeSize = last - maxNumSegments + 1;\n\n        // Consider all possible starting points:\n        long bestSize = 0;\n        int bestStart = 0;\n\n        for(int i=0;i<last-finalMergeSize+1;i++) {\n          long sumSize = 0;\n          for(int j=0;j<finalMergeSize;j++)\n            sumSize += size(infos.info(j+i));\n          if (i == 0 || (sumSize < 2*size(infos.info(i-1)) && sumSize < bestSize)) {\n            bestStart = i;\n            bestSize = sumSize;\n          }\n        }\n\n        spec.add(new OneMerge(infos.range(bestStart, bestStart + finalMergeSize)));\n      }\n    }\n    return spec.merges.size() == 0 ? null : spec;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c3a8a449466c1ff7ce2274fe73dab487256964b4","date":1305735867,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/LogMergePolicy#findMergesForOptimizeMaxNumSegments(SegmentInfos,int,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/LogMergePolicy#findMergesForOptimizeMaxNumSegments(SegmentInfos,int,int).mjava","sourceNew":"  /**\n   * Returns the merges necessary to optimize the index. This method constraints\n   * the returned merges only by the {@code maxNumSegments} parameter, and\n   * guaranteed that exactly that number of segments will remain in the index.\n   */\n  private MergeSpecification findMergesForOptimizeMaxNumSegments(SegmentInfos infos, int maxNumSegments, int last) throws IOException {\n    MergeSpecification spec = new MergeSpecification();\n    final List<SegmentInfo> segments = infos.asList();\n    \n    // First, enroll all \"full\" merges (size\n    // mergeFactor) to potentially be run concurrently:\n    while (last - maxNumSegments + 1 >= mergeFactor) {\n      spec.add(new OneMerge(segments.subList(last - mergeFactor, last)));\n      last -= mergeFactor;\n    }\n\n    // Only if there are no full merges pending do we\n    // add a final partial (< mergeFactor segments) merge:\n    if (0 == spec.merges.size()) {\n      if (maxNumSegments == 1) {\n\n        // Since we must optimize down to 1 segment, the\n        // choice is simple:\n        if (last > 1 || !isOptimized(infos.info(0))) {\n          spec.add(new OneMerge(segments.subList(0, last)));\n        }\n      } else if (last > maxNumSegments) {\n\n        // Take care to pick a partial merge that is\n        // least cost, but does not make the index too\n        // lopsided.  If we always just picked the\n        // partial tail then we could produce a highly\n        // lopsided index over time:\n\n        // We must merge this many segments to leave\n        // maxNumSegments in the index (from when\n        // optimize was first kicked off):\n        final int finalMergeSize = last - maxNumSegments + 1;\n\n        // Consider all possible starting points:\n        long bestSize = 0;\n        int bestStart = 0;\n\n        for(int i=0;i<last-finalMergeSize+1;i++) {\n          long sumSize = 0;\n          for(int j=0;j<finalMergeSize;j++)\n            sumSize += size(infos.info(j+i));\n          if (i == 0 || (sumSize < 2*size(infos.info(i-1)) && sumSize < bestSize)) {\n            bestStart = i;\n            bestSize = sumSize;\n          }\n        }\n\n        spec.add(new OneMerge(segments.subList(bestStart, bestStart + finalMergeSize)));\n      }\n    }\n    return spec.merges.size() == 0 ? null : spec;\n  }\n\n","sourceOld":"  /**\n   * Returns the merges necessary to optimize the index. This method constraints\n   * the returned merges only by the {@code maxNumSegments} parameter, and\n   * guaranteed that exactly that number of segments will remain in the index.\n   */\n  private MergeSpecification findMergesForOptimizeMaxNumSegments(SegmentInfos infos, int maxNumSegments, int last) throws IOException {\n    MergeSpecification spec = new MergeSpecification();\n    \n    // First, enroll all \"full\" merges (size\n    // mergeFactor) to potentially be run concurrently:\n    while (last - maxNumSegments + 1 >= mergeFactor) {\n      spec.add(new OneMerge(infos.range(last - mergeFactor, last)));\n      last -= mergeFactor;\n    }\n\n    // Only if there are no full merges pending do we\n    // add a final partial (< mergeFactor segments) merge:\n    if (0 == spec.merges.size()) {\n      if (maxNumSegments == 1) {\n\n        // Since we must optimize down to 1 segment, the\n        // choice is simple:\n        if (last > 1 || !isOptimized(infos.info(0))) {\n          spec.add(new OneMerge(infos.range(0, last)));\n        }\n      } else if (last > maxNumSegments) {\n\n        // Take care to pick a partial merge that is\n        // least cost, but does not make the index too\n        // lopsided.  If we always just picked the\n        // partial tail then we could produce a highly\n        // lopsided index over time:\n\n        // We must merge this many segments to leave\n        // maxNumSegments in the index (from when\n        // optimize was first kicked off):\n        final int finalMergeSize = last - maxNumSegments + 1;\n\n        // Consider all possible starting points:\n        long bestSize = 0;\n        int bestStart = 0;\n\n        for(int i=0;i<last-finalMergeSize+1;i++) {\n          long sumSize = 0;\n          for(int j=0;j<finalMergeSize;j++)\n            sumSize += size(infos.info(j+i));\n          if (i == 0 || (sumSize < 2*size(infos.info(i-1)) && sumSize < bestSize)) {\n            bestStart = i;\n            bestSize = sumSize;\n          }\n        }\n\n        spec.add(new OneMerge(infos.range(bestStart, bestStart + finalMergeSize)));\n      }\n    }\n    return spec.merges.size() == 0 ? null : spec;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/LogMergePolicy#findMergesForOptimizeMaxNumSegments(SegmentInfos,int,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/LogMergePolicy#findMergesForOptimizeMaxNumSegments(SegmentInfos,int,int).mjava","sourceNew":"  /**\n   * Returns the merges necessary to optimize the index. This method constraints\n   * the returned merges only by the {@code maxNumSegments} parameter, and\n   * guaranteed that exactly that number of segments will remain in the index.\n   */\n  private MergeSpecification findMergesForOptimizeMaxNumSegments(SegmentInfos infos, int maxNumSegments, int last) throws IOException {\n    MergeSpecification spec = new MergeSpecification();\n    final List<SegmentInfo> segments = infos.asList();\n    \n    // First, enroll all \"full\" merges (size\n    // mergeFactor) to potentially be run concurrently:\n    while (last - maxNumSegments + 1 >= mergeFactor) {\n      spec.add(new OneMerge(segments.subList(last - mergeFactor, last)));\n      last -= mergeFactor;\n    }\n\n    // Only if there are no full merges pending do we\n    // add a final partial (< mergeFactor segments) merge:\n    if (0 == spec.merges.size()) {\n      if (maxNumSegments == 1) {\n\n        // Since we must optimize down to 1 segment, the\n        // choice is simple:\n        if (last > 1 || !isOptimized(infos.info(0))) {\n          spec.add(new OneMerge(segments.subList(0, last)));\n        }\n      } else if (last > maxNumSegments) {\n\n        // Take care to pick a partial merge that is\n        // least cost, but does not make the index too\n        // lopsided.  If we always just picked the\n        // partial tail then we could produce a highly\n        // lopsided index over time:\n\n        // We must merge this many segments to leave\n        // maxNumSegments in the index (from when\n        // optimize was first kicked off):\n        final int finalMergeSize = last - maxNumSegments + 1;\n\n        // Consider all possible starting points:\n        long bestSize = 0;\n        int bestStart = 0;\n\n        for(int i=0;i<last-finalMergeSize+1;i++) {\n          long sumSize = 0;\n          for(int j=0;j<finalMergeSize;j++)\n            sumSize += size(infos.info(j+i));\n          if (i == 0 || (sumSize < 2*size(infos.info(i-1)) && sumSize < bestSize)) {\n            bestStart = i;\n            bestSize = sumSize;\n          }\n        }\n\n        spec.add(new OneMerge(segments.subList(bestStart, bestStart + finalMergeSize)));\n      }\n    }\n    return spec.merges.size() == 0 ? null : spec;\n  }\n\n","sourceOld":"  /**\n   * Returns the merges necessary to optimize the index. This method constraints\n   * the returned merges only by the {@code maxNumSegments} parameter, and\n   * guaranteed that exactly that number of segments will remain in the index.\n   */\n  private MergeSpecification findMergesForOptimizeMaxNumSegments(SegmentInfos infos, int maxNumSegments, int last) throws IOException {\n    MergeSpecification spec = new MergeSpecification();\n    \n    // First, enroll all \"full\" merges (size\n    // mergeFactor) to potentially be run concurrently:\n    while (last - maxNumSegments + 1 >= mergeFactor) {\n      spec.add(new OneMerge(infos.range(last - mergeFactor, last)));\n      last -= mergeFactor;\n    }\n\n    // Only if there are no full merges pending do we\n    // add a final partial (< mergeFactor segments) merge:\n    if (0 == spec.merges.size()) {\n      if (maxNumSegments == 1) {\n\n        // Since we must optimize down to 1 segment, the\n        // choice is simple:\n        if (last > 1 || !isOptimized(infos.info(0))) {\n          spec.add(new OneMerge(infos.range(0, last)));\n        }\n      } else if (last > maxNumSegments) {\n\n        // Take care to pick a partial merge that is\n        // least cost, but does not make the index too\n        // lopsided.  If we always just picked the\n        // partial tail then we could produce a highly\n        // lopsided index over time:\n\n        // We must merge this many segments to leave\n        // maxNumSegments in the index (from when\n        // optimize was first kicked off):\n        final int finalMergeSize = last - maxNumSegments + 1;\n\n        // Consider all possible starting points:\n        long bestSize = 0;\n        int bestStart = 0;\n\n        for(int i=0;i<last-finalMergeSize+1;i++) {\n          long sumSize = 0;\n          for(int j=0;j<finalMergeSize;j++)\n            sumSize += size(infos.info(j+i));\n          if (i == 0 || (sumSize < 2*size(infos.info(i-1)) && sumSize < bestSize)) {\n            bestStart = i;\n            bestSize = sumSize;\n          }\n        }\n\n        spec.add(new OneMerge(infos.range(bestStart, bestStart + finalMergeSize)));\n      }\n    }\n    return spec.merges.size() == 0 ? null : spec;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9f8b6801dbaf49c247119734f6e4516cce94e49a","date":1308478532,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/LogMergePolicy#findMergesForOptimizeMaxNumSegments(SegmentInfos,int,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/LogMergePolicy#findMergesForOptimizeMaxNumSegments(SegmentInfos,int,int).mjava","sourceNew":"  /**\n   * Returns the merges necessary to optimize the index. This method constraints\n   * the returned merges only by the {@code maxNumSegments} parameter, and\n   * guaranteed that exactly that number of segments will remain in the index.\n   */\n  private MergeSpecification findMergesForOptimizeMaxNumSegments(SegmentInfos infos, int maxNumSegments, int last) throws IOException {\n    MergeSpecification spec = new MergeSpecification();\n    final List<SegmentInfo> segments = infos.asList();\n\n    // First, enroll all \"full\" merges (size\n    // mergeFactor) to potentially be run concurrently:\n    while (last - maxNumSegments + 1 >= mergeFactor) {\n      spec.add(new OneMerge(segments.subList(last - mergeFactor, last)));\n      last -= mergeFactor;\n    }\n\n    // Only if there are no full merges pending do we\n    // add a final partial (< mergeFactor segments) merge:\n    if (0 == spec.merges.size()) {\n      if (maxNumSegments == 1) {\n\n        // Since we must optimize down to 1 segment, the\n        // choice is simple:\n        if (last > 1 || !isOptimized(infos.info(0))) {\n          spec.add(new OneMerge(segments.subList(0, last)));\n        }\n      } else if (last > maxNumSegments) {\n\n        // Take care to pick a partial merge that is\n        // least cost, but does not make the index too\n        // lopsided.  If we always just picked the\n        // partial tail then we could produce a highly\n        // lopsided index over time:\n\n        // We must merge this many segments to leave\n        // maxNumSegments in the index (from when\n        // optimize was first kicked off):\n        final int finalMergeSize = last - maxNumSegments + 1;\n\n        // Consider all possible starting points:\n        long bestSize = 0;\n        int bestStart = 0;\n\n        for(int i=0;i<last-finalMergeSize+1;i++) {\n          long sumSize = 0;\n          for(int j=0;j<finalMergeSize;j++)\n            sumSize += size(infos.info(j+i));\n          if (i == 0 || (sumSize < 2*size(infos.info(i-1)) && sumSize < bestSize)) {\n            bestStart = i;\n            bestSize = sumSize;\n          }\n        }\n\n        spec.add(new OneMerge(segments.subList(bestStart, bestStart + finalMergeSize)));\n      }\n    }\n    return spec.merges.size() == 0 ? null : spec;\n  }\n\n","sourceOld":"  /**\n   * Returns the merges necessary to optimize the index. This method constraints\n   * the returned merges only by the {@code maxNumSegments} parameter, and\n   * guaranteed that exactly that number of segments will remain in the index.\n   */\n  private MergeSpecification findMergesForOptimizeMaxNumSegments(SegmentInfos infos, int maxNumSegments, int last) throws IOException {\n    MergeSpecification spec = new MergeSpecification();\n    final List<SegmentInfo> segments = infos.asList();\n    \n    // First, enroll all \"full\" merges (size\n    // mergeFactor) to potentially be run concurrently:\n    while (last - maxNumSegments + 1 >= mergeFactor) {\n      spec.add(new OneMerge(segments.subList(last - mergeFactor, last)));\n      last -= mergeFactor;\n    }\n\n    // Only if there are no full merges pending do we\n    // add a final partial (< mergeFactor segments) merge:\n    if (0 == spec.merges.size()) {\n      if (maxNumSegments == 1) {\n\n        // Since we must optimize down to 1 segment, the\n        // choice is simple:\n        if (last > 1 || !isOptimized(infos.info(0))) {\n          spec.add(new OneMerge(segments.subList(0, last)));\n        }\n      } else if (last > maxNumSegments) {\n\n        // Take care to pick a partial merge that is\n        // least cost, but does not make the index too\n        // lopsided.  If we always just picked the\n        // partial tail then we could produce a highly\n        // lopsided index over time:\n\n        // We must merge this many segments to leave\n        // maxNumSegments in the index (from when\n        // optimize was first kicked off):\n        final int finalMergeSize = last - maxNumSegments + 1;\n\n        // Consider all possible starting points:\n        long bestSize = 0;\n        int bestStart = 0;\n\n        for(int i=0;i<last-finalMergeSize+1;i++) {\n          long sumSize = 0;\n          for(int j=0;j<finalMergeSize;j++)\n            sumSize += size(infos.info(j+i));\n          if (i == 0 || (sumSize < 2*size(infos.info(i-1)) && sumSize < bestSize)) {\n            bestStart = i;\n            bestSize = sumSize;\n          }\n        }\n\n        spec.add(new OneMerge(segments.subList(bestStart, bestStart + finalMergeSize)));\n      }\n    }\n    return spec.merges.size() == 0 ? null : spec;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f902dca0fec763317e17fa91ff6543fc8120c609","date":1308553979,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/LogMergePolicy#findMergesForOptimizeMaxNumSegments(SegmentInfos,int,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/LogMergePolicy#findMergesForOptimizeMaxNumSegments(SegmentInfos,int,int).mjava","sourceNew":"  /**\n   * Returns the merges necessary to optimize the index. This method constraints\n   * the returned merges only by the {@code maxNumSegments} parameter, and\n   * guaranteed that exactly that number of segments will remain in the index.\n   */\n  private MergeSpecification findMergesForOptimizeMaxNumSegments(SegmentInfos infos, int maxNumSegments, int last) throws IOException {\n    MergeSpecification spec = new MergeSpecification();\n    final List<SegmentInfo> segments = infos.asList();\n\n    // First, enroll all \"full\" merges (size\n    // mergeFactor) to potentially be run concurrently:\n    while (last - maxNumSegments + 1 >= mergeFactor) {\n      spec.add(new OneMerge(segments.subList(last - mergeFactor, last)));\n      last -= mergeFactor;\n    }\n\n    // Only if there are no full merges pending do we\n    // add a final partial (< mergeFactor segments) merge:\n    if (0 == spec.merges.size()) {\n      if (maxNumSegments == 1) {\n\n        // Since we must optimize down to 1 segment, the\n        // choice is simple:\n        if (last > 1 || !isOptimized(infos.info(0))) {\n          spec.add(new OneMerge(segments.subList(0, last)));\n        }\n      } else if (last > maxNumSegments) {\n\n        // Take care to pick a partial merge that is\n        // least cost, but does not make the index too\n        // lopsided.  If we always just picked the\n        // partial tail then we could produce a highly\n        // lopsided index over time:\n\n        // We must merge this many segments to leave\n        // maxNumSegments in the index (from when\n        // optimize was first kicked off):\n        final int finalMergeSize = last - maxNumSegments + 1;\n\n        // Consider all possible starting points:\n        long bestSize = 0;\n        int bestStart = 0;\n\n        for(int i=0;i<last-finalMergeSize+1;i++) {\n          long sumSize = 0;\n          for(int j=0;j<finalMergeSize;j++)\n            sumSize += size(infos.info(j+i));\n          if (i == 0 || (sumSize < 2*size(infos.info(i-1)) && sumSize < bestSize)) {\n            bestStart = i;\n            bestSize = sumSize;\n          }\n        }\n\n        spec.add(new OneMerge(segments.subList(bestStart, bestStart + finalMergeSize)));\n      }\n    }\n    return spec.merges.size() == 0 ? null : spec;\n  }\n\n","sourceOld":"  /**\n   * Returns the merges necessary to optimize the index. This method constraints\n   * the returned merges only by the {@code maxNumSegments} parameter, and\n   * guaranteed that exactly that number of segments will remain in the index.\n   */\n  private MergeSpecification findMergesForOptimizeMaxNumSegments(SegmentInfos infos, int maxNumSegments, int last) throws IOException {\n    MergeSpecification spec = new MergeSpecification();\n    final List<SegmentInfo> segments = infos.asList();\n    \n    // First, enroll all \"full\" merges (size\n    // mergeFactor) to potentially be run concurrently:\n    while (last - maxNumSegments + 1 >= mergeFactor) {\n      spec.add(new OneMerge(segments.subList(last - mergeFactor, last)));\n      last -= mergeFactor;\n    }\n\n    // Only if there are no full merges pending do we\n    // add a final partial (< mergeFactor segments) merge:\n    if (0 == spec.merges.size()) {\n      if (maxNumSegments == 1) {\n\n        // Since we must optimize down to 1 segment, the\n        // choice is simple:\n        if (last > 1 || !isOptimized(infos.info(0))) {\n          spec.add(new OneMerge(segments.subList(0, last)));\n        }\n      } else if (last > maxNumSegments) {\n\n        // Take care to pick a partial merge that is\n        // least cost, but does not make the index too\n        // lopsided.  If we always just picked the\n        // partial tail then we could produce a highly\n        // lopsided index over time:\n\n        // We must merge this many segments to leave\n        // maxNumSegments in the index (from when\n        // optimize was first kicked off):\n        final int finalMergeSize = last - maxNumSegments + 1;\n\n        // Consider all possible starting points:\n        long bestSize = 0;\n        int bestStart = 0;\n\n        for(int i=0;i<last-finalMergeSize+1;i++) {\n          long sumSize = 0;\n          for(int j=0;j<finalMergeSize;j++)\n            sumSize += size(infos.info(j+i));\n          if (i == 0 || (sumSize < 2*size(infos.info(i-1)) && sumSize < bestSize)) {\n            bestStart = i;\n            bestSize = sumSize;\n          }\n        }\n\n        spec.add(new OneMerge(segments.subList(bestStart, bestStart + finalMergeSize)));\n      }\n    }\n    return spec.merges.size() == 0 ? null : spec;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d14e8d18c0e3970c20354dbeeb49da11bd587fbd","date":1321041051,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/LogMergePolicy#findForcedMergesMaxNumSegments(SegmentInfos,int,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/LogMergePolicy#findMergesForOptimizeMaxNumSegments(SegmentInfos,int,int).mjava","sourceNew":"  /**\n   * Returns the merges necessary to forceMerge the index. This method constraints\n   * the returned merges only by the {@code maxNumSegments} parameter, and\n   * guaranteed that exactly that number of segments will remain in the index.\n   */\n  private MergeSpecification findForcedMergesMaxNumSegments(SegmentInfos infos, int maxNumSegments, int last) throws IOException {\n    MergeSpecification spec = new MergeSpecification();\n    final List<SegmentInfo> segments = infos.asList();\n\n    // First, enroll all \"full\" merges (size\n    // mergeFactor) to potentially be run concurrently:\n    while (last - maxNumSegments + 1 >= mergeFactor) {\n      spec.add(new OneMerge(segments.subList(last - mergeFactor, last)));\n      last -= mergeFactor;\n    }\n\n    // Only if there are no full merges pending do we\n    // add a final partial (< mergeFactor segments) merge:\n    if (0 == spec.merges.size()) {\n      if (maxNumSegments == 1) {\n\n        // Since we must merge down to 1 segment, the\n        // choice is simple:\n        if (last > 1 || !isMerged(infos.info(0))) {\n          spec.add(new OneMerge(segments.subList(0, last)));\n        }\n      } else if (last > maxNumSegments) {\n\n        // Take care to pick a partial merge that is\n        // least cost, but does not make the index too\n        // lopsided.  If we always just picked the\n        // partial tail then we could produce a highly\n        // lopsided index over time:\n\n        // We must merge this many segments to leave\n        // maxNumSegments in the index (from when\n        // forceMerge was first kicked off):\n        final int finalMergeSize = last - maxNumSegments + 1;\n\n        // Consider all possible starting points:\n        long bestSize = 0;\n        int bestStart = 0;\n\n        for(int i=0;i<last-finalMergeSize+1;i++) {\n          long sumSize = 0;\n          for(int j=0;j<finalMergeSize;j++)\n            sumSize += size(infos.info(j+i));\n          if (i == 0 || (sumSize < 2*size(infos.info(i-1)) && sumSize < bestSize)) {\n            bestStart = i;\n            bestSize = sumSize;\n          }\n        }\n\n        spec.add(new OneMerge(segments.subList(bestStart, bestStart + finalMergeSize)));\n      }\n    }\n    return spec.merges.size() == 0 ? null : spec;\n  }\n\n","sourceOld":"  /**\n   * Returns the merges necessary to optimize the index. This method constraints\n   * the returned merges only by the {@code maxNumSegments} parameter, and\n   * guaranteed that exactly that number of segments will remain in the index.\n   */\n  private MergeSpecification findMergesForOptimizeMaxNumSegments(SegmentInfos infos, int maxNumSegments, int last) throws IOException {\n    MergeSpecification spec = new MergeSpecification();\n    final List<SegmentInfo> segments = infos.asList();\n\n    // First, enroll all \"full\" merges (size\n    // mergeFactor) to potentially be run concurrently:\n    while (last - maxNumSegments + 1 >= mergeFactor) {\n      spec.add(new OneMerge(segments.subList(last - mergeFactor, last)));\n      last -= mergeFactor;\n    }\n\n    // Only if there are no full merges pending do we\n    // add a final partial (< mergeFactor segments) merge:\n    if (0 == spec.merges.size()) {\n      if (maxNumSegments == 1) {\n\n        // Since we must optimize down to 1 segment, the\n        // choice is simple:\n        if (last > 1 || !isOptimized(infos.info(0))) {\n          spec.add(new OneMerge(segments.subList(0, last)));\n        }\n      } else if (last > maxNumSegments) {\n\n        // Take care to pick a partial merge that is\n        // least cost, but does not make the index too\n        // lopsided.  If we always just picked the\n        // partial tail then we could produce a highly\n        // lopsided index over time:\n\n        // We must merge this many segments to leave\n        // maxNumSegments in the index (from when\n        // optimize was first kicked off):\n        final int finalMergeSize = last - maxNumSegments + 1;\n\n        // Consider all possible starting points:\n        long bestSize = 0;\n        int bestStart = 0;\n\n        for(int i=0;i<last-finalMergeSize+1;i++) {\n          long sumSize = 0;\n          for(int j=0;j<finalMergeSize;j++)\n            sumSize += size(infos.info(j+i));\n          if (i == 0 || (sumSize < 2*size(infos.info(i-1)) && sumSize < bestSize)) {\n            bestStart = i;\n            bestSize = sumSize;\n          }\n        }\n\n        spec.add(new OneMerge(segments.subList(bestStart, bestStart + finalMergeSize)));\n      }\n    }\n    return spec.merges.size() == 0 ? null : spec;\n  }\n\n","bugFix":null,"bugIntro":["67bcec391f8e94564afde5a0f0e6538d07a96255"],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"38a62612cfa4e104080d89d7751a8f1a258ac335":["5390d5f5bc8bf5d65eff4c1d596cf9547ead0c56"],"f902dca0fec763317e17fa91ff6543fc8120c609":["a3776dccca01c11e7046323cfad46a3b4a471233","9f8b6801dbaf49c247119734f6e4516cce94e49a"],"4a69e5860d014751cc9329dfeb441a6d8fd1ed8e":["3bb13258feba31ab676502787ab2e1779f129b7a"],"d14e8d18c0e3970c20354dbeeb49da11bd587fbd":["9f8b6801dbaf49c247119734f6e4516cce94e49a"],"5390d5f5bc8bf5d65eff4c1d596cf9547ead0c56":["8723a3379c08ae0b4ba0cf4f246306f86ad8362d"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","38a62612cfa4e104080d89d7751a8f1a258ac335"],"9f8b6801dbaf49c247119734f6e4516cce94e49a":["786a4d25ca958a1f315a9d6a74f0441fdafcd522"],"786a4d25ca958a1f315a9d6a74f0441fdafcd522":["38a62612cfa4e104080d89d7751a8f1a258ac335"],"c3a8a449466c1ff7ce2274fe73dab487256964b4":["4a69e5860d014751cc9329dfeb441a6d8fd1ed8e","786a4d25ca958a1f315a9d6a74f0441fdafcd522"],"a3776dccca01c11e7046323cfad46a3b4a471233":["38a62612cfa4e104080d89d7751a8f1a258ac335","786a4d25ca958a1f315a9d6a74f0441fdafcd522"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"8017ab6544f30f93b106e419e7298173bad77f69":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","8723a3379c08ae0b4ba0cf4f246306f86ad8362d"],"8723a3379c08ae0b4ba0cf4f246306f86ad8362d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"3bb13258feba31ab676502787ab2e1779f129b7a":["8017ab6544f30f93b106e419e7298173bad77f69","5390d5f5bc8bf5d65eff4c1d596cf9547ead0c56"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd"]},"commit2Childs":{"38a62612cfa4e104080d89d7751a8f1a258ac335":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","786a4d25ca958a1f315a9d6a74f0441fdafcd522","a3776dccca01c11e7046323cfad46a3b4a471233"],"f902dca0fec763317e17fa91ff6543fc8120c609":[],"4a69e5860d014751cc9329dfeb441a6d8fd1ed8e":["c3a8a449466c1ff7ce2274fe73dab487256964b4"],"d14e8d18c0e3970c20354dbeeb49da11bd587fbd":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"5390d5f5bc8bf5d65eff4c1d596cf9547ead0c56":["38a62612cfa4e104080d89d7751a8f1a258ac335","3bb13258feba31ab676502787ab2e1779f129b7a"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":[],"9f8b6801dbaf49c247119734f6e4516cce94e49a":["f902dca0fec763317e17fa91ff6543fc8120c609","d14e8d18c0e3970c20354dbeeb49da11bd587fbd"],"786a4d25ca958a1f315a9d6a74f0441fdafcd522":["9f8b6801dbaf49c247119734f6e4516cce94e49a","c3a8a449466c1ff7ce2274fe73dab487256964b4","a3776dccca01c11e7046323cfad46a3b4a471233"],"c3a8a449466c1ff7ce2274fe73dab487256964b4":[],"a3776dccca01c11e7046323cfad46a3b4a471233":["f902dca0fec763317e17fa91ff6543fc8120c609"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","8017ab6544f30f93b106e419e7298173bad77f69","8723a3379c08ae0b4ba0cf4f246306f86ad8362d"],"8017ab6544f30f93b106e419e7298173bad77f69":["3bb13258feba31ab676502787ab2e1779f129b7a"],"8723a3379c08ae0b4ba0cf4f246306f86ad8362d":["5390d5f5bc8bf5d65eff4c1d596cf9547ead0c56","8017ab6544f30f93b106e419e7298173bad77f69"],"3bb13258feba31ab676502787ab2e1779f129b7a":["4a69e5860d014751cc9329dfeb441a6d8fd1ed8e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["f902dca0fec763317e17fa91ff6543fc8120c609","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","c3a8a449466c1ff7ce2274fe73dab487256964b4","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}