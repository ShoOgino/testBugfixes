{"path":"solr/core/src/test/org/apache/solr/cloud/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String).mjava","commits":[{"id":"4c623a7f72be34d6c45bee682028c50327d9e4b7","date":1467791293,"type":1,"author":"Varun Thacker","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/TestCloudBackupRestore#testBackupAndRestore(String).mjava","sourceNew":"  private void testBackupAndRestore(String collectionName) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    {\n      CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n      if (origShardToDocCount.size() > cluster.getJettySolrRunners().size()) {\n        // may need to increase maxShardsPerNode (e.g. if it was shard split, then now we need more)\n        restore.setMaxShardsPerNode(origShardToDocCount.size());\n      }\n      Properties props = new Properties();\n      props.setProperty(\"customKey\", \"customVal\");\n      restore.setProperties(props);\n      if (sameConfig==false) {\n        restore.setConfigName(\"customConfigName\");\n      }\n      if (random().nextBoolean()) {\n        assertEquals(0, restore.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n      }\n      AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n          restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n    }\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getReplicationFactor(), restoreCollection.getReplicationFactor());\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(backupCollection.getActiveSlices().iterator().next().getReplicas().size(),\n        restoreCollection.getActiveSlices().iterator().next().getReplicas().size());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","sourceOld":"  private void testBackupAndRestore(String collectionName) throws Exception {\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    String location = createTempDir().toFile().getAbsolutePath();\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(location);\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    {\n      CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n              .setLocation(location);\n      if (origShardToDocCount.size() > cluster.getJettySolrRunners().size()) {\n        // may need to increase maxShardsPerNode (e.g. if it was shard split, then now we need more)\n        restore.setMaxShardsPerNode(origShardToDocCount.size());\n      }\n      Properties props = new Properties();\n      props.setProperty(\"customKey\", \"customVal\");\n      restore.setProperties(props);\n      if (sameConfig==false) {\n        restore.setConfigName(\"customConfigName\");\n      }\n      if (random().nextBoolean()) {\n        assertEquals(0, restore.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n      }\n      AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n          restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n    }\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getReplicationFactor(), restoreCollection.getReplicationFactor());\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(backupCollection.getActiveSlices().iterator().next().getReplicas().size(),\n        restoreCollection.getActiveSlices().iterator().next().getReplicas().size());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String).mjava","pathOld":"/dev/null","sourceNew":"  private void testBackupAndRestore(String collectionName) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    {\n      CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n      if (origShardToDocCount.size() > cluster.getJettySolrRunners().size()) {\n        // may need to increase maxShardsPerNode (e.g. if it was shard split, then now we need more)\n        restore.setMaxShardsPerNode(origShardToDocCount.size());\n      }\n      Properties props = new Properties();\n      props.setProperty(\"customKey\", \"customVal\");\n      restore.setProperties(props);\n      if (sameConfig==false) {\n        restore.setConfigName(\"customConfigName\");\n      }\n      if (random().nextBoolean()) {\n        assertEquals(0, restore.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n      }\n      AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n          restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n    }\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getReplicationFactor(), restoreCollection.getReplicationFactor());\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(backupCollection.getActiveSlices().iterator().next().getReplicas().size(),\n        restoreCollection.getActiveSlices().iterator().next().getReplicas().size());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d18dd44acd824af8b51a5994c9475b32b094fb76","date":1494427167,"type":3,"author":"Varun Thacker","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String).mjava","sourceNew":"  private void testBackupAndRestore(String collectionName) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    {\n      CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n      if (origShardToDocCount.size() > cluster.getJettySolrRunners().size()) {\n        // may need to increase maxShardsPerNode (e.g. if it was shard split, then now we need more)\n        restore.setMaxShardsPerNode(origShardToDocCount.size());\n      }\n\n      if (rarely()) { // Try with createNodeSet configuration\n        int nodeSetSize = cluster.getJettySolrRunners().size() / 2;\n        List<String> nodeStrs = new ArrayList<>(nodeSetSize);\n        Iterator<JettySolrRunner> iter = cluster.getJettySolrRunners().iterator();\n        for (int i = 0; i < nodeSetSize ; i++) {\n          nodeStrs.add(iter.next().getNodeName());\n        }\n        restore.setCreateNodeSet(String.join(\",\", nodeStrs));\n        restore.setCreateNodeSetShuffle(usually());\n        // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n        restore.setMaxShardsPerNode(origShardToDocCount.size() * 2);\n      }\n\n      Properties props = new Properties();\n      props.setProperty(\"customKey\", \"customVal\");\n      restore.setProperties(props);\n\n      if (sameConfig==false) {\n        restore.setConfigName(\"customConfigName\");\n      }\n      if (random().nextBoolean()) {\n        assertEquals(0, restore.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n      }\n      AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n          restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n    }\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getReplicationFactor(), restoreCollection.getReplicationFactor());\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(backupCollection.getActiveSlices().iterator().next().getReplicas().size(),\n        restoreCollection.getActiveSlices().iterator().next().getReplicas().size());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreCollection.getMaxShardsPerNode() ,\n          v <= restoreCollection.getMaxShardsPerNode());\n    });\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","sourceOld":"  private void testBackupAndRestore(String collectionName) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    {\n      CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n      if (origShardToDocCount.size() > cluster.getJettySolrRunners().size()) {\n        // may need to increase maxShardsPerNode (e.g. if it was shard split, then now we need more)\n        restore.setMaxShardsPerNode(origShardToDocCount.size());\n      }\n      Properties props = new Properties();\n      props.setProperty(\"customKey\", \"customVal\");\n      restore.setProperties(props);\n      if (sameConfig==false) {\n        restore.setConfigName(\"customConfigName\");\n      }\n      if (random().nextBoolean()) {\n        assertEquals(0, restore.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n      }\n      AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n          restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n    }\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getReplicationFactor(), restoreCollection.getReplicationFactor());\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(backupCollection.getActiveSlices().iterator().next().getReplicas().size(),\n        restoreCollection.getActiveSlices().iterator().next().getReplicas().size());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b31ebc7a867ddea79d438a8fca876a94e644d11a","date":1494496172,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String).mjava","sourceNew":"  private void testBackupAndRestore(String collectionName) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    {\n      CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n      if (origShardToDocCount.size() > cluster.getJettySolrRunners().size()) {\n        // may need to increase maxShardsPerNode (e.g. if it was shard split, then now we need more)\n        restore.setMaxShardsPerNode(origShardToDocCount.size());\n      }\n\n      if (rarely()) { // Try with createNodeSet configuration\n        int nodeSetSize = cluster.getJettySolrRunners().size() / 2;\n        List<String> nodeStrs = new ArrayList<>(nodeSetSize);\n        Iterator<JettySolrRunner> iter = cluster.getJettySolrRunners().iterator();\n        for (int i = 0; i < nodeSetSize ; i++) {\n          nodeStrs.add(iter.next().getNodeName());\n        }\n        restore.setCreateNodeSet(String.join(\",\", nodeStrs));\n        restore.setCreateNodeSetShuffle(usually());\n        // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n        restore.setMaxShardsPerNode(origShardToDocCount.size() * 2);\n      }\n\n      Properties props = new Properties();\n      props.setProperty(\"customKey\", \"customVal\");\n      restore.setProperties(props);\n\n      if (sameConfig==false) {\n        restore.setConfigName(\"customConfigName\");\n      }\n      if (random().nextBoolean()) {\n        assertEquals(0, restore.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n      }\n      AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n          restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n    }\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getReplicationFactor(), restoreCollection.getReplicationFactor());\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(backupCollection.getActiveSlices().iterator().next().getReplicas().size(),\n        restoreCollection.getActiveSlices().iterator().next().getReplicas().size());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreCollection.getMaxShardsPerNode() ,\n          v <= restoreCollection.getMaxShardsPerNode());\n    });\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","sourceOld":"  private void testBackupAndRestore(String collectionName) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    {\n      CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n      if (origShardToDocCount.size() > cluster.getJettySolrRunners().size()) {\n        // may need to increase maxShardsPerNode (e.g. if it was shard split, then now we need more)\n        restore.setMaxShardsPerNode(origShardToDocCount.size());\n      }\n      Properties props = new Properties();\n      props.setProperty(\"customKey\", \"customVal\");\n      restore.setProperties(props);\n      if (sameConfig==false) {\n        restore.setConfigName(\"customConfigName\");\n      }\n      if (random().nextBoolean()) {\n        assertEquals(0, restore.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n      }\n      AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n          restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n    }\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getReplicationFactor(), restoreCollection.getReplicationFactor());\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(backupCollection.getActiveSlices().iterator().next().getReplicas().size(),\n        restoreCollection.getActiveSlices().iterator().next().getReplicas().size());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"61c45e99cf6676da48f19d7511c73712ad39402b","date":1495508331,"type":3,"author":"Tomas Fernandez Lobbe","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String).mjava","sourceNew":"  private void testBackupAndRestore(String collectionName) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    {\n      CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n      if (backupCollection.getReplicas().size() > cluster.getJettySolrRunners().size()) {\n        // may need to increase maxShardsPerNode (e.g. if it was shard split, then now we need more)\n        restore.setMaxShardsPerNode((int)Math.ceil(backupCollection.getReplicas().size()/cluster.getJettySolrRunners().size()));\n      }\n\n      if (rarely()) { // Try with createNodeSet configuration\n        int nodeSetSize = cluster.getJettySolrRunners().size() / 2;\n        List<String> nodeStrs = new ArrayList<>(nodeSetSize);\n        Iterator<JettySolrRunner> iter = cluster.getJettySolrRunners().iterator();\n        for (int i = 0; i < nodeSetSize ; i++) {\n          nodeStrs.add(iter.next().getNodeName());\n        }\n        restore.setCreateNodeSet(String.join(\",\", nodeStrs));\n        restore.setCreateNodeSetShuffle(usually());\n        // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n        restore.setMaxShardsPerNode(origShardToDocCount.size() * 2);\n      }\n\n      Properties props = new Properties();\n      props.setProperty(\"customKey\", \"customVal\");\n      restore.setProperties(props);\n\n      if (sameConfig==false) {\n        restore.setConfigName(\"customConfigName\");\n      }\n      if (random().nextBoolean()) {\n        assertEquals(0, restore.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n      }\n      AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n          restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n    }\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getReplicationFactor(), restoreCollection.getReplicationFactor());\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(backupCollection.getActiveSlices().iterator().next().getReplicas().size(),\n        restoreCollection.getActiveSlices().iterator().next().getReplicas().size());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreCollection.getMaxShardsPerNode() ,\n          v <= restoreCollection.getMaxShardsPerNode());\n    });\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","sourceOld":"  private void testBackupAndRestore(String collectionName) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    {\n      CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n      if (origShardToDocCount.size() > cluster.getJettySolrRunners().size()) {\n        // may need to increase maxShardsPerNode (e.g. if it was shard split, then now we need more)\n        restore.setMaxShardsPerNode(origShardToDocCount.size());\n      }\n\n      if (rarely()) { // Try with createNodeSet configuration\n        int nodeSetSize = cluster.getJettySolrRunners().size() / 2;\n        List<String> nodeStrs = new ArrayList<>(nodeSetSize);\n        Iterator<JettySolrRunner> iter = cluster.getJettySolrRunners().iterator();\n        for (int i = 0; i < nodeSetSize ; i++) {\n          nodeStrs.add(iter.next().getNodeName());\n        }\n        restore.setCreateNodeSet(String.join(\",\", nodeStrs));\n        restore.setCreateNodeSetShuffle(usually());\n        // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n        restore.setMaxShardsPerNode(origShardToDocCount.size() * 2);\n      }\n\n      Properties props = new Properties();\n      props.setProperty(\"customKey\", \"customVal\");\n      restore.setProperties(props);\n\n      if (sameConfig==false) {\n        restore.setConfigName(\"customConfigName\");\n      }\n      if (random().nextBoolean()) {\n        assertEquals(0, restore.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n      }\n      AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n          restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n    }\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getReplicationFactor(), restoreCollection.getReplicationFactor());\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(backupCollection.getActiveSlices().iterator().next().getReplicas().size(),\n        restoreCollection.getActiveSlices().iterator().next().getReplicas().size());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreCollection.getMaxShardsPerNode() ,\n          v <= restoreCollection.getMaxShardsPerNode());\n    });\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c84db5cfadecb30316e2cc080dd63920b5e158bc","date":1495835358,"type":3,"author":"Tomas Fernandez Lobbe","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String).mjava","sourceNew":"  private void testBackupAndRestore(String collectionName) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    {\n      CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n      if (backupCollection.getReplicas().size() > cluster.getJettySolrRunners().size()) {\n        // may need to increase maxShardsPerNode (e.g. if it was shard split, then now we need more)\n        restore.setMaxShardsPerNode((int)Math.ceil(backupCollection.getReplicas().size()/cluster.getJettySolrRunners().size()));\n      }\n      \n\n      if (rarely()) { // Try with createNodeSet configuration\n        int nodeSetSize = cluster.getJettySolrRunners().size() / 2;\n        List<String> nodeStrs = new ArrayList<>(nodeSetSize);\n        Iterator<JettySolrRunner> iter = cluster.getJettySolrRunners().iterator();\n        for (int i = 0; i < nodeSetSize ; i++) {\n          nodeStrs.add(iter.next().getNodeName());\n        }\n        restore.setCreateNodeSet(String.join(\",\", nodeStrs));\n        restore.setCreateNodeSetShuffle(usually());\n        // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n        if (restore.getMaxShardsPerNode() != null) {\n          restore.setMaxShardsPerNode(restore.getMaxShardsPerNode() * 2);\n        } else {\n          restore.setMaxShardsPerNode(origShardToDocCount.size() * 2);\n        }\n      }\n\n      Properties props = new Properties();\n      props.setProperty(\"customKey\", \"customVal\");\n      restore.setProperties(props);\n\n      if (sameConfig==false) {\n        restore.setConfigName(\"customConfigName\");\n      }\n      if (random().nextBoolean()) {\n        assertEquals(0, restore.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n      }\n      AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n          restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n    }\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getReplicationFactor(), restoreCollection.getReplicationFactor());\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(backupCollection.getActiveSlices().iterator().next().getReplicas().size(),\n        restoreCollection.getActiveSlices().iterator().next().getReplicas().size());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreCollection.getMaxShardsPerNode() ,\n          v <= restoreCollection.getMaxShardsPerNode());\n    });\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","sourceOld":"  private void testBackupAndRestore(String collectionName) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    {\n      CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n      if (backupCollection.getReplicas().size() > cluster.getJettySolrRunners().size()) {\n        // may need to increase maxShardsPerNode (e.g. if it was shard split, then now we need more)\n        restore.setMaxShardsPerNode((int)Math.ceil(backupCollection.getReplicas().size()/cluster.getJettySolrRunners().size()));\n      }\n\n      if (rarely()) { // Try with createNodeSet configuration\n        int nodeSetSize = cluster.getJettySolrRunners().size() / 2;\n        List<String> nodeStrs = new ArrayList<>(nodeSetSize);\n        Iterator<JettySolrRunner> iter = cluster.getJettySolrRunners().iterator();\n        for (int i = 0; i < nodeSetSize ; i++) {\n          nodeStrs.add(iter.next().getNodeName());\n        }\n        restore.setCreateNodeSet(String.join(\",\", nodeStrs));\n        restore.setCreateNodeSetShuffle(usually());\n        // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n        restore.setMaxShardsPerNode(origShardToDocCount.size() * 2);\n      }\n\n      Properties props = new Properties();\n      props.setProperty(\"customKey\", \"customVal\");\n      restore.setProperties(props);\n\n      if (sameConfig==false) {\n        restore.setConfigName(\"customConfigName\");\n      }\n      if (random().nextBoolean()) {\n        assertEquals(0, restore.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n      }\n      AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n          restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n    }\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getReplicationFactor(), restoreCollection.getReplicationFactor());\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(backupCollection.getActiveSlices().iterator().next().getReplicas().size(),\n        restoreCollection.getActiveSlices().iterator().next().getReplicas().size());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreCollection.getMaxShardsPerNode() ,\n          v <= restoreCollection.getMaxShardsPerNode());\n    });\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","bugFix":null,"bugIntro":["076d58da25128e8a4c511abf07c5d86c4ebddcbf","076d58da25128e8a4c511abf07c5d86c4ebddcbf"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d1f5728f32a4a256b36cfabd7a2636452f599bb9","date":1496231774,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String).mjava","sourceNew":"  private void testBackupAndRestore(String collectionName) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    {\n      CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n      if (backupCollection.getReplicas().size() > cluster.getJettySolrRunners().size()) {\n        // may need to increase maxShardsPerNode (e.g. if it was shard split, then now we need more)\n        restore.setMaxShardsPerNode((int)Math.ceil(backupCollection.getReplicas().size()/cluster.getJettySolrRunners().size()));\n      }\n      \n\n      if (rarely()) { // Try with createNodeSet configuration\n        int nodeSetSize = cluster.getJettySolrRunners().size() / 2;\n        List<String> nodeStrs = new ArrayList<>(nodeSetSize);\n        Iterator<JettySolrRunner> iter = cluster.getJettySolrRunners().iterator();\n        for (int i = 0; i < nodeSetSize ; i++) {\n          nodeStrs.add(iter.next().getNodeName());\n        }\n        restore.setCreateNodeSet(String.join(\",\", nodeStrs));\n        restore.setCreateNodeSetShuffle(usually());\n        // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n        if (restore.getMaxShardsPerNode() != null) {\n          restore.setMaxShardsPerNode(restore.getMaxShardsPerNode() * 2);\n        } else {\n          restore.setMaxShardsPerNode(origShardToDocCount.size() * 2);\n        }\n      }\n\n      Properties props = new Properties();\n      props.setProperty(\"customKey\", \"customVal\");\n      restore.setProperties(props);\n\n      if (sameConfig==false) {\n        restore.setConfigName(\"customConfigName\");\n      }\n      if (random().nextBoolean()) {\n        assertEquals(0, restore.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n      }\n      AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n          restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n    }\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getReplicationFactor(), restoreCollection.getReplicationFactor());\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(backupCollection.getActiveSlices().iterator().next().getReplicas().size(),\n        restoreCollection.getActiveSlices().iterator().next().getReplicas().size());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreCollection.getMaxShardsPerNode() ,\n          v <= restoreCollection.getMaxShardsPerNode());\n    });\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","sourceOld":"  private void testBackupAndRestore(String collectionName) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    {\n      CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n      if (backupCollection.getReplicas().size() > cluster.getJettySolrRunners().size()) {\n        // may need to increase maxShardsPerNode (e.g. if it was shard split, then now we need more)\n        restore.setMaxShardsPerNode((int)Math.ceil(backupCollection.getReplicas().size()/cluster.getJettySolrRunners().size()));\n      }\n\n      if (rarely()) { // Try with createNodeSet configuration\n        int nodeSetSize = cluster.getJettySolrRunners().size() / 2;\n        List<String> nodeStrs = new ArrayList<>(nodeSetSize);\n        Iterator<JettySolrRunner> iter = cluster.getJettySolrRunners().iterator();\n        for (int i = 0; i < nodeSetSize ; i++) {\n          nodeStrs.add(iter.next().getNodeName());\n        }\n        restore.setCreateNodeSet(String.join(\",\", nodeStrs));\n        restore.setCreateNodeSetShuffle(usually());\n        // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n        restore.setMaxShardsPerNode(origShardToDocCount.size() * 2);\n      }\n\n      Properties props = new Properties();\n      props.setProperty(\"customKey\", \"customVal\");\n      restore.setProperties(props);\n\n      if (sameConfig==false) {\n        restore.setConfigName(\"customConfigName\");\n      }\n      if (random().nextBoolean()) {\n        assertEquals(0, restore.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n      }\n      AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n          restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n    }\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getReplicationFactor(), restoreCollection.getReplicationFactor());\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(backupCollection.getActiveSlices().iterator().next().getReplicas().size(),\n        restoreCollection.getActiveSlices().iterator().next().getReplicas().size());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreCollection.getMaxShardsPerNode() ,\n          v <= restoreCollection.getMaxShardsPerNode());\n    });\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e9017cf144952056066919f1ebc7897ff9bd71b1","date":1496757600,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String).mjava","sourceNew":"  private void testBackupAndRestore(String collectionName) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    {\n      CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n      if (backupCollection.getReplicas().size() > cluster.getJettySolrRunners().size()) {\n        // may need to increase maxShardsPerNode (e.g. if it was shard split, then now we need more)\n        restore.setMaxShardsPerNode((int)Math.ceil(backupCollection.getReplicas().size()/cluster.getJettySolrRunners().size()));\n      }\n      \n\n      if (rarely()) { // Try with createNodeSet configuration\n        int nodeSetSize = cluster.getJettySolrRunners().size() / 2;\n        List<String> nodeStrs = new ArrayList<>(nodeSetSize);\n        Iterator<JettySolrRunner> iter = cluster.getJettySolrRunners().iterator();\n        for (int i = 0; i < nodeSetSize ; i++) {\n          nodeStrs.add(iter.next().getNodeName());\n        }\n        restore.setCreateNodeSet(String.join(\",\", nodeStrs));\n        restore.setCreateNodeSetShuffle(usually());\n        // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n        if (restore.getMaxShardsPerNode() != null) {\n          restore.setMaxShardsPerNode(restore.getMaxShardsPerNode() * 2);\n        } else {\n          restore.setMaxShardsPerNode(origShardToDocCount.size() * 2);\n        }\n      }\n\n      Properties props = new Properties();\n      props.setProperty(\"customKey\", \"customVal\");\n      restore.setProperties(props);\n\n      if (sameConfig==false) {\n        restore.setConfigName(\"customConfigName\");\n      }\n      if (random().nextBoolean()) {\n        assertEquals(0, restore.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n      }\n      AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n          restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n    }\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getReplicationFactor(), restoreCollection.getReplicationFactor());\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(backupCollection.getActiveSlices().iterator().next().getReplicas().size(),\n        restoreCollection.getActiveSlices().iterator().next().getReplicas().size());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreCollection.getMaxShardsPerNode() ,\n          v <= restoreCollection.getMaxShardsPerNode());\n    });\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","sourceOld":"  private void testBackupAndRestore(String collectionName) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    {\n      CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n      if (origShardToDocCount.size() > cluster.getJettySolrRunners().size()) {\n        // may need to increase maxShardsPerNode (e.g. if it was shard split, then now we need more)\n        restore.setMaxShardsPerNode(origShardToDocCount.size());\n      }\n      Properties props = new Properties();\n      props.setProperty(\"customKey\", \"customVal\");\n      restore.setProperties(props);\n      if (sameConfig==false) {\n        restore.setConfigName(\"customConfigName\");\n      }\n      if (random().nextBoolean()) {\n        assertEquals(0, restore.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n      }\n      AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n          restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n    }\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getReplicationFactor(), restoreCollection.getReplicationFactor());\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(backupCollection.getActiveSlices().iterator().next().getReplicas().size(),\n        restoreCollection.getActiveSlices().iterator().next().getReplicas().size());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe190f27cc4076d6681cb10c6a326503ad5a17e7","date":1509475399,"type":3,"author":"Varun Thacker","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String).mjava","sourceNew":"  private void testBackupAndRestore(String collectionName) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    {\n      CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n\n      //explicitly specify the replicationFactor/pullReplicas/nrtReplicas/tlogReplicas .\n      //Value is still the same as the original. maybe test with different values that the original for better test coverage\n      if (random().nextBoolean())  {\n        restore.setReplicationFactor(replFactor);\n      }\n      if (backupCollection.getReplicas().size() > cluster.getJettySolrRunners().size()) {\n        // may need to increase maxShardsPerNode (e.g. if it was shard split, then now we need more)\n        restore.setMaxShardsPerNode((int)Math.ceil(backupCollection.getReplicas().size()/cluster.getJettySolrRunners().size()));\n      }\n      \n\n      if (rarely()) { // Try with createNodeSet configuration\n        int nodeSetSize = cluster.getJettySolrRunners().size() / 2;\n        List<String> nodeStrs = new ArrayList<>(nodeSetSize);\n        Iterator<JettySolrRunner> iter = cluster.getJettySolrRunners().iterator();\n        for (int i = 0; i < nodeSetSize ; i++) {\n          nodeStrs.add(iter.next().getNodeName());\n        }\n        restore.setCreateNodeSet(String.join(\",\", nodeStrs));\n        restore.setCreateNodeSetShuffle(usually());\n        // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n        if (restore.getMaxShardsPerNode() != null) {\n          restore.setMaxShardsPerNode(restore.getMaxShardsPerNode() * 2);\n        } else {\n          restore.setMaxShardsPerNode(origShardToDocCount.size() * 2);\n        }\n      }\n\n      Properties props = new Properties();\n      props.setProperty(\"customKey\", \"customVal\");\n      restore.setProperties(props);\n\n      if (sameConfig==false) {\n        restore.setConfigName(\"customConfigName\");\n      }\n      if (random().nextBoolean()) {\n        assertEquals(0, restore.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n      }\n      AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n          restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n    }\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getReplicationFactor(), restoreCollection.getReplicationFactor());\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(backupCollection.getActiveSlices().iterator().next().getReplicas().size(),\n        restoreCollection.getActiveSlices().iterator().next().getReplicas().size());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreCollection.getMaxShardsPerNode() ,\n          v <= restoreCollection.getMaxShardsPerNode());\n    });\n\n    assertEquals(\"Different count of nrtReplicas. Backup collection state=\" + backupCollection + \"\\nRestore \" +\n        \"collection state=\" + restoreCollection, replFactor, restoreCollection.getNumNrtReplicas().intValue());\n    assertEquals(\"Different count of pullReplicas. Backup collection state=\" + backupCollection + \"\\nRestore\" +\n        \" collection state=\" + restoreCollection, numPullReplicas, restoreCollection.getNumPullReplicas().intValue());\n    assertEquals(\"Different count of TlogReplica. Backup collection state=\" + backupCollection + \"\\nRestore\" +\n        \" collection state=\" + restoreCollection, numTlogReplicas, restoreCollection.getNumTlogReplicas().intValue());\n\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","sourceOld":"  private void testBackupAndRestore(String collectionName) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    {\n      CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n      if (backupCollection.getReplicas().size() > cluster.getJettySolrRunners().size()) {\n        // may need to increase maxShardsPerNode (e.g. if it was shard split, then now we need more)\n        restore.setMaxShardsPerNode((int)Math.ceil(backupCollection.getReplicas().size()/cluster.getJettySolrRunners().size()));\n      }\n      \n\n      if (rarely()) { // Try with createNodeSet configuration\n        int nodeSetSize = cluster.getJettySolrRunners().size() / 2;\n        List<String> nodeStrs = new ArrayList<>(nodeSetSize);\n        Iterator<JettySolrRunner> iter = cluster.getJettySolrRunners().iterator();\n        for (int i = 0; i < nodeSetSize ; i++) {\n          nodeStrs.add(iter.next().getNodeName());\n        }\n        restore.setCreateNodeSet(String.join(\",\", nodeStrs));\n        restore.setCreateNodeSetShuffle(usually());\n        // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n        if (restore.getMaxShardsPerNode() != null) {\n          restore.setMaxShardsPerNode(restore.getMaxShardsPerNode() * 2);\n        } else {\n          restore.setMaxShardsPerNode(origShardToDocCount.size() * 2);\n        }\n      }\n\n      Properties props = new Properties();\n      props.setProperty(\"customKey\", \"customVal\");\n      restore.setProperties(props);\n\n      if (sameConfig==false) {\n        restore.setConfigName(\"customConfigName\");\n      }\n      if (random().nextBoolean()) {\n        assertEquals(0, restore.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n      }\n      AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n          restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n    }\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getReplicationFactor(), restoreCollection.getReplicationFactor());\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(backupCollection.getActiveSlices().iterator().next().getReplicas().size(),\n        restoreCollection.getActiveSlices().iterator().next().getReplicas().size());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreCollection.getMaxShardsPerNode() ,\n          v <= restoreCollection.getMaxShardsPerNode());\n    });\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"529c69423fc9de95d4d764f4c998f095fead50bd","date":1509480852,"type":3,"author":"Varun Thacker","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String).mjava","sourceNew":"  private void testBackupAndRestore(String collectionName) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    {\n      CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n\n      //explicitly specify the replicationFactor/pullReplicas/nrtReplicas/tlogReplicas .\n      //Value is still the same as the original. maybe test with different values that the original for better test coverage\n      if (random().nextBoolean())  {\n        restore.setReplicationFactor(replFactor);\n      }\n      if (backupCollection.getReplicas().size() > cluster.getJettySolrRunners().size()) {\n        // may need to increase maxShardsPerNode (e.g. if it was shard split, then now we need more)\n        restore.setMaxShardsPerNode((int)Math.ceil(backupCollection.getReplicas().size()/cluster.getJettySolrRunners().size()));\n      }\n      \n\n      if (rarely()) { // Try with createNodeSet configuration\n        int nodeSetSize = cluster.getJettySolrRunners().size() / 2;\n        List<String> nodeStrs = new ArrayList<>(nodeSetSize);\n        Iterator<JettySolrRunner> iter = cluster.getJettySolrRunners().iterator();\n        for (int i = 0; i < nodeSetSize ; i++) {\n          nodeStrs.add(iter.next().getNodeName());\n        }\n        restore.setCreateNodeSet(String.join(\",\", nodeStrs));\n        restore.setCreateNodeSetShuffle(usually());\n        // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n        if (restore.getMaxShardsPerNode() != null) {\n          restore.setMaxShardsPerNode(restore.getMaxShardsPerNode() * 2);\n        } else {\n          restore.setMaxShardsPerNode(origShardToDocCount.size() * 2);\n        }\n      }\n\n      Properties props = new Properties();\n      props.setProperty(\"customKey\", \"customVal\");\n      restore.setProperties(props);\n\n      if (sameConfig==false) {\n        restore.setConfigName(\"customConfigName\");\n      }\n      if (random().nextBoolean()) {\n        assertEquals(0, restore.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n      }\n      AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n          restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n    }\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getReplicationFactor(), restoreCollection.getReplicationFactor());\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(backupCollection.getActiveSlices().iterator().next().getReplicas().size(),\n        restoreCollection.getActiveSlices().iterator().next().getReplicas().size());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreCollection.getMaxShardsPerNode() ,\n          v <= restoreCollection.getMaxShardsPerNode());\n    });\n\n    assertEquals(\"Different count of nrtReplicas. Backup collection state=\" + backupCollection + \"\\nRestore \" +\n        \"collection state=\" + restoreCollection, replFactor, restoreCollection.getNumNrtReplicas().intValue());\n    assertEquals(\"Different count of pullReplicas. Backup collection state=\" + backupCollection + \"\\nRestore\" +\n        \" collection state=\" + restoreCollection, numPullReplicas, restoreCollection.getNumPullReplicas().intValue());\n    assertEquals(\"Different count of TlogReplica. Backup collection state=\" + backupCollection + \"\\nRestore\" +\n        \" collection state=\" + restoreCollection, numTlogReplicas, restoreCollection.getNumTlogReplicas().intValue());\n\n    assertEquals(\"Restore collection should use stateFormat=2\", 2, restoreCollection.getStateFormat());\n\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","sourceOld":"  private void testBackupAndRestore(String collectionName) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    {\n      CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n\n      //explicitly specify the replicationFactor/pullReplicas/nrtReplicas/tlogReplicas .\n      //Value is still the same as the original. maybe test with different values that the original for better test coverage\n      if (random().nextBoolean())  {\n        restore.setReplicationFactor(replFactor);\n      }\n      if (backupCollection.getReplicas().size() > cluster.getJettySolrRunners().size()) {\n        // may need to increase maxShardsPerNode (e.g. if it was shard split, then now we need more)\n        restore.setMaxShardsPerNode((int)Math.ceil(backupCollection.getReplicas().size()/cluster.getJettySolrRunners().size()));\n      }\n      \n\n      if (rarely()) { // Try with createNodeSet configuration\n        int nodeSetSize = cluster.getJettySolrRunners().size() / 2;\n        List<String> nodeStrs = new ArrayList<>(nodeSetSize);\n        Iterator<JettySolrRunner> iter = cluster.getJettySolrRunners().iterator();\n        for (int i = 0; i < nodeSetSize ; i++) {\n          nodeStrs.add(iter.next().getNodeName());\n        }\n        restore.setCreateNodeSet(String.join(\",\", nodeStrs));\n        restore.setCreateNodeSetShuffle(usually());\n        // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n        if (restore.getMaxShardsPerNode() != null) {\n          restore.setMaxShardsPerNode(restore.getMaxShardsPerNode() * 2);\n        } else {\n          restore.setMaxShardsPerNode(origShardToDocCount.size() * 2);\n        }\n      }\n\n      Properties props = new Properties();\n      props.setProperty(\"customKey\", \"customVal\");\n      restore.setProperties(props);\n\n      if (sameConfig==false) {\n        restore.setConfigName(\"customConfigName\");\n      }\n      if (random().nextBoolean()) {\n        assertEquals(0, restore.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n      }\n      AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n          restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n    }\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getReplicationFactor(), restoreCollection.getReplicationFactor());\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(backupCollection.getActiveSlices().iterator().next().getReplicas().size(),\n        restoreCollection.getActiveSlices().iterator().next().getReplicas().size());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreCollection.getMaxShardsPerNode() ,\n          v <= restoreCollection.getMaxShardsPerNode());\n    });\n\n    assertEquals(\"Different count of nrtReplicas. Backup collection state=\" + backupCollection + \"\\nRestore \" +\n        \"collection state=\" + restoreCollection, replFactor, restoreCollection.getNumNrtReplicas().intValue());\n    assertEquals(\"Different count of pullReplicas. Backup collection state=\" + backupCollection + \"\\nRestore\" +\n        \" collection state=\" + restoreCollection, numPullReplicas, restoreCollection.getNumPullReplicas().intValue());\n    assertEquals(\"Different count of TlogReplica. Backup collection state=\" + backupCollection + \"\\nRestore\" +\n        \" collection state=\" + restoreCollection, numTlogReplicas, restoreCollection.getNumTlogReplicas().intValue());\n\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","bugFix":null,"bugIntro":["5ad9c35f926b4bf8da0336d1300efc709c8d5a56","5ad9c35f926b4bf8da0336d1300efc709c8d5a56"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6146c07c0dee1ae1e42926167acd127fed5ef59d","date":1516129420,"type":5,"author":"Varun Thacker","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/api/collections/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String).mjava","sourceNew":"  private void testBackupAndRestore(String collectionName) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    {\n      CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n\n      //explicitly specify the replicationFactor/pullReplicas/nrtReplicas/tlogReplicas .\n      //Value is still the same as the original. maybe test with different values that the original for better test coverage\n      if (random().nextBoolean())  {\n        restore.setReplicationFactor(replFactor);\n      }\n      if (backupCollection.getReplicas().size() > cluster.getJettySolrRunners().size()) {\n        // may need to increase maxShardsPerNode (e.g. if it was shard split, then now we need more)\n        restore.setMaxShardsPerNode((int)Math.ceil(backupCollection.getReplicas().size()/cluster.getJettySolrRunners().size()));\n      }\n      \n\n      if (rarely()) { // Try with createNodeSet configuration\n        int nodeSetSize = cluster.getJettySolrRunners().size() / 2;\n        List<String> nodeStrs = new ArrayList<>(nodeSetSize);\n        Iterator<JettySolrRunner> iter = cluster.getJettySolrRunners().iterator();\n        for (int i = 0; i < nodeSetSize ; i++) {\n          nodeStrs.add(iter.next().getNodeName());\n        }\n        restore.setCreateNodeSet(String.join(\",\", nodeStrs));\n        restore.setCreateNodeSetShuffle(usually());\n        // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n        if (restore.getMaxShardsPerNode() != null) {\n          restore.setMaxShardsPerNode(restore.getMaxShardsPerNode() * 2);\n        } else {\n          restore.setMaxShardsPerNode(origShardToDocCount.size() * 2);\n        }\n      }\n\n      Properties props = new Properties();\n      props.setProperty(\"customKey\", \"customVal\");\n      restore.setProperties(props);\n\n      if (sameConfig==false) {\n        restore.setConfigName(\"customConfigName\");\n      }\n      if (random().nextBoolean()) {\n        assertEquals(0, restore.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n      }\n      AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n          restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n    }\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getReplicationFactor(), restoreCollection.getReplicationFactor());\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(backupCollection.getActiveSlices().iterator().next().getReplicas().size(),\n        restoreCollection.getActiveSlices().iterator().next().getReplicas().size());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreCollection.getMaxShardsPerNode() ,\n          v <= restoreCollection.getMaxShardsPerNode());\n    });\n\n    assertEquals(\"Different count of nrtReplicas. Backup collection state=\" + backupCollection + \"\\nRestore \" +\n        \"collection state=\" + restoreCollection, replFactor, restoreCollection.getNumNrtReplicas().intValue());\n    assertEquals(\"Different count of pullReplicas. Backup collection state=\" + backupCollection + \"\\nRestore\" +\n        \" collection state=\" + restoreCollection, numPullReplicas, restoreCollection.getNumPullReplicas().intValue());\n    assertEquals(\"Different count of TlogReplica. Backup collection state=\" + backupCollection + \"\\nRestore\" +\n        \" collection state=\" + restoreCollection, numTlogReplicas, restoreCollection.getNumTlogReplicas().intValue());\n\n    assertEquals(\"Restore collection should use stateFormat=2\", 2, restoreCollection.getStateFormat());\n\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","sourceOld":"  private void testBackupAndRestore(String collectionName) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    {\n      CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n\n      //explicitly specify the replicationFactor/pullReplicas/nrtReplicas/tlogReplicas .\n      //Value is still the same as the original. maybe test with different values that the original for better test coverage\n      if (random().nextBoolean())  {\n        restore.setReplicationFactor(replFactor);\n      }\n      if (backupCollection.getReplicas().size() > cluster.getJettySolrRunners().size()) {\n        // may need to increase maxShardsPerNode (e.g. if it was shard split, then now we need more)\n        restore.setMaxShardsPerNode((int)Math.ceil(backupCollection.getReplicas().size()/cluster.getJettySolrRunners().size()));\n      }\n      \n\n      if (rarely()) { // Try with createNodeSet configuration\n        int nodeSetSize = cluster.getJettySolrRunners().size() / 2;\n        List<String> nodeStrs = new ArrayList<>(nodeSetSize);\n        Iterator<JettySolrRunner> iter = cluster.getJettySolrRunners().iterator();\n        for (int i = 0; i < nodeSetSize ; i++) {\n          nodeStrs.add(iter.next().getNodeName());\n        }\n        restore.setCreateNodeSet(String.join(\",\", nodeStrs));\n        restore.setCreateNodeSetShuffle(usually());\n        // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n        if (restore.getMaxShardsPerNode() != null) {\n          restore.setMaxShardsPerNode(restore.getMaxShardsPerNode() * 2);\n        } else {\n          restore.setMaxShardsPerNode(origShardToDocCount.size() * 2);\n        }\n      }\n\n      Properties props = new Properties();\n      props.setProperty(\"customKey\", \"customVal\");\n      restore.setProperties(props);\n\n      if (sameConfig==false) {\n        restore.setConfigName(\"customConfigName\");\n      }\n      if (random().nextBoolean()) {\n        assertEquals(0, restore.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n      }\n      AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n          restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n    }\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getReplicationFactor(), restoreCollection.getReplicationFactor());\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(backupCollection.getActiveSlices().iterator().next().getReplicas().size(),\n        restoreCollection.getActiveSlices().iterator().next().getReplicas().size());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreCollection.getMaxShardsPerNode() ,\n          v <= restoreCollection.getMaxShardsPerNode());\n    });\n\n    assertEquals(\"Different count of nrtReplicas. Backup collection state=\" + backupCollection + \"\\nRestore \" +\n        \"collection state=\" + restoreCollection, replFactor, restoreCollection.getNumNrtReplicas().intValue());\n    assertEquals(\"Different count of pullReplicas. Backup collection state=\" + backupCollection + \"\\nRestore\" +\n        \" collection state=\" + restoreCollection, numPullReplicas, restoreCollection.getNumPullReplicas().intValue());\n    assertEquals(\"Different count of TlogReplica. Backup collection state=\" + backupCollection + \"\\nRestore\" +\n        \" collection state=\" + restoreCollection, numTlogReplicas, restoreCollection.getNumTlogReplicas().intValue());\n\n    assertEquals(\"Restore collection should use stateFormat=2\", 2, restoreCollection.getStateFormat());\n\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b94236357aaa22b76c10629851fe4e376e0cea82","date":1516710914,"type":5,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/api/collections/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String).mjava","sourceNew":"  private void testBackupAndRestore(String collectionName) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    {\n      CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n\n      //explicitly specify the replicationFactor/pullReplicas/nrtReplicas/tlogReplicas .\n      //Value is still the same as the original. maybe test with different values that the original for better test coverage\n      if (random().nextBoolean())  {\n        restore.setReplicationFactor(replFactor);\n      }\n      if (backupCollection.getReplicas().size() > cluster.getJettySolrRunners().size()) {\n        // may need to increase maxShardsPerNode (e.g. if it was shard split, then now we need more)\n        restore.setMaxShardsPerNode((int)Math.ceil(backupCollection.getReplicas().size()/cluster.getJettySolrRunners().size()));\n      }\n      \n\n      if (rarely()) { // Try with createNodeSet configuration\n        int nodeSetSize = cluster.getJettySolrRunners().size() / 2;\n        List<String> nodeStrs = new ArrayList<>(nodeSetSize);\n        Iterator<JettySolrRunner> iter = cluster.getJettySolrRunners().iterator();\n        for (int i = 0; i < nodeSetSize ; i++) {\n          nodeStrs.add(iter.next().getNodeName());\n        }\n        restore.setCreateNodeSet(String.join(\",\", nodeStrs));\n        restore.setCreateNodeSetShuffle(usually());\n        // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n        if (restore.getMaxShardsPerNode() != null) {\n          restore.setMaxShardsPerNode(restore.getMaxShardsPerNode() * 2);\n        } else {\n          restore.setMaxShardsPerNode(origShardToDocCount.size() * 2);\n        }\n      }\n\n      Properties props = new Properties();\n      props.setProperty(\"customKey\", \"customVal\");\n      restore.setProperties(props);\n\n      if (sameConfig==false) {\n        restore.setConfigName(\"customConfigName\");\n      }\n      if (random().nextBoolean()) {\n        assertEquals(0, restore.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n      }\n      AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n          restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n    }\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getReplicationFactor(), restoreCollection.getReplicationFactor());\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(backupCollection.getActiveSlices().iterator().next().getReplicas().size(),\n        restoreCollection.getActiveSlices().iterator().next().getReplicas().size());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreCollection.getMaxShardsPerNode() ,\n          v <= restoreCollection.getMaxShardsPerNode());\n    });\n\n    assertEquals(\"Different count of nrtReplicas. Backup collection state=\" + backupCollection + \"\\nRestore \" +\n        \"collection state=\" + restoreCollection, replFactor, restoreCollection.getNumNrtReplicas().intValue());\n    assertEquals(\"Different count of pullReplicas. Backup collection state=\" + backupCollection + \"\\nRestore\" +\n        \" collection state=\" + restoreCollection, numPullReplicas, restoreCollection.getNumPullReplicas().intValue());\n    assertEquals(\"Different count of TlogReplica. Backup collection state=\" + backupCollection + \"\\nRestore\" +\n        \" collection state=\" + restoreCollection, numTlogReplicas, restoreCollection.getNumTlogReplicas().intValue());\n\n    assertEquals(\"Restore collection should use stateFormat=2\", 2, restoreCollection.getStateFormat());\n\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","sourceOld":"  private void testBackupAndRestore(String collectionName) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    {\n      CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n\n      //explicitly specify the replicationFactor/pullReplicas/nrtReplicas/tlogReplicas .\n      //Value is still the same as the original. maybe test with different values that the original for better test coverage\n      if (random().nextBoolean())  {\n        restore.setReplicationFactor(replFactor);\n      }\n      if (backupCollection.getReplicas().size() > cluster.getJettySolrRunners().size()) {\n        // may need to increase maxShardsPerNode (e.g. if it was shard split, then now we need more)\n        restore.setMaxShardsPerNode((int)Math.ceil(backupCollection.getReplicas().size()/cluster.getJettySolrRunners().size()));\n      }\n      \n\n      if (rarely()) { // Try with createNodeSet configuration\n        int nodeSetSize = cluster.getJettySolrRunners().size() / 2;\n        List<String> nodeStrs = new ArrayList<>(nodeSetSize);\n        Iterator<JettySolrRunner> iter = cluster.getJettySolrRunners().iterator();\n        for (int i = 0; i < nodeSetSize ; i++) {\n          nodeStrs.add(iter.next().getNodeName());\n        }\n        restore.setCreateNodeSet(String.join(\",\", nodeStrs));\n        restore.setCreateNodeSetShuffle(usually());\n        // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n        if (restore.getMaxShardsPerNode() != null) {\n          restore.setMaxShardsPerNode(restore.getMaxShardsPerNode() * 2);\n        } else {\n          restore.setMaxShardsPerNode(origShardToDocCount.size() * 2);\n        }\n      }\n\n      Properties props = new Properties();\n      props.setProperty(\"customKey\", \"customVal\");\n      restore.setProperties(props);\n\n      if (sameConfig==false) {\n        restore.setConfigName(\"customConfigName\");\n      }\n      if (random().nextBoolean()) {\n        assertEquals(0, restore.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n      }\n      AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n          restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n    }\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getReplicationFactor(), restoreCollection.getReplicationFactor());\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(backupCollection.getActiveSlices().iterator().next().getReplicas().size(),\n        restoreCollection.getActiveSlices().iterator().next().getReplicas().size());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreCollection.getMaxShardsPerNode() ,\n          v <= restoreCollection.getMaxShardsPerNode());\n    });\n\n    assertEquals(\"Different count of nrtReplicas. Backup collection state=\" + backupCollection + \"\\nRestore \" +\n        \"collection state=\" + restoreCollection, replFactor, restoreCollection.getNumNrtReplicas().intValue());\n    assertEquals(\"Different count of pullReplicas. Backup collection state=\" + backupCollection + \"\\nRestore\" +\n        \" collection state=\" + restoreCollection, numPullReplicas, restoreCollection.getNumPullReplicas().intValue());\n    assertEquals(\"Different count of TlogReplica. Backup collection state=\" + backupCollection + \"\\nRestore\" +\n        \" collection state=\" + restoreCollection, numTlogReplicas, restoreCollection.getNumTlogReplicas().intValue());\n\n    assertEquals(\"Restore collection should use stateFormat=2\", 2, restoreCollection.getStateFormat());\n\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"4c623a7f72be34d6c45bee682028c50327d9e4b7":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"b94236357aaa22b76c10629851fe4e376e0cea82":["529c69423fc9de95d4d764f4c998f095fead50bd","6146c07c0dee1ae1e42926167acd127fed5ef59d"],"b31ebc7a867ddea79d438a8fca876a94e644d11a":["4c623a7f72be34d6c45bee682028c50327d9e4b7","d18dd44acd824af8b51a5994c9475b32b094fb76"],"61c45e99cf6676da48f19d7511c73712ad39402b":["d18dd44acd824af8b51a5994c9475b32b094fb76"],"6146c07c0dee1ae1e42926167acd127fed5ef59d":["529c69423fc9de95d4d764f4c998f095fead50bd"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","4c623a7f72be34d6c45bee682028c50327d9e4b7"],"d18dd44acd824af8b51a5994c9475b32b094fb76":["4c623a7f72be34d6c45bee682028c50327d9e4b7"],"e9017cf144952056066919f1ebc7897ff9bd71b1":["4c623a7f72be34d6c45bee682028c50327d9e4b7","c84db5cfadecb30316e2cc080dd63920b5e158bc"],"529c69423fc9de95d4d764f4c998f095fead50bd":["fe190f27cc4076d6681cb10c6a326503ad5a17e7"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"c84db5cfadecb30316e2cc080dd63920b5e158bc":["61c45e99cf6676da48f19d7511c73712ad39402b"],"fe190f27cc4076d6681cb10c6a326503ad5a17e7":["d1f5728f32a4a256b36cfabd7a2636452f599bb9"],"d1f5728f32a4a256b36cfabd7a2636452f599bb9":["61c45e99cf6676da48f19d7511c73712ad39402b","c84db5cfadecb30316e2cc080dd63920b5e158bc"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b94236357aaa22b76c10629851fe4e376e0cea82"]},"commit2Childs":{"4c623a7f72be34d6c45bee682028c50327d9e4b7":["b31ebc7a867ddea79d438a8fca876a94e644d11a","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","d18dd44acd824af8b51a5994c9475b32b094fb76","e9017cf144952056066919f1ebc7897ff9bd71b1"],"b94236357aaa22b76c10629851fe4e376e0cea82":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"b31ebc7a867ddea79d438a8fca876a94e644d11a":[],"61c45e99cf6676da48f19d7511c73712ad39402b":["c84db5cfadecb30316e2cc080dd63920b5e158bc","d1f5728f32a4a256b36cfabd7a2636452f599bb9"],"6146c07c0dee1ae1e42926167acd127fed5ef59d":["b94236357aaa22b76c10629851fe4e376e0cea82"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"d18dd44acd824af8b51a5994c9475b32b094fb76":["b31ebc7a867ddea79d438a8fca876a94e644d11a","61c45e99cf6676da48f19d7511c73712ad39402b"],"e9017cf144952056066919f1ebc7897ff9bd71b1":[],"529c69423fc9de95d4d764f4c998f095fead50bd":["b94236357aaa22b76c10629851fe4e376e0cea82","6146c07c0dee1ae1e42926167acd127fed5ef59d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["4c623a7f72be34d6c45bee682028c50327d9e4b7","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"c84db5cfadecb30316e2cc080dd63920b5e158bc":["e9017cf144952056066919f1ebc7897ff9bd71b1","d1f5728f32a4a256b36cfabd7a2636452f599bb9"],"fe190f27cc4076d6681cb10c6a326503ad5a17e7":["529c69423fc9de95d4d764f4c998f095fead50bd"],"d1f5728f32a4a256b36cfabd7a2636452f599bb9":["fe190f27cc4076d6681cb10c6a326503ad5a17e7"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["b31ebc7a867ddea79d438a8fca876a94e644d11a","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","e9017cf144952056066919f1ebc7897ff9bd71b1","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}