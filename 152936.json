{"path":"lucene/analysis/common/src/java/org/apache/lucene/analysis/cjk/CJKBigramFilter#refill().mjava","commits":[{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/cjk/CJKBigramFilter#refill().mjava","pathOld":"modules/analysis/common/src/java/org/apache/lucene/analysis/cjk/CJKBigramFilter#refill().mjava","sourceNew":"  /**\n   * refills buffers with new data from the current token.\n   */\n  private void refill() throws IOException {\n    // compact buffers to keep them smallish if they become large\n    // just a safety check, but technically we only need the last codepoint\n    if (bufferLen > 64) {\n      int last = bufferLen - 1;\n      buffer[0] = buffer[last];\n      startOffset[0] = startOffset[last];\n      endOffset[0] = endOffset[last];\n      bufferLen = 1;\n      index -= last;\n    }\n\n    char termBuffer[] = termAtt.buffer();\n    int len = termAtt.length();\n    int start = offsetAtt.startOffset();\n    int end = offsetAtt.endOffset();\n    \n    int newSize = bufferLen + len;\n    buffer = ArrayUtil.grow(buffer, newSize);\n    startOffset = ArrayUtil.grow(startOffset, newSize);\n    endOffset = ArrayUtil.grow(endOffset, newSize);\n    lastEndOffset = end;\n\n    if (end - start != len) {\n      // crazy offsets (modified by synonym or charfilter): just preserve\n      for (int i = 0, cp = 0; i < len; i += Character.charCount(cp)) {\n        cp = buffer[bufferLen] = Character.codePointAt(termBuffer, i, len);\n        startOffset[bufferLen] = start;\n        endOffset[bufferLen] = end;\n        bufferLen++;\n      }\n    } else {\n      // normal offsets\n      for (int i = 0, cp = 0, cpLen = 0; i < len; i += cpLen) {\n        cp = buffer[bufferLen] = Character.codePointAt(termBuffer, i, len);\n        cpLen = Character.charCount(cp);\n        startOffset[bufferLen] = start;\n        start = endOffset[bufferLen] = start + cpLen;\n        bufferLen++;\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * refills buffers with new data from the current token.\n   */\n  private void refill() throws IOException {\n    // compact buffers to keep them smallish if they become large\n    // just a safety check, but technically we only need the last codepoint\n    if (bufferLen > 64) {\n      int last = bufferLen - 1;\n      buffer[0] = buffer[last];\n      startOffset[0] = startOffset[last];\n      endOffset[0] = endOffset[last];\n      bufferLen = 1;\n      index -= last;\n    }\n\n    char termBuffer[] = termAtt.buffer();\n    int len = termAtt.length();\n    int start = offsetAtt.startOffset();\n    int end = offsetAtt.endOffset();\n    \n    int newSize = bufferLen + len;\n    buffer = ArrayUtil.grow(buffer, newSize);\n    startOffset = ArrayUtil.grow(startOffset, newSize);\n    endOffset = ArrayUtil.grow(endOffset, newSize);\n    lastEndOffset = end;\n\n    if (end - start != len) {\n      // crazy offsets (modified by synonym or charfilter): just preserve\n      for (int i = 0, cp = 0; i < len; i += Character.charCount(cp)) {\n        cp = buffer[bufferLen] = Character.codePointAt(termBuffer, i, len);\n        startOffset[bufferLen] = start;\n        endOffset[bufferLen] = end;\n        bufferLen++;\n      }\n    } else {\n      // normal offsets\n      for (int i = 0, cp = 0, cpLen = 0; i < len; i += cpLen) {\n        cp = buffer[bufferLen] = Character.codePointAt(termBuffer, i, len);\n        cpLen = Character.charCount(cp);\n        startOffset[bufferLen] = start;\n        start = endOffset[bufferLen] = start + cpLen;\n        bufferLen++;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4d3e8520fd031bab31fd0e4d480e55958bc45efe","date":1340901565,"type":3,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/cjk/CJKBigramFilter#refill().mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/cjk/CJKBigramFilter#refill().mjava","sourceNew":"  /**\n   * refills buffers with new data from the current token.\n   */\n  private void refill() {\n    // compact buffers to keep them smallish if they become large\n    // just a safety check, but technically we only need the last codepoint\n    if (bufferLen > 64) {\n      int last = bufferLen - 1;\n      buffer[0] = buffer[last];\n      startOffset[0] = startOffset[last];\n      endOffset[0] = endOffset[last];\n      bufferLen = 1;\n      index -= last;\n    }\n\n    char termBuffer[] = termAtt.buffer();\n    int len = termAtt.length();\n    int start = offsetAtt.startOffset();\n    int end = offsetAtt.endOffset();\n    \n    int newSize = bufferLen + len;\n    buffer = ArrayUtil.grow(buffer, newSize);\n    startOffset = ArrayUtil.grow(startOffset, newSize);\n    endOffset = ArrayUtil.grow(endOffset, newSize);\n    lastEndOffset = end;\n\n    if (end - start != len) {\n      // crazy offsets (modified by synonym or charfilter): just preserve\n      for (int i = 0, cp = 0; i < len; i += Character.charCount(cp)) {\n        cp = buffer[bufferLen] = Character.codePointAt(termBuffer, i, len);\n        startOffset[bufferLen] = start;\n        endOffset[bufferLen] = end;\n        bufferLen++;\n      }\n    } else {\n      // normal offsets\n      for (int i = 0, cp = 0, cpLen = 0; i < len; i += cpLen) {\n        cp = buffer[bufferLen] = Character.codePointAt(termBuffer, i, len);\n        cpLen = Character.charCount(cp);\n        startOffset[bufferLen] = start;\n        start = endOffset[bufferLen] = start + cpLen;\n        bufferLen++;\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * refills buffers with new data from the current token.\n   */\n  private void refill() throws IOException {\n    // compact buffers to keep them smallish if they become large\n    // just a safety check, but technically we only need the last codepoint\n    if (bufferLen > 64) {\n      int last = bufferLen - 1;\n      buffer[0] = buffer[last];\n      startOffset[0] = startOffset[last];\n      endOffset[0] = endOffset[last];\n      bufferLen = 1;\n      index -= last;\n    }\n\n    char termBuffer[] = termAtt.buffer();\n    int len = termAtt.length();\n    int start = offsetAtt.startOffset();\n    int end = offsetAtt.endOffset();\n    \n    int newSize = bufferLen + len;\n    buffer = ArrayUtil.grow(buffer, newSize);\n    startOffset = ArrayUtil.grow(startOffset, newSize);\n    endOffset = ArrayUtil.grow(endOffset, newSize);\n    lastEndOffset = end;\n\n    if (end - start != len) {\n      // crazy offsets (modified by synonym or charfilter): just preserve\n      for (int i = 0, cp = 0; i < len; i += Character.charCount(cp)) {\n        cp = buffer[bufferLen] = Character.codePointAt(termBuffer, i, len);\n        startOffset[bufferLen] = start;\n        endOffset[bufferLen] = end;\n        bufferLen++;\n      }\n    } else {\n      // normal offsets\n      for (int i = 0, cp = 0, cpLen = 0; i < len; i += cpLen) {\n        cp = buffer[bufferLen] = Character.codePointAt(termBuffer, i, len);\n        cpLen = Character.charCount(cp);\n        startOffset[bufferLen] = start;\n        start = endOffset[bufferLen] = start + cpLen;\n        bufferLen++;\n      }\n    }\n  }\n\n","bugFix":["680e7f726eaa1fd44ade8bc1f8a02f452ca7c4ee"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe33227f6805edab2036cbb80645cc4e2d1fa424","date":1342713534,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/cjk/CJKBigramFilter#refill().mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/cjk/CJKBigramFilter#refill().mjava","sourceNew":"  /**\n   * refills buffers with new data from the current token.\n   */\n  private void refill() {\n    // compact buffers to keep them smallish if they become large\n    // just a safety check, but technically we only need the last codepoint\n    if (bufferLen > 64) {\n      int last = bufferLen - 1;\n      buffer[0] = buffer[last];\n      startOffset[0] = startOffset[last];\n      endOffset[0] = endOffset[last];\n      bufferLen = 1;\n      index -= last;\n    }\n\n    char termBuffer[] = termAtt.buffer();\n    int len = termAtt.length();\n    int start = offsetAtt.startOffset();\n    int end = offsetAtt.endOffset();\n    \n    int newSize = bufferLen + len;\n    buffer = ArrayUtil.grow(buffer, newSize);\n    startOffset = ArrayUtil.grow(startOffset, newSize);\n    endOffset = ArrayUtil.grow(endOffset, newSize);\n    lastEndOffset = end;\n\n    if (end - start != len) {\n      // crazy offsets (modified by synonym or charfilter): just preserve\n      for (int i = 0, cp = 0; i < len; i += Character.charCount(cp)) {\n        cp = buffer[bufferLen] = Character.codePointAt(termBuffer, i, len);\n        startOffset[bufferLen] = start;\n        endOffset[bufferLen] = end;\n        bufferLen++;\n      }\n    } else {\n      // normal offsets\n      for (int i = 0, cp = 0, cpLen = 0; i < len; i += cpLen) {\n        cp = buffer[bufferLen] = Character.codePointAt(termBuffer, i, len);\n        cpLen = Character.charCount(cp);\n        startOffset[bufferLen] = start;\n        start = endOffset[bufferLen] = start + cpLen;\n        bufferLen++;\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * refills buffers with new data from the current token.\n   */\n  private void refill() throws IOException {\n    // compact buffers to keep them smallish if they become large\n    // just a safety check, but technically we only need the last codepoint\n    if (bufferLen > 64) {\n      int last = bufferLen - 1;\n      buffer[0] = buffer[last];\n      startOffset[0] = startOffset[last];\n      endOffset[0] = endOffset[last];\n      bufferLen = 1;\n      index -= last;\n    }\n\n    char termBuffer[] = termAtt.buffer();\n    int len = termAtt.length();\n    int start = offsetAtt.startOffset();\n    int end = offsetAtt.endOffset();\n    \n    int newSize = bufferLen + len;\n    buffer = ArrayUtil.grow(buffer, newSize);\n    startOffset = ArrayUtil.grow(startOffset, newSize);\n    endOffset = ArrayUtil.grow(endOffset, newSize);\n    lastEndOffset = end;\n\n    if (end - start != len) {\n      // crazy offsets (modified by synonym or charfilter): just preserve\n      for (int i = 0, cp = 0; i < len; i += Character.charCount(cp)) {\n        cp = buffer[bufferLen] = Character.codePointAt(termBuffer, i, len);\n        startOffset[bufferLen] = start;\n        endOffset[bufferLen] = end;\n        bufferLen++;\n      }\n    } else {\n      // normal offsets\n      for (int i = 0, cp = 0, cpLen = 0; i < len; i += cpLen) {\n        cp = buffer[bufferLen] = Character.codePointAt(termBuffer, i, len);\n        cpLen = Character.charCount(cp);\n        startOffset[bufferLen] = start;\n        start = endOffset[bufferLen] = start + cpLen;\n        bufferLen++;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":["b89678825b68eccaf09e6ab71675fc0b0af1e099","4d3e8520fd031bab31fd0e4d480e55958bc45efe"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"4d3e8520fd031bab31fd0e4d480e55958bc45efe":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["4d3e8520fd031bab31fd0e4d480e55958bc45efe"]},"commit2Childs":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["fe33227f6805edab2036cbb80645cc4e2d1fa424","4d3e8520fd031bab31fd0e4d480e55958bc45efe"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"4d3e8520fd031bab31fd0e4d480e55958bc45efe":["fe33227f6805edab2036cbb80645cc4e2d1fa424","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["fe33227f6805edab2036cbb80645cc4e2d1fa424","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}