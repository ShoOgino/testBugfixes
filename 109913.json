{"path":"lucene/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum#testReuseDocsEnumNoReuse().mjava","commits":[{"id":"a0ae5e3ed1232483b7b8a014f175a5fe43595982","date":1324062192,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum#testReuseDocsEnumNoReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/codecs/lucene40/TestReuseDocsEnum#testReuseDocsEnumNoReuse().mjava","sourceNew":"  public void testReuseDocsEnumNoReuse() throws IOException {\n    Directory dir = newDirectory();\n    Codec cp = _TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat());\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));\n    int numdocs = atLeast(20);\n    createRandomIndex(numdocs, writer, random);\n    writer.commit();\n\n    IndexReader open = IndexReader.open(dir);\n    IndexReader[] sequentialSubReaders = open.getSequentialSubReaders();\n    for (IndexReader indexReader : sequentialSubReaders) {\n      Terms terms = indexReader.terms(\"body\");\n      TermsEnum iterator = terms.iterator(null);\n      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<DocsEnum, Boolean>();\n      MatchNoBits bits = new Bits.MatchNoBits(open.maxDoc());\n      while ((iterator.next()) != null) {\n        DocsEnum docs = iterator.docs(random.nextBoolean() ? bits : new Bits.MatchNoBits(open.maxDoc()), null, random.nextBoolean());\n        enums.put(docs, true);\n      }\n      \n      assertEquals(terms.getUniqueTermCount(), enums.size());  \n    }\n    IOUtils.close(writer, open, dir);\n  }\n\n","sourceOld":"  public void testReuseDocsEnumNoReuse() throws IOException {\n    Directory dir = newDirectory();\n    Codec cp = _TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat());\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));\n    int numdocs = atLeast(20);\n    createRandomIndex(numdocs, writer, random);\n    writer.commit();\n\n    IndexReader open = IndexReader.open(dir);\n    IndexReader[] sequentialSubReaders = open.getSequentialSubReaders();\n    for (IndexReader indexReader : sequentialSubReaders) {\n      Terms terms = indexReader.terms(\"body\");\n      TermsEnum iterator = terms.iterator(null);\n      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<DocsEnum, Boolean>();\n      MatchNoBits bits = new Bits.MatchNoBits(open.maxDoc());\n      while ((iterator.next()) != null) {\n        DocsEnum docs = iterator.docs(random.nextBoolean() ? bits : new Bits.MatchNoBits(open.maxDoc()), null, random.nextBoolean());\n        enums.put(docs, true);\n      }\n      \n      assertEquals(terms.getUniqueTermCount(), enums.size());  \n    }\n    IOUtils.close(writer, open, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b4cf6c21bb1d9b45355c090dc32ef9a4fdb6a1c6","date":1327848875,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum#testReuseDocsEnumNoReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum#testReuseDocsEnumNoReuse().mjava","sourceNew":"  public void testReuseDocsEnumNoReuse() throws IOException {\n    Directory dir = newDirectory();\n    Codec cp = _TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat());\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));\n    int numdocs = atLeast(20);\n    createRandomIndex(numdocs, writer, random);\n    writer.commit();\n\n    IndexReader open = IndexReader.open(dir);\n    new ReaderUtil.Gather(open) {\n      @Override\n      protected void add(int base, AtomicIndexReader r) throws IOException {\n        Terms terms = r.terms(\"body\");\n        TermsEnum iterator = terms.iterator(null);\n        IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<DocsEnum, Boolean>();\n        MatchNoBits bits = new Bits.MatchNoBits(r.maxDoc());\n        while ((iterator.next()) != null) {\n          DocsEnum docs = iterator.docs(random.nextBoolean() ? bits : new Bits.MatchNoBits(r.maxDoc()), null, random.nextBoolean());\n          enums.put(docs, true);\n        }\n        \n        assertEquals(terms.getUniqueTermCount(), enums.size());  \n      }\n    }.run();\n    IOUtils.close(writer, open, dir);\n  }\n\n","sourceOld":"  public void testReuseDocsEnumNoReuse() throws IOException {\n    Directory dir = newDirectory();\n    Codec cp = _TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat());\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));\n    int numdocs = atLeast(20);\n    createRandomIndex(numdocs, writer, random);\n    writer.commit();\n\n    IndexReader open = IndexReader.open(dir);\n    IndexReader[] sequentialSubReaders = open.getSequentialSubReaders();\n    for (IndexReader indexReader : sequentialSubReaders) {\n      Terms terms = indexReader.terms(\"body\");\n      TermsEnum iterator = terms.iterator(null);\n      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<DocsEnum, Boolean>();\n      MatchNoBits bits = new Bits.MatchNoBits(open.maxDoc());\n      while ((iterator.next()) != null) {\n        DocsEnum docs = iterator.docs(random.nextBoolean() ? bits : new Bits.MatchNoBits(open.maxDoc()), null, random.nextBoolean());\n        enums.put(docs, true);\n      }\n      \n      assertEquals(terms.getUniqueTermCount(), enums.size());  \n    }\n    IOUtils.close(writer, open, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d40b62adb64d8f7b2f85ee849349cfb0bef03f45","date":1327855938,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum#testReuseDocsEnumNoReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum#testReuseDocsEnumNoReuse().mjava","sourceNew":"  public void testReuseDocsEnumNoReuse() throws IOException {\n    Directory dir = newDirectory();\n    Codec cp = _TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat());\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));\n    int numdocs = atLeast(20);\n    createRandomIndex(numdocs, writer, random);\n    writer.commit();\n\n    DirectoryReader open = DirectoryReader.open(dir);\n    new ReaderUtil.Gather(open) {\n      @Override\n      protected void add(int base, AtomicIndexReader r) throws IOException {\n        Terms terms = r.terms(\"body\");\n        TermsEnum iterator = terms.iterator(null);\n        IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<DocsEnum, Boolean>();\n        MatchNoBits bits = new Bits.MatchNoBits(r.maxDoc());\n        while ((iterator.next()) != null) {\n          DocsEnum docs = iterator.docs(random.nextBoolean() ? bits : new Bits.MatchNoBits(r.maxDoc()), null, random.nextBoolean());\n          enums.put(docs, true);\n        }\n        \n        assertEquals(terms.getUniqueTermCount(), enums.size());  \n      }\n    }.run();\n    IOUtils.close(writer, open, dir);\n  }\n\n","sourceOld":"  public void testReuseDocsEnumNoReuse() throws IOException {\n    Directory dir = newDirectory();\n    Codec cp = _TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat());\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));\n    int numdocs = atLeast(20);\n    createRandomIndex(numdocs, writer, random);\n    writer.commit();\n\n    IndexReader open = IndexReader.open(dir);\n    new ReaderUtil.Gather(open) {\n      @Override\n      protected void add(int base, AtomicIndexReader r) throws IOException {\n        Terms terms = r.terms(\"body\");\n        TermsEnum iterator = terms.iterator(null);\n        IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<DocsEnum, Boolean>();\n        MatchNoBits bits = new Bits.MatchNoBits(r.maxDoc());\n        while ((iterator.next()) != null) {\n          DocsEnum docs = iterator.docs(random.nextBoolean() ? bits : new Bits.MatchNoBits(r.maxDoc()), null, random.nextBoolean());\n          enums.put(docs, true);\n        }\n        \n        assertEquals(terms.getUniqueTermCount(), enums.size());  \n      }\n    }.run();\n    IOUtils.close(writer, open, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"da6d5ac19a80d65b1e864251f155d30960353b7e","date":1327881054,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum#testReuseDocsEnumNoReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum#testReuseDocsEnumNoReuse().mjava","sourceNew":"  public void testReuseDocsEnumNoReuse() throws IOException {\n    Directory dir = newDirectory();\n    Codec cp = _TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat());\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));\n    int numdocs = atLeast(20);\n    createRandomIndex(numdocs, writer, random);\n    writer.commit();\n\n    DirectoryReader open = DirectoryReader.open(dir);\n    new ReaderUtil.Gather(open) {\n      @Override\n      protected void add(int base, AtomicReader r) throws IOException {\n        Terms terms = r.terms(\"body\");\n        TermsEnum iterator = terms.iterator(null);\n        IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<DocsEnum, Boolean>();\n        MatchNoBits bits = new Bits.MatchNoBits(r.maxDoc());\n        while ((iterator.next()) != null) {\n          DocsEnum docs = iterator.docs(random.nextBoolean() ? bits : new Bits.MatchNoBits(r.maxDoc()), null, random.nextBoolean());\n          enums.put(docs, true);\n        }\n        \n        assertEquals(terms.getUniqueTermCount(), enums.size());  \n      }\n    }.run();\n    IOUtils.close(writer, open, dir);\n  }\n\n","sourceOld":"  public void testReuseDocsEnumNoReuse() throws IOException {\n    Directory dir = newDirectory();\n    Codec cp = _TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat());\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));\n    int numdocs = atLeast(20);\n    createRandomIndex(numdocs, writer, random);\n    writer.commit();\n\n    DirectoryReader open = DirectoryReader.open(dir);\n    new ReaderUtil.Gather(open) {\n      @Override\n      protected void add(int base, AtomicIndexReader r) throws IOException {\n        Terms terms = r.terms(\"body\");\n        TermsEnum iterator = terms.iterator(null);\n        IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<DocsEnum, Boolean>();\n        MatchNoBits bits = new Bits.MatchNoBits(r.maxDoc());\n        while ((iterator.next()) != null) {\n          DocsEnum docs = iterator.docs(random.nextBoolean() ? bits : new Bits.MatchNoBits(r.maxDoc()), null, random.nextBoolean());\n          enums.put(docs, true);\n        }\n        \n        assertEquals(terms.getUniqueTermCount(), enums.size());  \n      }\n    }.run();\n    IOUtils.close(writer, open, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5cab9a86bd67202d20b6adc463008c8e982b070a","date":1327966443,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum#testReuseDocsEnumNoReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum#testReuseDocsEnumNoReuse().mjava","sourceNew":"  public void testReuseDocsEnumNoReuse() throws IOException {\n    Directory dir = newDirectory();\n    Codec cp = _TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat());\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));\n    int numdocs = atLeast(20);\n    createRandomIndex(numdocs, writer, random);\n    writer.commit();\n\n    DirectoryReader open = DirectoryReader.open(dir);\n    new ReaderUtil.Gather(open) {\n      @Override\n      protected void add(int base, AtomicReader r) throws IOException {\n        Terms terms = r.terms(\"body\");\n        TermsEnum iterator = terms.iterator(null);\n        IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<DocsEnum, Boolean>();\n        MatchNoBits bits = new Bits.MatchNoBits(r.maxDoc());\n        while ((iterator.next()) != null) {\n          DocsEnum docs = iterator.docs(random.nextBoolean() ? bits : new Bits.MatchNoBits(r.maxDoc()), null, random.nextBoolean());\n          enums.put(docs, true);\n        }\n        \n        assertEquals(terms.getUniqueTermCount(), enums.size());  \n      }\n    }.run();\n    IOUtils.close(writer, open, dir);\n  }\n\n","sourceOld":"  public void testReuseDocsEnumNoReuse() throws IOException {\n    Directory dir = newDirectory();\n    Codec cp = _TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat());\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));\n    int numdocs = atLeast(20);\n    createRandomIndex(numdocs, writer, random);\n    writer.commit();\n\n    IndexReader open = IndexReader.open(dir);\n    IndexReader[] sequentialSubReaders = open.getSequentialSubReaders();\n    for (IndexReader indexReader : sequentialSubReaders) {\n      Terms terms = indexReader.terms(\"body\");\n      TermsEnum iterator = terms.iterator(null);\n      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<DocsEnum, Boolean>();\n      MatchNoBits bits = new Bits.MatchNoBits(open.maxDoc());\n      while ((iterator.next()) != null) {\n        DocsEnum docs = iterator.docs(random.nextBoolean() ? bits : new Bits.MatchNoBits(open.maxDoc()), null, random.nextBoolean());\n        enums.put(docs, true);\n      }\n      \n      assertEquals(terms.getUniqueTermCount(), enums.size());  \n    }\n    IOUtils.close(writer, open, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum#testReuseDocsEnumNoReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum#testReuseDocsEnumNoReuse().mjava","sourceNew":"  public void testReuseDocsEnumNoReuse() throws IOException {\n    Directory dir = newDirectory();\n    Codec cp = _TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat());\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));\n    int numdocs = atLeast(20);\n    createRandomIndex(numdocs, writer, random);\n    writer.commit();\n\n    DirectoryReader open = DirectoryReader.open(dir);\n    new ReaderUtil.Gather(open) {\n      @Override\n      protected void add(int base, AtomicReader r) throws IOException {\n        Terms terms = r.terms(\"body\");\n        TermsEnum iterator = terms.iterator(null);\n        IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<DocsEnum, Boolean>();\n        MatchNoBits bits = new Bits.MatchNoBits(r.maxDoc());\n        while ((iterator.next()) != null) {\n          DocsEnum docs = iterator.docs(random.nextBoolean() ? bits : new Bits.MatchNoBits(r.maxDoc()), null, random.nextBoolean());\n          enums.put(docs, true);\n        }\n        \n        assertEquals(terms.getUniqueTermCount(), enums.size());  \n      }\n    }.run();\n    IOUtils.close(writer, open, dir);\n  }\n\n","sourceOld":"  public void testReuseDocsEnumNoReuse() throws IOException {\n    Directory dir = newDirectory();\n    Codec cp = _TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat());\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));\n    int numdocs = atLeast(20);\n    createRandomIndex(numdocs, writer, random);\n    writer.commit();\n\n    DirectoryReader open = DirectoryReader.open(dir);\n    new ReaderUtil.Gather(open) {\n      @Override\n      protected void add(int base, AtomicReader r) throws IOException {\n        Terms terms = r.terms(\"body\");\n        TermsEnum iterator = terms.iterator(null);\n        IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<DocsEnum, Boolean>();\n        MatchNoBits bits = new Bits.MatchNoBits(r.maxDoc());\n        while ((iterator.next()) != null) {\n          DocsEnum docs = iterator.docs(random.nextBoolean() ? bits : new Bits.MatchNoBits(r.maxDoc()), null, random.nextBoolean());\n          enums.put(docs, true);\n        }\n        \n        assertEquals(terms.getUniqueTermCount(), enums.size());  \n      }\n    }.run();\n    IOUtils.close(writer, open, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"da6d5ac19a80d65b1e864251f155d30960353b7e":["d40b62adb64d8f7b2f85ee849349cfb0bef03f45"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"b4cf6c21bb1d9b45355c090dc32ef9a4fdb6a1c6":["a0ae5e3ed1232483b7b8a014f175a5fe43595982"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d40b62adb64d8f7b2f85ee849349cfb0bef03f45":["b4cf6c21bb1d9b45355c090dc32ef9a4fdb6a1c6"],"a0ae5e3ed1232483b7b8a014f175a5fe43595982":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["a0ae5e3ed1232483b7b8a014f175a5fe43595982","da6d5ac19a80d65b1e864251f155d30960353b7e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"da6d5ac19a80d65b1e864251f155d30960353b7e":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"b4cf6c21bb1d9b45355c090dc32ef9a4fdb6a1c6":["d40b62adb64d8f7b2f85ee849349cfb0bef03f45"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a0ae5e3ed1232483b7b8a014f175a5fe43595982"],"d40b62adb64d8f7b2f85ee849349cfb0bef03f45":["da6d5ac19a80d65b1e864251f155d30960353b7e"],"a0ae5e3ed1232483b7b8a014f175a5fe43595982":["b4cf6c21bb1d9b45355c090dc32ef9a4fdb6a1c6","5cab9a86bd67202d20b6adc463008c8e982b070a"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}