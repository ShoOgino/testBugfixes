{"path":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","commits":[{"id":"ba8cd9ac7a0f3ad09c6c94f53c4c415d13e72723","date":1286186570,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","pathOld":"/dev/null","sourceNew":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefaultCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    int TERMS_PER_DOC = 1000000;\n\n    Directory dir = FSDirectory.open(_TestUtil.getTempDir(\"2BTerms\"));\n    IndexWriter w = new IndexWriter(dir,\n                                    newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())\n                                                  .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                                .setRAMBufferSizeMB(256.0).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((LogMergePolicy) w.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) w.getConfig().getMergePolicy()).setUseCompoundDocStore(false);\n    ((LogMergePolicy) w.getConfig().getMergePolicy()).setMergeFactor(10);\n\n    Document doc = new Document();\n    Field field = new Field(\"field\", new MyTokenStream(TERMS_PER_DOC));\n    field.setOmitTermFreqAndPositions(true);\n    field.setOmitNorms(true);\n    doc.add(field);\n    //w.setInfoStream(System.out);\n    final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n    for(int i=0;i<numDocs;i++) {\n      w.addDocument(doc);\n      System.out.println(i + \" of \" + numDocs);\n    }\n    System.out.println(\"now optimize...\");\n    w.optimize();\n    w.close();\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["985161df9b1c14b4aa862d74d48cf51561579e62"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a6c7b564e6275fb0c0e137d84fda55b447c19d9c","date":1286438356,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","sourceNew":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefaultCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    int TERMS_PER_DOC = 1000000;\n\n    Directory dir = FSDirectory.open(_TestUtil.getTempDir(\"2BTerms\"));\n    IndexWriter w = new IndexWriter(dir,\n                                    new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())\n                                                  .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                                .setRAMBufferSizeMB(256.0).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((LogMergePolicy) w.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) w.getConfig().getMergePolicy()).setUseCompoundDocStore(false);\n    ((LogMergePolicy) w.getConfig().getMergePolicy()).setMergeFactor(10);\n\n    Document doc = new Document();\n    Field field = new Field(\"field\", new MyTokenStream(TERMS_PER_DOC));\n    field.setOmitTermFreqAndPositions(true);\n    field.setOmitNorms(true);\n    doc.add(field);\n    //w.setInfoStream(System.out);\n    final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n    for(int i=0;i<numDocs;i++) {\n      final long t0 = System.currentTimeMillis();\n      w.addDocument(doc);\n      System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n    }\n    System.out.println(\"now optimize...\");\n    w.optimize();\n    w.close();\n\n    System.out.println(\"now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n  }\n\n","sourceOld":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefaultCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    int TERMS_PER_DOC = 1000000;\n\n    Directory dir = FSDirectory.open(_TestUtil.getTempDir(\"2BTerms\"));\n    IndexWriter w = new IndexWriter(dir,\n                                    newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())\n                                                  .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                                .setRAMBufferSizeMB(256.0).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((LogMergePolicy) w.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) w.getConfig().getMergePolicy()).setUseCompoundDocStore(false);\n    ((LogMergePolicy) w.getConfig().getMergePolicy()).setMergeFactor(10);\n\n    Document doc = new Document();\n    Field field = new Field(\"field\", new MyTokenStream(TERMS_PER_DOC));\n    field.setOmitTermFreqAndPositions(true);\n    field.setOmitNorms(true);\n    doc.add(field);\n    //w.setInfoStream(System.out);\n    final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n    for(int i=0;i<numDocs;i++) {\n      w.addDocument(doc);\n      System.out.println(i + \" of \" + numDocs);\n    }\n    System.out.println(\"now optimize...\");\n    w.optimize();\n    w.close();\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["985161df9b1c14b4aa862d74d48cf51561579e62"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"13452165d8bf3d45a72f572aaed3c679735d3af2","date":1290101307,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","sourceNew":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    int TERMS_PER_DOC = 1000000;\n\n    Directory dir = FSDirectory.open(_TestUtil.getTempDir(\"2BTerms\"));\n    IndexWriter w = new IndexWriter(dir,\n                                    new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())\n                                                  .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                                .setRAMBufferSizeMB(256.0).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((LogMergePolicy) w.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) w.getConfig().getMergePolicy()).setUseCompoundDocStore(false);\n    ((LogMergePolicy) w.getConfig().getMergePolicy()).setMergeFactor(10);\n\n    Document doc = new Document();\n    Field field = new Field(\"field\", new MyTokenStream(TERMS_PER_DOC));\n    field.setOmitTermFreqAndPositions(true);\n    field.setOmitNorms(true);\n    doc.add(field);\n    //w.setInfoStream(System.out);\n    final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n    for(int i=0;i<numDocs;i++) {\n      final long t0 = System.currentTimeMillis();\n      w.addDocument(doc);\n      System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n    }\n    System.out.println(\"now optimize...\");\n    w.optimize();\n    w.close();\n\n    System.out.println(\"now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n  }\n\n","sourceOld":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefaultCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    int TERMS_PER_DOC = 1000000;\n\n    Directory dir = FSDirectory.open(_TestUtil.getTempDir(\"2BTerms\"));\n    IndexWriter w = new IndexWriter(dir,\n                                    new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())\n                                                  .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                                .setRAMBufferSizeMB(256.0).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((LogMergePolicy) w.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) w.getConfig().getMergePolicy()).setUseCompoundDocStore(false);\n    ((LogMergePolicy) w.getConfig().getMergePolicy()).setMergeFactor(10);\n\n    Document doc = new Document();\n    Field field = new Field(\"field\", new MyTokenStream(TERMS_PER_DOC));\n    field.setOmitTermFreqAndPositions(true);\n    field.setOmitNorms(true);\n    doc.add(field);\n    //w.setInfoStream(System.out);\n    final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n    for(int i=0;i<numDocs;i++) {\n      final long t0 = System.currentTimeMillis();\n      w.addDocument(doc);\n      System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n    }\n    System.out.println(\"now optimize...\");\n    w.optimize();\n    w.close();\n\n    System.out.println(\"now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bce89597a7c3a4535b5b7f8100c2078e520f6e57","date":1290106041,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","sourceNew":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefaultCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    int TERMS_PER_DOC = 1000000;\n\n    Directory dir = FSDirectory.open(_TestUtil.getTempDir(\"2BTerms\"));\n    IndexWriter w = new IndexWriter(dir,\n                                    new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())\n                                                  .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                                .setRAMBufferSizeMB(256.0).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((LogMergePolicy) w.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) w.getConfig().getMergePolicy()).setUseCompoundDocStore(false);\n    ((LogMergePolicy) w.getConfig().getMergePolicy()).setMergeFactor(10);\n\n    Document doc = new Document();\n    Field field = new Field(\"field\", new MyTokenStream(TERMS_PER_DOC));\n    field.setOmitTermFreqAndPositions(true);\n    field.setOmitNorms(true);\n    doc.add(field);\n    //w.setInfoStream(System.out);\n    final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n    for(int i=0;i<numDocs;i++) {\n      final long t0 = System.currentTimeMillis();\n      w.addDocument(doc);\n      System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n    }\n    System.out.println(\"now optimize...\");\n    w.optimize();\n    w.close();\n\n    System.out.println(\"now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n  }\n\n","sourceOld":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    int TERMS_PER_DOC = 1000000;\n\n    Directory dir = FSDirectory.open(_TestUtil.getTempDir(\"2BTerms\"));\n    IndexWriter w = new IndexWriter(dir,\n                                    new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())\n                                                  .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                                .setRAMBufferSizeMB(256.0).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((LogMergePolicy) w.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) w.getConfig().getMergePolicy()).setUseCompoundDocStore(false);\n    ((LogMergePolicy) w.getConfig().getMergePolicy()).setMergeFactor(10);\n\n    Document doc = new Document();\n    Field field = new Field(\"field\", new MyTokenStream(TERMS_PER_DOC));\n    field.setOmitTermFreqAndPositions(true);\n    field.setOmitNorms(true);\n    doc.add(field);\n    //w.setInfoStream(System.out);\n    final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n    for(int i=0;i<numDocs;i++) {\n      final long t0 = System.currentTimeMillis();\n      w.addDocument(doc);\n      System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n    }\n    System.out.println(\"now optimize...\");\n    w.optimize();\n    w.close();\n\n    System.out.println(\"now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"eeebcf026b55d8ce3ac8165210782b26cc4efe30","date":1290108396,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","sourceNew":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    int TERMS_PER_DOC = 1000000;\n\n    Directory dir = FSDirectory.open(_TestUtil.getTempDir(\"2BTerms\"));\n    IndexWriter w = new IndexWriter(dir,\n                                    new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())\n                                                  .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                                .setRAMBufferSizeMB(256.0).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((LogMergePolicy) w.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) w.getConfig().getMergePolicy()).setUseCompoundDocStore(false);\n    ((LogMergePolicy) w.getConfig().getMergePolicy()).setMergeFactor(10);\n\n    Document doc = new Document();\n    Field field = new Field(\"field\", new MyTokenStream(TERMS_PER_DOC));\n    field.setOmitTermFreqAndPositions(true);\n    field.setOmitNorms(true);\n    doc.add(field);\n    //w.setInfoStream(System.out);\n    final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n    for(int i=0;i<numDocs;i++) {\n      final long t0 = System.currentTimeMillis();\n      w.addDocument(doc);\n      System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n    }\n    System.out.println(\"now optimize...\");\n    w.optimize();\n    w.close();\n\n    System.out.println(\"now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n  }\n\n","sourceOld":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefaultCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    int TERMS_PER_DOC = 1000000;\n\n    Directory dir = FSDirectory.open(_TestUtil.getTempDir(\"2BTerms\"));\n    IndexWriter w = new IndexWriter(dir,\n                                    new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())\n                                                  .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                                .setRAMBufferSizeMB(256.0).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((LogMergePolicy) w.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) w.getConfig().getMergePolicy()).setUseCompoundDocStore(false);\n    ((LogMergePolicy) w.getConfig().getMergePolicy()).setMergeFactor(10);\n\n    Document doc = new Document();\n    Field field = new Field(\"field\", new MyTokenStream(TERMS_PER_DOC));\n    field.setOmitTermFreqAndPositions(true);\n    field.setOmitNorms(true);\n    doc.add(field);\n    //w.setInfoStream(System.out);\n    final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n    for(int i=0;i<numDocs;i++) {\n      final long t0 = System.currentTimeMillis();\n      w.addDocument(doc);\n      System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n    }\n    System.out.println(\"now optimize...\");\n    w.optimize();\n    w.close();\n\n    System.out.println(\"now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4e8cc373c801e54cec75daf9f52792cb4b17f536","date":1291116159,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","sourceNew":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    int TERMS_PER_DOC = 1000000;\n\n    Directory dir = FSDirectory.open(_TestUtil.getTempDir(\"2BTerms\"));\n    IndexWriter w = new IndexWriter(\n        dir,\n        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).\n            setRAMBufferSizeMB(256.0).\n            setMergeScheduler(new ConcurrentMergeScheduler()).\n            setMergePolicy(newLogMergePolicy(false, 10))\n    );\n\n    Document doc = new Document();\n    Field field = new Field(\"field\", new MyTokenStream(TERMS_PER_DOC));\n    field.setOmitTermFreqAndPositions(true);\n    field.setOmitNorms(true);\n    doc.add(field);\n    //w.setInfoStream(System.out);\n    final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n    for(int i=0;i<numDocs;i++) {\n      final long t0 = System.currentTimeMillis();\n      w.addDocument(doc);\n      System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n    }\n    System.out.println(\"now optimize...\");\n    w.optimize();\n    w.close();\n\n    System.out.println(\"now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n  }\n\n","sourceOld":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    int TERMS_PER_DOC = 1000000;\n\n    Directory dir = FSDirectory.open(_TestUtil.getTempDir(\"2BTerms\"));\n    IndexWriter w = new IndexWriter(dir,\n                                    new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())\n                                                  .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                                .setRAMBufferSizeMB(256.0).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((LogMergePolicy) w.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) w.getConfig().getMergePolicy()).setUseCompoundDocStore(false);\n    ((LogMergePolicy) w.getConfig().getMergePolicy()).setMergeFactor(10);\n\n    Document doc = new Document();\n    Field field = new Field(\"field\", new MyTokenStream(TERMS_PER_DOC));\n    field.setOmitTermFreqAndPositions(true);\n    field.setOmitNorms(true);\n    doc.add(field);\n    //w.setInfoStream(System.out);\n    final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n    for(int i=0;i<numDocs;i++) {\n      final long t0 = System.currentTimeMillis();\n      w.addDocument(doc);\n      System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n    }\n    System.out.println(\"now optimize...\");\n    w.optimize();\n    w.close();\n\n    System.out.println(\"now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["985161df9b1c14b4aa862d74d48cf51561579e62"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3bb13258feba31ab676502787ab2e1779f129b7a","date":1291596436,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","sourceNew":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    int TERMS_PER_DOC = 1000000;\n\n    Directory dir = FSDirectory.open(_TestUtil.getTempDir(\"2BTerms\"));\n    IndexWriter w = new IndexWriter(\n        dir,\n        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).\n            setRAMBufferSizeMB(256.0).\n            setMergeScheduler(new ConcurrentMergeScheduler()).\n            setMergePolicy(newLogMergePolicy(false, 10))\n    );\n\n    Document doc = new Document();\n    Field field = new Field(\"field\", new MyTokenStream(TERMS_PER_DOC));\n    field.setOmitTermFreqAndPositions(true);\n    field.setOmitNorms(true);\n    doc.add(field);\n    //w.setInfoStream(System.out);\n    final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n    for(int i=0;i<numDocs;i++) {\n      final long t0 = System.currentTimeMillis();\n      w.addDocument(doc);\n      System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n    }\n    System.out.println(\"now optimize...\");\n    w.optimize();\n    w.close();\n\n    System.out.println(\"now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n  }\n\n","sourceOld":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefaultCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    int TERMS_PER_DOC = 1000000;\n\n    Directory dir = FSDirectory.open(_TestUtil.getTempDir(\"2BTerms\"));\n    IndexWriter w = new IndexWriter(dir,\n                                    new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())\n                                                  .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                                .setRAMBufferSizeMB(256.0).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((LogMergePolicy) w.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) w.getConfig().getMergePolicy()).setUseCompoundDocStore(false);\n    ((LogMergePolicy) w.getConfig().getMergePolicy()).setMergeFactor(10);\n\n    Document doc = new Document();\n    Field field = new Field(\"field\", new MyTokenStream(TERMS_PER_DOC));\n    field.setOmitTermFreqAndPositions(true);\n    field.setOmitNorms(true);\n    doc.add(field);\n    //w.setInfoStream(System.out);\n    final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n    for(int i=0;i<numDocs;i++) {\n      final long t0 = System.currentTimeMillis();\n      w.addDocument(doc);\n      System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n    }\n    System.out.println(\"now optimize...\");\n    w.optimize();\n    w.close();\n\n    System.out.println(\"now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a13a126d15299d5c1e117ea99ddae6fb0fa3f209","date":1291909583,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","sourceNew":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    int TERMS_PER_DOC = 1000000;\n\n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    IndexWriter w = new IndexWriter(\n        dir,\n        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).\n            setRAMBufferSizeMB(256.0).\n            setMergeScheduler(new ConcurrentMergeScheduler()).\n            setMergePolicy(newLogMergePolicy(false, 10))\n    );\n\n    Document doc = new Document();\n    Field field = new Field(\"field\", new MyTokenStream(TERMS_PER_DOC));\n    field.setOmitTermFreqAndPositions(true);\n    field.setOmitNorms(true);\n    doc.add(field);\n    //w.setInfoStream(System.out);\n    final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n    for(int i=0;i<numDocs;i++) {\n      final long t0 = System.currentTimeMillis();\n      w.addDocument(doc);\n      System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n    }\n    System.out.println(\"now optimize...\");\n    w.optimize();\n    w.close();\n\n    System.out.println(\"now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n  }\n\n","sourceOld":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    int TERMS_PER_DOC = 1000000;\n\n    Directory dir = FSDirectory.open(_TestUtil.getTempDir(\"2BTerms\"));\n    IndexWriter w = new IndexWriter(\n        dir,\n        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).\n            setRAMBufferSizeMB(256.0).\n            setMergeScheduler(new ConcurrentMergeScheduler()).\n            setMergePolicy(newLogMergePolicy(false, 10))\n    );\n\n    Document doc = new Document();\n    Field field = new Field(\"field\", new MyTokenStream(TERMS_PER_DOC));\n    field.setOmitTermFreqAndPositions(true);\n    field.setOmitNorms(true);\n    doc.add(field);\n    //w.setInfoStream(System.out);\n    final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n    for(int i=0;i<numDocs;i++) {\n      final long t0 = System.currentTimeMillis();\n      w.addDocument(doc);\n      System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n    }\n    System.out.println(\"now optimize...\");\n    w.optimize();\n    w.close();\n\n    System.out.println(\"now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ab5cb6a74aefb78aa0569857970b9151dfe2e787","date":1292842407,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","sourceNew":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    int TERMS_PER_DOC = 1000000;\n\n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    IndexWriter w = new IndexWriter(\n        dir,\n        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).\n            setRAMBufferSizeMB(256.0).\n            setMergeScheduler(new ConcurrentMergeScheduler()).\n            setMergePolicy(newLogMergePolicy(false, 10))\n    );\n\n    Document doc = new Document();\n    Field field = new Field(\"field\", new MyTokenStream(TERMS_PER_DOC));\n    field.setOmitTermFreqAndPositions(true);\n    field.setOmitNorms(true);\n    doc.add(field);\n    //w.setInfoStream(System.out);\n    final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n    for(int i=0;i<numDocs;i++) {\n      final long t0 = System.currentTimeMillis();\n      w.addDocument(doc);\n      System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n    }\n    System.out.println(\"now optimize...\");\n    w.optimize();\n    w.close();\n\n    System.out.println(\"now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n  }\n\n","sourceOld":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    int TERMS_PER_DOC = 1000000;\n\n    Directory dir = FSDirectory.open(_TestUtil.getTempDir(\"2BTerms\"));\n    IndexWriter w = new IndexWriter(\n        dir,\n        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).\n            setRAMBufferSizeMB(256.0).\n            setMergeScheduler(new ConcurrentMergeScheduler()).\n            setMergePolicy(newLogMergePolicy(false, 10))\n    );\n\n    Document doc = new Document();\n    Field field = new Field(\"field\", new MyTokenStream(TERMS_PER_DOC));\n    field.setOmitTermFreqAndPositions(true);\n    field.setOmitNorms(true);\n    doc.add(field);\n    //w.setInfoStream(System.out);\n    final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n    for(int i=0;i<numDocs;i++) {\n      final long t0 = System.currentTimeMillis();\n      w.addDocument(doc);\n      System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n    }\n    System.out.println(\"now optimize...\");\n    w.optimize();\n    w.close();\n\n    System.out.println(\"now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":0,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","pathOld":"/dev/null","sourceNew":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    int TERMS_PER_DOC = 1000000;\n\n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    IndexWriter w = new IndexWriter(\n        dir,\n        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).\n            setRAMBufferSizeMB(256.0).\n            setMergeScheduler(new ConcurrentMergeScheduler()).\n            setMergePolicy(newLogMergePolicy(false, 10))\n    );\n\n    Document doc = new Document();\n    Field field = new Field(\"field\", new MyTokenStream(TERMS_PER_DOC));\n    field.setOmitTermFreqAndPositions(true);\n    field.setOmitNorms(true);\n    doc.add(field);\n    //w.setInfoStream(System.out);\n    final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n    for(int i=0;i<numDocs;i++) {\n      final long t0 = System.currentTimeMillis();\n      w.addDocument(doc);\n      System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n    }\n    System.out.println(\"now optimize...\");\n    w.optimize();\n    w.close();\n\n    System.out.println(\"now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1b589b416d5461faed99d25118babbda4b89acb4","date":1294225264,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","sourceNew":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    int TERMS_PER_DOC = 1000000;\n\n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    IndexWriter w = new IndexWriter(\n        dir,\n        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).\n            setRAMBufferSizeMB(256.0).\n            setMergeScheduler(new ConcurrentMergeScheduler()).\n            setMergePolicy(newLogMergePolicy(false, 10))\n    );\n\n    MergePolicy mp = w.getConfig().getMergePolicy();\n    if (mp instanceof LogByteSizeMergePolicy) {\n      // 1 petabyte:\n      ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n    }\n\n    Document doc = new Document();\n    Field field = new Field(\"field\", new MyTokenStream(TERMS_PER_DOC));\n    field.setOmitTermFreqAndPositions(true);\n    field.setOmitNorms(true);\n    doc.add(field);\n    //w.setInfoStream(System.out);\n    final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n    for(int i=0;i<numDocs;i++) {\n      final long t0 = System.currentTimeMillis();\n      w.addDocument(doc);\n      System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n    }\n    System.out.println(\"now optimize...\");\n    w.optimize();\n    w.close();\n\n    System.out.println(\"now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n  }\n\n","sourceOld":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    int TERMS_PER_DOC = 1000000;\n\n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    IndexWriter w = new IndexWriter(\n        dir,\n        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).\n            setRAMBufferSizeMB(256.0).\n            setMergeScheduler(new ConcurrentMergeScheduler()).\n            setMergePolicy(newLogMergePolicy(false, 10))\n    );\n\n    Document doc = new Document();\n    Field field = new Field(\"field\", new MyTokenStream(TERMS_PER_DOC));\n    field.setOmitTermFreqAndPositions(true);\n    field.setOmitNorms(true);\n    doc.add(field);\n    //w.setInfoStream(System.out);\n    final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n    for(int i=0;i<numDocs;i++) {\n      final long t0 = System.currentTimeMillis();\n      w.addDocument(doc);\n      System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n    }\n    System.out.println(\"now optimize...\");\n    w.optimize();\n    w.close();\n\n    System.out.println(\"now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["985161df9b1c14b4aa862d74d48cf51561579e62"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"70ad682703b8585f5d0a637efec044d57ec05efb","date":1294259117,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","sourceNew":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    int TERMS_PER_DOC = 1000000;\n\n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    IndexWriter w = new IndexWriter(\n        dir,\n        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).\n            setRAMBufferSizeMB(256.0).\n            setMergeScheduler(new ConcurrentMergeScheduler()).\n            setMergePolicy(newLogMergePolicy(false, 10))\n    );\n\n    MergePolicy mp = w.getConfig().getMergePolicy();\n    if (mp instanceof LogByteSizeMergePolicy) {\n      // 1 petabyte:\n      ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n    }\n\n    Document doc = new Document();\n    Field field = new Field(\"field\", new MyTokenStream(TERMS_PER_DOC));\n    field.setOmitTermFreqAndPositions(true);\n    field.setOmitNorms(true);\n    doc.add(field);\n    //w.setInfoStream(System.out);\n    final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n    for(int i=0;i<numDocs;i++) {\n      final long t0 = System.currentTimeMillis();\n      w.addDocument(doc);\n      System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n    }\n    System.out.println(\"now optimize...\");\n    w.optimize();\n    w.close();\n\n    System.out.println(\"now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n  }\n\n","sourceOld":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    int TERMS_PER_DOC = 1000000;\n\n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    IndexWriter w = new IndexWriter(\n        dir,\n        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).\n            setRAMBufferSizeMB(256.0).\n            setMergeScheduler(new ConcurrentMergeScheduler()).\n            setMergePolicy(newLogMergePolicy(false, 10))\n    );\n\n    Document doc = new Document();\n    Field field = new Field(\"field\", new MyTokenStream(TERMS_PER_DOC));\n    field.setOmitTermFreqAndPositions(true);\n    field.setOmitNorms(true);\n    doc.add(field);\n    //w.setInfoStream(System.out);\n    final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n    for(int i=0;i<numDocs;i++) {\n      final long t0 = System.currentTimeMillis();\n      w.addDocument(doc);\n      System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n    }\n    System.out.println(\"now optimize...\");\n    w.optimize();\n    w.close();\n\n    System.out.println(\"now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"868da859b43505d9d2a023bfeae6dd0c795f5295","date":1294948401,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","sourceNew":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    int TERMS_PER_DOC = 1000000;\n\n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    IndexWriter w = new IndexWriter(\n        dir,\n        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).\n            setRAMBufferSizeMB(256.0).\n            setMergeScheduler(new ConcurrentMergeScheduler()).\n            setMergePolicy(newLogMergePolicy(false, 10))\n    );\n\n    MergePolicy mp = w.getConfig().getMergePolicy();\n    if (mp instanceof LogByteSizeMergePolicy) {\n      // 1 petabyte:\n      ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n    }\n\n    Document doc = new Document();\n    Field field = new Field(\"field\", new MyTokenStream(TERMS_PER_DOC));\n    field.setOmitTermFreqAndPositions(true);\n    field.setOmitNorms(true);\n    doc.add(field);\n    //w.setInfoStream(System.out);\n    final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n    for(int i=0;i<numDocs;i++) {\n      final long t0 = System.currentTimeMillis();\n      w.addDocument(doc);\n      System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n    }\n    System.out.println(\"now optimize...\");\n    w.optimize();\n    w.close();\n\n    System.out.println(\"now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n  }\n\n","sourceOld":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    int TERMS_PER_DOC = 1000000;\n\n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    IndexWriter w = new IndexWriter(\n        dir,\n        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).\n            setRAMBufferSizeMB(256.0).\n            setMergeScheduler(new ConcurrentMergeScheduler()).\n            setMergePolicy(newLogMergePolicy(false, 10))\n    );\n\n    Document doc = new Document();\n    Field field = new Field(\"field\", new MyTokenStream(TERMS_PER_DOC));\n    field.setOmitTermFreqAndPositions(true);\n    field.setOmitNorms(true);\n    doc.add(field);\n    //w.setInfoStream(System.out);\n    final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n    for(int i=0;i<numDocs;i++) {\n      final long t0 = System.currentTimeMillis();\n      w.addDocument(doc);\n      System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n    }\n    System.out.println(\"now optimize...\");\n    w.optimize();\n    w.close();\n\n    System.out.println(\"now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f2c5f0cb44df114db4228c8f77861714b5cabaea","date":1302542431,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","sourceNew":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    int TERMS_PER_DOC = 1000000;\n\n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    IndexWriter w = new IndexWriter(\n        dir,\n        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).\n            setRAMBufferSizeMB(256.0).\n            setMergeScheduler(new ConcurrentMergeScheduler()).\n            setMergePolicy(newLogMergePolicy(false, 10))\n    );\n\n    MergePolicy mp = w.getConfig().getMergePolicy();\n    if (mp instanceof LogByteSizeMergePolicy) {\n      // 1 petabyte:\n      ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n    }\n\n    Document doc = new Document();\n    Field field = new Field(\"field\", new MyTokenStream(TERMS_PER_DOC));\n    field.setOmitTermFreqAndPositions(true);\n    field.setOmitNorms(true);\n    doc.add(field);\n    //w.setInfoStream(System.out);\n    final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n    for(int i=0;i<numDocs;i++) {\n      final long t0 = System.currentTimeMillis();\n      w.addDocument(doc);\n      System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n    }\n    System.out.println(\"now optimize...\");\n    w.optimize();\n    w.close();\n\n    System.out.println(\"now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n  }\n\n","sourceOld":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    int TERMS_PER_DOC = 1000000;\n\n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    IndexWriter w = new IndexWriter(\n        dir,\n        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).\n            setRAMBufferSizeMB(256.0).\n            setMergeScheduler(new ConcurrentMergeScheduler()).\n            setMergePolicy(newLogMergePolicy(false, 10))\n    );\n\n    MergePolicy mp = w.getConfig().getMergePolicy();\n    if (mp instanceof LogByteSizeMergePolicy) {\n      // 1 petabyte:\n      ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n    }\n\n    Document doc = new Document();\n    Field field = new Field(\"field\", new MyTokenStream(TERMS_PER_DOC));\n    field.setOmitTermFreqAndPositions(true);\n    field.setOmitNorms(true);\n    doc.add(field);\n    //w.setInfoStream(System.out);\n    final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n    for(int i=0;i<numDocs;i++) {\n      final long t0 = System.currentTimeMillis();\n      w.addDocument(doc);\n      System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n    }\n    System.out.println(\"now optimize...\");\n    w.optimize();\n    w.close();\n\n    System.out.println(\"now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["985161df9b1c14b4aa862d74d48cf51561579e62"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"985161df9b1c14b4aa862d74d48cf51561579e62","date":1302636954,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","sourceNew":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    final long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    final int TERMS_PER_DOC = _TestUtil.nextInt(random, 100000, 1000000);\n\n    List<BytesRef> savedTerms = null;\n\n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    //Directory dir = newFSDirectory(new File(\"/p/lucene/indices/2bindex\"));\n    if (true) {\n      IndexWriter w = new IndexWriter(dir,\n                                      new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n                                      .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                      .setRAMBufferSizeMB(256.0)\n                                      .setMergeScheduler(new ConcurrentMergeScheduler())\n                                      .setMergePolicy(newLogMergePolicy(false, 10)));\n\n      MergePolicy mp = w.getConfig().getMergePolicy();\n      if (mp instanceof LogByteSizeMergePolicy) {\n        // 1 petabyte:\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n      }\n\n      Document doc = new Document();\n      final MyTokenStream ts = new MyTokenStream(random, TERMS_PER_DOC);\n      Field field = new Field(\"field\", ts);\n      field.setOmitTermFreqAndPositions(true);\n      field.setOmitNorms(true);\n      doc.add(field);\n      //w.setInfoStream(System.out);\n      final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n\n      System.out.println(\"TERMS_PER_DOC=\" + TERMS_PER_DOC);\n      System.out.println(\"numDocs=\" + numDocs);\n\n      for(int i=0;i<numDocs;i++) {\n        final long t0 = System.currentTimeMillis();\n        w.addDocument(doc);\n        System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n      }\n      savedTerms = ts.savedTerms;\n\n      System.out.println(\"TEST: optimize\");\n      w.optimize();\n      System.out.println(\"TEST: close writer\");\n      w.close();\n    }\n\n    System.out.println(\"TEST: open reader\");\n    final IndexReader r = IndexReader.open(dir);\n    if (savedTerms == null) {\n      savedTerms = findTerms(r);\n    }\n    final int numSavedTerms = savedTerms.size();\n    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));\n    System.out.println(\"TEST: test big ord terms...\");\n    testSavedTerms(r, bigOrdTerms);\n    System.out.println(\"TEST: test all saved terms...\");\n    testSavedTerms(r, savedTerms);\n    r.close();\n\n    System.out.println(\"TEST: now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n  }\n\n","sourceOld":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    int TERMS_PER_DOC = 1000000;\n\n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    IndexWriter w = new IndexWriter(\n        dir,\n        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).\n            setRAMBufferSizeMB(256.0).\n            setMergeScheduler(new ConcurrentMergeScheduler()).\n            setMergePolicy(newLogMergePolicy(false, 10))\n    );\n\n    MergePolicy mp = w.getConfig().getMergePolicy();\n    if (mp instanceof LogByteSizeMergePolicy) {\n      // 1 petabyte:\n      ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n    }\n\n    Document doc = new Document();\n    Field field = new Field(\"field\", new MyTokenStream(TERMS_PER_DOC));\n    field.setOmitTermFreqAndPositions(true);\n    field.setOmitNorms(true);\n    doc.add(field);\n    //w.setInfoStream(System.out);\n    final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n    for(int i=0;i<numDocs;i++) {\n      final long t0 = System.currentTimeMillis();\n      w.addDocument(doc);\n      System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n    }\n    System.out.println(\"now optimize...\");\n    w.optimize();\n    w.close();\n\n    System.out.println(\"now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n  }\n\n","bugFix":["f2c5f0cb44df114db4228c8f77861714b5cabaea","a6c7b564e6275fb0c0e137d84fda55b447c19d9c","4e8cc373c801e54cec75daf9f52792cb4b17f536","1b589b416d5461faed99d25118babbda4b89acb4","ba8cd9ac7a0f3ad09c6c94f53c4c415d13e72723"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"962d04139994fce5193143ef35615499a9a96d78","date":1302693744,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","sourceNew":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    final long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    final int TERMS_PER_DOC = _TestUtil.nextInt(random, 100000, 1000000);\n\n    List<BytesRef> savedTerms = null;\n\n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    //Directory dir = newFSDirectory(new File(\"/p/lucene/indices/2bindex\"));\n    if (true) {\n      IndexWriter w = new IndexWriter(dir,\n                                      new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n                                      .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                      .setRAMBufferSizeMB(256.0)\n                                      .setMergeScheduler(new ConcurrentMergeScheduler())\n                                      .setMergePolicy(newLogMergePolicy(false, 10)));\n\n      MergePolicy mp = w.getConfig().getMergePolicy();\n      if (mp instanceof LogByteSizeMergePolicy) {\n        // 1 petabyte:\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n      }\n\n      Document doc = new Document();\n      final MyTokenStream ts = new MyTokenStream(random, TERMS_PER_DOC);\n      Field field = new Field(\"field\", ts);\n      field.setOmitTermFreqAndPositions(true);\n      field.setOmitNorms(true);\n      doc.add(field);\n      //w.setInfoStream(System.out);\n      final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n\n      System.out.println(\"TERMS_PER_DOC=\" + TERMS_PER_DOC);\n      System.out.println(\"numDocs=\" + numDocs);\n\n      for(int i=0;i<numDocs;i++) {\n        final long t0 = System.currentTimeMillis();\n        w.addDocument(doc);\n        System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n      }\n      savedTerms = ts.savedTerms;\n\n      System.out.println(\"TEST: optimize\");\n      w.optimize();\n      System.out.println(\"TEST: close writer\");\n      w.close();\n    }\n\n    System.out.println(\"TEST: open reader\");\n    final IndexReader r = IndexReader.open(dir);\n    if (savedTerms == null) {\n      savedTerms = findTerms(r);\n    }\n    final int numSavedTerms = savedTerms.size();\n    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));\n    System.out.println(\"TEST: test big ord terms...\");\n    testSavedTerms(r, bigOrdTerms);\n    System.out.println(\"TEST: test all saved terms...\");\n    testSavedTerms(r, savedTerms);\n    r.close();\n\n    System.out.println(\"TEST: now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n  }\n\n","sourceOld":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    int TERMS_PER_DOC = 1000000;\n\n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    IndexWriter w = new IndexWriter(\n        dir,\n        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).\n            setRAMBufferSizeMB(256.0).\n            setMergeScheduler(new ConcurrentMergeScheduler()).\n            setMergePolicy(newLogMergePolicy(false, 10))\n    );\n\n    MergePolicy mp = w.getConfig().getMergePolicy();\n    if (mp instanceof LogByteSizeMergePolicy) {\n      // 1 petabyte:\n      ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n    }\n\n    Document doc = new Document();\n    Field field = new Field(\"field\", new MyTokenStream(TERMS_PER_DOC));\n    field.setOmitTermFreqAndPositions(true);\n    field.setOmitNorms(true);\n    doc.add(field);\n    //w.setInfoStream(System.out);\n    final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n    for(int i=0;i<numDocs;i++) {\n      final long t0 = System.currentTimeMillis();\n      w.addDocument(doc);\n      System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n    }\n    System.out.println(\"now optimize...\");\n    w.optimize();\n    w.close();\n\n    System.out.println(\"now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2506f4a214732b20f56ab2fa8b975dc5d85c4d45","date":1302809898,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","sourceNew":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    final long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    final int TERMS_PER_DOC = _TestUtil.nextInt(random, 100000, 1000000);\n\n    List<BytesRef> savedTerms = null;\n\n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    //Directory dir = newFSDirectory(new File(\"/p/lucene/indices/2bindex\"));\n\n    if (true) {\n\n      IndexWriter w = new IndexWriter(dir,\n                                      new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n                                      .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                      .setRAMBufferSizeMB(256.0)\n                                      .setMergeScheduler(new ConcurrentMergeScheduler())\n                                      .setMergePolicy(newLogMergePolicy(false, 10))\n                                      .setOpenMode(IndexWriterConfig.OpenMode.CREATE));\n\n      MergePolicy mp = w.getConfig().getMergePolicy();\n      if (mp instanceof LogByteSizeMergePolicy) {\n        // 1 petabyte:\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n      }\n\n      Document doc = new Document();\n      final MyTokenStream ts = new MyTokenStream(random, TERMS_PER_DOC);\n      Field field = new Field(\"field\", ts);\n      field.setOmitTermFreqAndPositions(true);\n      field.setOmitNorms(true);\n      doc.add(field);\n      //w.setInfoStream(System.out);\n      final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n\n      System.out.println(\"TERMS_PER_DOC=\" + TERMS_PER_DOC);\n      System.out.println(\"numDocs=\" + numDocs);\n\n      for(int i=0;i<numDocs;i++) {\n        final long t0 = System.currentTimeMillis();\n        w.addDocument(doc);\n        System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n      }\n      savedTerms = ts.savedTerms;\n\n      System.out.println(\"TEST: optimize\");\n      w.optimize();\n      System.out.println(\"TEST: close writer\");\n      w.close();\n    }\n\n    System.out.println(\"TEST: open reader\");\n    final IndexReader r = IndexReader.open(dir);\n    if (savedTerms == null) {\n      savedTerms = findTerms(r);\n    }\n    final int numSavedTerms = savedTerms.size();\n    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));\n    System.out.println(\"TEST: test big ord terms...\");\n    testSavedTerms(r, bigOrdTerms);\n    System.out.println(\"TEST: test all saved terms...\");\n    testSavedTerms(r, savedTerms);\n    r.close();\n\n    System.out.println(\"TEST: now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n    System.out.println(\"TEST: done!\");\n  }\n\n","sourceOld":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    final long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    final int TERMS_PER_DOC = _TestUtil.nextInt(random, 100000, 1000000);\n\n    List<BytesRef> savedTerms = null;\n\n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    //Directory dir = newFSDirectory(new File(\"/p/lucene/indices/2bindex\"));\n    if (true) {\n      IndexWriter w = new IndexWriter(dir,\n                                      new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n                                      .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                      .setRAMBufferSizeMB(256.0)\n                                      .setMergeScheduler(new ConcurrentMergeScheduler())\n                                      .setMergePolicy(newLogMergePolicy(false, 10)));\n\n      MergePolicy mp = w.getConfig().getMergePolicy();\n      if (mp instanceof LogByteSizeMergePolicy) {\n        // 1 petabyte:\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n      }\n\n      Document doc = new Document();\n      final MyTokenStream ts = new MyTokenStream(random, TERMS_PER_DOC);\n      Field field = new Field(\"field\", ts);\n      field.setOmitTermFreqAndPositions(true);\n      field.setOmitNorms(true);\n      doc.add(field);\n      //w.setInfoStream(System.out);\n      final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n\n      System.out.println(\"TERMS_PER_DOC=\" + TERMS_PER_DOC);\n      System.out.println(\"numDocs=\" + numDocs);\n\n      for(int i=0;i<numDocs;i++) {\n        final long t0 = System.currentTimeMillis();\n        w.addDocument(doc);\n        System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n      }\n      savedTerms = ts.savedTerms;\n\n      System.out.println(\"TEST: optimize\");\n      w.optimize();\n      System.out.println(\"TEST: close writer\");\n      w.close();\n    }\n\n    System.out.println(\"TEST: open reader\");\n    final IndexReader r = IndexReader.open(dir);\n    if (savedTerms == null) {\n      savedTerms = findTerms(r);\n    }\n    final int numSavedTerms = savedTerms.size();\n    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));\n    System.out.println(\"TEST: test big ord terms...\");\n    testSavedTerms(r, bigOrdTerms);\n    System.out.println(\"TEST: test all saved terms...\");\n    testSavedTerms(r, savedTerms);\n    r.close();\n\n    System.out.println(\"TEST: now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["656cfb06eff2244ff5a25ffb3ed3a79942ece85c"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1eb46686a27187e42311e77666a2c7026f461ebc","date":1302858020,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","sourceNew":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    final long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    final int TERMS_PER_DOC = _TestUtil.nextInt(random, 100000, 1000000);\n\n    List<BytesRef> savedTerms = null;\n\n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    //Directory dir = newFSDirectory(new File(\"/p/lucene/indices/2bindex\"));\n\n    if (true) {\n\n      IndexWriter w = new IndexWriter(dir,\n                                      new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n                                      .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                      .setRAMBufferSizeMB(256.0)\n                                      .setMergeScheduler(new ConcurrentMergeScheduler())\n                                      .setMergePolicy(newLogMergePolicy(false, 10))\n                                      .setOpenMode(IndexWriterConfig.OpenMode.CREATE));\n\n      MergePolicy mp = w.getConfig().getMergePolicy();\n      if (mp instanceof LogByteSizeMergePolicy) {\n        // 1 petabyte:\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n      }\n\n      Document doc = new Document();\n      final MyTokenStream ts = new MyTokenStream(random, TERMS_PER_DOC);\n      Field field = new Field(\"field\", ts);\n      field.setOmitTermFreqAndPositions(true);\n      field.setOmitNorms(true);\n      doc.add(field);\n      //w.setInfoStream(System.out);\n      final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n\n      System.out.println(\"TERMS_PER_DOC=\" + TERMS_PER_DOC);\n      System.out.println(\"numDocs=\" + numDocs);\n\n      for(int i=0;i<numDocs;i++) {\n        final long t0 = System.currentTimeMillis();\n        w.addDocument(doc);\n        System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n      }\n      savedTerms = ts.savedTerms;\n\n      System.out.println(\"TEST: optimize\");\n      w.optimize();\n      System.out.println(\"TEST: close writer\");\n      w.close();\n    }\n\n    System.out.println(\"TEST: open reader\");\n    final IndexReader r = IndexReader.open(dir);\n    if (savedTerms == null) {\n      savedTerms = findTerms(r);\n    }\n    final int numSavedTerms = savedTerms.size();\n    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));\n    System.out.println(\"TEST: test big ord terms...\");\n    testSavedTerms(r, bigOrdTerms);\n    System.out.println(\"TEST: test all saved terms...\");\n    testSavedTerms(r, savedTerms);\n    r.close();\n\n    System.out.println(\"TEST: now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n    System.out.println(\"TEST: done!\");\n  }\n\n","sourceOld":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    final long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    final int TERMS_PER_DOC = _TestUtil.nextInt(random, 100000, 1000000);\n\n    List<BytesRef> savedTerms = null;\n\n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    //Directory dir = newFSDirectory(new File(\"/p/lucene/indices/2bindex\"));\n    if (true) {\n      IndexWriter w = new IndexWriter(dir,\n                                      new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n                                      .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                      .setRAMBufferSizeMB(256.0)\n                                      .setMergeScheduler(new ConcurrentMergeScheduler())\n                                      .setMergePolicy(newLogMergePolicy(false, 10)));\n\n      MergePolicy mp = w.getConfig().getMergePolicy();\n      if (mp instanceof LogByteSizeMergePolicy) {\n        // 1 petabyte:\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n      }\n\n      Document doc = new Document();\n      final MyTokenStream ts = new MyTokenStream(random, TERMS_PER_DOC);\n      Field field = new Field(\"field\", ts);\n      field.setOmitTermFreqAndPositions(true);\n      field.setOmitNorms(true);\n      doc.add(field);\n      //w.setInfoStream(System.out);\n      final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n\n      System.out.println(\"TERMS_PER_DOC=\" + TERMS_PER_DOC);\n      System.out.println(\"numDocs=\" + numDocs);\n\n      for(int i=0;i<numDocs;i++) {\n        final long t0 = System.currentTimeMillis();\n        w.addDocument(doc);\n        System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n      }\n      savedTerms = ts.savedTerms;\n\n      System.out.println(\"TEST: optimize\");\n      w.optimize();\n      System.out.println(\"TEST: close writer\");\n      w.close();\n    }\n\n    System.out.println(\"TEST: open reader\");\n    final IndexReader r = IndexReader.open(dir);\n    if (savedTerms == null) {\n      savedTerms = findTerms(r);\n    }\n    final int numSavedTerms = savedTerms.size();\n    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));\n    System.out.println(\"TEST: test big ord terms...\");\n    testSavedTerms(r, bigOrdTerms);\n    System.out.println(\"TEST: test all saved terms...\");\n    testSavedTerms(r, savedTerms);\n    r.close();\n\n    System.out.println(\"TEST: now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","sourceNew":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    final long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    final int TERMS_PER_DOC = _TestUtil.nextInt(random, 100000, 1000000);\n\n    List<BytesRef> savedTerms = null;\n\n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    //Directory dir = newFSDirectory(new File(\"/p/lucene/indices/2bindex\"));\n\n    if (true) {\n\n      IndexWriter w = new IndexWriter(dir,\n                                      new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n                                      .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                      .setRAMBufferSizeMB(256.0)\n                                      .setMergeScheduler(new ConcurrentMergeScheduler())\n                                      .setMergePolicy(newLogMergePolicy(false, 10))\n                                      .setOpenMode(IndexWriterConfig.OpenMode.CREATE));\n\n      MergePolicy mp = w.getConfig().getMergePolicy();\n      if (mp instanceof LogByteSizeMergePolicy) {\n        // 1 petabyte:\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n      }\n\n      Document doc = new Document();\n      final MyTokenStream ts = new MyTokenStream(random, TERMS_PER_DOC);\n      Field field = new Field(\"field\", ts);\n      field.setOmitTermFreqAndPositions(true);\n      field.setOmitNorms(true);\n      doc.add(field);\n      //w.setInfoStream(System.out);\n      final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n\n      System.out.println(\"TERMS_PER_DOC=\" + TERMS_PER_DOC);\n      System.out.println(\"numDocs=\" + numDocs);\n\n      for(int i=0;i<numDocs;i++) {\n        final long t0 = System.currentTimeMillis();\n        w.addDocument(doc);\n        System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n      }\n      savedTerms = ts.savedTerms;\n\n      System.out.println(\"TEST: optimize\");\n      w.optimize();\n      System.out.println(\"TEST: close writer\");\n      w.close();\n    }\n\n    System.out.println(\"TEST: open reader\");\n    final IndexReader r = IndexReader.open(dir);\n    if (savedTerms == null) {\n      savedTerms = findTerms(r);\n    }\n    final int numSavedTerms = savedTerms.size();\n    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));\n    System.out.println(\"TEST: test big ord terms...\");\n    testSavedTerms(r, bigOrdTerms);\n    System.out.println(\"TEST: test all saved terms...\");\n    testSavedTerms(r, savedTerms);\n    r.close();\n\n    System.out.println(\"TEST: now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n    System.out.println(\"TEST: done!\");\n  }\n\n","sourceOld":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    int TERMS_PER_DOC = 1000000;\n\n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    IndexWriter w = new IndexWriter(\n        dir,\n        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).\n            setRAMBufferSizeMB(256.0).\n            setMergeScheduler(new ConcurrentMergeScheduler()).\n            setMergePolicy(newLogMergePolicy(false, 10))\n    );\n\n    MergePolicy mp = w.getConfig().getMergePolicy();\n    if (mp instanceof LogByteSizeMergePolicy) {\n      // 1 petabyte:\n      ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n    }\n\n    Document doc = new Document();\n    Field field = new Field(\"field\", new MyTokenStream(TERMS_PER_DOC));\n    field.setOmitTermFreqAndPositions(true);\n    field.setOmitNorms(true);\n    doc.add(field);\n    //w.setInfoStream(System.out);\n    final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n    for(int i=0;i<numDocs;i++) {\n      final long t0 = System.currentTimeMillis();\n      w.addDocument(doc);\n      System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n    }\n    System.out.println(\"now optimize...\");\n    w.optimize();\n    w.close();\n\n    System.out.println(\"now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cf46d950a2020ee31dbae60879a0b0a519224524","date":1305045957,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","sourceNew":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    final long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    final int TERMS_PER_DOC = _TestUtil.nextInt(random, 100000, 1000000);\n\n    List<BytesRef> savedTerms = null;\n\n    MockDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    dir.setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    //Directory dir = newFSDirectory(new File(\"/p/lucene/indices/2bindex\"));\n\n    if (true) {\n\n      IndexWriter w = new IndexWriter(dir,\n                                      new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n                                      .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                      .setRAMBufferSizeMB(256.0)\n                                      .setMergeScheduler(new ConcurrentMergeScheduler())\n                                      .setMergePolicy(newLogMergePolicy(false, 10))\n                                      .setOpenMode(IndexWriterConfig.OpenMode.CREATE));\n\n      MergePolicy mp = w.getConfig().getMergePolicy();\n      if (mp instanceof LogByteSizeMergePolicy) {\n        // 1 petabyte:\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n      }\n\n      Document doc = new Document();\n      final MyTokenStream ts = new MyTokenStream(random, TERMS_PER_DOC);\n      Field field = new Field(\"field\", ts);\n      field.setOmitTermFreqAndPositions(true);\n      field.setOmitNorms(true);\n      doc.add(field);\n      //w.setInfoStream(System.out);\n      final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n\n      System.out.println(\"TERMS_PER_DOC=\" + TERMS_PER_DOC);\n      System.out.println(\"numDocs=\" + numDocs);\n\n      for(int i=0;i<numDocs;i++) {\n        final long t0 = System.currentTimeMillis();\n        w.addDocument(doc);\n        System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n      }\n      savedTerms = ts.savedTerms;\n\n      System.out.println(\"TEST: optimize\");\n      w.optimize();\n      System.out.println(\"TEST: close writer\");\n      w.close();\n    }\n\n    System.out.println(\"TEST: open reader\");\n    final IndexReader r = IndexReader.open(dir);\n    if (savedTerms == null) {\n      savedTerms = findTerms(r);\n    }\n    final int numSavedTerms = savedTerms.size();\n    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));\n    System.out.println(\"TEST: test big ord terms...\");\n    testSavedTerms(r, bigOrdTerms);\n    System.out.println(\"TEST: test all saved terms...\");\n    testSavedTerms(r, savedTerms);\n    r.close();\n\n    System.out.println(\"TEST: now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n    System.out.println(\"TEST: done!\");\n  }\n\n","sourceOld":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    final long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    final int TERMS_PER_DOC = _TestUtil.nextInt(random, 100000, 1000000);\n\n    List<BytesRef> savedTerms = null;\n\n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    //Directory dir = newFSDirectory(new File(\"/p/lucene/indices/2bindex\"));\n\n    if (true) {\n\n      IndexWriter w = new IndexWriter(dir,\n                                      new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n                                      .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                      .setRAMBufferSizeMB(256.0)\n                                      .setMergeScheduler(new ConcurrentMergeScheduler())\n                                      .setMergePolicy(newLogMergePolicy(false, 10))\n                                      .setOpenMode(IndexWriterConfig.OpenMode.CREATE));\n\n      MergePolicy mp = w.getConfig().getMergePolicy();\n      if (mp instanceof LogByteSizeMergePolicy) {\n        // 1 petabyte:\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n      }\n\n      Document doc = new Document();\n      final MyTokenStream ts = new MyTokenStream(random, TERMS_PER_DOC);\n      Field field = new Field(\"field\", ts);\n      field.setOmitTermFreqAndPositions(true);\n      field.setOmitNorms(true);\n      doc.add(field);\n      //w.setInfoStream(System.out);\n      final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n\n      System.out.println(\"TERMS_PER_DOC=\" + TERMS_PER_DOC);\n      System.out.println(\"numDocs=\" + numDocs);\n\n      for(int i=0;i<numDocs;i++) {\n        final long t0 = System.currentTimeMillis();\n        w.addDocument(doc);\n        System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n      }\n      savedTerms = ts.savedTerms;\n\n      System.out.println(\"TEST: optimize\");\n      w.optimize();\n      System.out.println(\"TEST: close writer\");\n      w.close();\n    }\n\n    System.out.println(\"TEST: open reader\");\n    final IndexReader r = IndexReader.open(dir);\n    if (savedTerms == null) {\n      savedTerms = findTerms(r);\n    }\n    final int numSavedTerms = savedTerms.size();\n    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));\n    System.out.println(\"TEST: test big ord terms...\");\n    testSavedTerms(r, bigOrdTerms);\n    System.out.println(\"TEST: test all saved terms...\");\n    testSavedTerms(r, savedTerms);\n    r.close();\n\n    System.out.println(\"TEST: now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n    System.out.println(\"TEST: done!\");\n  }\n\n","bugFix":null,"bugIntro":["d19974432be9aed28ee7dca73bdf01d139e763a9"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c700f8d0842d3e52bb2bdfbfdc046a137e836edb","date":1305285499,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","sourceNew":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    final long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    final int TERMS_PER_DOC = _TestUtil.nextInt(random, 100000, 1000000);\n\n    List<BytesRef> savedTerms = null;\n\n    MockDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    dir.setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    //Directory dir = newFSDirectory(new File(\"/p/lucene/indices/2bindex\"));\n\n    if (true) {\n\n      IndexWriter w = new IndexWriter(dir,\n                                      new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n                                      .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                      .setRAMBufferSizeMB(256.0)\n                                      .setMergeScheduler(new ConcurrentMergeScheduler())\n                                      .setMergePolicy(newLogMergePolicy(false, 10))\n                                      .setOpenMode(IndexWriterConfig.OpenMode.CREATE));\n\n      MergePolicy mp = w.getConfig().getMergePolicy();\n      if (mp instanceof LogByteSizeMergePolicy) {\n        // 1 petabyte:\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n      }\n\n      Document doc = new Document();\n      final MyTokenStream ts = new MyTokenStream(random, TERMS_PER_DOC);\n      Field field = new Field(\"field\", ts);\n      field.setOmitTermFreqAndPositions(true);\n      field.setOmitNorms(true);\n      doc.add(field);\n      //w.setInfoStream(System.out);\n      final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n\n      System.out.println(\"TERMS_PER_DOC=\" + TERMS_PER_DOC);\n      System.out.println(\"numDocs=\" + numDocs);\n\n      for(int i=0;i<numDocs;i++) {\n        final long t0 = System.currentTimeMillis();\n        w.addDocument(doc);\n        System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n      }\n      savedTerms = ts.savedTerms;\n\n      System.out.println(\"TEST: optimize\");\n      w.optimize();\n      System.out.println(\"TEST: close writer\");\n      w.close();\n    }\n\n    System.out.println(\"TEST: open reader\");\n    final IndexReader r = IndexReader.open(dir);\n    if (savedTerms == null) {\n      savedTerms = findTerms(r);\n    }\n    final int numSavedTerms = savedTerms.size();\n    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));\n    System.out.println(\"TEST: test big ord terms...\");\n    testSavedTerms(r, bigOrdTerms);\n    System.out.println(\"TEST: test all saved terms...\");\n    testSavedTerms(r, savedTerms);\n    r.close();\n\n    System.out.println(\"TEST: now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n    System.out.println(\"TEST: done!\");\n  }\n\n","sourceOld":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    final long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    final int TERMS_PER_DOC = _TestUtil.nextInt(random, 100000, 1000000);\n\n    List<BytesRef> savedTerms = null;\n\n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    //Directory dir = newFSDirectory(new File(\"/p/lucene/indices/2bindex\"));\n\n    if (true) {\n\n      IndexWriter w = new IndexWriter(dir,\n                                      new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n                                      .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                      .setRAMBufferSizeMB(256.0)\n                                      .setMergeScheduler(new ConcurrentMergeScheduler())\n                                      .setMergePolicy(newLogMergePolicy(false, 10))\n                                      .setOpenMode(IndexWriterConfig.OpenMode.CREATE));\n\n      MergePolicy mp = w.getConfig().getMergePolicy();\n      if (mp instanceof LogByteSizeMergePolicy) {\n        // 1 petabyte:\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n      }\n\n      Document doc = new Document();\n      final MyTokenStream ts = new MyTokenStream(random, TERMS_PER_DOC);\n      Field field = new Field(\"field\", ts);\n      field.setOmitTermFreqAndPositions(true);\n      field.setOmitNorms(true);\n      doc.add(field);\n      //w.setInfoStream(System.out);\n      final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n\n      System.out.println(\"TERMS_PER_DOC=\" + TERMS_PER_DOC);\n      System.out.println(\"numDocs=\" + numDocs);\n\n      for(int i=0;i<numDocs;i++) {\n        final long t0 = System.currentTimeMillis();\n        w.addDocument(doc);\n        System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n      }\n      savedTerms = ts.savedTerms;\n\n      System.out.println(\"TEST: optimize\");\n      w.optimize();\n      System.out.println(\"TEST: close writer\");\n      w.close();\n    }\n\n    System.out.println(\"TEST: open reader\");\n    final IndexReader r = IndexReader.open(dir);\n    if (savedTerms == null) {\n      savedTerms = findTerms(r);\n    }\n    final int numSavedTerms = savedTerms.size();\n    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));\n    System.out.println(\"TEST: test big ord terms...\");\n    testSavedTerms(r, bigOrdTerms);\n    System.out.println(\"TEST: test all saved terms...\");\n    testSavedTerms(r, savedTerms);\n    r.close();\n\n    System.out.println(\"TEST: now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n    System.out.println(\"TEST: done!\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","sourceNew":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    final long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    final int TERMS_PER_DOC = _TestUtil.nextInt(random, 100000, 1000000);\n\n    List<BytesRef> savedTerms = null;\n\n    MockDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    dir.setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    //Directory dir = newFSDirectory(new File(\"/p/lucene/indices/2bindex\"));\n\n    if (true) {\n\n      IndexWriter w = new IndexWriter(dir,\n                                      new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n                                      .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                      .setRAMBufferSizeMB(256.0)\n                                      .setMergeScheduler(new ConcurrentMergeScheduler())\n                                      .setMergePolicy(newLogMergePolicy(false, 10))\n                                      .setOpenMode(IndexWriterConfig.OpenMode.CREATE));\n\n      MergePolicy mp = w.getConfig().getMergePolicy();\n      if (mp instanceof LogByteSizeMergePolicy) {\n        // 1 petabyte:\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n      }\n\n      Document doc = new Document();\n      final MyTokenStream ts = new MyTokenStream(random, TERMS_PER_DOC);\n      Field field = new Field(\"field\", ts);\n      field.setOmitTermFreqAndPositions(true);\n      field.setOmitNorms(true);\n      doc.add(field);\n      //w.setInfoStream(System.out);\n      final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n\n      System.out.println(\"TERMS_PER_DOC=\" + TERMS_PER_DOC);\n      System.out.println(\"numDocs=\" + numDocs);\n\n      for(int i=0;i<numDocs;i++) {\n        final long t0 = System.currentTimeMillis();\n        w.addDocument(doc);\n        System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n      }\n      savedTerms = ts.savedTerms;\n\n      System.out.println(\"TEST: optimize\");\n      w.optimize();\n      System.out.println(\"TEST: close writer\");\n      w.close();\n    }\n\n    System.out.println(\"TEST: open reader\");\n    final IndexReader r = IndexReader.open(dir);\n    if (savedTerms == null) {\n      savedTerms = findTerms(r);\n    }\n    final int numSavedTerms = savedTerms.size();\n    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));\n    System.out.println(\"TEST: test big ord terms...\");\n    testSavedTerms(r, bigOrdTerms);\n    System.out.println(\"TEST: test all saved terms...\");\n    testSavedTerms(r, savedTerms);\n    r.close();\n\n    System.out.println(\"TEST: now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n    System.out.println(\"TEST: done!\");\n  }\n\n","sourceOld":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    int TERMS_PER_DOC = 1000000;\n\n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    IndexWriter w = new IndexWriter(\n        dir,\n        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).\n            setRAMBufferSizeMB(256.0).\n            setMergeScheduler(new ConcurrentMergeScheduler()).\n            setMergePolicy(newLogMergePolicy(false, 10))\n    );\n\n    MergePolicy mp = w.getConfig().getMergePolicy();\n    if (mp instanceof LogByteSizeMergePolicy) {\n      // 1 petabyte:\n      ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n    }\n\n    Document doc = new Document();\n    Field field = new Field(\"field\", new MyTokenStream(TERMS_PER_DOC));\n    field.setOmitTermFreqAndPositions(true);\n    field.setOmitNorms(true);\n    doc.add(field);\n    //w.setInfoStream(System.out);\n    final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n    for(int i=0;i<numDocs;i++) {\n      final long t0 = System.currentTimeMillis();\n      w.addDocument(doc);\n      System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n    }\n    System.out.println(\"now optimize...\");\n    w.optimize();\n    w.close();\n\n    System.out.println(\"now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ff6fd241dc6610f7f81b62e3ba4cedf105939623","date":1307331653,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","sourceNew":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    final long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    final int TERMS_PER_DOC = _TestUtil.nextInt(random, 100000, 1000000);\n\n    List<BytesRef> savedTerms = null;\n\n    MockDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    dir.setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    dir.setCheckIndexOnClose(false); // don't double-checkindex\n    //Directory dir = newFSDirectory(new File(\"/p/lucene/indices/2bindex\"));\n\n    if (true) {\n\n      IndexWriter w = new IndexWriter(dir,\n                                      new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n                                      .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                      .setRAMBufferSizeMB(256.0)\n                                      .setMergeScheduler(new ConcurrentMergeScheduler())\n                                      .setMergePolicy(newLogMergePolicy(false, 10))\n                                      .setOpenMode(IndexWriterConfig.OpenMode.CREATE));\n\n      MergePolicy mp = w.getConfig().getMergePolicy();\n      if (mp instanceof LogByteSizeMergePolicy) {\n        // 1 petabyte:\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n      }\n\n      Document doc = new Document();\n      final MyTokenStream ts = new MyTokenStream(random, TERMS_PER_DOC);\n      Field field = new Field(\"field\", ts);\n      field.setOmitTermFreqAndPositions(true);\n      field.setOmitNorms(true);\n      doc.add(field);\n      //w.setInfoStream(System.out);\n      final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n\n      System.out.println(\"TERMS_PER_DOC=\" + TERMS_PER_DOC);\n      System.out.println(\"numDocs=\" + numDocs);\n\n      for(int i=0;i<numDocs;i++) {\n        final long t0 = System.currentTimeMillis();\n        w.addDocument(doc);\n        System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n      }\n      savedTerms = ts.savedTerms;\n\n      System.out.println(\"TEST: optimize\");\n      w.optimize();\n      System.out.println(\"TEST: close writer\");\n      w.close();\n    }\n\n    System.out.println(\"TEST: open reader\");\n    final IndexReader r = IndexReader.open(dir);\n    if (savedTerms == null) {\n      savedTerms = findTerms(r);\n    }\n    final int numSavedTerms = savedTerms.size();\n    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));\n    System.out.println(\"TEST: test big ord terms...\");\n    testSavedTerms(r, bigOrdTerms);\n    System.out.println(\"TEST: test all saved terms...\");\n    testSavedTerms(r, savedTerms);\n    r.close();\n\n    System.out.println(\"TEST: now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n    System.out.println(\"TEST: done!\");\n  }\n\n","sourceOld":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    final long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    final int TERMS_PER_DOC = _TestUtil.nextInt(random, 100000, 1000000);\n\n    List<BytesRef> savedTerms = null;\n\n    MockDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    dir.setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    //Directory dir = newFSDirectory(new File(\"/p/lucene/indices/2bindex\"));\n\n    if (true) {\n\n      IndexWriter w = new IndexWriter(dir,\n                                      new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n                                      .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                      .setRAMBufferSizeMB(256.0)\n                                      .setMergeScheduler(new ConcurrentMergeScheduler())\n                                      .setMergePolicy(newLogMergePolicy(false, 10))\n                                      .setOpenMode(IndexWriterConfig.OpenMode.CREATE));\n\n      MergePolicy mp = w.getConfig().getMergePolicy();\n      if (mp instanceof LogByteSizeMergePolicy) {\n        // 1 petabyte:\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n      }\n\n      Document doc = new Document();\n      final MyTokenStream ts = new MyTokenStream(random, TERMS_PER_DOC);\n      Field field = new Field(\"field\", ts);\n      field.setOmitTermFreqAndPositions(true);\n      field.setOmitNorms(true);\n      doc.add(field);\n      //w.setInfoStream(System.out);\n      final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n\n      System.out.println(\"TERMS_PER_DOC=\" + TERMS_PER_DOC);\n      System.out.println(\"numDocs=\" + numDocs);\n\n      for(int i=0;i<numDocs;i++) {\n        final long t0 = System.currentTimeMillis();\n        w.addDocument(doc);\n        System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n      }\n      savedTerms = ts.savedTerms;\n\n      System.out.println(\"TEST: optimize\");\n      w.optimize();\n      System.out.println(\"TEST: close writer\");\n      w.close();\n    }\n\n    System.out.println(\"TEST: open reader\");\n    final IndexReader r = IndexReader.open(dir);\n    if (savedTerms == null) {\n      savedTerms = findTerms(r);\n    }\n    final int numSavedTerms = savedTerms.size();\n    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));\n    System.out.println(\"TEST: test big ord terms...\");\n    testSavedTerms(r, bigOrdTerms);\n    System.out.println(\"TEST: test all saved terms...\");\n    testSavedTerms(r, savedTerms);\n    r.close();\n\n    System.out.println(\"TEST: now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n    System.out.println(\"TEST: done!\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"79c2cb24929f2649a8875fb629086171f914d5ce","date":1307332717,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","sourceNew":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    final long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    final int TERMS_PER_DOC = _TestUtil.nextInt(random, 100000, 1000000);\n\n    List<BytesRef> savedTerms = null;\n\n    MockDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    dir.setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    dir.setCheckIndexOnClose(false); // don't double-checkindex\n    //Directory dir = newFSDirectory(new File(\"/p/lucene/indices/2bindex\"));\n\n    if (true) {\n\n      IndexWriter w = new IndexWriter(dir,\n                                      new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n                                      .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                      .setRAMBufferSizeMB(256.0)\n                                      .setMergeScheduler(new ConcurrentMergeScheduler())\n                                      .setMergePolicy(newLogMergePolicy(false, 10))\n                                      .setOpenMode(IndexWriterConfig.OpenMode.CREATE));\n\n      MergePolicy mp = w.getConfig().getMergePolicy();\n      if (mp instanceof LogByteSizeMergePolicy) {\n        // 1 petabyte:\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n      }\n\n      Document doc = new Document();\n      final MyTokenStream ts = new MyTokenStream(random, TERMS_PER_DOC);\n      Field field = new Field(\"field\", ts);\n      field.setOmitTermFreqAndPositions(true);\n      field.setOmitNorms(true);\n      doc.add(field);\n      //w.setInfoStream(System.out);\n      final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n\n      System.out.println(\"TERMS_PER_DOC=\" + TERMS_PER_DOC);\n      System.out.println(\"numDocs=\" + numDocs);\n\n      for(int i=0;i<numDocs;i++) {\n        final long t0 = System.currentTimeMillis();\n        w.addDocument(doc);\n        System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n      }\n      savedTerms = ts.savedTerms;\n\n      System.out.println(\"TEST: optimize\");\n      w.optimize();\n      System.out.println(\"TEST: close writer\");\n      w.close();\n    }\n\n    System.out.println(\"TEST: open reader\");\n    final IndexReader r = IndexReader.open(dir);\n    if (savedTerms == null) {\n      savedTerms = findTerms(r);\n    }\n    final int numSavedTerms = savedTerms.size();\n    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));\n    System.out.println(\"TEST: test big ord terms...\");\n    testSavedTerms(r, bigOrdTerms);\n    System.out.println(\"TEST: test all saved terms...\");\n    testSavedTerms(r, savedTerms);\n    r.close();\n\n    System.out.println(\"TEST: now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n    System.out.println(\"TEST: done!\");\n  }\n\n","sourceOld":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    final long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    final int TERMS_PER_DOC = _TestUtil.nextInt(random, 100000, 1000000);\n\n    List<BytesRef> savedTerms = null;\n\n    MockDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    dir.setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    //Directory dir = newFSDirectory(new File(\"/p/lucene/indices/2bindex\"));\n\n    if (true) {\n\n      IndexWriter w = new IndexWriter(dir,\n                                      new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n                                      .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                      .setRAMBufferSizeMB(256.0)\n                                      .setMergeScheduler(new ConcurrentMergeScheduler())\n                                      .setMergePolicy(newLogMergePolicy(false, 10))\n                                      .setOpenMode(IndexWriterConfig.OpenMode.CREATE));\n\n      MergePolicy mp = w.getConfig().getMergePolicy();\n      if (mp instanceof LogByteSizeMergePolicy) {\n        // 1 petabyte:\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n      }\n\n      Document doc = new Document();\n      final MyTokenStream ts = new MyTokenStream(random, TERMS_PER_DOC);\n      Field field = new Field(\"field\", ts);\n      field.setOmitTermFreqAndPositions(true);\n      field.setOmitNorms(true);\n      doc.add(field);\n      //w.setInfoStream(System.out);\n      final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n\n      System.out.println(\"TERMS_PER_DOC=\" + TERMS_PER_DOC);\n      System.out.println(\"numDocs=\" + numDocs);\n\n      for(int i=0;i<numDocs;i++) {\n        final long t0 = System.currentTimeMillis();\n        w.addDocument(doc);\n        System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n      }\n      savedTerms = ts.savedTerms;\n\n      System.out.println(\"TEST: optimize\");\n      w.optimize();\n      System.out.println(\"TEST: close writer\");\n      w.close();\n    }\n\n    System.out.println(\"TEST: open reader\");\n    final IndexReader r = IndexReader.open(dir);\n    if (savedTerms == null) {\n      savedTerms = findTerms(r);\n    }\n    final int numSavedTerms = savedTerms.size();\n    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));\n    System.out.println(\"TEST: test big ord terms...\");\n    testSavedTerms(r, bigOrdTerms);\n    System.out.println(\"TEST: test all saved terms...\");\n    testSavedTerms(r, savedTerms);\n    r.close();\n\n    System.out.println(\"TEST: now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n    System.out.println(\"TEST: done!\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"77cf4379b2824f6ea34b091c495d6e95c38ff9e2","date":1307610475,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","sourceNew":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    final long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    final int TERMS_PER_DOC = _TestUtil.nextInt(random, 100000, 1000000);\n\n    List<BytesRef> savedTerms = null;\n\n    MockDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    dir.setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    dir.setCheckIndexOnClose(false); // don't double-checkindex\n    //Directory dir = newFSDirectory(new File(\"/p/lucene/indices/2bindex\"));\n\n    if (true) {\n\n      IndexWriter w = new IndexWriter(dir,\n                                      new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n                                      .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                      .setRAMBufferSizeMB(256.0)\n                                      .setMergeScheduler(new ConcurrentMergeScheduler())\n                                      .setMergePolicy(newLogMergePolicy(false, 10))\n                                      .setOpenMode(IndexWriterConfig.OpenMode.CREATE));\n\n      MergePolicy mp = w.getConfig().getMergePolicy();\n      if (mp instanceof LogByteSizeMergePolicy) {\n        // 1 petabyte:\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n      }\n\n      Document doc = new Document();\n      final MyTokenStream ts = new MyTokenStream(random, TERMS_PER_DOC);\n      Field field = new Field(\"field\", ts);\n      field.setOmitTermFreqAndPositions(true);\n      field.setOmitNorms(true);\n      doc.add(field);\n      //w.setInfoStream(System.out);\n      final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n\n      System.out.println(\"TERMS_PER_DOC=\" + TERMS_PER_DOC);\n      System.out.println(\"numDocs=\" + numDocs);\n\n      for(int i=0;i<numDocs;i++) {\n        final long t0 = System.currentTimeMillis();\n        w.addDocument(doc);\n        System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n      }\n      savedTerms = ts.savedTerms;\n\n      System.out.println(\"TEST: optimize\");\n      w.optimize();\n      System.out.println(\"TEST: close writer\");\n      w.close();\n    }\n\n    System.out.println(\"TEST: open reader\");\n    final IndexReader r = IndexReader.open(dir);\n    if (savedTerms == null) {\n      savedTerms = findTerms(r);\n    }\n    final int numSavedTerms = savedTerms.size();\n    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));\n    System.out.println(\"TEST: test big ord terms...\");\n    testSavedTerms(r, bigOrdTerms);\n    System.out.println(\"TEST: test all saved terms...\");\n    testSavedTerms(r, savedTerms);\n    r.close();\n\n    System.out.println(\"TEST: now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n    System.out.println(\"TEST: done!\");\n  }\n\n","sourceOld":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    final long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    final int TERMS_PER_DOC = _TestUtil.nextInt(random, 100000, 1000000);\n\n    List<BytesRef> savedTerms = null;\n\n    MockDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    dir.setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    //Directory dir = newFSDirectory(new File(\"/p/lucene/indices/2bindex\"));\n\n    if (true) {\n\n      IndexWriter w = new IndexWriter(dir,\n                                      new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n                                      .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                      .setRAMBufferSizeMB(256.0)\n                                      .setMergeScheduler(new ConcurrentMergeScheduler())\n                                      .setMergePolicy(newLogMergePolicy(false, 10))\n                                      .setOpenMode(IndexWriterConfig.OpenMode.CREATE));\n\n      MergePolicy mp = w.getConfig().getMergePolicy();\n      if (mp instanceof LogByteSizeMergePolicy) {\n        // 1 petabyte:\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n      }\n\n      Document doc = new Document();\n      final MyTokenStream ts = new MyTokenStream(random, TERMS_PER_DOC);\n      Field field = new Field(\"field\", ts);\n      field.setOmitTermFreqAndPositions(true);\n      field.setOmitNorms(true);\n      doc.add(field);\n      //w.setInfoStream(System.out);\n      final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n\n      System.out.println(\"TERMS_PER_DOC=\" + TERMS_PER_DOC);\n      System.out.println(\"numDocs=\" + numDocs);\n\n      for(int i=0;i<numDocs;i++) {\n        final long t0 = System.currentTimeMillis();\n        w.addDocument(doc);\n        System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n      }\n      savedTerms = ts.savedTerms;\n\n      System.out.println(\"TEST: optimize\");\n      w.optimize();\n      System.out.println(\"TEST: close writer\");\n      w.close();\n    }\n\n    System.out.println(\"TEST: open reader\");\n    final IndexReader r = IndexReader.open(dir);\n    if (savedTerms == null) {\n      savedTerms = findTerms(r);\n    }\n    final int numSavedTerms = savedTerms.size();\n    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));\n    System.out.println(\"TEST: test big ord terms...\");\n    testSavedTerms(r, bigOrdTerms);\n    System.out.println(\"TEST: test all saved terms...\");\n    testSavedTerms(r, savedTerms);\n    r.close();\n\n    System.out.println(\"TEST: now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n    System.out.println(\"TEST: done!\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2afd23a6f1242190c3409d8d81d5c5912d607fc9","date":1310477482,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","sourceNew":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    final long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    final int TERMS_PER_DOC = _TestUtil.nextInt(random, 100000, 1000000);\n\n    List<BytesRef> savedTerms = null;\n\n    MockDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    dir.setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    dir.setCheckIndexOnClose(false); // don't double-checkindex\n    //Directory dir = newFSDirectory(new File(\"/p/lucene/indices/2bindex\"));\n\n    if (true) {\n\n      IndexWriter w = new IndexWriter(dir,\n                                      new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n                                      .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                      .setRAMBufferSizeMB(256.0)\n                                      .setMergeScheduler(new ConcurrentMergeScheduler())\n                                      .setMergePolicy(newLogMergePolicy(false, 10))\n                                      .setOpenMode(IndexWriterConfig.OpenMode.CREATE));\n\n      MergePolicy mp = w.getConfig().getMergePolicy();\n      if (mp instanceof LogByteSizeMergePolicy) {\n        // 1 petabyte:\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n      }\n\n      Document doc = new Document();\n      final MyTokenStream ts = new MyTokenStream(random, TERMS_PER_DOC);\n      Field field = new Field(\"field\", ts);\n      field.setIndexOptions(IndexOptions.DOCS_ONLY);\n      field.setOmitNorms(true);\n      doc.add(field);\n      //w.setInfoStream(System.out);\n      final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n\n      System.out.println(\"TERMS_PER_DOC=\" + TERMS_PER_DOC);\n      System.out.println(\"numDocs=\" + numDocs);\n\n      for(int i=0;i<numDocs;i++) {\n        final long t0 = System.currentTimeMillis();\n        w.addDocument(doc);\n        System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n      }\n      savedTerms = ts.savedTerms;\n\n      System.out.println(\"TEST: optimize\");\n      w.optimize();\n      System.out.println(\"TEST: close writer\");\n      w.close();\n    }\n\n    System.out.println(\"TEST: open reader\");\n    final IndexReader r = IndexReader.open(dir);\n    if (savedTerms == null) {\n      savedTerms = findTerms(r);\n    }\n    final int numSavedTerms = savedTerms.size();\n    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));\n    System.out.println(\"TEST: test big ord terms...\");\n    testSavedTerms(r, bigOrdTerms);\n    System.out.println(\"TEST: test all saved terms...\");\n    testSavedTerms(r, savedTerms);\n    r.close();\n\n    System.out.println(\"TEST: now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n    System.out.println(\"TEST: done!\");\n  }\n\n","sourceOld":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    final long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    final int TERMS_PER_DOC = _TestUtil.nextInt(random, 100000, 1000000);\n\n    List<BytesRef> savedTerms = null;\n\n    MockDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    dir.setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    dir.setCheckIndexOnClose(false); // don't double-checkindex\n    //Directory dir = newFSDirectory(new File(\"/p/lucene/indices/2bindex\"));\n\n    if (true) {\n\n      IndexWriter w = new IndexWriter(dir,\n                                      new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n                                      .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                      .setRAMBufferSizeMB(256.0)\n                                      .setMergeScheduler(new ConcurrentMergeScheduler())\n                                      .setMergePolicy(newLogMergePolicy(false, 10))\n                                      .setOpenMode(IndexWriterConfig.OpenMode.CREATE));\n\n      MergePolicy mp = w.getConfig().getMergePolicy();\n      if (mp instanceof LogByteSizeMergePolicy) {\n        // 1 petabyte:\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n      }\n\n      Document doc = new Document();\n      final MyTokenStream ts = new MyTokenStream(random, TERMS_PER_DOC);\n      Field field = new Field(\"field\", ts);\n      field.setOmitTermFreqAndPositions(true);\n      field.setOmitNorms(true);\n      doc.add(field);\n      //w.setInfoStream(System.out);\n      final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n\n      System.out.println(\"TERMS_PER_DOC=\" + TERMS_PER_DOC);\n      System.out.println(\"numDocs=\" + numDocs);\n\n      for(int i=0;i<numDocs;i++) {\n        final long t0 = System.currentTimeMillis();\n        w.addDocument(doc);\n        System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n      }\n      savedTerms = ts.savedTerms;\n\n      System.out.println(\"TEST: optimize\");\n      w.optimize();\n      System.out.println(\"TEST: close writer\");\n      w.close();\n    }\n\n    System.out.println(\"TEST: open reader\");\n    final IndexReader r = IndexReader.open(dir);\n    if (savedTerms == null) {\n      savedTerms = findTerms(r);\n    }\n    final int numSavedTerms = savedTerms.size();\n    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));\n    System.out.println(\"TEST: test big ord terms...\");\n    testSavedTerms(r, bigOrdTerms);\n    System.out.println(\"TEST: test all saved terms...\");\n    testSavedTerms(r, savedTerms);\n    r.close();\n\n    System.out.println(\"TEST: now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n    System.out.println(\"TEST: done!\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"60ba444201d2570214b6fcf1d15600dc1a01f548","date":1313868045,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","sourceNew":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    final long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    final int TERMS_PER_DOC = _TestUtil.nextInt(random, 100000, 1000000);\n\n    List<BytesRef> savedTerms = null;\n\n    MockDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    //MockDirectoryWrapper dir = newFSDirectory(new File(\"/p/lucene/indices/2bindex\"));\n    dir.setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    dir.setCheckIndexOnClose(false); // don't double-checkindex\n\n    if (true) {\n\n      IndexWriter w = new IndexWriter(dir,\n                                      new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n                                      .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                      .setRAMBufferSizeMB(256.0)\n                                      .setMergeScheduler(new ConcurrentMergeScheduler())\n                                      .setMergePolicy(newLogMergePolicy(false, 10))\n                                      .setOpenMode(IndexWriterConfig.OpenMode.CREATE));\n\n      w.setInfoStream(VERBOSE ? System.out : null);\n      MergePolicy mp = w.getConfig().getMergePolicy();\n      if (mp instanceof LogByteSizeMergePolicy) {\n        // 1 petabyte:\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n      }\n\n      Document doc = new Document();\n      final MyTokenStream ts = new MyTokenStream(random, TERMS_PER_DOC);\n      Field field = new Field(\"field\", ts);\n      field.setIndexOptions(IndexOptions.DOCS_ONLY);\n      field.setOmitNorms(true);\n      doc.add(field);\n      //w.setInfoStream(System.out);\n      final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n\n      System.out.println(\"TERMS_PER_DOC=\" + TERMS_PER_DOC);\n      System.out.println(\"numDocs=\" + numDocs);\n\n      for(int i=0;i<numDocs;i++) {\n        final long t0 = System.currentTimeMillis();\n        w.addDocument(doc);\n        System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n      }\n      savedTerms = ts.savedTerms;\n\n      System.out.println(\"TEST: optimize\");\n      w.optimize();\n      System.out.println(\"TEST: close writer\");\n      w.close();\n    }\n\n    System.out.println(\"TEST: open reader\");\n    final IndexReader r = IndexReader.open(dir);\n    if (savedTerms == null) {\n      savedTerms = findTerms(r);\n    }\n    final int numSavedTerms = savedTerms.size();\n    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));\n    System.out.println(\"TEST: test big ord terms...\");\n    testSavedTerms(r, bigOrdTerms);\n    System.out.println(\"TEST: test all saved terms...\");\n    testSavedTerms(r, savedTerms);\n    r.close();\n\n    System.out.println(\"TEST: now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n    System.out.println(\"TEST: done!\");\n  }\n\n","sourceOld":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    final long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    final int TERMS_PER_DOC = _TestUtil.nextInt(random, 100000, 1000000);\n\n    List<BytesRef> savedTerms = null;\n\n    MockDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    dir.setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    dir.setCheckIndexOnClose(false); // don't double-checkindex\n    //Directory dir = newFSDirectory(new File(\"/p/lucene/indices/2bindex\"));\n\n    if (true) {\n\n      IndexWriter w = new IndexWriter(dir,\n                                      new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n                                      .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                      .setRAMBufferSizeMB(256.0)\n                                      .setMergeScheduler(new ConcurrentMergeScheduler())\n                                      .setMergePolicy(newLogMergePolicy(false, 10))\n                                      .setOpenMode(IndexWriterConfig.OpenMode.CREATE));\n\n      MergePolicy mp = w.getConfig().getMergePolicy();\n      if (mp instanceof LogByteSizeMergePolicy) {\n        // 1 petabyte:\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n      }\n\n      Document doc = new Document();\n      final MyTokenStream ts = new MyTokenStream(random, TERMS_PER_DOC);\n      Field field = new Field(\"field\", ts);\n      field.setIndexOptions(IndexOptions.DOCS_ONLY);\n      field.setOmitNorms(true);\n      doc.add(field);\n      //w.setInfoStream(System.out);\n      final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n\n      System.out.println(\"TERMS_PER_DOC=\" + TERMS_PER_DOC);\n      System.out.println(\"numDocs=\" + numDocs);\n\n      for(int i=0;i<numDocs;i++) {\n        final long t0 = System.currentTimeMillis();\n        w.addDocument(doc);\n        System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n      }\n      savedTerms = ts.savedTerms;\n\n      System.out.println(\"TEST: optimize\");\n      w.optimize();\n      System.out.println(\"TEST: close writer\");\n      w.close();\n    }\n\n    System.out.println(\"TEST: open reader\");\n    final IndexReader r = IndexReader.open(dir);\n    if (savedTerms == null) {\n      savedTerms = findTerms(r);\n    }\n    final int numSavedTerms = savedTerms.size();\n    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));\n    System.out.println(\"TEST: test big ord terms...\");\n    testSavedTerms(r, bigOrdTerms);\n    System.out.println(\"TEST: test all saved terms...\");\n    testSavedTerms(r, savedTerms);\n    r.close();\n\n    System.out.println(\"TEST: now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n    System.out.println(\"TEST: done!\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1509f151d7692d84fae414b2b799ac06ba60fcb4","date":1314451621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","sourceNew":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    final long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    final int TERMS_PER_DOC = _TestUtil.nextInt(random, 100000, 1000000);\n\n    List<BytesRef> savedTerms = null;\n\n    MockDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    //MockDirectoryWrapper dir = newFSDirectory(new File(\"/p/lucene/indices/2bindex\"));\n    dir.setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    dir.setCheckIndexOnClose(false); // don't double-checkindex\n\n    if (true) {\n\n      IndexWriter w = new IndexWriter(dir,\n                                      new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n                                      .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                      .setRAMBufferSizeMB(256.0)\n                                      .setMergeScheduler(new ConcurrentMergeScheduler())\n                                      .setMergePolicy(newLogMergePolicy(false, 10))\n                                      .setOpenMode(IndexWriterConfig.OpenMode.CREATE));\n\n      w.setInfoStream(VERBOSE ? System.out : null);\n      MergePolicy mp = w.getConfig().getMergePolicy();\n      if (mp instanceof LogByteSizeMergePolicy) {\n        // 1 petabyte:\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n      }\n\n      Document doc = new Document();\n      final MyTokenStream ts = new MyTokenStream(random, TERMS_PER_DOC);\n\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setIndexOptions(IndexOptions.DOCS_ONLY);\n      customType.setOmitNorms(true);\n      Field field = new Field(\"field\", customType, ts);\n      doc.add(field);\n      //w.setInfoStream(System.out);\n      final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n\n      System.out.println(\"TERMS_PER_DOC=\" + TERMS_PER_DOC);\n      System.out.println(\"numDocs=\" + numDocs);\n\n      for(int i=0;i<numDocs;i++) {\n        final long t0 = System.currentTimeMillis();\n        w.addDocument(doc);\n        System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n      }\n      savedTerms = ts.savedTerms;\n\n      System.out.println(\"TEST: optimize\");\n      w.optimize();\n      System.out.println(\"TEST: close writer\");\n      w.close();\n    }\n\n    System.out.println(\"TEST: open reader\");\n    final IndexReader r = IndexReader.open(dir);\n    if (savedTerms == null) {\n      savedTerms = findTerms(r);\n    }\n    final int numSavedTerms = savedTerms.size();\n    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));\n    System.out.println(\"TEST: test big ord terms...\");\n    testSavedTerms(r, bigOrdTerms);\n    System.out.println(\"TEST: test all saved terms...\");\n    testSavedTerms(r, savedTerms);\n    r.close();\n\n    System.out.println(\"TEST: now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n    System.out.println(\"TEST: done!\");\n  }\n\n","sourceOld":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    final long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    final int TERMS_PER_DOC = _TestUtil.nextInt(random, 100000, 1000000);\n\n    List<BytesRef> savedTerms = null;\n\n    MockDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    //MockDirectoryWrapper dir = newFSDirectory(new File(\"/p/lucene/indices/2bindex\"));\n    dir.setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    dir.setCheckIndexOnClose(false); // don't double-checkindex\n\n    if (true) {\n\n      IndexWriter w = new IndexWriter(dir,\n                                      new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n                                      .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                      .setRAMBufferSizeMB(256.0)\n                                      .setMergeScheduler(new ConcurrentMergeScheduler())\n                                      .setMergePolicy(newLogMergePolicy(false, 10))\n                                      .setOpenMode(IndexWriterConfig.OpenMode.CREATE));\n\n      w.setInfoStream(VERBOSE ? System.out : null);\n      MergePolicy mp = w.getConfig().getMergePolicy();\n      if (mp instanceof LogByteSizeMergePolicy) {\n        // 1 petabyte:\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n      }\n\n      Document doc = new Document();\n      final MyTokenStream ts = new MyTokenStream(random, TERMS_PER_DOC);\n      Field field = new Field(\"field\", ts);\n      field.setIndexOptions(IndexOptions.DOCS_ONLY);\n      field.setOmitNorms(true);\n      doc.add(field);\n      //w.setInfoStream(System.out);\n      final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n\n      System.out.println(\"TERMS_PER_DOC=\" + TERMS_PER_DOC);\n      System.out.println(\"numDocs=\" + numDocs);\n\n      for(int i=0;i<numDocs;i++) {\n        final long t0 = System.currentTimeMillis();\n        w.addDocument(doc);\n        System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n      }\n      savedTerms = ts.savedTerms;\n\n      System.out.println(\"TEST: optimize\");\n      w.optimize();\n      System.out.println(\"TEST: close writer\");\n      w.close();\n    }\n\n    System.out.println(\"TEST: open reader\");\n    final IndexReader r = IndexReader.open(dir);\n    if (savedTerms == null) {\n      savedTerms = findTerms(r);\n    }\n    final int numSavedTerms = savedTerms.size();\n    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));\n    System.out.println(\"TEST: test big ord terms...\");\n    testSavedTerms(r, bigOrdTerms);\n    System.out.println(\"TEST: test all saved terms...\");\n    testSavedTerms(r, savedTerms);\n    r.close();\n\n    System.out.println(\"TEST: now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n    System.out.println(\"TEST: done!\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7e4db59c6b6c10e25322cfb41c4c19d78b4298bd","date":1317197236,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","sourceNew":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    final long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    final int TERMS_PER_DOC = _TestUtil.nextInt(random, 100000, 1000000);\n\n    List<BytesRef> savedTerms = null;\n\n    MockDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    //MockDirectoryWrapper dir = newFSDirectory(new File(\"/p/lucene/indices/2bindex\"));\n    dir.setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    dir.setCheckIndexOnClose(false); // don't double-checkindex\n\n    if (true) {\n\n      IndexWriter w = new IndexWriter(dir,\n                                      new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n                                      .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                      .setRAMBufferSizeMB(256.0)\n                                      .setMergeScheduler(new ConcurrentMergeScheduler())\n                                      .setMergePolicy(newLogMergePolicy(false, 10))\n                                      .setOpenMode(IndexWriterConfig.OpenMode.CREATE));\n\n      w.setInfoStream(VERBOSE ? System.out : null);\n      MergePolicy mp = w.getConfig().getMergePolicy();\n      if (mp instanceof LogByteSizeMergePolicy) {\n        // 1 petabyte:\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n      }\n\n      Document doc = new Document();\n      final MyTokenStream ts = new MyTokenStream(random, TERMS_PER_DOC);\n\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setIndexOptions(IndexOptions.DOCS_ONLY);\n      customType.setOmitNorms(true);\n      Field field = new Field(\"field\", ts, customType);\n      doc.add(field);\n      //w.setInfoStream(System.out);\n      final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n\n      System.out.println(\"TERMS_PER_DOC=\" + TERMS_PER_DOC);\n      System.out.println(\"numDocs=\" + numDocs);\n\n      for(int i=0;i<numDocs;i++) {\n        final long t0 = System.currentTimeMillis();\n        w.addDocument(doc);\n        System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n      }\n      savedTerms = ts.savedTerms;\n\n      System.out.println(\"TEST: optimize\");\n      w.optimize();\n      System.out.println(\"TEST: close writer\");\n      w.close();\n    }\n\n    System.out.println(\"TEST: open reader\");\n    final IndexReader r = IndexReader.open(dir);\n    if (savedTerms == null) {\n      savedTerms = findTerms(r);\n    }\n    final int numSavedTerms = savedTerms.size();\n    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));\n    System.out.println(\"TEST: test big ord terms...\");\n    testSavedTerms(r, bigOrdTerms);\n    System.out.println(\"TEST: test all saved terms...\");\n    testSavedTerms(r, savedTerms);\n    r.close();\n\n    System.out.println(\"TEST: now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n    System.out.println(\"TEST: done!\");\n  }\n\n","sourceOld":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    final long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    final int TERMS_PER_DOC = _TestUtil.nextInt(random, 100000, 1000000);\n\n    List<BytesRef> savedTerms = null;\n\n    MockDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    //MockDirectoryWrapper dir = newFSDirectory(new File(\"/p/lucene/indices/2bindex\"));\n    dir.setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    dir.setCheckIndexOnClose(false); // don't double-checkindex\n\n    if (true) {\n\n      IndexWriter w = new IndexWriter(dir,\n                                      new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n                                      .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                      .setRAMBufferSizeMB(256.0)\n                                      .setMergeScheduler(new ConcurrentMergeScheduler())\n                                      .setMergePolicy(newLogMergePolicy(false, 10))\n                                      .setOpenMode(IndexWriterConfig.OpenMode.CREATE));\n\n      w.setInfoStream(VERBOSE ? System.out : null);\n      MergePolicy mp = w.getConfig().getMergePolicy();\n      if (mp instanceof LogByteSizeMergePolicy) {\n        // 1 petabyte:\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n      }\n\n      Document doc = new Document();\n      final MyTokenStream ts = new MyTokenStream(random, TERMS_PER_DOC);\n\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setIndexOptions(IndexOptions.DOCS_ONLY);\n      customType.setOmitNorms(true);\n      Field field = new Field(\"field\", customType, ts);\n      doc.add(field);\n      //w.setInfoStream(System.out);\n      final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n\n      System.out.println(\"TERMS_PER_DOC=\" + TERMS_PER_DOC);\n      System.out.println(\"numDocs=\" + numDocs);\n\n      for(int i=0;i<numDocs;i++) {\n        final long t0 = System.currentTimeMillis();\n        w.addDocument(doc);\n        System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n      }\n      savedTerms = ts.savedTerms;\n\n      System.out.println(\"TEST: optimize\");\n      w.optimize();\n      System.out.println(\"TEST: close writer\");\n      w.close();\n    }\n\n    System.out.println(\"TEST: open reader\");\n    final IndexReader r = IndexReader.open(dir);\n    if (savedTerms == null) {\n      savedTerms = findTerms(r);\n    }\n    final int numSavedTerms = savedTerms.size();\n    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));\n    System.out.println(\"TEST: test big ord terms...\");\n    testSavedTerms(r, bigOrdTerms);\n    System.out.println(\"TEST: test all saved terms...\");\n    testSavedTerms(r, savedTerms);\n    r.close();\n\n    System.out.println(\"TEST: now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n    System.out.println(\"TEST: done!\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7b91922b55d15444d554721b352861d028eb8278","date":1320421415,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","sourceNew":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"Lucene3x\".equals(Codec.getDefault().getName())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    final long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    final int TERMS_PER_DOC = _TestUtil.nextInt(random, 100000, 1000000);\n\n    List<BytesRef> savedTerms = null;\n\n    MockDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    //MockDirectoryWrapper dir = newFSDirectory(new File(\"/p/lucene/indices/2bindex\"));\n    dir.setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    dir.setCheckIndexOnClose(false); // don't double-checkindex\n\n    if (true) {\n\n      IndexWriter w = new IndexWriter(dir,\n                                      new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n                                      .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                      .setRAMBufferSizeMB(256.0)\n                                      .setMergeScheduler(new ConcurrentMergeScheduler())\n                                      .setMergePolicy(newLogMergePolicy(false, 10))\n                                      .setOpenMode(IndexWriterConfig.OpenMode.CREATE));\n\n      w.setInfoStream(VERBOSE ? System.out : null);\n      MergePolicy mp = w.getConfig().getMergePolicy();\n      if (mp instanceof LogByteSizeMergePolicy) {\n        // 1 petabyte:\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n      }\n\n      Document doc = new Document();\n      final MyTokenStream ts = new MyTokenStream(random, TERMS_PER_DOC);\n\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setIndexOptions(IndexOptions.DOCS_ONLY);\n      customType.setOmitNorms(true);\n      Field field = new Field(\"field\", ts, customType);\n      doc.add(field);\n      //w.setInfoStream(System.out);\n      final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n\n      System.out.println(\"TERMS_PER_DOC=\" + TERMS_PER_DOC);\n      System.out.println(\"numDocs=\" + numDocs);\n\n      for(int i=0;i<numDocs;i++) {\n        final long t0 = System.currentTimeMillis();\n        w.addDocument(doc);\n        System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n      }\n      savedTerms = ts.savedTerms;\n\n      System.out.println(\"TEST: optimize\");\n      w.optimize();\n      System.out.println(\"TEST: close writer\");\n      w.close();\n    }\n\n    System.out.println(\"TEST: open reader\");\n    final IndexReader r = IndexReader.open(dir);\n    if (savedTerms == null) {\n      savedTerms = findTerms(r);\n    }\n    final int numSavedTerms = savedTerms.size();\n    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));\n    System.out.println(\"TEST: test big ord terms...\");\n    testSavedTerms(r, bigOrdTerms);\n    System.out.println(\"TEST: test all saved terms...\");\n    testSavedTerms(r, savedTerms);\n    r.close();\n\n    System.out.println(\"TEST: now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n    System.out.println(\"TEST: done!\");\n  }\n\n","sourceOld":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"PreFlex\".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    final long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    final int TERMS_PER_DOC = _TestUtil.nextInt(random, 100000, 1000000);\n\n    List<BytesRef> savedTerms = null;\n\n    MockDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    //MockDirectoryWrapper dir = newFSDirectory(new File(\"/p/lucene/indices/2bindex\"));\n    dir.setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    dir.setCheckIndexOnClose(false); // don't double-checkindex\n\n    if (true) {\n\n      IndexWriter w = new IndexWriter(dir,\n                                      new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n                                      .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                      .setRAMBufferSizeMB(256.0)\n                                      .setMergeScheduler(new ConcurrentMergeScheduler())\n                                      .setMergePolicy(newLogMergePolicy(false, 10))\n                                      .setOpenMode(IndexWriterConfig.OpenMode.CREATE));\n\n      w.setInfoStream(VERBOSE ? System.out : null);\n      MergePolicy mp = w.getConfig().getMergePolicy();\n      if (mp instanceof LogByteSizeMergePolicy) {\n        // 1 petabyte:\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n      }\n\n      Document doc = new Document();\n      final MyTokenStream ts = new MyTokenStream(random, TERMS_PER_DOC);\n\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setIndexOptions(IndexOptions.DOCS_ONLY);\n      customType.setOmitNorms(true);\n      Field field = new Field(\"field\", ts, customType);\n      doc.add(field);\n      //w.setInfoStream(System.out);\n      final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n\n      System.out.println(\"TERMS_PER_DOC=\" + TERMS_PER_DOC);\n      System.out.println(\"numDocs=\" + numDocs);\n\n      for(int i=0;i<numDocs;i++) {\n        final long t0 = System.currentTimeMillis();\n        w.addDocument(doc);\n        System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n      }\n      savedTerms = ts.savedTerms;\n\n      System.out.println(\"TEST: optimize\");\n      w.optimize();\n      System.out.println(\"TEST: close writer\");\n      w.close();\n    }\n\n    System.out.println(\"TEST: open reader\");\n    final IndexReader r = IndexReader.open(dir);\n    if (savedTerms == null) {\n      savedTerms = findTerms(r);\n    }\n    final int numSavedTerms = savedTerms.size();\n    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));\n    System.out.println(\"TEST: test big ord terms...\");\n    testSavedTerms(r, bigOrdTerms);\n    System.out.println(\"TEST: test all saved terms...\");\n    testSavedTerms(r, savedTerms);\n    r.close();\n\n    System.out.println(\"TEST: now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n    System.out.println(\"TEST: done!\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"06584e6e98d592b34e1329b384182f368d2025e8","date":1320850353,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","sourceNew":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"Lucene3x\".equals(Codec.getDefault().getName())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    final long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    final int TERMS_PER_DOC = _TestUtil.nextInt(random, 100000, 1000000);\n\n    List<BytesRef> savedTerms = null;\n\n    MockDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    //MockDirectoryWrapper dir = newFSDirectory(new File(\"/p/lucene/indices/2bindex\"));\n    dir.setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    dir.setCheckIndexOnClose(false); // don't double-checkindex\n\n    if (true) {\n\n      IndexWriter w = new IndexWriter(dir,\n                                      new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n                                      .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                      .setRAMBufferSizeMB(256.0)\n                                      .setMergeScheduler(new ConcurrentMergeScheduler())\n                                      .setMergePolicy(newLogMergePolicy(false, 10))\n                                      .setOpenMode(IndexWriterConfig.OpenMode.CREATE));\n\n      MergePolicy mp = w.getConfig().getMergePolicy();\n      if (mp instanceof LogByteSizeMergePolicy) {\n        // 1 petabyte:\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n      }\n\n      Document doc = new Document();\n      final MyTokenStream ts = new MyTokenStream(random, TERMS_PER_DOC);\n\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setIndexOptions(IndexOptions.DOCS_ONLY);\n      customType.setOmitNorms(true);\n      Field field = new Field(\"field\", ts, customType);\n      doc.add(field);\n      //w.setInfoStream(System.out);\n      final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n\n      System.out.println(\"TERMS_PER_DOC=\" + TERMS_PER_DOC);\n      System.out.println(\"numDocs=\" + numDocs);\n\n      for(int i=0;i<numDocs;i++) {\n        final long t0 = System.currentTimeMillis();\n        w.addDocument(doc);\n        System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n      }\n      savedTerms = ts.savedTerms;\n\n      System.out.println(\"TEST: optimize\");\n      w.optimize();\n      System.out.println(\"TEST: close writer\");\n      w.close();\n    }\n\n    System.out.println(\"TEST: open reader\");\n    final IndexReader r = IndexReader.open(dir);\n    if (savedTerms == null) {\n      savedTerms = findTerms(r);\n    }\n    final int numSavedTerms = savedTerms.size();\n    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));\n    System.out.println(\"TEST: test big ord terms...\");\n    testSavedTerms(r, bigOrdTerms);\n    System.out.println(\"TEST: test all saved terms...\");\n    testSavedTerms(r, savedTerms);\n    r.close();\n\n    System.out.println(\"TEST: now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n    System.out.println(\"TEST: done!\");\n  }\n\n","sourceOld":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"Lucene3x\".equals(Codec.getDefault().getName())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    final long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    final int TERMS_PER_DOC = _TestUtil.nextInt(random, 100000, 1000000);\n\n    List<BytesRef> savedTerms = null;\n\n    MockDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    //MockDirectoryWrapper dir = newFSDirectory(new File(\"/p/lucene/indices/2bindex\"));\n    dir.setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    dir.setCheckIndexOnClose(false); // don't double-checkindex\n\n    if (true) {\n\n      IndexWriter w = new IndexWriter(dir,\n                                      new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n                                      .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                      .setRAMBufferSizeMB(256.0)\n                                      .setMergeScheduler(new ConcurrentMergeScheduler())\n                                      .setMergePolicy(newLogMergePolicy(false, 10))\n                                      .setOpenMode(IndexWriterConfig.OpenMode.CREATE));\n\n      w.setInfoStream(VERBOSE ? System.out : null);\n      MergePolicy mp = w.getConfig().getMergePolicy();\n      if (mp instanceof LogByteSizeMergePolicy) {\n        // 1 petabyte:\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n      }\n\n      Document doc = new Document();\n      final MyTokenStream ts = new MyTokenStream(random, TERMS_PER_DOC);\n\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setIndexOptions(IndexOptions.DOCS_ONLY);\n      customType.setOmitNorms(true);\n      Field field = new Field(\"field\", ts, customType);\n      doc.add(field);\n      //w.setInfoStream(System.out);\n      final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n\n      System.out.println(\"TERMS_PER_DOC=\" + TERMS_PER_DOC);\n      System.out.println(\"numDocs=\" + numDocs);\n\n      for(int i=0;i<numDocs;i++) {\n        final long t0 = System.currentTimeMillis();\n        w.addDocument(doc);\n        System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n      }\n      savedTerms = ts.savedTerms;\n\n      System.out.println(\"TEST: optimize\");\n      w.optimize();\n      System.out.println(\"TEST: close writer\");\n      w.close();\n    }\n\n    System.out.println(\"TEST: open reader\");\n    final IndexReader r = IndexReader.open(dir);\n    if (savedTerms == null) {\n      savedTerms = findTerms(r);\n    }\n    final int numSavedTerms = savedTerms.size();\n    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));\n    System.out.println(\"TEST: test big ord terms...\");\n    testSavedTerms(r, bigOrdTerms);\n    System.out.println(\"TEST: test all saved terms...\");\n    testSavedTerms(r, savedTerms);\n    r.close();\n\n    System.out.println(\"TEST: now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n    System.out.println(\"TEST: done!\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d14e8d18c0e3970c20354dbeeb49da11bd587fbd","date":1321041051,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","sourceNew":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"Lucene3x\".equals(Codec.getDefault().getName())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    final long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    final int TERMS_PER_DOC = _TestUtil.nextInt(random, 100000, 1000000);\n\n    List<BytesRef> savedTerms = null;\n\n    MockDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    //MockDirectoryWrapper dir = newFSDirectory(new File(\"/p/lucene/indices/2bindex\"));\n    dir.setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    dir.setCheckIndexOnClose(false); // don't double-checkindex\n\n    if (true) {\n\n      IndexWriter w = new IndexWriter(dir,\n                                      new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n                                      .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                      .setRAMBufferSizeMB(256.0)\n                                      .setMergeScheduler(new ConcurrentMergeScheduler())\n                                      .setMergePolicy(newLogMergePolicy(false, 10))\n                                      .setOpenMode(IndexWriterConfig.OpenMode.CREATE));\n\n      MergePolicy mp = w.getConfig().getMergePolicy();\n      if (mp instanceof LogByteSizeMergePolicy) {\n        // 1 petabyte:\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n      }\n\n      Document doc = new Document();\n      final MyTokenStream ts = new MyTokenStream(random, TERMS_PER_DOC);\n\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setIndexOptions(IndexOptions.DOCS_ONLY);\n      customType.setOmitNorms(true);\n      Field field = new Field(\"field\", ts, customType);\n      doc.add(field);\n      //w.setInfoStream(System.out);\n      final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n\n      System.out.println(\"TERMS_PER_DOC=\" + TERMS_PER_DOC);\n      System.out.println(\"numDocs=\" + numDocs);\n\n      for(int i=0;i<numDocs;i++) {\n        final long t0 = System.currentTimeMillis();\n        w.addDocument(doc);\n        System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n      }\n      savedTerms = ts.savedTerms;\n\n      System.out.println(\"TEST: full merge\");\n      w.forceMerge(1);\n      System.out.println(\"TEST: close writer\");\n      w.close();\n    }\n\n    System.out.println(\"TEST: open reader\");\n    final IndexReader r = IndexReader.open(dir);\n    if (savedTerms == null) {\n      savedTerms = findTerms(r);\n    }\n    final int numSavedTerms = savedTerms.size();\n    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));\n    System.out.println(\"TEST: test big ord terms...\");\n    testSavedTerms(r, bigOrdTerms);\n    System.out.println(\"TEST: test all saved terms...\");\n    testSavedTerms(r, savedTerms);\n    r.close();\n\n    System.out.println(\"TEST: now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n    System.out.println(\"TEST: done!\");\n  }\n\n","sourceOld":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"Lucene3x\".equals(Codec.getDefault().getName())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    final long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    final int TERMS_PER_DOC = _TestUtil.nextInt(random, 100000, 1000000);\n\n    List<BytesRef> savedTerms = null;\n\n    MockDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    //MockDirectoryWrapper dir = newFSDirectory(new File(\"/p/lucene/indices/2bindex\"));\n    dir.setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    dir.setCheckIndexOnClose(false); // don't double-checkindex\n\n    if (true) {\n\n      IndexWriter w = new IndexWriter(dir,\n                                      new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n                                      .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                      .setRAMBufferSizeMB(256.0)\n                                      .setMergeScheduler(new ConcurrentMergeScheduler())\n                                      .setMergePolicy(newLogMergePolicy(false, 10))\n                                      .setOpenMode(IndexWriterConfig.OpenMode.CREATE));\n\n      MergePolicy mp = w.getConfig().getMergePolicy();\n      if (mp instanceof LogByteSizeMergePolicy) {\n        // 1 petabyte:\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n      }\n\n      Document doc = new Document();\n      final MyTokenStream ts = new MyTokenStream(random, TERMS_PER_DOC);\n\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setIndexOptions(IndexOptions.DOCS_ONLY);\n      customType.setOmitNorms(true);\n      Field field = new Field(\"field\", ts, customType);\n      doc.add(field);\n      //w.setInfoStream(System.out);\n      final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n\n      System.out.println(\"TERMS_PER_DOC=\" + TERMS_PER_DOC);\n      System.out.println(\"numDocs=\" + numDocs);\n\n      for(int i=0;i<numDocs;i++) {\n        final long t0 = System.currentTimeMillis();\n        w.addDocument(doc);\n        System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n      }\n      savedTerms = ts.savedTerms;\n\n      System.out.println(\"TEST: optimize\");\n      w.optimize();\n      System.out.println(\"TEST: close writer\");\n      w.close();\n    }\n\n    System.out.println(\"TEST: open reader\");\n    final IndexReader r = IndexReader.open(dir);\n    if (savedTerms == null) {\n      savedTerms = findTerms(r);\n    }\n    final int numSavedTerms = savedTerms.size();\n    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));\n    System.out.println(\"TEST: test big ord terms...\");\n    testSavedTerms(r, bigOrdTerms);\n    System.out.println(\"TEST: test all saved terms...\");\n    testSavedTerms(r, savedTerms);\n    r.close();\n\n    System.out.println(\"TEST: now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n    System.out.println(\"TEST: done!\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0b48e6a5f05226090ba50ec1fbf53eda6b1d3647","date":1322850938,"type":3,"author":"Grant Ingersoll","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","sourceNew":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"Lucene3x\".equals(Codec.getDefault().getName())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    final long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    final int TERMS_PER_DOC = _TestUtil.nextInt(random, 100000, 1000000);\n\n    List<BytesRef> savedTerms = null;\n\n    MockDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    //MockDirectoryWrapper dir = newFSDirectory(new File(\"/p/lucene/indices/2bindex\"));\n    dir.setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    dir.setCheckIndexOnClose(false); // don't double-checkindex\n\n    if (true) {\n\n      IndexWriter w = new IndexWriter(dir,\n                                      new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n                                      .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                      .setRAMBufferSizeMB(256.0)\n                                      .setMergeScheduler(new ConcurrentMergeScheduler())\n                                      .setMergePolicy(newLogMergePolicy(false, 10))\n                                      .setOpenMode(IndexWriterConfig.OpenMode.CREATE));\n\n      MergePolicy mp = w.getConfig().getMergePolicy();\n      if (mp instanceof LogByteSizeMergePolicy) {\n        // 1 petabyte:\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n      }\n\n      Document doc = new Document();\n      final MyTokenStream ts = new MyTokenStream(random, TERMS_PER_DOC);\n\n      FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n      customType.setIndexOptions(IndexOptions.DOCS_ONLY);\n      customType.setOmitNorms(true);\n      Field field = new Field(\"field\", ts, customType);\n      doc.add(field);\n      //w.setInfoStream(System.out);\n      final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n\n      System.out.println(\"TERMS_PER_DOC=\" + TERMS_PER_DOC);\n      System.out.println(\"numDocs=\" + numDocs);\n\n      for(int i=0;i<numDocs;i++) {\n        final long t0 = System.currentTimeMillis();\n        w.addDocument(doc);\n        System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n      }\n      savedTerms = ts.savedTerms;\n\n      System.out.println(\"TEST: full merge\");\n      w.forceMerge(1);\n      System.out.println(\"TEST: close writer\");\n      w.close();\n    }\n\n    System.out.println(\"TEST: open reader\");\n    final IndexReader r = IndexReader.open(dir);\n    if (savedTerms == null) {\n      savedTerms = findTerms(r);\n    }\n    final int numSavedTerms = savedTerms.size();\n    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));\n    System.out.println(\"TEST: test big ord terms...\");\n    testSavedTerms(r, bigOrdTerms);\n    System.out.println(\"TEST: test all saved terms...\");\n    testSavedTerms(r, savedTerms);\n    r.close();\n\n    System.out.println(\"TEST: now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n    System.out.println(\"TEST: done!\");\n  }\n\n","sourceOld":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"Lucene3x\".equals(Codec.getDefault().getName())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    final long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    final int TERMS_PER_DOC = _TestUtil.nextInt(random, 100000, 1000000);\n\n    List<BytesRef> savedTerms = null;\n\n    MockDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    //MockDirectoryWrapper dir = newFSDirectory(new File(\"/p/lucene/indices/2bindex\"));\n    dir.setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    dir.setCheckIndexOnClose(false); // don't double-checkindex\n\n    if (true) {\n\n      IndexWriter w = new IndexWriter(dir,\n                                      new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n                                      .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                      .setRAMBufferSizeMB(256.0)\n                                      .setMergeScheduler(new ConcurrentMergeScheduler())\n                                      .setMergePolicy(newLogMergePolicy(false, 10))\n                                      .setOpenMode(IndexWriterConfig.OpenMode.CREATE));\n\n      MergePolicy mp = w.getConfig().getMergePolicy();\n      if (mp instanceof LogByteSizeMergePolicy) {\n        // 1 petabyte:\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n      }\n\n      Document doc = new Document();\n      final MyTokenStream ts = new MyTokenStream(random, TERMS_PER_DOC);\n\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setIndexOptions(IndexOptions.DOCS_ONLY);\n      customType.setOmitNorms(true);\n      Field field = new Field(\"field\", ts, customType);\n      doc.add(field);\n      //w.setInfoStream(System.out);\n      final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n\n      System.out.println(\"TERMS_PER_DOC=\" + TERMS_PER_DOC);\n      System.out.println(\"numDocs=\" + numDocs);\n\n      for(int i=0;i<numDocs;i++) {\n        final long t0 = System.currentTimeMillis();\n        w.addDocument(doc);\n        System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n      }\n      savedTerms = ts.savedTerms;\n\n      System.out.println(\"TEST: full merge\");\n      w.forceMerge(1);\n      System.out.println(\"TEST: close writer\");\n      w.close();\n    }\n\n    System.out.println(\"TEST: open reader\");\n    final IndexReader r = IndexReader.open(dir);\n    if (savedTerms == null) {\n      savedTerms = findTerms(r);\n    }\n    final int numSavedTerms = savedTerms.size();\n    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));\n    System.out.println(\"TEST: test big ord terms...\");\n    testSavedTerms(r, bigOrdTerms);\n    System.out.println(\"TEST: test all saved terms...\");\n    testSavedTerms(r, savedTerms);\n    r.close();\n\n    System.out.println(\"TEST: now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n    System.out.println(\"TEST: done!\");\n  }\n\n","bugFix":null,"bugIntro":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"be27627d0f58706cf69d633fea573aeb078dcb22","date":1322958023,"type":3,"author":"Grant Ingersoll","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","sourceNew":"  @Slow\n  public void test2BTerms() throws IOException {\n\n    if (\"Lucene3x\".equals(Codec.getDefault().getName())) {\n      throw new RuntimeException(\"this test cannot run with PreFlex codec\");\n    }\n    System.out.println(\"Starting Test2B\");\n    final long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    final int TERMS_PER_DOC = _TestUtil.nextInt(random, 100000, 1000000);\n\n    List<BytesRef> savedTerms = null;\n\n    MockDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    //MockDirectoryWrapper dir = newFSDirectory(new File(\"/p/lucene/indices/2bindex\"));\n    dir.setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    dir.setCheckIndexOnClose(false); // don't double-checkindex\n\n    if (true) {\n\n      IndexWriter w = new IndexWriter(dir,\n                                      new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n                                      .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                      .setRAMBufferSizeMB(256.0)\n                                      .setMergeScheduler(new ConcurrentMergeScheduler())\n                                      .setMergePolicy(newLogMergePolicy(false, 10))\n                                      .setOpenMode(IndexWriterConfig.OpenMode.CREATE));\n\n      MergePolicy mp = w.getConfig().getMergePolicy();\n      if (mp instanceof LogByteSizeMergePolicy) {\n        // 1 petabyte:\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n      }\n\n      Document doc = new Document();\n      final MyTokenStream ts = new MyTokenStream(random, TERMS_PER_DOC);\n\n      FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n      customType.setIndexOptions(IndexOptions.DOCS_ONLY);\n      customType.setOmitNorms(true);\n      Field field = new Field(\"field\", ts, customType);\n      doc.add(field);\n      //w.setInfoStream(System.out);\n      final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n\n      System.out.println(\"TERMS_PER_DOC=\" + TERMS_PER_DOC);\n      System.out.println(\"numDocs=\" + numDocs);\n\n      for(int i=0;i<numDocs;i++) {\n        final long t0 = System.currentTimeMillis();\n        w.addDocument(doc);\n        System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n      }\n      savedTerms = ts.savedTerms;\n\n      System.out.println(\"TEST: full merge\");\n      w.forceMerge(1);\n      System.out.println(\"TEST: close writer\");\n      w.close();\n    }\n\n    System.out.println(\"TEST: open reader\");\n    final IndexReader r = IndexReader.open(dir);\n    if (savedTerms == null) {\n      savedTerms = findTerms(r);\n    }\n    final int numSavedTerms = savedTerms.size();\n    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));\n    System.out.println(\"TEST: test big ord terms...\");\n    testSavedTerms(r, bigOrdTerms);\n    System.out.println(\"TEST: test all saved terms...\");\n    testSavedTerms(r, savedTerms);\n    r.close();\n\n    System.out.println(\"TEST: now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n    System.out.println(\"TEST: done!\");\n  }\n\n","sourceOld":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"Lucene3x\".equals(Codec.getDefault().getName())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    final long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    final int TERMS_PER_DOC = _TestUtil.nextInt(random, 100000, 1000000);\n\n    List<BytesRef> savedTerms = null;\n\n    MockDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    //MockDirectoryWrapper dir = newFSDirectory(new File(\"/p/lucene/indices/2bindex\"));\n    dir.setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    dir.setCheckIndexOnClose(false); // don't double-checkindex\n\n    if (true) {\n\n      IndexWriter w = new IndexWriter(dir,\n                                      new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n                                      .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                      .setRAMBufferSizeMB(256.0)\n                                      .setMergeScheduler(new ConcurrentMergeScheduler())\n                                      .setMergePolicy(newLogMergePolicy(false, 10))\n                                      .setOpenMode(IndexWriterConfig.OpenMode.CREATE));\n\n      MergePolicy mp = w.getConfig().getMergePolicy();\n      if (mp instanceof LogByteSizeMergePolicy) {\n        // 1 petabyte:\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n      }\n\n      Document doc = new Document();\n      final MyTokenStream ts = new MyTokenStream(random, TERMS_PER_DOC);\n\n      FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n      customType.setIndexOptions(IndexOptions.DOCS_ONLY);\n      customType.setOmitNorms(true);\n      Field field = new Field(\"field\", ts, customType);\n      doc.add(field);\n      //w.setInfoStream(System.out);\n      final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n\n      System.out.println(\"TERMS_PER_DOC=\" + TERMS_PER_DOC);\n      System.out.println(\"numDocs=\" + numDocs);\n\n      for(int i=0;i<numDocs;i++) {\n        final long t0 = System.currentTimeMillis();\n        w.addDocument(doc);\n        System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n      }\n      savedTerms = ts.savedTerms;\n\n      System.out.println(\"TEST: full merge\");\n      w.forceMerge(1);\n      System.out.println(\"TEST: close writer\");\n      w.close();\n    }\n\n    System.out.println(\"TEST: open reader\");\n    final IndexReader r = IndexReader.open(dir);\n    if (savedTerms == null) {\n      savedTerms = findTerms(r);\n    }\n    final int numSavedTerms = savedTerms.size();\n    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));\n    System.out.println(\"TEST: test big ord terms...\");\n    testSavedTerms(r, bigOrdTerms);\n    System.out.println(\"TEST: test all saved terms...\");\n    testSavedTerms(r, savedTerms);\n    r.close();\n\n    System.out.println(\"TEST: now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n    System.out.println(\"TEST: done!\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7733816f2c0d688c0bbc68468196f1a848606f21","date":1323021691,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","sourceNew":"  @Slow\n  public void test2BTerms() throws IOException {\n\n    if (\"Lucene3x\".equals(Codec.getDefault().getName())) {\n      throw new RuntimeException(\"this test cannot run with PreFlex codec\");\n    }\n    System.out.println(\"Starting Test2B\");\n    final long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    final int TERMS_PER_DOC = _TestUtil.nextInt(random, 100000, 1000000);\n\n    List<BytesRef> savedTerms = null;\n\n    MockDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    //MockDirectoryWrapper dir = newFSDirectory(new File(\"/p/lucene/indices/2bindex\"));\n    dir.setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    dir.setCheckIndexOnClose(false); // don't double-checkindex\n\n    if (true) {\n\n      IndexWriter w = new IndexWriter(dir,\n                                      new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n                                      .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                      .setRAMBufferSizeMB(256.0)\n                                      .setMergeScheduler(new ConcurrentMergeScheduler())\n                                      .setMergePolicy(newLogMergePolicy(false, 10))\n                                      .setOpenMode(IndexWriterConfig.OpenMode.CREATE));\n\n      MergePolicy mp = w.getConfig().getMergePolicy();\n      if (mp instanceof LogByteSizeMergePolicy) {\n        // 1 petabyte:\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n      }\n\n      Document doc = new Document();\n      final MyTokenStream ts = new MyTokenStream(random, TERMS_PER_DOC);\n\n      FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n      customType.setIndexOptions(IndexOptions.DOCS_ONLY);\n      customType.setOmitNorms(true);\n      Field field = new Field(\"field\", ts, customType);\n      doc.add(field);\n      //w.setInfoStream(System.out);\n      final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n\n      System.out.println(\"TERMS_PER_DOC=\" + TERMS_PER_DOC);\n      System.out.println(\"numDocs=\" + numDocs);\n\n      for(int i=0;i<numDocs;i++) {\n        final long t0 = System.currentTimeMillis();\n        w.addDocument(doc);\n        System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n      }\n      savedTerms = ts.savedTerms;\n\n      System.out.println(\"TEST: full merge\");\n      w.forceMerge(1);\n      System.out.println(\"TEST: close writer\");\n      w.close();\n    }\n\n    System.out.println(\"TEST: open reader\");\n    final IndexReader r = IndexReader.open(dir);\n    if (savedTerms == null) {\n      savedTerms = findTerms(r);\n    }\n    final int numSavedTerms = savedTerms.size();\n    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));\n    System.out.println(\"TEST: test big ord terms...\");\n    testSavedTerms(r, bigOrdTerms);\n    System.out.println(\"TEST: test all saved terms...\");\n    testSavedTerms(r, savedTerms);\n    r.close();\n\n    System.out.println(\"TEST: now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n    System.out.println(\"TEST: done!\");\n  }\n\n","sourceOld":"  @Ignore(\"Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\")\n  public void test2BTerms() throws IOException {\n\n    if (\"Lucene3x\".equals(Codec.getDefault().getName())) {\n      throw new RuntimeException(\"thist test cannot run with PreFlex codec\");\n    }\n\n    final long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    final int TERMS_PER_DOC = _TestUtil.nextInt(random, 100000, 1000000);\n\n    List<BytesRef> savedTerms = null;\n\n    MockDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    //MockDirectoryWrapper dir = newFSDirectory(new File(\"/p/lucene/indices/2bindex\"));\n    dir.setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    dir.setCheckIndexOnClose(false); // don't double-checkindex\n\n    if (true) {\n\n      IndexWriter w = new IndexWriter(dir,\n                                      new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n                                      .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                      .setRAMBufferSizeMB(256.0)\n                                      .setMergeScheduler(new ConcurrentMergeScheduler())\n                                      .setMergePolicy(newLogMergePolicy(false, 10))\n                                      .setOpenMode(IndexWriterConfig.OpenMode.CREATE));\n\n      MergePolicy mp = w.getConfig().getMergePolicy();\n      if (mp instanceof LogByteSizeMergePolicy) {\n        // 1 petabyte:\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n      }\n\n      Document doc = new Document();\n      final MyTokenStream ts = new MyTokenStream(random, TERMS_PER_DOC);\n\n      FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n      customType.setIndexOptions(IndexOptions.DOCS_ONLY);\n      customType.setOmitNorms(true);\n      Field field = new Field(\"field\", ts, customType);\n      doc.add(field);\n      //w.setInfoStream(System.out);\n      final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n\n      System.out.println(\"TERMS_PER_DOC=\" + TERMS_PER_DOC);\n      System.out.println(\"numDocs=\" + numDocs);\n\n      for(int i=0;i<numDocs;i++) {\n        final long t0 = System.currentTimeMillis();\n        w.addDocument(doc);\n        System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n      }\n      savedTerms = ts.savedTerms;\n\n      System.out.println(\"TEST: full merge\");\n      w.forceMerge(1);\n      System.out.println(\"TEST: close writer\");\n      w.close();\n    }\n\n    System.out.println(\"TEST: open reader\");\n    final IndexReader r = IndexReader.open(dir);\n    if (savedTerms == null) {\n      savedTerms = findTerms(r);\n    }\n    final int numSavedTerms = savedTerms.size();\n    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));\n    System.out.println(\"TEST: test big ord terms...\");\n    testSavedTerms(r, bigOrdTerms);\n    System.out.println(\"TEST: test all saved terms...\");\n    testSavedTerms(r, savedTerms);\n    r.close();\n\n    System.out.println(\"TEST: now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n    System.out.println(\"TEST: done!\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/Test2BTerms#test2BTerms().mjava","sourceNew":"  @Slow\n  public void test2BTerms() throws IOException {\n\n    if (\"Lucene3x\".equals(Codec.getDefault().getName())) {\n      throw new RuntimeException(\"this test cannot run with PreFlex codec\");\n    }\n    System.out.println(\"Starting Test2B\");\n    final long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    final int TERMS_PER_DOC = _TestUtil.nextInt(random, 100000, 1000000);\n\n    List<BytesRef> savedTerms = null;\n\n    MockDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    //MockDirectoryWrapper dir = newFSDirectory(new File(\"/p/lucene/indices/2bindex\"));\n    dir.setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    dir.setCheckIndexOnClose(false); // don't double-checkindex\n\n    if (true) {\n\n      IndexWriter w = new IndexWriter(dir,\n                                      new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n                                      .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                      .setRAMBufferSizeMB(256.0)\n                                      .setMergeScheduler(new ConcurrentMergeScheduler())\n                                      .setMergePolicy(newLogMergePolicy(false, 10))\n                                      .setOpenMode(IndexWriterConfig.OpenMode.CREATE));\n\n      MergePolicy mp = w.getConfig().getMergePolicy();\n      if (mp instanceof LogByteSizeMergePolicy) {\n        // 1 petabyte:\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n      }\n\n      Document doc = new Document();\n      final MyTokenStream ts = new MyTokenStream(random, TERMS_PER_DOC);\n\n      FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n      customType.setIndexOptions(IndexOptions.DOCS_ONLY);\n      customType.setOmitNorms(true);\n      Field field = new Field(\"field\", ts, customType);\n      doc.add(field);\n      //w.setInfoStream(System.out);\n      final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n\n      System.out.println(\"TERMS_PER_DOC=\" + TERMS_PER_DOC);\n      System.out.println(\"numDocs=\" + numDocs);\n\n      for(int i=0;i<numDocs;i++) {\n        final long t0 = System.currentTimeMillis();\n        w.addDocument(doc);\n        System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n      }\n      savedTerms = ts.savedTerms;\n\n      System.out.println(\"TEST: full merge\");\n      w.forceMerge(1);\n      System.out.println(\"TEST: close writer\");\n      w.close();\n    }\n\n    System.out.println(\"TEST: open reader\");\n    final IndexReader r = IndexReader.open(dir);\n    if (savedTerms == null) {\n      savedTerms = findTerms(r);\n    }\n    final int numSavedTerms = savedTerms.size();\n    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));\n    System.out.println(\"TEST: test big ord terms...\");\n    testSavedTerms(r, bigOrdTerms);\n    System.out.println(\"TEST: test all saved terms...\");\n    testSavedTerms(r, savedTerms);\n    r.close();\n\n    System.out.println(\"TEST: now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n    System.out.println(\"TEST: done!\");\n  }\n\n","sourceOld":"  @Slow\n  public void test2BTerms() throws IOException {\n\n    if (\"Lucene3x\".equals(Codec.getDefault().getName())) {\n      throw new RuntimeException(\"this test cannot run with PreFlex codec\");\n    }\n    System.out.println(\"Starting Test2B\");\n    final long TERM_COUNT = ((long) Integer.MAX_VALUE) + 100000000;\n\n    final int TERMS_PER_DOC = _TestUtil.nextInt(random, 100000, 1000000);\n\n    List<BytesRef> savedTerms = null;\n\n    MockDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir(\"2BTerms\"));\n    //MockDirectoryWrapper dir = newFSDirectory(new File(\"/p/lucene/indices/2bindex\"));\n    dir.setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    dir.setCheckIndexOnClose(false); // don't double-checkindex\n\n    if (true) {\n\n      IndexWriter w = new IndexWriter(dir,\n                                      new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n                                      .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                      .setRAMBufferSizeMB(256.0)\n                                      .setMergeScheduler(new ConcurrentMergeScheduler())\n                                      .setMergePolicy(newLogMergePolicy(false, 10))\n                                      .setOpenMode(IndexWriterConfig.OpenMode.CREATE));\n\n      MergePolicy mp = w.getConfig().getMergePolicy();\n      if (mp instanceof LogByteSizeMergePolicy) {\n        // 1 petabyte:\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);\n      }\n\n      Document doc = new Document();\n      final MyTokenStream ts = new MyTokenStream(random, TERMS_PER_DOC);\n\n      FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n      customType.setIndexOptions(IndexOptions.DOCS_ONLY);\n      customType.setOmitNorms(true);\n      Field field = new Field(\"field\", ts, customType);\n      doc.add(field);\n      //w.setInfoStream(System.out);\n      final int numDocs = (int) (TERM_COUNT/TERMS_PER_DOC);\n\n      System.out.println(\"TERMS_PER_DOC=\" + TERMS_PER_DOC);\n      System.out.println(\"numDocs=\" + numDocs);\n\n      for(int i=0;i<numDocs;i++) {\n        final long t0 = System.currentTimeMillis();\n        w.addDocument(doc);\n        System.out.println(i + \" of \" + numDocs + \" \" + (System.currentTimeMillis()-t0) + \" msec\");\n      }\n      savedTerms = ts.savedTerms;\n\n      System.out.println(\"TEST: full merge\");\n      w.forceMerge(1);\n      System.out.println(\"TEST: close writer\");\n      w.close();\n    }\n\n    System.out.println(\"TEST: open reader\");\n    final IndexReader r = IndexReader.open(dir);\n    if (savedTerms == null) {\n      savedTerms = findTerms(r);\n    }\n    final int numSavedTerms = savedTerms.size();\n    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));\n    System.out.println(\"TEST: test big ord terms...\");\n    testSavedTerms(r, bigOrdTerms);\n    System.out.println(\"TEST: test all saved terms...\");\n    testSavedTerms(r, savedTerms);\n    r.close();\n\n    System.out.println(\"TEST: now CheckIndex...\");\n    CheckIndex.Status status = _TestUtil.checkIndex(dir);\n    final long tc = status.segmentInfos.get(0).termIndexStatus.termCount;\n    assertTrue(\"count \" + tc + \" is not > \" + Integer.MAX_VALUE, tc > Integer.MAX_VALUE);\n\n    dir.close();\n    System.out.println(\"TEST: done!\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"2afd23a6f1242190c3409d8d81d5c5912d607fc9":["ff6fd241dc6610f7f81b62e3ba4cedf105939623"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["be27627d0f58706cf69d633fea573aeb078dcb22"],"d14e8d18c0e3970c20354dbeeb49da11bd587fbd":["06584e6e98d592b34e1329b384182f368d2025e8"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","a13a126d15299d5c1e117ea99ddae6fb0fa3f209"],"cf46d950a2020ee31dbae60879a0b0a519224524":["2506f4a214732b20f56ab2fa8b975dc5d85c4d45"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["1b589b416d5461faed99d25118babbda4b89acb4"],"79c2cb24929f2649a8875fb629086171f914d5ce":["a3776dccca01c11e7046323cfad46a3b4a471233","ff6fd241dc6610f7f81b62e3ba4cedf105939623"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":["135621f3a0670a9394eb563224a3b76cc4dddc0f","cf46d950a2020ee31dbae60879a0b0a519224524"],"eeebcf026b55d8ce3ac8165210782b26cc4efe30":["bce89597a7c3a4535b5b7f8100c2078e520f6e57"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["60ba444201d2570214b6fcf1d15600dc1a01f548"],"1b589b416d5461faed99d25118babbda4b89acb4":["a13a126d15299d5c1e117ea99ddae6fb0fa3f209"],"0b48e6a5f05226090ba50ec1fbf53eda6b1d3647":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd"],"a6c7b564e6275fb0c0e137d84fda55b447c19d9c":["ba8cd9ac7a0f3ad09c6c94f53c4c415d13e72723"],"be27627d0f58706cf69d633fea573aeb078dcb22":["0b48e6a5f05226090ba50ec1fbf53eda6b1d3647"],"7e4db59c6b6c10e25322cfb41c4c19d78b4298bd":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"a13a126d15299d5c1e117ea99ddae6fb0fa3f209":["4e8cc373c801e54cec75daf9f52792cb4b17f536"],"70ad682703b8585f5d0a637efec044d57ec05efb":["ab5cb6a74aefb78aa0569857970b9151dfe2e787","1b589b416d5461faed99d25118babbda4b89acb4"],"ff6fd241dc6610f7f81b62e3ba4cedf105939623":["cf46d950a2020ee31dbae60879a0b0a519224524"],"06584e6e98d592b34e1329b384182f368d2025e8":["7b91922b55d15444d554721b352861d028eb8278"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["70ad682703b8585f5d0a637efec044d57ec05efb","2506f4a214732b20f56ab2fa8b975dc5d85c4d45"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["3bb13258feba31ab676502787ab2e1779f129b7a","a13a126d15299d5c1e117ea99ddae6fb0fa3f209"],"962d04139994fce5193143ef35615499a9a96d78":["868da859b43505d9d2a023bfeae6dd0c795f5295","985161df9b1c14b4aa862d74d48cf51561579e62"],"7733816f2c0d688c0bbc68468196f1a848606f21":["0b48e6a5f05226090ba50ec1fbf53eda6b1d3647","be27627d0f58706cf69d633fea573aeb078dcb22"],"60ba444201d2570214b6fcf1d15600dc1a01f548":["2afd23a6f1242190c3409d8d81d5c5912d607fc9"],"7b91922b55d15444d554721b352861d028eb8278":["7e4db59c6b6c10e25322cfb41c4c19d78b4298bd"],"1eb46686a27187e42311e77666a2c7026f461ebc":["962d04139994fce5193143ef35615499a9a96d78","2506f4a214732b20f56ab2fa8b975dc5d85c4d45"],"bce89597a7c3a4535b5b7f8100c2078e520f6e57":["13452165d8bf3d45a72f572aaed3c679735d3af2"],"13452165d8bf3d45a72f572aaed3c679735d3af2":["a6c7b564e6275fb0c0e137d84fda55b447c19d9c"],"a3776dccca01c11e7046323cfad46a3b4a471233":["1b589b416d5461faed99d25118babbda4b89acb4","cf46d950a2020ee31dbae60879a0b0a519224524"],"77cf4379b2824f6ea34b091c495d6e95c38ff9e2":["c700f8d0842d3e52bb2bdfbfdc046a137e836edb","ff6fd241dc6610f7f81b62e3ba4cedf105939623"],"2506f4a214732b20f56ab2fa8b975dc5d85c4d45":["985161df9b1c14b4aa862d74d48cf51561579e62"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","1b589b416d5461faed99d25118babbda4b89acb4"],"3bb13258feba31ab676502787ab2e1779f129b7a":["a6c7b564e6275fb0c0e137d84fda55b447c19d9c","4e8cc373c801e54cec75daf9f52792cb4b17f536"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"985161df9b1c14b4aa862d74d48cf51561579e62":["f2c5f0cb44df114db4228c8f77861714b5cabaea"],"4e8cc373c801e54cec75daf9f52792cb4b17f536":["eeebcf026b55d8ce3ac8165210782b26cc4efe30"],"ba8cd9ac7a0f3ad09c6c94f53c4c415d13e72723":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"2afd23a6f1242190c3409d8d81d5c5912d607fc9":["60ba444201d2570214b6fcf1d15600dc1a01f548"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d14e8d18c0e3970c20354dbeeb49da11bd587fbd":["0b48e6a5f05226090ba50ec1fbf53eda6b1d3647"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["868da859b43505d9d2a023bfeae6dd0c795f5295"],"cf46d950a2020ee31dbae60879a0b0a519224524":["c700f8d0842d3e52bb2bdfbfdc046a137e836edb","ff6fd241dc6610f7f81b62e3ba4cedf105939623","a3776dccca01c11e7046323cfad46a3b4a471233"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["985161df9b1c14b4aa862d74d48cf51561579e62"],"79c2cb24929f2649a8875fb629086171f914d5ce":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","ba8cd9ac7a0f3ad09c6c94f53c4c415d13e72723"],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":["77cf4379b2824f6ea34b091c495d6e95c38ff9e2"],"eeebcf026b55d8ce3ac8165210782b26cc4efe30":["4e8cc373c801e54cec75daf9f52792cb4b17f536"],"1b589b416d5461faed99d25118babbda4b89acb4":["f2c5f0cb44df114db4228c8f77861714b5cabaea","70ad682703b8585f5d0a637efec044d57ec05efb","a3776dccca01c11e7046323cfad46a3b4a471233","868da859b43505d9d2a023bfeae6dd0c795f5295"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["7e4db59c6b6c10e25322cfb41c4c19d78b4298bd"],"0b48e6a5f05226090ba50ec1fbf53eda6b1d3647":["be27627d0f58706cf69d633fea573aeb078dcb22","7733816f2c0d688c0bbc68468196f1a848606f21"],"a6c7b564e6275fb0c0e137d84fda55b447c19d9c":["13452165d8bf3d45a72f572aaed3c679735d3af2","3bb13258feba31ab676502787ab2e1779f129b7a"],"be27627d0f58706cf69d633fea573aeb078dcb22":["3a119bbc8703c10faa329ec201c654b3a35a1e3e","7733816f2c0d688c0bbc68468196f1a848606f21"],"7e4db59c6b6c10e25322cfb41c4c19d78b4298bd":["7b91922b55d15444d554721b352861d028eb8278"],"a13a126d15299d5c1e117ea99ddae6fb0fa3f209":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","1b589b416d5461faed99d25118babbda4b89acb4","ab5cb6a74aefb78aa0569857970b9151dfe2e787"],"70ad682703b8585f5d0a637efec044d57ec05efb":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"ff6fd241dc6610f7f81b62e3ba4cedf105939623":["2afd23a6f1242190c3409d8d81d5c5912d607fc9","79c2cb24929f2649a8875fb629086171f914d5ce","77cf4379b2824f6ea34b091c495d6e95c38ff9e2"],"06584e6e98d592b34e1329b384182f368d2025e8":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["c700f8d0842d3e52bb2bdfbfdc046a137e836edb"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["70ad682703b8585f5d0a637efec044d57ec05efb"],"962d04139994fce5193143ef35615499a9a96d78":["1eb46686a27187e42311e77666a2c7026f461ebc"],"7733816f2c0d688c0bbc68468196f1a848606f21":[],"60ba444201d2570214b6fcf1d15600dc1a01f548":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"7b91922b55d15444d554721b352861d028eb8278":["06584e6e98d592b34e1329b384182f368d2025e8"],"1eb46686a27187e42311e77666a2c7026f461ebc":[],"bce89597a7c3a4535b5b7f8100c2078e520f6e57":["eeebcf026b55d8ce3ac8165210782b26cc4efe30"],"a3776dccca01c11e7046323cfad46a3b4a471233":["79c2cb24929f2649a8875fb629086171f914d5ce"],"13452165d8bf3d45a72f572aaed3c679735d3af2":["bce89597a7c3a4535b5b7f8100c2078e520f6e57"],"77cf4379b2824f6ea34b091c495d6e95c38ff9e2":[],"2506f4a214732b20f56ab2fa8b975dc5d85c4d45":["cf46d950a2020ee31dbae60879a0b0a519224524","135621f3a0670a9394eb563224a3b76cc4dddc0f","1eb46686a27187e42311e77666a2c7026f461ebc"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["962d04139994fce5193143ef35615499a9a96d78"],"3bb13258feba31ab676502787ab2e1779f129b7a":["ab5cb6a74aefb78aa0569857970b9151dfe2e787"],"ba8cd9ac7a0f3ad09c6c94f53c4c415d13e72723":["a6c7b564e6275fb0c0e137d84fda55b447c19d9c"],"4e8cc373c801e54cec75daf9f52792cb4b17f536":["a13a126d15299d5c1e117ea99ddae6fb0fa3f209","3bb13258feba31ab676502787ab2e1779f129b7a"],"985161df9b1c14b4aa862d74d48cf51561579e62":["962d04139994fce5193143ef35615499a9a96d78","2506f4a214732b20f56ab2fa8b975dc5d85c4d45"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["79c2cb24929f2649a8875fb629086171f914d5ce","7733816f2c0d688c0bbc68468196f1a848606f21","1eb46686a27187e42311e77666a2c7026f461ebc","77cf4379b2824f6ea34b091c495d6e95c38ff9e2","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}