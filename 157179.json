{"path":"contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/WordSegmenter#segmentSentence(Token,int).mjava","commits":[{"id":"05ff0cc6e864c7d71a48579f2acfca4f58943568","date":1242295762,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/WordSegmenter#segmentSentence(Token,int).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * 调用HHMMSegment程序将当前的sentence Token分词，返回分词结果，保存在Token List中\n   * \n   * @param sentenceToken 句子的Token\n   * @param shortPathCount HHMM算法分词所需要的优化前的最短路径个数。一般越大分词结果越精确，但是计算代价也较高。\n   * @return 分词结果的Token List\n   */\n  public List segmentSentence(Token sentenceToken, int shortPathCount) {\n    String sentence = sentenceToken.term();\n\n    List segTokenList = hhmmSegmenter.process(sentence);\n\n    List result = new ArrayList();\n\n    // i从1到rawTokens.length-2，也就是说将“始##始”，“末##末”两个RawToken去掉\n    for (int i = 1; i < segTokenList.size() - 1; i++) {\n      result.add(convertSegToken((SegToken) segTokenList.get(i), sentence,\n          sentenceToken.startOffset(), \"word\"));\n    }\n    return result;\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"be5ef2f970a6c1141562b06dd26ed04c3dc29d70","date":1246444343,"type":5,"author":"Simon Willnauer","isMerge":false,"pathNew":"contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/WordSegmenter#segmentSentence(Token).mjava","pathOld":"contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/WordSegmenter#segmentSentence(Token,int).mjava","sourceNew":"  /**\n   * Segment a sentence into words with {@link HHMMSegmenter}\n   * \n   * @param sentenceToken sentence {@link Token}\n   * @return {@link List} of {@link SegToken}\n   */\n  public List segmentSentence(Token sentenceToken) {\n    String sentence = sentenceToken.term();\n\n    List segTokenList = hhmmSegmenter.process(sentence);\n\n    List result = new ArrayList();\n\n    // tokens from sentence, excluding WordType.SENTENCE_BEGIN and WordType.SENTENCE_END\n    for (int i = 1; i < segTokenList.size() - 1; i++) {\n      result.add(convertSegToken((SegToken) segTokenList.get(i), sentence,\n          sentenceToken.startOffset(), \"word\"));\n    }\n    return result;\n\n  }\n\n","sourceOld":"  /**\n   * 调用HHMMSegment程序将当前的sentence Token分词，返回分词结果，保存在Token List中\n   * \n   * @param sentenceToken 句子的Token\n   * @param shortPathCount HHMM算法分词所需要的优化前的最短路径个数。一般越大分词结果越精确，但是计算代价也较高。\n   * @return 分词结果的Token List\n   */\n  public List segmentSentence(Token sentenceToken, int shortPathCount) {\n    String sentence = sentenceToken.term();\n\n    List segTokenList = hhmmSegmenter.process(sentence);\n\n    List result = new ArrayList();\n\n    // i从1到rawTokens.length-2，也就是说将“始##始”，“末##末”两个RawToken去掉\n    for (int i = 1; i < segTokenList.size() - 1; i++) {\n      result.add(convertSegToken((SegToken) segTokenList.get(i), sentence,\n          sentenceToken.startOffset(), \"word\"));\n    }\n    return result;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"be5ef2f970a6c1141562b06dd26ed04c3dc29d70":["05ff0cc6e864c7d71a48579f2acfca4f58943568"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"05ff0cc6e864c7d71a48579f2acfca4f58943568":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["be5ef2f970a6c1141562b06dd26ed04c3dc29d70"]},"commit2Childs":{"be5ef2f970a6c1141562b06dd26ed04c3dc29d70":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["05ff0cc6e864c7d71a48579f2acfca4f58943568"],"05ff0cc6e864c7d71a48579f2acfca4f58943568":["be5ef2f970a6c1141562b06dd26ed04c3dc29d70"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}