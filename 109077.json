{"path":"solr/contrib/solr-mr/src/java/org/apache/solr/hadoop/MapReduceIndexerTool#randomizeManyInputFiles(Configuration,Path,Path,int).mjava","commits":[{"id":"d6e604e9030fb0cabf0c5a85ae6039921a81419c","date":1386009743,"type":0,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/solr-mr/src/java/org/apache/solr/hadoop/MapReduceIndexerTool#randomizeManyInputFiles(Configuration,Path,Path,int).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * To uniformly spread load across all mappers we randomize fullInputList\n   * with a separate small Mapper & Reducer preprocessing step. This way\n   * each input line ends up on a random position in the output file list.\n   * Each mapper indexes a disjoint consecutive set of files such that each\n   * set has roughly the same size, at least from a probabilistic\n   * perspective.\n   * \n   * For example an input file with the following input list of URLs:\n   * \n   * A\n   * B\n   * C\n   * D\n   * \n   * might be randomized into the following output list of URLs:\n   * \n   * C\n   * A\n   * D\n   * B\n   * \n   * The implementation sorts the list of lines by randomly generated numbers.\n   */\n  private Job randomizeManyInputFiles(Configuration baseConfig, Path fullInputList, Path outputStep2Dir, int numLinesPerSplit) \n      throws IOException {\n    \n    Job job2 = Job.getInstance(baseConfig);\n    job2.setJarByClass(getClass());\n    job2.setJobName(getClass().getName() + \"/\" + Utils.getShortClassName(LineRandomizerMapper.class));\n    job2.setInputFormatClass(NLineInputFormat.class);\n    NLineInputFormat.addInputPath(job2, fullInputList);\n    NLineInputFormat.setNumLinesPerSplit(job2, numLinesPerSplit);          \n    job2.setMapperClass(LineRandomizerMapper.class);\n    job2.setReducerClass(LineRandomizerReducer.class);\n    job2.setOutputFormatClass(TextOutputFormat.class);\n    FileOutputFormat.setOutputPath(job2, outputStep2Dir);\n    job2.setNumReduceTasks(1);\n    job2.setOutputKeyClass(LongWritable.class);\n    job2.setOutputValueClass(Text.class);\n    return job2;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"70f91c8322fbffe3a3a897ef20ea19119cac10cd","date":1386170124,"type":5,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/MapReduceIndexerTool#randomizeManyInputFiles(Configuration,Path,Path,int).mjava","pathOld":"solr/contrib/solr-mr/src/java/org/apache/solr/hadoop/MapReduceIndexerTool#randomizeManyInputFiles(Configuration,Path,Path,int).mjava","sourceNew":"  /**\n   * To uniformly spread load across all mappers we randomize fullInputList\n   * with a separate small Mapper & Reducer preprocessing step. This way\n   * each input line ends up on a random position in the output file list.\n   * Each mapper indexes a disjoint consecutive set of files such that each\n   * set has roughly the same size, at least from a probabilistic\n   * perspective.\n   * \n   * For example an input file with the following input list of URLs:\n   * \n   * A\n   * B\n   * C\n   * D\n   * \n   * might be randomized into the following output list of URLs:\n   * \n   * C\n   * A\n   * D\n   * B\n   * \n   * The implementation sorts the list of lines by randomly generated numbers.\n   */\n  private Job randomizeManyInputFiles(Configuration baseConfig, Path fullInputList, Path outputStep2Dir, int numLinesPerSplit) \n      throws IOException {\n    \n    Job job2 = Job.getInstance(baseConfig);\n    job2.setJarByClass(getClass());\n    job2.setJobName(getClass().getName() + \"/\" + Utils.getShortClassName(LineRandomizerMapper.class));\n    job2.setInputFormatClass(NLineInputFormat.class);\n    NLineInputFormat.addInputPath(job2, fullInputList);\n    NLineInputFormat.setNumLinesPerSplit(job2, numLinesPerSplit);          \n    job2.setMapperClass(LineRandomizerMapper.class);\n    job2.setReducerClass(LineRandomizerReducer.class);\n    job2.setOutputFormatClass(TextOutputFormat.class);\n    FileOutputFormat.setOutputPath(job2, outputStep2Dir);\n    job2.setNumReduceTasks(1);\n    job2.setOutputKeyClass(LongWritable.class);\n    job2.setOutputValueClass(Text.class);\n    return job2;\n  }\n\n","sourceOld":"  /**\n   * To uniformly spread load across all mappers we randomize fullInputList\n   * with a separate small Mapper & Reducer preprocessing step. This way\n   * each input line ends up on a random position in the output file list.\n   * Each mapper indexes a disjoint consecutive set of files such that each\n   * set has roughly the same size, at least from a probabilistic\n   * perspective.\n   * \n   * For example an input file with the following input list of URLs:\n   * \n   * A\n   * B\n   * C\n   * D\n   * \n   * might be randomized into the following output list of URLs:\n   * \n   * C\n   * A\n   * D\n   * B\n   * \n   * The implementation sorts the list of lines by randomly generated numbers.\n   */\n  private Job randomizeManyInputFiles(Configuration baseConfig, Path fullInputList, Path outputStep2Dir, int numLinesPerSplit) \n      throws IOException {\n    \n    Job job2 = Job.getInstance(baseConfig);\n    job2.setJarByClass(getClass());\n    job2.setJobName(getClass().getName() + \"/\" + Utils.getShortClassName(LineRandomizerMapper.class));\n    job2.setInputFormatClass(NLineInputFormat.class);\n    NLineInputFormat.addInputPath(job2, fullInputList);\n    NLineInputFormat.setNumLinesPerSplit(job2, numLinesPerSplit);          \n    job2.setMapperClass(LineRandomizerMapper.class);\n    job2.setReducerClass(LineRandomizerReducer.class);\n    job2.setOutputFormatClass(TextOutputFormat.class);\n    FileOutputFormat.setOutputPath(job2, outputStep2Dir);\n    job2.setNumReduceTasks(1);\n    job2.setOutputKeyClass(LongWritable.class);\n    job2.setOutputValueClass(Text.class);\n    return job2;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"70f91c8322fbffe3a3a897ef20ea19119cac10cd":["d6e604e9030fb0cabf0c5a85ae6039921a81419c"],"d6e604e9030fb0cabf0c5a85ae6039921a81419c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["70f91c8322fbffe3a3a897ef20ea19119cac10cd"]},"commit2Childs":{"70f91c8322fbffe3a3a897ef20ea19119cac10cd":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d6e604e9030fb0cabf0c5a85ae6039921a81419c":["70f91c8322fbffe3a3a897ef20ea19119cac10cd"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d6e604e9030fb0cabf0c5a85ae6039921a81419c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}