{"path":"lucene/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue#innerPurge(DocumentsWriter).mjava","commits":[{"id":"33e096accda90a8bd2279f890efe2e287f47c743","date":1326726424,"type":0,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue#innerPurge(DocumentsWriter).mjava","pathOld":"/dev/null","sourceNew":"  private void innerPurge(DocumentsWriter writer) throws IOException {\n    assert purgeLock.isHeldByCurrentThread();\n    while (true) {\n      final FlushTicket head;\n      final boolean canPublish;\n      synchronized (this) {\n        head = queue.peek();\n        canPublish = head != null && head.canPublish(); // do this synced \n      }\n      if (canPublish) {\n        try {\n          /*\n           * if we bock on publish -> lock IW -> lock BufferedDeletes we don't block\n           * concurrent segment flushes just because they want to append to the queue.\n           * the downside is that we need to force a purge on fullFlush since ther could\n           * be a ticket still in the queue. \n           */\n          head.publish(writer);\n        } finally {\n          synchronized (this) {\n            // finally remove the publised ticket from the queue\n            final FlushTicket poll = queue.poll();\n            ticketCount.decrementAndGet();\n            assert poll == head;\n          }\n        }\n      } else {\n        break;\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue#innerPurge(DocumentsWriter).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue#innerPurge(DocumentsWriter).mjava","sourceNew":"  private void innerPurge(DocumentsWriter writer) throws IOException {\n    assert purgeLock.isHeldByCurrentThread();\n    while (true) {\n      final FlushTicket head;\n      final boolean canPublish;\n      synchronized (this) {\n        head = queue.peek();\n        canPublish = head != null && head.canPublish(); // do this synced \n      }\n      if (canPublish) {\n        try {\n          /*\n           * if we bock on publish -> lock IW -> lock BufferedDeletes we don't block\n           * concurrent segment flushes just because they want to append to the queue.\n           * the downside is that we need to force a purge on fullFlush since ther could\n           * be a ticket still in the queue. \n           */\n          head.publish(writer);\n        } finally {\n          synchronized (this) {\n            // finally remove the publised ticket from the queue\n            final FlushTicket poll = queue.poll();\n            ticketCount.decrementAndGet();\n            assert poll == head;\n          }\n        }\n      } else {\n        break;\n      }\n    }\n  }\n\n","sourceOld":"  private void innerPurge(DocumentsWriter writer) throws IOException {\n    assert purgeLock.isHeldByCurrentThread();\n    while (true) {\n      final FlushTicket head;\n      final boolean canPublish;\n      synchronized (this) {\n        head = queue.peek();\n        canPublish = head != null && head.canPublish(); // do this synced \n      }\n      if (canPublish) {\n        try {\n          /*\n           * if we bock on publish -> lock IW -> lock BufferedDeletes we don't block\n           * concurrent segment flushes just because they want to append to the queue.\n           * the downside is that we need to force a purge on fullFlush since ther could\n           * be a ticket still in the queue. \n           */\n          head.publish(writer);\n        } finally {\n          synchronized (this) {\n            // finally remove the publised ticket from the queue\n            final FlushTicket poll = queue.poll();\n            ticketCount.decrementAndGet();\n            assert poll == head;\n          }\n        }\n      } else {\n        break;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["33e096accda90a8bd2279f890efe2e287f47c743"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"33e096accda90a8bd2279f890efe2e287f47c743":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["33e096accda90a8bd2279f890efe2e287f47c743"],"33e096accda90a8bd2279f890efe2e287f47c743":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}