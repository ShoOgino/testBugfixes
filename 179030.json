{"path":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#writeDocument().mjava","commits":[{"id":"4350b17bd363cd13a95171b8df1ca62ea4c3e71c","date":1183562198,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#writeDocument().mjava","pathOld":"/dev/null","sourceNew":"    /** Move all per-document state that was accumulated in\n     *  the ThreadState into the \"real\" stores. */\n    public void writeDocument() throws IOException {\n\n      // Append stored fields to the real FieldsWriter:\n      fieldsWriter.flushDocument(fdtLocal);\n      fdtLocal.reset();\n\n      // Append term vectors to the real outputs:\n      if (tvx != null) {\n        tvx.writeLong(tvd.getFilePointer());\n        if (numVectorFields > 0) {\n          tvd.writeVInt(numVectorFields);\n          for(int i=0;i<numVectorFields;i++)\n            tvd.writeVInt(vectorFieldNumbers[i]);\n          assert 0 == vectorFieldPointers[0];\n          tvd.writeVLong(tvf.getFilePointer());\n          long lastPos = vectorFieldPointers[0];\n          for(int i=1;i<numVectorFields;i++) {\n            long pos = vectorFieldPointers[i];\n            tvd.writeVLong(pos-lastPos);\n            lastPos = pos;\n          }\n          tvfLocal.writeTo(tvf);\n          tvfLocal.reset();\n        }\n      }\n\n      // Append norms for the fields we saw:\n      for(int i=0;i<numFieldData;i++) {\n        FieldData fp = fieldDataArray[i];\n        if (fp.doNorms) {\n          BufferedNorms bn = norms[fp.fieldInfo.number];\n          assert bn != null;\n          assert bn.upto <= docID;\n          bn.fill(docID);\n          float norm = fp.boost * writer.getSimilarity().lengthNorm(fp.fieldInfo.name, fp.length);\n          bn.add(norm);\n        }\n      }\n\n      if (postingsIsFull && !flushPending) {\n        flushPending = true;\n        doFlushAfter = true;\n      }\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["c9a0deca56efc5191d6b3c41047fd538f3fae1d8","83bbb041887bbef07b8a98d08a0e1713ce137039","c1213252cc474b006ac3675f030910be3895e10d"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c1213252cc474b006ac3675f030910be3895e10d","date":1191070365,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#writeDocument().mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#writeDocument().mjava","sourceNew":"    /** Move all per-document state that was accumulated in\n     *  the ThreadState into the \"real\" stores. */\n    public void writeDocument() throws IOException {\n\n      // Append stored fields to the real FieldsWriter:\n      fieldsWriter.flushDocument(fdtLocal);\n      fdtLocal.reset();\n\n      // Append term vectors to the real outputs:\n      if (tvx != null) {\n        tvx.writeLong(tvd.getFilePointer());\n        tvd.writeVInt(numVectorFields);\n        if (numVectorFields > 0) {\n          for(int i=0;i<numVectorFields;i++)\n            tvd.writeVInt(vectorFieldNumbers[i]);\n          assert 0 == vectorFieldPointers[0];\n          tvd.writeVLong(tvf.getFilePointer());\n          long lastPos = vectorFieldPointers[0];\n          for(int i=1;i<numVectorFields;i++) {\n            long pos = vectorFieldPointers[i];\n            tvd.writeVLong(pos-lastPos);\n            lastPos = pos;\n          }\n          tvfLocal.writeTo(tvf);\n          tvfLocal.reset();\n        }\n      }\n\n      // Append norms for the fields we saw:\n      for(int i=0;i<numFieldData;i++) {\n        FieldData fp = fieldDataArray[i];\n        if (fp.doNorms) {\n          BufferedNorms bn = norms[fp.fieldInfo.number];\n          assert bn != null;\n          assert bn.upto <= docID;\n          bn.fill(docID);\n          float norm = fp.boost * writer.getSimilarity().lengthNorm(fp.fieldInfo.name, fp.length);\n          bn.add(norm);\n        }\n      }\n\n      if (postingsIsFull && !flushPending) {\n        flushPending = true;\n        doFlushAfter = true;\n      }\n    }\n\n","sourceOld":"    /** Move all per-document state that was accumulated in\n     *  the ThreadState into the \"real\" stores. */\n    public void writeDocument() throws IOException {\n\n      // Append stored fields to the real FieldsWriter:\n      fieldsWriter.flushDocument(fdtLocal);\n      fdtLocal.reset();\n\n      // Append term vectors to the real outputs:\n      if (tvx != null) {\n        tvx.writeLong(tvd.getFilePointer());\n        if (numVectorFields > 0) {\n          tvd.writeVInt(numVectorFields);\n          for(int i=0;i<numVectorFields;i++)\n            tvd.writeVInt(vectorFieldNumbers[i]);\n          assert 0 == vectorFieldPointers[0];\n          tvd.writeVLong(tvf.getFilePointer());\n          long lastPos = vectorFieldPointers[0];\n          for(int i=1;i<numVectorFields;i++) {\n            long pos = vectorFieldPointers[i];\n            tvd.writeVLong(pos-lastPos);\n            lastPos = pos;\n          }\n          tvfLocal.writeTo(tvf);\n          tvfLocal.reset();\n        }\n      }\n\n      // Append norms for the fields we saw:\n      for(int i=0;i<numFieldData;i++) {\n        FieldData fp = fieldDataArray[i];\n        if (fp.doNorms) {\n          BufferedNorms bn = norms[fp.fieldInfo.number];\n          assert bn != null;\n          assert bn.upto <= docID;\n          bn.fill(docID);\n          float norm = fp.boost * writer.getSimilarity().lengthNorm(fp.fieldInfo.name, fp.length);\n          bn.add(norm);\n        }\n      }\n\n      if (postingsIsFull && !flushPending) {\n        flushPending = true;\n        doFlushAfter = true;\n      }\n    }\n\n","bugFix":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"bugIntro":["83bbb041887bbef07b8a98d08a0e1713ce137039"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2558ddf9e14a97bc597f5b72bb3ecb5b7f6bba8e","date":1191352543,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#writeDocument().mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#writeDocument().mjava","sourceNew":"    /** Move all per-document state that was accumulated in\n     *  the ThreadState into the \"real\" stores. */\n    public void writeDocument() throws IOException {\n\n      // Append stored fields to the real FieldsWriter:\n      fieldsWriter.flushDocument(fdtLocal);\n      fdtLocal.reset();\n\n      // Append term vectors to the real outputs:\n      if (tvx != null) {\n        tvx.writeLong(tvd.getFilePointer());\n        tvd.writeVInt(numVectorFields);\n        if (numVectorFields > 0) {\n          for(int i=0;i<numVectorFields;i++)\n            tvd.writeVInt(vectorFieldNumbers[i]);\n          assert 0 == vectorFieldPointers[0];\n          tvd.writeVLong(tvf.getFilePointer());\n          long lastPos = vectorFieldPointers[0];\n          for(int i=1;i<numVectorFields;i++) {\n            long pos = vectorFieldPointers[i];\n            tvd.writeVLong(pos-lastPos);\n            lastPos = pos;\n          }\n          tvfLocal.writeTo(tvf);\n          tvfLocal.reset();\n        }\n      }\n\n      // Append norms for the fields we saw:\n      for(int i=0;i<numFieldData;i++) {\n        FieldData fp = fieldDataArray[i];\n        if (fp.doNorms) {\n          BufferedNorms bn = norms[fp.fieldInfo.number];\n          assert bn != null;\n          assert bn.upto <= docID;\n          bn.fill(docID);\n          float norm = fp.boost * writer.getSimilarity().lengthNorm(fp.fieldInfo.name, fp.length);\n          bn.add(norm);\n        }\n      }\n\n      if (bufferIsFull && !flushPending) {\n        flushPending = true;\n        doFlushAfter = true;\n      }\n    }\n\n","sourceOld":"    /** Move all per-document state that was accumulated in\n     *  the ThreadState into the \"real\" stores. */\n    public void writeDocument() throws IOException {\n\n      // Append stored fields to the real FieldsWriter:\n      fieldsWriter.flushDocument(fdtLocal);\n      fdtLocal.reset();\n\n      // Append term vectors to the real outputs:\n      if (tvx != null) {\n        tvx.writeLong(tvd.getFilePointer());\n        tvd.writeVInt(numVectorFields);\n        if (numVectorFields > 0) {\n          for(int i=0;i<numVectorFields;i++)\n            tvd.writeVInt(vectorFieldNumbers[i]);\n          assert 0 == vectorFieldPointers[0];\n          tvd.writeVLong(tvf.getFilePointer());\n          long lastPos = vectorFieldPointers[0];\n          for(int i=1;i<numVectorFields;i++) {\n            long pos = vectorFieldPointers[i];\n            tvd.writeVLong(pos-lastPos);\n            lastPos = pos;\n          }\n          tvfLocal.writeTo(tvf);\n          tvfLocal.reset();\n        }\n      }\n\n      // Append norms for the fields we saw:\n      for(int i=0;i<numFieldData;i++) {\n        FieldData fp = fieldDataArray[i];\n        if (fp.doNorms) {\n          BufferedNorms bn = norms[fp.fieldInfo.number];\n          assert bn != null;\n          assert bn.upto <= docID;\n          bn.fill(docID);\n          float norm = fp.boost * writer.getSimilarity().lengthNorm(fp.fieldInfo.name, fp.length);\n          bn.add(norm);\n        }\n      }\n\n      if (postingsIsFull && !flushPending) {\n        flushPending = true;\n        doFlushAfter = true;\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8560794cda5bcd510c60e38ed553e9c5a6204983","date":1196807382,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#writeDocument().mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#writeDocument().mjava","sourceNew":"    /** Move all per-document state that was accumulated in\n     *  the ThreadState into the \"real\" stores. */\n    public void writeDocument() throws IOException {\n\n      // If we hit an exception while appending to the\n      // stored fields or term vectors files, we have to\n      // abort because it means those files are possibly\n      // inconsistent.\n      abortOnExc = true;\n\n      // Append stored fields to the real FieldsWriter:\n      fieldsWriter.flushDocument(fdtLocal);\n      fdtLocal.reset();\n\n      // Append term vectors to the real outputs:\n      if (tvx != null) {\n        tvx.writeLong(tvd.getFilePointer());\n        tvd.writeVInt(numVectorFields);\n        if (numVectorFields > 0) {\n          for(int i=0;i<numVectorFields;i++)\n            tvd.writeVInt(vectorFieldNumbers[i]);\n          assert 0 == vectorFieldPointers[0];\n          tvd.writeVLong(tvf.getFilePointer());\n          long lastPos = vectorFieldPointers[0];\n          for(int i=1;i<numVectorFields;i++) {\n            long pos = vectorFieldPointers[i];\n            tvd.writeVLong(pos-lastPos);\n            lastPos = pos;\n          }\n          tvfLocal.writeTo(tvf);\n          tvfLocal.reset();\n        }\n      }\n      abortOnExc = false;\n\n      // Append norms for the fields we saw:\n      for(int i=0;i<numFieldData;i++) {\n        FieldData fp = fieldDataArray[i];\n        if (fp.doNorms) {\n          BufferedNorms bn = norms[fp.fieldInfo.number];\n          assert bn != null;\n          assert bn.upto <= docID;\n          bn.fill(docID);\n          float norm = fp.boost * writer.getSimilarity().lengthNorm(fp.fieldInfo.name, fp.length);\n          bn.add(norm);\n        }\n      }\n\n      if (bufferIsFull && !flushPending) {\n        flushPending = true;\n        doFlushAfter = true;\n      }\n    }\n\n","sourceOld":"    /** Move all per-document state that was accumulated in\n     *  the ThreadState into the \"real\" stores. */\n    public void writeDocument() throws IOException {\n\n      // Append stored fields to the real FieldsWriter:\n      fieldsWriter.flushDocument(fdtLocal);\n      fdtLocal.reset();\n\n      // Append term vectors to the real outputs:\n      if (tvx != null) {\n        tvx.writeLong(tvd.getFilePointer());\n        tvd.writeVInt(numVectorFields);\n        if (numVectorFields > 0) {\n          for(int i=0;i<numVectorFields;i++)\n            tvd.writeVInt(vectorFieldNumbers[i]);\n          assert 0 == vectorFieldPointers[0];\n          tvd.writeVLong(tvf.getFilePointer());\n          long lastPos = vectorFieldPointers[0];\n          for(int i=1;i<numVectorFields;i++) {\n            long pos = vectorFieldPointers[i];\n            tvd.writeVLong(pos-lastPos);\n            lastPos = pos;\n          }\n          tvfLocal.writeTo(tvf);\n          tvfLocal.reset();\n        }\n      }\n\n      // Append norms for the fields we saw:\n      for(int i=0;i<numFieldData;i++) {\n        FieldData fp = fieldDataArray[i];\n        if (fp.doNorms) {\n          BufferedNorms bn = norms[fp.fieldInfo.number];\n          assert bn != null;\n          assert bn.upto <= docID;\n          bn.fill(docID);\n          float norm = fp.boost * writer.getSimilarity().lengthNorm(fp.fieldInfo.name, fp.length);\n          bn.add(norm);\n        }\n      }\n\n      if (bufferIsFull && !flushPending) {\n        flushPending = true;\n        doFlushAfter = true;\n      }\n    }\n\n","bugFix":null,"bugIntro":["c9a0deca56efc5191d6b3c41047fd538f3fae1d8","83bbb041887bbef07b8a98d08a0e1713ce137039"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c9a0deca56efc5191d6b3c41047fd538f3fae1d8","date":1198156049,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#writeDocument().mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#writeDocument().mjava","sourceNew":"    /** Move all per-document state that was accumulated in\n     *  the ThreadState into the \"real\" stores. */\n    public void writeDocument() throws IOException {\n\n      // If we hit an exception while appending to the\n      // stored fields or term vectors files, we have to\n      // abort all documents since we last flushed because\n      // it means those files are possibly inconsistent.\n      abortOnExc = true;\n\n      // Append stored fields to the real FieldsWriter:\n      fieldsWriter.flushDocument(numStoredFields, fdtLocal);\n      fdtLocal.reset();\n      numStoredFields = 0;\n\n      // Append term vectors to the real outputs:\n      if (tvx != null) {\n        tvx.writeLong(tvd.getFilePointer());\n        tvd.writeVInt(numVectorFields);\n        if (numVectorFields > 0) {\n          for(int i=0;i<numVectorFields;i++)\n            tvd.writeVInt(vectorFieldNumbers[i]);\n          assert 0 == vectorFieldPointers[0];\n          tvd.writeVLong(tvf.getFilePointer());\n          long lastPos = vectorFieldPointers[0];\n          for(int i=1;i<numVectorFields;i++) {\n            long pos = vectorFieldPointers[i];\n            tvd.writeVLong(pos-lastPos);\n            lastPos = pos;\n          }\n          tvfLocal.writeTo(tvf);\n          tvfLocal.reset();\n        }\n      }\n\n      // Append norms for the fields we saw:\n      for(int i=0;i<numFieldData;i++) {\n        FieldData fp = fieldDataArray[i];\n        if (fp.doNorms) {\n          BufferedNorms bn = norms[fp.fieldInfo.number];\n          assert bn != null;\n          assert bn.upto <= docID;\n          bn.fill(docID);\n          float norm = fp.boost * writer.getSimilarity().lengthNorm(fp.fieldInfo.name, fp.length);\n          bn.add(norm);\n        }\n      }\n      abortOnExc = false;\n\n      if (bufferIsFull && !flushPending) {\n        flushPending = true;\n        doFlushAfter = true;\n      }\n    }\n\n","sourceOld":"    /** Move all per-document state that was accumulated in\n     *  the ThreadState into the \"real\" stores. */\n    public void writeDocument() throws IOException {\n\n      // If we hit an exception while appending to the\n      // stored fields or term vectors files, we have to\n      // abort because it means those files are possibly\n      // inconsistent.\n      abortOnExc = true;\n\n      // Append stored fields to the real FieldsWriter:\n      fieldsWriter.flushDocument(fdtLocal);\n      fdtLocal.reset();\n\n      // Append term vectors to the real outputs:\n      if (tvx != null) {\n        tvx.writeLong(tvd.getFilePointer());\n        tvd.writeVInt(numVectorFields);\n        if (numVectorFields > 0) {\n          for(int i=0;i<numVectorFields;i++)\n            tvd.writeVInt(vectorFieldNumbers[i]);\n          assert 0 == vectorFieldPointers[0];\n          tvd.writeVLong(tvf.getFilePointer());\n          long lastPos = vectorFieldPointers[0];\n          for(int i=1;i<numVectorFields;i++) {\n            long pos = vectorFieldPointers[i];\n            tvd.writeVLong(pos-lastPos);\n            lastPos = pos;\n          }\n          tvfLocal.writeTo(tvf);\n          tvfLocal.reset();\n        }\n      }\n      abortOnExc = false;\n\n      // Append norms for the fields we saw:\n      for(int i=0;i<numFieldData;i++) {\n        FieldData fp = fieldDataArray[i];\n        if (fp.doNorms) {\n          BufferedNorms bn = norms[fp.fieldInfo.number];\n          assert bn != null;\n          assert bn.upto <= docID;\n          bn.fill(docID);\n          float norm = fp.boost * writer.getSimilarity().lengthNorm(fp.fieldInfo.name, fp.length);\n          bn.add(norm);\n        }\n      }\n\n      if (bufferIsFull && !flushPending) {\n        flushPending = true;\n        doFlushAfter = true;\n      }\n    }\n\n","bugFix":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c","8560794cda5bcd510c60e38ed553e9c5a6204983"],"bugIntro":["83bbb041887bbef07b8a98d08a0e1713ce137039"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"83bbb041887bbef07b8a98d08a0e1713ce137039","date":1200330381,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#writeDocument().mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#writeDocument().mjava","sourceNew":"    /** Move all per-document state that was accumulated in\n     *  the ThreadState into the \"real\" stores. */\n    public void writeDocument() throws IOException, AbortException {\n\n      // If we hit an exception while appending to the\n      // stored fields or term vectors files, we have to\n      // abort all documents since we last flushed because\n      // it means those files are possibly inconsistent.\n      try {\n\n        // Append stored fields to the real FieldsWriter:\n        fieldsWriter.flushDocument(numStoredFields, fdtLocal);\n        fdtLocal.reset();\n\n        // Append term vectors to the real outputs:\n        if (tvx != null) {\n          tvx.writeLong(tvd.getFilePointer());\n          tvd.writeVInt(numVectorFields);\n          if (numVectorFields > 0) {\n            for(int i=0;i<numVectorFields;i++)\n              tvd.writeVInt(vectorFieldNumbers[i]);\n            assert 0 == vectorFieldPointers[0];\n            tvd.writeVLong(tvf.getFilePointer());\n            long lastPos = vectorFieldPointers[0];\n            for(int i=1;i<numVectorFields;i++) {\n              long pos = vectorFieldPointers[i];\n              tvd.writeVLong(pos-lastPos);\n              lastPos = pos;\n            }\n            tvfLocal.writeTo(tvf);\n            tvfLocal.reset();\n          }\n        }\n\n        // Append norms for the fields we saw:\n        for(int i=0;i<numFieldData;i++) {\n          FieldData fp = fieldDataArray[i];\n          if (fp.doNorms) {\n            BufferedNorms bn = norms[fp.fieldInfo.number];\n            assert bn != null;\n            assert bn.upto <= docID;\n            bn.fill(docID);\n            float norm = fp.boost * writer.getSimilarity().lengthNorm(fp.fieldInfo.name, fp.length);\n            bn.add(norm);\n          }\n        }\n      } catch (Throwable t) {\n        // Forcefully idle this threadstate -- its state will\n        // be reset by abort()\n        isIdle = true;\n        throw new AbortException(t, DocumentsWriter.this);\n      }\n\n      if (bufferIsFull && !flushPending) {\n        flushPending = true;\n        doFlushAfter = true;\n      }\n    }\n\n","sourceOld":"    /** Move all per-document state that was accumulated in\n     *  the ThreadState into the \"real\" stores. */\n    public void writeDocument() throws IOException {\n\n      // If we hit an exception while appending to the\n      // stored fields or term vectors files, we have to\n      // abort all documents since we last flushed because\n      // it means those files are possibly inconsistent.\n      abortOnExc = true;\n\n      // Append stored fields to the real FieldsWriter:\n      fieldsWriter.flushDocument(numStoredFields, fdtLocal);\n      fdtLocal.reset();\n      numStoredFields = 0;\n\n      // Append term vectors to the real outputs:\n      if (tvx != null) {\n        tvx.writeLong(tvd.getFilePointer());\n        tvd.writeVInt(numVectorFields);\n        if (numVectorFields > 0) {\n          for(int i=0;i<numVectorFields;i++)\n            tvd.writeVInt(vectorFieldNumbers[i]);\n          assert 0 == vectorFieldPointers[0];\n          tvd.writeVLong(tvf.getFilePointer());\n          long lastPos = vectorFieldPointers[0];\n          for(int i=1;i<numVectorFields;i++) {\n            long pos = vectorFieldPointers[i];\n            tvd.writeVLong(pos-lastPos);\n            lastPos = pos;\n          }\n          tvfLocal.writeTo(tvf);\n          tvfLocal.reset();\n        }\n      }\n\n      // Append norms for the fields we saw:\n      for(int i=0;i<numFieldData;i++) {\n        FieldData fp = fieldDataArray[i];\n        if (fp.doNorms) {\n          BufferedNorms bn = norms[fp.fieldInfo.number];\n          assert bn != null;\n          assert bn.upto <= docID;\n          bn.fill(docID);\n          float norm = fp.boost * writer.getSimilarity().lengthNorm(fp.fieldInfo.name, fp.length);\n          bn.add(norm);\n        }\n      }\n      abortOnExc = false;\n\n      if (bufferIsFull && !flushPending) {\n        flushPending = true;\n        doFlushAfter = true;\n      }\n    }\n\n","bugFix":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c","c9a0deca56efc5191d6b3c41047fd538f3fae1d8","c1213252cc474b006ac3675f030910be3895e10d","8560794cda5bcd510c60e38ed553e9c5a6204983"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b3d08461c77d39c25ea6ff0cd05b32f948fa2a33","date":1201260752,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#writeDocument().mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#writeDocument().mjava","sourceNew":"    /** Move all per-document state that was accumulated in\n     *  the ThreadState into the \"real\" stores. */\n    public void writeDocument() throws IOException, AbortException {\n\n      // If we hit an exception while appending to the\n      // stored fields or term vectors files, we have to\n      // abort all documents since we last flushed because\n      // it means those files are possibly inconsistent.\n      try {\n\n        // Append stored fields to the real FieldsWriter:\n        fieldsWriter.flushDocument(numStoredFields, fdtLocal);\n        fdtLocal.reset();\n\n        // Append term vectors to the real outputs:\n        if (tvx != null) {\n          tvx.writeLong(tvd.getFilePointer());\n          tvx.writeLong(tvf.getFilePointer());\n          tvd.writeVInt(numVectorFields);\n          if (numVectorFields > 0) {\n            for(int i=0;i<numVectorFields;i++)\n              tvd.writeVInt(vectorFieldNumbers[i]);\n            assert 0 == vectorFieldPointers[0];\n            long lastPos = vectorFieldPointers[0];\n            for(int i=1;i<numVectorFields;i++) {\n              long pos = vectorFieldPointers[i];\n              tvd.writeVLong(pos-lastPos);\n              lastPos = pos;\n            }\n            tvfLocal.writeTo(tvf);\n            tvfLocal.reset();\n          }\n        }\n\n        // Append norms for the fields we saw:\n        for(int i=0;i<numFieldData;i++) {\n          FieldData fp = fieldDataArray[i];\n          if (fp.doNorms) {\n            BufferedNorms bn = norms[fp.fieldInfo.number];\n            assert bn != null;\n            assert bn.upto <= docID;\n            bn.fill(docID);\n            float norm = fp.boost * writer.getSimilarity().lengthNorm(fp.fieldInfo.name, fp.length);\n            bn.add(norm);\n          }\n        }\n      } catch (Throwable t) {\n        // Forcefully idle this threadstate -- its state will\n        // be reset by abort()\n        isIdle = true;\n        throw new AbortException(t, DocumentsWriter.this);\n      }\n\n      if (bufferIsFull && !flushPending) {\n        flushPending = true;\n        doFlushAfter = true;\n      }\n    }\n\n","sourceOld":"    /** Move all per-document state that was accumulated in\n     *  the ThreadState into the \"real\" stores. */\n    public void writeDocument() throws IOException, AbortException {\n\n      // If we hit an exception while appending to the\n      // stored fields or term vectors files, we have to\n      // abort all documents since we last flushed because\n      // it means those files are possibly inconsistent.\n      try {\n\n        // Append stored fields to the real FieldsWriter:\n        fieldsWriter.flushDocument(numStoredFields, fdtLocal);\n        fdtLocal.reset();\n\n        // Append term vectors to the real outputs:\n        if (tvx != null) {\n          tvx.writeLong(tvd.getFilePointer());\n          tvd.writeVInt(numVectorFields);\n          if (numVectorFields > 0) {\n            for(int i=0;i<numVectorFields;i++)\n              tvd.writeVInt(vectorFieldNumbers[i]);\n            assert 0 == vectorFieldPointers[0];\n            tvd.writeVLong(tvf.getFilePointer());\n            long lastPos = vectorFieldPointers[0];\n            for(int i=1;i<numVectorFields;i++) {\n              long pos = vectorFieldPointers[i];\n              tvd.writeVLong(pos-lastPos);\n              lastPos = pos;\n            }\n            tvfLocal.writeTo(tvf);\n            tvfLocal.reset();\n          }\n        }\n\n        // Append norms for the fields we saw:\n        for(int i=0;i<numFieldData;i++) {\n          FieldData fp = fieldDataArray[i];\n          if (fp.doNorms) {\n            BufferedNorms bn = norms[fp.fieldInfo.number];\n            assert bn != null;\n            assert bn.upto <= docID;\n            bn.fill(docID);\n            float norm = fp.boost * writer.getSimilarity().lengthNorm(fp.fieldInfo.name, fp.length);\n            bn.add(norm);\n          }\n        }\n      } catch (Throwable t) {\n        // Forcefully idle this threadstate -- its state will\n        // be reset by abort()\n        isIdle = true;\n        throw new AbortException(t, DocumentsWriter.this);\n      }\n\n      if (bufferIsFull && !flushPending) {\n        flushPending = true;\n        doFlushAfter = true;\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"49adbad5232116eb2448ea8166464e6a68bca007","date":1202851885,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#writeDocument().mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#writeDocument().mjava","sourceNew":"    /** Move all per-document state that was accumulated in\n     *  the ThreadState into the \"real\" stores. */\n    public void writeDocument() throws IOException, AbortException {\n\n      // If we hit an exception while appending to the\n      // stored fields or term vectors files, we have to\n      // abort all documents since we last flushed because\n      // it means those files are possibly inconsistent.\n      try {\n\n        numDocsInStore++;\n\n        // Append stored fields to the real FieldsWriter:\n        fieldsWriter.flushDocument(numStoredFields, fdtLocal);\n        fdtLocal.reset();\n\n        // Append term vectors to the real outputs:\n        if (tvx != null) {\n          tvx.writeLong(tvd.getFilePointer());\n          tvx.writeLong(tvf.getFilePointer());\n          tvd.writeVInt(numVectorFields);\n          if (numVectorFields > 0) {\n            for(int i=0;i<numVectorFields;i++)\n              tvd.writeVInt(vectorFieldNumbers[i]);\n            assert 0 == vectorFieldPointers[0];\n            long lastPos = vectorFieldPointers[0];\n            for(int i=1;i<numVectorFields;i++) {\n              long pos = vectorFieldPointers[i];\n              tvd.writeVLong(pos-lastPos);\n              lastPos = pos;\n            }\n            tvfLocal.writeTo(tvf);\n            tvfLocal.reset();\n          }\n        }\n\n        // Append norms for the fields we saw:\n        for(int i=0;i<numFieldData;i++) {\n          FieldData fp = fieldDataArray[i];\n          if (fp.doNorms) {\n            BufferedNorms bn = norms[fp.fieldInfo.number];\n            assert bn != null;\n            assert bn.upto <= docID;\n            bn.fill(docID);\n            float norm = fp.boost * writer.getSimilarity().lengthNorm(fp.fieldInfo.name, fp.length);\n            bn.add(norm);\n          }\n        }\n      } catch (Throwable t) {\n        // Forcefully idle this threadstate -- its state will\n        // be reset by abort()\n        isIdle = true;\n        throw new AbortException(t, DocumentsWriter.this);\n      }\n\n      if (bufferIsFull && !flushPending) {\n        flushPending = true;\n        doFlushAfter = true;\n      }\n    }\n\n","sourceOld":"    /** Move all per-document state that was accumulated in\n     *  the ThreadState into the \"real\" stores. */\n    public void writeDocument() throws IOException, AbortException {\n\n      // If we hit an exception while appending to the\n      // stored fields or term vectors files, we have to\n      // abort all documents since we last flushed because\n      // it means those files are possibly inconsistent.\n      try {\n\n        // Append stored fields to the real FieldsWriter:\n        fieldsWriter.flushDocument(numStoredFields, fdtLocal);\n        fdtLocal.reset();\n\n        // Append term vectors to the real outputs:\n        if (tvx != null) {\n          tvx.writeLong(tvd.getFilePointer());\n          tvx.writeLong(tvf.getFilePointer());\n          tvd.writeVInt(numVectorFields);\n          if (numVectorFields > 0) {\n            for(int i=0;i<numVectorFields;i++)\n              tvd.writeVInt(vectorFieldNumbers[i]);\n            assert 0 == vectorFieldPointers[0];\n            long lastPos = vectorFieldPointers[0];\n            for(int i=1;i<numVectorFields;i++) {\n              long pos = vectorFieldPointers[i];\n              tvd.writeVLong(pos-lastPos);\n              lastPos = pos;\n            }\n            tvfLocal.writeTo(tvf);\n            tvfLocal.reset();\n          }\n        }\n\n        // Append norms for the fields we saw:\n        for(int i=0;i<numFieldData;i++) {\n          FieldData fp = fieldDataArray[i];\n          if (fp.doNorms) {\n            BufferedNorms bn = norms[fp.fieldInfo.number];\n            assert bn != null;\n            assert bn.upto <= docID;\n            bn.fill(docID);\n            float norm = fp.boost * writer.getSimilarity().lengthNorm(fp.fieldInfo.name, fp.length);\n            bn.add(norm);\n          }\n        }\n      } catch (Throwable t) {\n        // Forcefully idle this threadstate -- its state will\n        // be reset by abort()\n        isIdle = true;\n        throw new AbortException(t, DocumentsWriter.this);\n      }\n\n      if (bufferIsFull && !flushPending) {\n        flushPending = true;\n        doFlushAfter = true;\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5a0af3a442be522899177e5e11384a45a6784a3f","date":1205348952,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#writeDocument().mjava","sourceNew":null,"sourceOld":"    /** Move all per-document state that was accumulated in\n     *  the ThreadState into the \"real\" stores. */\n    public void writeDocument() throws IOException, AbortException {\n\n      // If we hit an exception while appending to the\n      // stored fields or term vectors files, we have to\n      // abort all documents since we last flushed because\n      // it means those files are possibly inconsistent.\n      try {\n\n        numDocsInStore++;\n\n        // Append stored fields to the real FieldsWriter:\n        fieldsWriter.flushDocument(numStoredFields, fdtLocal);\n        fdtLocal.reset();\n\n        // Append term vectors to the real outputs:\n        if (tvx != null) {\n          tvx.writeLong(tvd.getFilePointer());\n          tvx.writeLong(tvf.getFilePointer());\n          tvd.writeVInt(numVectorFields);\n          if (numVectorFields > 0) {\n            for(int i=0;i<numVectorFields;i++)\n              tvd.writeVInt(vectorFieldNumbers[i]);\n            assert 0 == vectorFieldPointers[0];\n            long lastPos = vectorFieldPointers[0];\n            for(int i=1;i<numVectorFields;i++) {\n              long pos = vectorFieldPointers[i];\n              tvd.writeVLong(pos-lastPos);\n              lastPos = pos;\n            }\n            tvfLocal.writeTo(tvf);\n            tvfLocal.reset();\n          }\n        }\n\n        // Append norms for the fields we saw:\n        for(int i=0;i<numFieldData;i++) {\n          FieldData fp = fieldDataArray[i];\n          if (fp.doNorms) {\n            BufferedNorms bn = norms[fp.fieldInfo.number];\n            assert bn != null;\n            assert bn.upto <= docID;\n            bn.fill(docID);\n            float norm = fp.boost * writer.getSimilarity().lengthNorm(fp.fieldInfo.name, fp.length);\n            bn.add(norm);\n          }\n        }\n      } catch (Throwable t) {\n        // Forcefully idle this threadstate -- its state will\n        // be reset by abort()\n        isIdle = true;\n        throw new AbortException(t, DocumentsWriter.this);\n      }\n\n      if (bufferIsFull && !flushPending) {\n        flushPending = true;\n        doFlushAfter = true;\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"49adbad5232116eb2448ea8166464e6a68bca007":["b3d08461c77d39c25ea6ff0cd05b32f948fa2a33"],"83bbb041887bbef07b8a98d08a0e1713ce137039":["c9a0deca56efc5191d6b3c41047fd538f3fae1d8"],"c9a0deca56efc5191d6b3c41047fd538f3fae1d8":["8560794cda5bcd510c60e38ed553e9c5a6204983"],"b3d08461c77d39c25ea6ff0cd05b32f948fa2a33":["83bbb041887bbef07b8a98d08a0e1713ce137039"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"2558ddf9e14a97bc597f5b72bb3ecb5b7f6bba8e":["c1213252cc474b006ac3675f030910be3895e10d"],"8560794cda5bcd510c60e38ed553e9c5a6204983":["2558ddf9e14a97bc597f5b72bb3ecb5b7f6bba8e"],"5a0af3a442be522899177e5e11384a45a6784a3f":["49adbad5232116eb2448ea8166464e6a68bca007"],"c1213252cc474b006ac3675f030910be3895e10d":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"4350b17bd363cd13a95171b8df1ca62ea4c3e71c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["5a0af3a442be522899177e5e11384a45a6784a3f"]},"commit2Childs":{"49adbad5232116eb2448ea8166464e6a68bca007":["5a0af3a442be522899177e5e11384a45a6784a3f"],"83bbb041887bbef07b8a98d08a0e1713ce137039":["b3d08461c77d39c25ea6ff0cd05b32f948fa2a33"],"c9a0deca56efc5191d6b3c41047fd538f3fae1d8":["83bbb041887bbef07b8a98d08a0e1713ce137039"],"b3d08461c77d39c25ea6ff0cd05b32f948fa2a33":["49adbad5232116eb2448ea8166464e6a68bca007"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"2558ddf9e14a97bc597f5b72bb3ecb5b7f6bba8e":["8560794cda5bcd510c60e38ed553e9c5a6204983"],"8560794cda5bcd510c60e38ed553e9c5a6204983":["c9a0deca56efc5191d6b3c41047fd538f3fae1d8"],"5a0af3a442be522899177e5e11384a45a6784a3f":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"c1213252cc474b006ac3675f030910be3895e10d":["2558ddf9e14a97bc597f5b72bb3ecb5b7f6bba8e"],"4350b17bd363cd13a95171b8df1ca62ea4c3e71c":["c1213252cc474b006ac3675f030910be3895e10d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}