{"path":"src/java/org/apache/lucene/index/SegmentReader#initialize(SegmentInfo,int).mjava","commits":[{"id":"f6dba7919de4ff4ed6ff17f90619203772722f08","date":1180451647,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentReader#initialize(SegmentInfo,int).mjava","pathOld":"src/java/org/apache/lucene/index/SegmentReader#initialize(SegmentInfo).mjava","sourceNew":"  private void initialize(SegmentInfo si, int readBufferSize) throws CorruptIndexException, IOException {\n    segment = si.name;\n    this.si = si;\n\n    boolean success = false;\n\n    try {\n      // Use compound file directory for some files, if it exists\n      Directory cfsDir = directory();\n      if (si.getUseCompoundFile()) {\n        cfsReader = new CompoundFileReader(directory(), segment + \".cfs\", readBufferSize);\n        cfsDir = cfsReader;\n      }\n\n      // No compound file exists - use the multi-file format\n      fieldInfos = new FieldInfos(cfsDir, segment + \".fnm\");\n      fieldsReader = new FieldsReader(cfsDir, segment, fieldInfos, readBufferSize);\n\n      // Verify two sources of \"maxDoc\" agree:\n      if (fieldsReader.size() != si.docCount) {\n        throw new CorruptIndexException(\"doc counts differ for segment \" + si.name + \": fieldsReader shows \" + fieldsReader.size() + \" but segmentInfo shows \" + si.docCount);\n      }\n\n      tis = new TermInfosReader(cfsDir, segment, fieldInfos, readBufferSize);\n      \n      // NOTE: the bitvector is stored using the regular directory, not cfs\n      if (hasDeletions(si)) {\n        deletedDocs = new BitVector(directory(), si.getDelFileName());\n\n        // Verify # deletes does not exceed maxDoc for this segment:\n        if (deletedDocs.count() > maxDoc()) {\n          throw new CorruptIndexException(\"number of deletes (\" + deletedDocs.count() + \") exceeds max doc (\" + maxDoc() + \") for segment \" + si.name);\n        }\n      }\n\n      // make sure that all index files have been read or are kept open\n      // so that if an index update removes them we'll still have them\n      freqStream = cfsDir.openInput(segment + \".frq\", readBufferSize);\n      proxStream = cfsDir.openInput(segment + \".prx\", readBufferSize);\n      openNorms(cfsDir, readBufferSize);\n\n      if (fieldInfos.hasVectors()) { // open term vector files only as needed\n        termVectorsReaderOrig = new TermVectorsReader(cfsDir, segment, fieldInfos, readBufferSize);\n      }\n      success = true;\n    } finally {\n\n      // With lock-less commits, it's entirely possible (and\n      // fine) to hit a FileNotFound exception above.  In\n      // this case, we want to explicitly close any subset\n      // of things that were opened so that we don't have to\n      // wait for a GC to do so.\n      if (!success) {\n        doClose();\n      }\n    }\n  }\n\n","sourceOld":"  private void initialize(SegmentInfo si) throws CorruptIndexException, IOException {\n    segment = si.name;\n    this.si = si;\n\n    boolean success = false;\n\n    try {\n      // Use compound file directory for some files, if it exists\n      Directory cfsDir = directory();\n      if (si.getUseCompoundFile()) {\n        cfsReader = new CompoundFileReader(directory(), segment + \".cfs\");\n        cfsDir = cfsReader;\n      }\n\n      // No compound file exists - use the multi-file format\n      fieldInfos = new FieldInfos(cfsDir, segment + \".fnm\");\n      fieldsReader = new FieldsReader(cfsDir, segment, fieldInfos);\n\n      // Verify two sources of \"maxDoc\" agree:\n      if (fieldsReader.size() != si.docCount) {\n        throw new CorruptIndexException(\"doc counts differ for segment \" + si.name + \": fieldsReader shows \" + fieldsReader.size() + \" but segmentInfo shows \" + si.docCount);\n      }\n\n      tis = new TermInfosReader(cfsDir, segment, fieldInfos);\n      \n      // NOTE: the bitvector is stored using the regular directory, not cfs\n      if (hasDeletions(si)) {\n        deletedDocs = new BitVector(directory(), si.getDelFileName());\n\n        // Verify # deletes does not exceed maxDoc for this segment:\n        if (deletedDocs.count() > maxDoc()) {\n          throw new CorruptIndexException(\"number of deletes (\" + deletedDocs.count() + \") exceeds max doc (\" + maxDoc() + \") for segment \" + si.name);\n        }\n      }\n\n      // make sure that all index files have been read or are kept open\n      // so that if an index update removes them we'll still have them\n      freqStream = cfsDir.openInput(segment + \".frq\");\n      proxStream = cfsDir.openInput(segment + \".prx\");\n      openNorms(cfsDir);\n\n      if (fieldInfos.hasVectors()) { // open term vector files only as needed\n        termVectorsReaderOrig = new TermVectorsReader(cfsDir, segment, fieldInfos);\n      }\n      success = true;\n    } finally {\n\n      // With lock-less commits, it's entirely possible (and\n      // fine) to hit a FileNotFound exception above.  In\n      // this case, we want to explicitly close any subset\n      // of things that were opened so that we don't have to\n      // wait for a GC to do so.\n      if (!success) {\n        doClose();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4350b17bd363cd13a95171b8df1ca62ea4c3e71c","date":1183562198,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentReader#initialize(SegmentInfo,int,boolean).mjava","pathOld":"src/java/org/apache/lucene/index/SegmentReader#initialize(SegmentInfo,int).mjava","sourceNew":"  private void initialize(SegmentInfo si, int readBufferSize, boolean doOpenStores) throws CorruptIndexException, IOException {\n    segment = si.name;\n    this.si = si;\n\n    boolean success = false;\n\n    try {\n      // Use compound file directory for some files, if it exists\n      Directory cfsDir = directory();\n      if (si.getUseCompoundFile()) {\n        cfsReader = new CompoundFileReader(directory(), segment + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION, readBufferSize);\n        cfsDir = cfsReader;\n      }\n\n      final Directory storeDir;\n\n      if (doOpenStores) {\n        if (si.getDocStoreOffset() != -1) {\n          if (si.getDocStoreIsCompoundFile()) {\n            storeCFSReader = new CompoundFileReader(directory(), si.getDocStoreSegment() + \".\" + IndexFileNames.COMPOUND_FILE_STORE_EXTENSION, readBufferSize);\n            storeDir = storeCFSReader;\n          } else {\n            storeDir = directory();\n          }\n        } else {\n          storeDir = cfsDir;\n        }\n      } else\n        storeDir = null;\n\n      // No compound file exists - use the multi-file format\n      fieldInfos = new FieldInfos(cfsDir, segment + \".fnm\");\n\n      final String fieldsSegment;\n      final Directory dir;\n\n      if (si.getDocStoreOffset() != -1)\n        fieldsSegment = si.getDocStoreSegment();\n      else\n        fieldsSegment = segment;\n\n      if (doOpenStores) {\n        fieldsReader = new FieldsReader(storeDir, fieldsSegment, fieldInfos, readBufferSize,\n                                        si.getDocStoreOffset(), si.docCount);\n\n        // Verify two sources of \"maxDoc\" agree:\n        if (si.getDocStoreOffset() == -1 && fieldsReader.size() != si.docCount) {\n          throw new CorruptIndexException(\"doc counts differ for segment \" + si.name + \": fieldsReader shows \" + fieldsReader.size() + \" but segmentInfo shows \" + si.docCount);\n        }\n      }\n\n      tis = new TermInfosReader(cfsDir, segment, fieldInfos, readBufferSize);\n      \n      // NOTE: the bitvector is stored using the regular directory, not cfs\n      if (hasDeletions(si)) {\n        deletedDocs = new BitVector(directory(), si.getDelFileName());\n\n        // Verify # deletes does not exceed maxDoc for this segment:\n        if (deletedDocs.count() > maxDoc()) {\n          throw new CorruptIndexException(\"number of deletes (\" + deletedDocs.count() + \") exceeds max doc (\" + maxDoc() + \") for segment \" + si.name);\n        }\n      }\n\n      // make sure that all index files have been read or are kept open\n      // so that if an index update removes them we'll still have them\n      freqStream = cfsDir.openInput(segment + \".frq\", readBufferSize);\n      proxStream = cfsDir.openInput(segment + \".prx\", readBufferSize);\n      openNorms(cfsDir, readBufferSize);\n\n      if (doOpenStores && fieldInfos.hasVectors()) { // open term vector files only as needed\n        final String vectorsSegment;\n        if (si.getDocStoreOffset() != -1)\n          vectorsSegment = si.getDocStoreSegment();\n        else\n          vectorsSegment = segment;\n        termVectorsReaderOrig = new TermVectorsReader(storeDir, vectorsSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n      }\n      success = true;\n    } finally {\n\n      // With lock-less commits, it's entirely possible (and\n      // fine) to hit a FileNotFound exception above.  In\n      // this case, we want to explicitly close any subset\n      // of things that were opened so that we don't have to\n      // wait for a GC to do so.\n      if (!success) {\n        doClose();\n      }\n    }\n  }\n\n","sourceOld":"  private void initialize(SegmentInfo si, int readBufferSize) throws CorruptIndexException, IOException {\n    segment = si.name;\n    this.si = si;\n\n    boolean success = false;\n\n    try {\n      // Use compound file directory for some files, if it exists\n      Directory cfsDir = directory();\n      if (si.getUseCompoundFile()) {\n        cfsReader = new CompoundFileReader(directory(), segment + \".cfs\", readBufferSize);\n        cfsDir = cfsReader;\n      }\n\n      // No compound file exists - use the multi-file format\n      fieldInfos = new FieldInfos(cfsDir, segment + \".fnm\");\n      fieldsReader = new FieldsReader(cfsDir, segment, fieldInfos, readBufferSize);\n\n      // Verify two sources of \"maxDoc\" agree:\n      if (fieldsReader.size() != si.docCount) {\n        throw new CorruptIndexException(\"doc counts differ for segment \" + si.name + \": fieldsReader shows \" + fieldsReader.size() + \" but segmentInfo shows \" + si.docCount);\n      }\n\n      tis = new TermInfosReader(cfsDir, segment, fieldInfos, readBufferSize);\n      \n      // NOTE: the bitvector is stored using the regular directory, not cfs\n      if (hasDeletions(si)) {\n        deletedDocs = new BitVector(directory(), si.getDelFileName());\n\n        // Verify # deletes does not exceed maxDoc for this segment:\n        if (deletedDocs.count() > maxDoc()) {\n          throw new CorruptIndexException(\"number of deletes (\" + deletedDocs.count() + \") exceeds max doc (\" + maxDoc() + \") for segment \" + si.name);\n        }\n      }\n\n      // make sure that all index files have been read or are kept open\n      // so that if an index update removes them we'll still have them\n      freqStream = cfsDir.openInput(segment + \".frq\", readBufferSize);\n      proxStream = cfsDir.openInput(segment + \".prx\", readBufferSize);\n      openNorms(cfsDir, readBufferSize);\n\n      if (fieldInfos.hasVectors()) { // open term vector files only as needed\n        termVectorsReaderOrig = new TermVectorsReader(cfsDir, segment, fieldInfos, readBufferSize);\n      }\n      success = true;\n    } finally {\n\n      // With lock-less commits, it's entirely possible (and\n      // fine) to hit a FileNotFound exception above.  In\n      // this case, we want to explicitly close any subset\n      // of things that were opened so that we don't have to\n      // wait for a GC to do so.\n      if (!success) {\n        doClose();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"f6dba7919de4ff4ed6ff17f90619203772722f08":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"4350b17bd363cd13a95171b8df1ca62ea4c3e71c":["f6dba7919de4ff4ed6ff17f90619203772722f08"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"]},"commit2Childs":{"f6dba7919de4ff4ed6ff17f90619203772722f08":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["f6dba7919de4ff4ed6ff17f90619203772722f08"],"4350b17bd363cd13a95171b8df1ca62ea4c3e71c":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}