{"path":"solr/solrj/src/java/org/apache/solr/common/cloud/ZkStateReader.AliasesManager#applyModificationAndExportToZk(UnaryOperator[Aliases]).mjava","commits":[{"id":"215e40821821b2df2e69355e208532c05ef095a5","date":1510858642,"type":0,"author":"David Smiley","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/common/cloud/ZkStateReader.AliasesManager#applyModificationAndExportToZk(UnaryOperator[Aliases]).mjava","pathOld":"/dev/null","sourceNew":"    /**\n     * Writes an updated {@link Aliases} to zk.\n     * It will retry if there are races with other modifications, giving up after 30 seconds with a SolrException.\n     * The caller should understand it's possible the aliases has further changed if it examines it.\n     */\n    public void applyModificationAndExportToZk(UnaryOperator<Aliases> op) {\n      final long deadlineNanos = System.nanoTime() + TimeUnit.SECONDS.toNanos(30);\n      int triesLeft = 5;\n      while (triesLeft > 0) {\n        triesLeft--;\n        // we could synchronize on \"this\" but there doesn't seem to be a point; we have a retry loop.\n        Aliases curAliases = getAliases();\n        Aliases modAliases = op.apply(curAliases);\n        final byte[] modAliasesJson = modAliases.toJSON();\n        if (curAliases == modAliases) {\n          LOG.debug(\"Current aliases has the desired modification; no further ZK interaction needed.\");\n          return;\n        }\n\n        try {\n          try {\n            final Stat stat = getZkClient().setData(ALIASES, modAliasesJson, curAliases.getZNodeVersion(), true);\n            setIfNewer(Aliases.fromJSON(modAliasesJson, stat.getVersion()));\n            return;\n          } catch (KeeperException.BadVersionException e) {\n            LOG.debug(e.toString(), e);\n            LOG.warn(\"Couldn't save aliases due to race with another modification; will update and retry until timeout\");\n            // considered a backoff here, but we really do want to compete strongly since the normal case is\n            // that we will do one update and succeed. This is left as a hot loop for 5 tries intentionally.\n            // More failures than that here probably indicate a bug or a very strange high write frequency usage for\n            // aliases.json, timeouts mean zk is being very slow to respond, or this node is being crushed\n            // by other processing and just can't find any cpu cycles at all.\n            update();\n            if (deadlineNanos < System.nanoTime()) {\n              throw new SolrException(ErrorCode.SERVER_ERROR, \"Timed out trying to update aliases! \" +\n                  \"Either zookeeper or this node may be overloaded.\");\n            }\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        } catch (KeeperException e) {\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        }\n      }\n      if (triesLeft == 0) {\n        throw new SolrException(ErrorCode.SERVER_ERROR, \"Too many successive version failures trying to update aliases\");\n      }\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["a980795a56f2fcbc94caeb3233071312d5684d59"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6857f8205f8c5b4ff39a54d8aebb4fdfb7cfb691","date":1516909549,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/common/cloud/ZkStateReader.AliasesManager#applyModificationAndExportToZk(UnaryOperator[Aliases]).mjava","pathOld":"solr/solrj/src/java/org/apache/solr/common/cloud/ZkStateReader.AliasesManager#applyModificationAndExportToZk(UnaryOperator[Aliases]).mjava","sourceNew":"    /**\n     * Writes an updated {@link Aliases} to zk.\n     * It will retry if there are races with other modifications, giving up after 30 seconds with a SolrException.\n     * The caller should understand it's possible the aliases has further changed if it examines it.\n     */\n    public void applyModificationAndExportToZk(UnaryOperator<Aliases> op) {\n      final long deadlineNanos = System.nanoTime() + TimeUnit.SECONDS.toNanos(30);\n      // note: triesLeft tuning is based on ConcurrentCreateRoutedAliasTest\n      int triesLeft = 30;\n      while (triesLeft > 0) {\n        triesLeft--;\n        // we could synchronize on \"this\" but there doesn't seem to be a point; we have a retry loop.\n        Aliases curAliases = getAliases();\n        Aliases modAliases = op.apply(curAliases);\n        final byte[] modAliasesJson = modAliases.toJSON();\n        if (curAliases == modAliases) {\n          LOG.debug(\"Current aliases has the desired modification; no further ZK interaction needed.\");\n          return;\n        }\n\n        try {\n          try {\n            final Stat stat = getZkClient().setData(ALIASES, modAliasesJson, curAliases.getZNodeVersion(), true);\n            setIfNewer(Aliases.fromJSON(modAliasesJson, stat.getVersion()));\n            return;\n          } catch (KeeperException.BadVersionException e) {\n            LOG.debug(e.toString(), e);\n            LOG.warn(\"Couldn't save aliases due to race with another modification; will update and retry until timeout\");\n            // considered a backoff here, but we really do want to compete strongly since the normal case is\n            // that we will do one update and succeed. This is left as a hot loop for 5 tries intentionally.\n            // More failures than that here probably indicate a bug or a very strange high write frequency usage for\n            // aliases.json, timeouts mean zk is being very slow to respond, or this node is being crushed\n            // by other processing and just can't find any cpu cycles at all.\n            update();\n            if (deadlineNanos < System.nanoTime()) {\n              throw new SolrException(ErrorCode.SERVER_ERROR, \"Timed out trying to update aliases! \" +\n                  \"Either zookeeper or this node may be overloaded.\");\n            }\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        } catch (KeeperException e) {\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        }\n      }\n      if (triesLeft == 0) {\n        throw new SolrException(ErrorCode.SERVER_ERROR, \"Too many successive version failures trying to update aliases\");\n      }\n    }\n\n","sourceOld":"    /**\n     * Writes an updated {@link Aliases} to zk.\n     * It will retry if there are races with other modifications, giving up after 30 seconds with a SolrException.\n     * The caller should understand it's possible the aliases has further changed if it examines it.\n     */\n    public void applyModificationAndExportToZk(UnaryOperator<Aliases> op) {\n      final long deadlineNanos = System.nanoTime() + TimeUnit.SECONDS.toNanos(30);\n      int triesLeft = 5;\n      while (triesLeft > 0) {\n        triesLeft--;\n        // we could synchronize on \"this\" but there doesn't seem to be a point; we have a retry loop.\n        Aliases curAliases = getAliases();\n        Aliases modAliases = op.apply(curAliases);\n        final byte[] modAliasesJson = modAliases.toJSON();\n        if (curAliases == modAliases) {\n          LOG.debug(\"Current aliases has the desired modification; no further ZK interaction needed.\");\n          return;\n        }\n\n        try {\n          try {\n            final Stat stat = getZkClient().setData(ALIASES, modAliasesJson, curAliases.getZNodeVersion(), true);\n            setIfNewer(Aliases.fromJSON(modAliasesJson, stat.getVersion()));\n            return;\n          } catch (KeeperException.BadVersionException e) {\n            LOG.debug(e.toString(), e);\n            LOG.warn(\"Couldn't save aliases due to race with another modification; will update and retry until timeout\");\n            // considered a backoff here, but we really do want to compete strongly since the normal case is\n            // that we will do one update and succeed. This is left as a hot loop for 5 tries intentionally.\n            // More failures than that here probably indicate a bug or a very strange high write frequency usage for\n            // aliases.json, timeouts mean zk is being very slow to respond, or this node is being crushed\n            // by other processing and just can't find any cpu cycles at all.\n            update();\n            if (deadlineNanos < System.nanoTime()) {\n              throw new SolrException(ErrorCode.SERVER_ERROR, \"Timed out trying to update aliases! \" +\n                  \"Either zookeeper or this node may be overloaded.\");\n            }\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        } catch (KeeperException e) {\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        }\n      }\n      if (triesLeft == 0) {\n        throw new SolrException(ErrorCode.SERVER_ERROR, \"Too many successive version failures trying to update aliases\");\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a980795a56f2fcbc94caeb3233071312d5684d59","date":1524255736,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/common/cloud/ZkStateReader.AliasesManager#applyModificationAndExportToZk(UnaryOperator[Aliases]).mjava","pathOld":"solr/solrj/src/java/org/apache/solr/common/cloud/ZkStateReader.AliasesManager#applyModificationAndExportToZk(UnaryOperator[Aliases]).mjava","sourceNew":"    /**\n     * Writes an updated {@link Aliases} to zk.\n     * It will retry if there are races with other modifications, giving up after 30 seconds with a SolrException.\n     * The caller should understand it's possible the aliases has further changed if it examines it.\n     */\n    public void applyModificationAndExportToZk(UnaryOperator<Aliases> op) {\n      final long deadlineNanos = System.nanoTime() + TimeUnit.SECONDS.toNanos(30);\n      // note: triesLeft tuning is based on ConcurrentCreateRoutedAliasTest\n      int triesLeft = 30;\n      while (triesLeft > 0) {\n        triesLeft--;\n        // we could synchronize on \"this\" but there doesn't seem to be a point; we have a retry loop.\n        Aliases curAliases = getAliases();\n        Aliases modAliases = op.apply(curAliases);\n        final byte[] modAliasesJson = modAliases.toJSON();\n        if (curAliases == modAliases) {\n          LOG.debug(\"Current aliases has the desired modification; no further ZK interaction needed.\");\n          return;\n        }\n\n        try {\n          try {\n            final Stat stat = getZkClient().setData(ALIASES, modAliasesJson, curAliases.getZNodeVersion(), true);\n            setIfNewer(Aliases.fromJSON(modAliasesJson, stat.getVersion()));\n            return;\n          } catch (KeeperException.BadVersionException e) {\n            LOG.debug(e.toString(), e);\n            LOG.warn(\"Couldn't save aliases due to race with another modification; will update and retry until timeout\");\n            // considered a backoff here, but we really do want to compete strongly since the normal case is\n            // that we will do one update and succeed. This is left as a hot loop for limited tries intentionally.\n            // More failures than that here probably indicate a bug or a very strange high write frequency usage for\n            // aliases.json, timeouts mean zk is being very slow to respond, or this node is being crushed\n            // by other processing and just can't find any cpu cycles at all.\n            update();\n            if (deadlineNanos < System.nanoTime()) {\n              throw new SolrException(ErrorCode.SERVER_ERROR, \"Timed out trying to update aliases! \" +\n                  \"Either zookeeper or this node may be overloaded.\");\n            }\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        } catch (KeeperException e) {\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        }\n      }\n      if (triesLeft == 0) {\n        throw new SolrException(ErrorCode.SERVER_ERROR, \"Too many successive version failures trying to update aliases\");\n      }\n    }\n\n","sourceOld":"    /**\n     * Writes an updated {@link Aliases} to zk.\n     * It will retry if there are races with other modifications, giving up after 30 seconds with a SolrException.\n     * The caller should understand it's possible the aliases has further changed if it examines it.\n     */\n    public void applyModificationAndExportToZk(UnaryOperator<Aliases> op) {\n      final long deadlineNanos = System.nanoTime() + TimeUnit.SECONDS.toNanos(30);\n      // note: triesLeft tuning is based on ConcurrentCreateRoutedAliasTest\n      int triesLeft = 30;\n      while (triesLeft > 0) {\n        triesLeft--;\n        // we could synchronize on \"this\" but there doesn't seem to be a point; we have a retry loop.\n        Aliases curAliases = getAliases();\n        Aliases modAliases = op.apply(curAliases);\n        final byte[] modAliasesJson = modAliases.toJSON();\n        if (curAliases == modAliases) {\n          LOG.debug(\"Current aliases has the desired modification; no further ZK interaction needed.\");\n          return;\n        }\n\n        try {\n          try {\n            final Stat stat = getZkClient().setData(ALIASES, modAliasesJson, curAliases.getZNodeVersion(), true);\n            setIfNewer(Aliases.fromJSON(modAliasesJson, stat.getVersion()));\n            return;\n          } catch (KeeperException.BadVersionException e) {\n            LOG.debug(e.toString(), e);\n            LOG.warn(\"Couldn't save aliases due to race with another modification; will update and retry until timeout\");\n            // considered a backoff here, but we really do want to compete strongly since the normal case is\n            // that we will do one update and succeed. This is left as a hot loop for 5 tries intentionally.\n            // More failures than that here probably indicate a bug or a very strange high write frequency usage for\n            // aliases.json, timeouts mean zk is being very slow to respond, or this node is being crushed\n            // by other processing and just can't find any cpu cycles at all.\n            update();\n            if (deadlineNanos < System.nanoTime()) {\n              throw new SolrException(ErrorCode.SERVER_ERROR, \"Timed out trying to update aliases! \" +\n                  \"Either zookeeper or this node may be overloaded.\");\n            }\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        } catch (KeeperException e) {\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        }\n      }\n      if (triesLeft == 0) {\n        throw new SolrException(ErrorCode.SERVER_ERROR, \"Too many successive version failures trying to update aliases\");\n      }\n    }\n\n","bugFix":["215e40821821b2df2e69355e208532c05ef095a5"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"db3a95645baea7e03cf8ae62147cba606639004e","date":1525745850,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/common/cloud/ZkStateReader.AliasesManager#applyModificationAndExportToZk(UnaryOperator[Aliases]).mjava","pathOld":"solr/solrj/src/java/org/apache/solr/common/cloud/ZkStateReader.AliasesManager#applyModificationAndExportToZk(UnaryOperator[Aliases]).mjava","sourceNew":"    /**\n     * Writes an updated {@link Aliases} to zk.\n     * It will retry if there are races with other modifications, giving up after 30 seconds with a SolrException.\n     * The caller should understand it's possible the aliases has further changed if it examines it.\n     */\n    public void applyModificationAndExportToZk(UnaryOperator<Aliases> op) {\n      final long deadlineNanos = System.nanoTime() + TimeUnit.SECONDS.toNanos(30);\n      // note: triesLeft tuning is based on ConcurrentCreateRoutedAliasTest\n      for (int triesLeft = 30; triesLeft > 0; triesLeft--) {\n        // we could synchronize on \"this\" but there doesn't seem to be a point; we have a retry loop.\n        Aliases curAliases = getAliases();\n        Aliases modAliases = op.apply(curAliases);\n        final byte[] modAliasesJson = modAliases.toJSON();\n        if (curAliases == modAliases) {\n          LOG.debug(\"Current aliases has the desired modification; no further ZK interaction needed.\");\n          return;\n        }\n\n        try {\n          try {\n            final Stat stat = getZkClient().setData(ALIASES, modAliasesJson, curAliases.getZNodeVersion(), true);\n            setIfNewer(Aliases.fromJSON(modAliasesJson, stat.getVersion()));\n            return;\n          } catch (KeeperException.BadVersionException e) {\n            LOG.debug(e.toString(), e);\n            LOG.warn(\"Couldn't save aliases due to race with another modification; will update and retry until timeout\");\n            // considered a backoff here, but we really do want to compete strongly since the normal case is\n            // that we will do one update and succeed. This is left as a hot loop for limited tries intentionally.\n            // More failures than that here probably indicate a bug or a very strange high write frequency usage for\n            // aliases.json, timeouts mean zk is being very slow to respond, or this node is being crushed\n            // by other processing and just can't find any cpu cycles at all.\n            update();\n            if (deadlineNanos < System.nanoTime()) {\n              throw new SolrException(ErrorCode.SERVER_ERROR, \"Timed out trying to update aliases! \" +\n                  \"Either zookeeper or this node may be overloaded.\");\n            }\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        } catch (KeeperException e) {\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        }\n      }\n      throw new SolrException(ErrorCode.SERVER_ERROR, \"Too many successive version failures trying to update aliases\");\n    }\n\n","sourceOld":"    /**\n     * Writes an updated {@link Aliases} to zk.\n     * It will retry if there are races with other modifications, giving up after 30 seconds with a SolrException.\n     * The caller should understand it's possible the aliases has further changed if it examines it.\n     */\n    public void applyModificationAndExportToZk(UnaryOperator<Aliases> op) {\n      final long deadlineNanos = System.nanoTime() + TimeUnit.SECONDS.toNanos(30);\n      // note: triesLeft tuning is based on ConcurrentCreateRoutedAliasTest\n      int triesLeft = 30;\n      while (triesLeft > 0) {\n        triesLeft--;\n        // we could synchronize on \"this\" but there doesn't seem to be a point; we have a retry loop.\n        Aliases curAliases = getAliases();\n        Aliases modAliases = op.apply(curAliases);\n        final byte[] modAliasesJson = modAliases.toJSON();\n        if (curAliases == modAliases) {\n          LOG.debug(\"Current aliases has the desired modification; no further ZK interaction needed.\");\n          return;\n        }\n\n        try {\n          try {\n            final Stat stat = getZkClient().setData(ALIASES, modAliasesJson, curAliases.getZNodeVersion(), true);\n            setIfNewer(Aliases.fromJSON(modAliasesJson, stat.getVersion()));\n            return;\n          } catch (KeeperException.BadVersionException e) {\n            LOG.debug(e.toString(), e);\n            LOG.warn(\"Couldn't save aliases due to race with another modification; will update and retry until timeout\");\n            // considered a backoff here, but we really do want to compete strongly since the normal case is\n            // that we will do one update and succeed. This is left as a hot loop for limited tries intentionally.\n            // More failures than that here probably indicate a bug or a very strange high write frequency usage for\n            // aliases.json, timeouts mean zk is being very slow to respond, or this node is being crushed\n            // by other processing and just can't find any cpu cycles at all.\n            update();\n            if (deadlineNanos < System.nanoTime()) {\n              throw new SolrException(ErrorCode.SERVER_ERROR, \"Timed out trying to update aliases! \" +\n                  \"Either zookeeper or this node may be overloaded.\");\n            }\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        } catch (KeeperException e) {\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        }\n      }\n      if (triesLeft == 0) {\n        throw new SolrException(ErrorCode.SERVER_ERROR, \"Too many successive version failures trying to update aliases\");\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d3de3780a0f74036499aad4df2a73159b14fbad2","date":1529604012,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/common/cloud/ZkStateReader.AliasesManager#applyModificationAndExportToZk(UnaryOperator[Aliases]).mjava","pathOld":"solr/solrj/src/java/org/apache/solr/common/cloud/ZkStateReader.AliasesManager#applyModificationAndExportToZk(UnaryOperator[Aliases]).mjava","sourceNew":"    /**\n     * Writes an updated {@link Aliases} to zk.\n     * It will retry if there are races with other modifications, giving up after 30 seconds with a SolrException.\n     * The caller should understand it's possible the aliases has further changed if it examines it.\n     */\n    public void applyModificationAndExportToZk(UnaryOperator<Aliases> op) {\n      // The current aliases hasn't been update()'ed yet -- which is impossible?  Any way just update it first.\n      if (aliases.getZNodeVersion() == -1) {\n        try {\n          boolean updated = update();\n          assert updated;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        } catch (KeeperException e) {\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        }\n      }\n\n      final long deadlineNanos = System.nanoTime() + TimeUnit.SECONDS.toNanos(30);\n      // note: triesLeft tuning is based on ConcurrentCreateRoutedAliasTest\n      for (int triesLeft = 30; triesLeft > 0; triesLeft--) {\n        // we could synchronize on \"this\" but there doesn't seem to be a point; we have a retry loop.\n        Aliases curAliases = getAliases();\n        Aliases modAliases = op.apply(curAliases);\n        final byte[] modAliasesJson = modAliases.toJSON();\n        if (curAliases == modAliases) {\n          LOG.debug(\"Current aliases has the desired modification; no further ZK interaction needed.\");\n          return;\n        }\n\n        try {\n          try {\n            final Stat stat = getZkClient().setData(ALIASES, modAliasesJson, curAliases.getZNodeVersion(), true);\n            setIfNewer(Aliases.fromJSON(modAliasesJson, stat.getVersion()));\n            return;\n          } catch (KeeperException.BadVersionException e) {\n            LOG.debug(e.toString(), e);\n            LOG.warn(\"Couldn't save aliases due to race with another modification; will update and retry until timeout\");\n            // considered a backoff here, but we really do want to compete strongly since the normal case is\n            // that we will do one update and succeed. This is left as a hot loop for limited tries intentionally.\n            // More failures than that here probably indicate a bug or a very strange high write frequency usage for\n            // aliases.json, timeouts mean zk is being very slow to respond, or this node is being crushed\n            // by other processing and just can't find any cpu cycles at all.\n            update();\n            if (deadlineNanos < System.nanoTime()) {\n              throw new SolrException(ErrorCode.SERVER_ERROR, \"Timed out trying to update aliases! \" +\n                  \"Either zookeeper or this node may be overloaded.\");\n            }\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        } catch (KeeperException e) {\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        }\n      }\n      throw new SolrException(ErrorCode.SERVER_ERROR, \"Too many successive version failures trying to update aliases\");\n    }\n\n","sourceOld":"    /**\n     * Writes an updated {@link Aliases} to zk.\n     * It will retry if there are races with other modifications, giving up after 30 seconds with a SolrException.\n     * The caller should understand it's possible the aliases has further changed if it examines it.\n     */\n    public void applyModificationAndExportToZk(UnaryOperator<Aliases> op) {\n      final long deadlineNanos = System.nanoTime() + TimeUnit.SECONDS.toNanos(30);\n      // note: triesLeft tuning is based on ConcurrentCreateRoutedAliasTest\n      for (int triesLeft = 30; triesLeft > 0; triesLeft--) {\n        // we could synchronize on \"this\" but there doesn't seem to be a point; we have a retry loop.\n        Aliases curAliases = getAliases();\n        Aliases modAliases = op.apply(curAliases);\n        final byte[] modAliasesJson = modAliases.toJSON();\n        if (curAliases == modAliases) {\n          LOG.debug(\"Current aliases has the desired modification; no further ZK interaction needed.\");\n          return;\n        }\n\n        try {\n          try {\n            final Stat stat = getZkClient().setData(ALIASES, modAliasesJson, curAliases.getZNodeVersion(), true);\n            setIfNewer(Aliases.fromJSON(modAliasesJson, stat.getVersion()));\n            return;\n          } catch (KeeperException.BadVersionException e) {\n            LOG.debug(e.toString(), e);\n            LOG.warn(\"Couldn't save aliases due to race with another modification; will update and retry until timeout\");\n            // considered a backoff here, but we really do want to compete strongly since the normal case is\n            // that we will do one update and succeed. This is left as a hot loop for limited tries intentionally.\n            // More failures than that here probably indicate a bug or a very strange high write frequency usage for\n            // aliases.json, timeouts mean zk is being very slow to respond, or this node is being crushed\n            // by other processing and just can't find any cpu cycles at all.\n            update();\n            if (deadlineNanos < System.nanoTime()) {\n              throw new SolrException(ErrorCode.SERVER_ERROR, \"Timed out trying to update aliases! \" +\n                  \"Either zookeeper or this node may be overloaded.\");\n            }\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        } catch (KeeperException e) {\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        }\n      }\n      throw new SolrException(ErrorCode.SERVER_ERROR, \"Too many successive version failures trying to update aliases\");\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe736fc2dca4bb2af36fa478366c29db9e31a1f9","date":1529608800,"type":3,"author":"Chris Hostetter","isMerge":true,"pathNew":"solr/solrj/src/java/org/apache/solr/common/cloud/ZkStateReader.AliasesManager#applyModificationAndExportToZk(UnaryOperator[Aliases]).mjava","pathOld":"solr/solrj/src/java/org/apache/solr/common/cloud/ZkStateReader.AliasesManager#applyModificationAndExportToZk(UnaryOperator[Aliases]).mjava","sourceNew":"    /**\n     * Writes an updated {@link Aliases} to zk.\n     * It will retry if there are races with other modifications, giving up after 30 seconds with a SolrException.\n     * The caller should understand it's possible the aliases has further changed if it examines it.\n     */\n    public void applyModificationAndExportToZk(UnaryOperator<Aliases> op) {\n      // The current aliases hasn't been update()'ed yet -- which is impossible?  Any way just update it first.\n      if (aliases.getZNodeVersion() == -1) {\n        try {\n          boolean updated = update();\n          assert updated;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        } catch (KeeperException e) {\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        }\n      }\n\n      final long deadlineNanos = System.nanoTime() + TimeUnit.SECONDS.toNanos(30);\n      // note: triesLeft tuning is based on ConcurrentCreateRoutedAliasTest\n      for (int triesLeft = 30; triesLeft > 0; triesLeft--) {\n        // we could synchronize on \"this\" but there doesn't seem to be a point; we have a retry loop.\n        Aliases curAliases = getAliases();\n        Aliases modAliases = op.apply(curAliases);\n        final byte[] modAliasesJson = modAliases.toJSON();\n        if (curAliases == modAliases) {\n          LOG.debug(\"Current aliases has the desired modification; no further ZK interaction needed.\");\n          return;\n        }\n\n        try {\n          try {\n            final Stat stat = getZkClient().setData(ALIASES, modAliasesJson, curAliases.getZNodeVersion(), true);\n            setIfNewer(Aliases.fromJSON(modAliasesJson, stat.getVersion()));\n            return;\n          } catch (KeeperException.BadVersionException e) {\n            LOG.debug(e.toString(), e);\n            LOG.warn(\"Couldn't save aliases due to race with another modification; will update and retry until timeout\");\n            // considered a backoff here, but we really do want to compete strongly since the normal case is\n            // that we will do one update and succeed. This is left as a hot loop for limited tries intentionally.\n            // More failures than that here probably indicate a bug or a very strange high write frequency usage for\n            // aliases.json, timeouts mean zk is being very slow to respond, or this node is being crushed\n            // by other processing and just can't find any cpu cycles at all.\n            update();\n            if (deadlineNanos < System.nanoTime()) {\n              throw new SolrException(ErrorCode.SERVER_ERROR, \"Timed out trying to update aliases! \" +\n                  \"Either zookeeper or this node may be overloaded.\");\n            }\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        } catch (KeeperException e) {\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        }\n      }\n      throw new SolrException(ErrorCode.SERVER_ERROR, \"Too many successive version failures trying to update aliases\");\n    }\n\n","sourceOld":"    /**\n     * Writes an updated {@link Aliases} to zk.\n     * It will retry if there are races with other modifications, giving up after 30 seconds with a SolrException.\n     * The caller should understand it's possible the aliases has further changed if it examines it.\n     */\n    public void applyModificationAndExportToZk(UnaryOperator<Aliases> op) {\n      final long deadlineNanos = System.nanoTime() + TimeUnit.SECONDS.toNanos(30);\n      // note: triesLeft tuning is based on ConcurrentCreateRoutedAliasTest\n      for (int triesLeft = 30; triesLeft > 0; triesLeft--) {\n        // we could synchronize on \"this\" but there doesn't seem to be a point; we have a retry loop.\n        Aliases curAliases = getAliases();\n        Aliases modAliases = op.apply(curAliases);\n        final byte[] modAliasesJson = modAliases.toJSON();\n        if (curAliases == modAliases) {\n          LOG.debug(\"Current aliases has the desired modification; no further ZK interaction needed.\");\n          return;\n        }\n\n        try {\n          try {\n            final Stat stat = getZkClient().setData(ALIASES, modAliasesJson, curAliases.getZNodeVersion(), true);\n            setIfNewer(Aliases.fromJSON(modAliasesJson, stat.getVersion()));\n            return;\n          } catch (KeeperException.BadVersionException e) {\n            LOG.debug(e.toString(), e);\n            LOG.warn(\"Couldn't save aliases due to race with another modification; will update and retry until timeout\");\n            // considered a backoff here, but we really do want to compete strongly since the normal case is\n            // that we will do one update and succeed. This is left as a hot loop for limited tries intentionally.\n            // More failures than that here probably indicate a bug or a very strange high write frequency usage for\n            // aliases.json, timeouts mean zk is being very slow to respond, or this node is being crushed\n            // by other processing and just can't find any cpu cycles at all.\n            update();\n            if (deadlineNanos < System.nanoTime()) {\n              throw new SolrException(ErrorCode.SERVER_ERROR, \"Timed out trying to update aliases! \" +\n                  \"Either zookeeper or this node may be overloaded.\");\n            }\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        } catch (KeeperException e) {\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        }\n      }\n      throw new SolrException(ErrorCode.SERVER_ERROR, \"Too many successive version failures trying to update aliases\");\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26","date":1531589977,"type":3,"author":"Michael Braun","isMerge":true,"pathNew":"solr/solrj/src/java/org/apache/solr/common/cloud/ZkStateReader.AliasesManager#applyModificationAndExportToZk(UnaryOperator[Aliases]).mjava","pathOld":"solr/solrj/src/java/org/apache/solr/common/cloud/ZkStateReader.AliasesManager#applyModificationAndExportToZk(UnaryOperator[Aliases]).mjava","sourceNew":"    /**\n     * Writes an updated {@link Aliases} to zk.\n     * It will retry if there are races with other modifications, giving up after 30 seconds with a SolrException.\n     * The caller should understand it's possible the aliases has further changed if it examines it.\n     */\n    public void applyModificationAndExportToZk(UnaryOperator<Aliases> op) {\n      // The current aliases hasn't been update()'ed yet -- which is impossible?  Any way just update it first.\n      if (aliases.getZNodeVersion() == -1) {\n        try {\n          boolean updated = update();\n          assert updated;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        } catch (KeeperException e) {\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        }\n      }\n\n      final long deadlineNanos = System.nanoTime() + TimeUnit.SECONDS.toNanos(30);\n      // note: triesLeft tuning is based on ConcurrentCreateRoutedAliasTest\n      for (int triesLeft = 30; triesLeft > 0; triesLeft--) {\n        // we could synchronize on \"this\" but there doesn't seem to be a point; we have a retry loop.\n        Aliases curAliases = getAliases();\n        Aliases modAliases = op.apply(curAliases);\n        final byte[] modAliasesJson = modAliases.toJSON();\n        if (curAliases == modAliases) {\n          LOG.debug(\"Current aliases has the desired modification; no further ZK interaction needed.\");\n          return;\n        }\n\n        try {\n          try {\n            final Stat stat = getZkClient().setData(ALIASES, modAliasesJson, curAliases.getZNodeVersion(), true);\n            setIfNewer(Aliases.fromJSON(modAliasesJson, stat.getVersion()));\n            return;\n          } catch (KeeperException.BadVersionException e) {\n            LOG.debug(e.toString(), e);\n            LOG.warn(\"Couldn't save aliases due to race with another modification; will update and retry until timeout\");\n            // considered a backoff here, but we really do want to compete strongly since the normal case is\n            // that we will do one update and succeed. This is left as a hot loop for limited tries intentionally.\n            // More failures than that here probably indicate a bug or a very strange high write frequency usage for\n            // aliases.json, timeouts mean zk is being very slow to respond, or this node is being crushed\n            // by other processing and just can't find any cpu cycles at all.\n            update();\n            if (deadlineNanos < System.nanoTime()) {\n              throw new SolrException(ErrorCode.SERVER_ERROR, \"Timed out trying to update aliases! \" +\n                  \"Either zookeeper or this node may be overloaded.\");\n            }\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        } catch (KeeperException e) {\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        }\n      }\n      throw new SolrException(ErrorCode.SERVER_ERROR, \"Too many successive version failures trying to update aliases\");\n    }\n\n","sourceOld":"    /**\n     * Writes an updated {@link Aliases} to zk.\n     * It will retry if there are races with other modifications, giving up after 30 seconds with a SolrException.\n     * The caller should understand it's possible the aliases has further changed if it examines it.\n     */\n    public void applyModificationAndExportToZk(UnaryOperator<Aliases> op) {\n      final long deadlineNanos = System.nanoTime() + TimeUnit.SECONDS.toNanos(30);\n      // note: triesLeft tuning is based on ConcurrentCreateRoutedAliasTest\n      for (int triesLeft = 30; triesLeft > 0; triesLeft--) {\n        // we could synchronize on \"this\" but there doesn't seem to be a point; we have a retry loop.\n        Aliases curAliases = getAliases();\n        Aliases modAliases = op.apply(curAliases);\n        final byte[] modAliasesJson = modAliases.toJSON();\n        if (curAliases == modAliases) {\n          LOG.debug(\"Current aliases has the desired modification; no further ZK interaction needed.\");\n          return;\n        }\n\n        try {\n          try {\n            final Stat stat = getZkClient().setData(ALIASES, modAliasesJson, curAliases.getZNodeVersion(), true);\n            setIfNewer(Aliases.fromJSON(modAliasesJson, stat.getVersion()));\n            return;\n          } catch (KeeperException.BadVersionException e) {\n            LOG.debug(e.toString(), e);\n            LOG.warn(\"Couldn't save aliases due to race with another modification; will update and retry until timeout\");\n            // considered a backoff here, but we really do want to compete strongly since the normal case is\n            // that we will do one update and succeed. This is left as a hot loop for limited tries intentionally.\n            // More failures than that here probably indicate a bug or a very strange high write frequency usage for\n            // aliases.json, timeouts mean zk is being very slow to respond, or this node is being crushed\n            // by other processing and just can't find any cpu cycles at all.\n            update();\n            if (deadlineNanos < System.nanoTime()) {\n              throw new SolrException(ErrorCode.SERVER_ERROR, \"Timed out trying to update aliases! \" +\n                  \"Either zookeeper or this node may be overloaded.\");\n            }\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        } catch (KeeperException e) {\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        }\n      }\n      throw new SolrException(ErrorCode.SERVER_ERROR, \"Too many successive version failures trying to update aliases\");\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","date":1531905561,"type":3,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"solr/solrj/src/java/org/apache/solr/common/cloud/ZkStateReader.AliasesManager#applyModificationAndExportToZk(UnaryOperator[Aliases]).mjava","pathOld":"solr/solrj/src/java/org/apache/solr/common/cloud/ZkStateReader.AliasesManager#applyModificationAndExportToZk(UnaryOperator[Aliases]).mjava","sourceNew":"    /**\n     * Writes an updated {@link Aliases} to zk.\n     * It will retry if there are races with other modifications, giving up after 30 seconds with a SolrException.\n     * The caller should understand it's possible the aliases has further changed if it examines it.\n     */\n    public void applyModificationAndExportToZk(UnaryOperator<Aliases> op) {\n      // The current aliases hasn't been update()'ed yet -- which is impossible?  Any way just update it first.\n      if (aliases.getZNodeVersion() == -1) {\n        try {\n          boolean updated = update();\n          assert updated;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        } catch (KeeperException e) {\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        }\n      }\n\n      final long deadlineNanos = System.nanoTime() + TimeUnit.SECONDS.toNanos(30);\n      // note: triesLeft tuning is based on ConcurrentCreateRoutedAliasTest\n      for (int triesLeft = 30; triesLeft > 0; triesLeft--) {\n        // we could synchronize on \"this\" but there doesn't seem to be a point; we have a retry loop.\n        Aliases curAliases = getAliases();\n        Aliases modAliases = op.apply(curAliases);\n        final byte[] modAliasesJson = modAliases.toJSON();\n        if (curAliases == modAliases) {\n          LOG.debug(\"Current aliases has the desired modification; no further ZK interaction needed.\");\n          return;\n        }\n\n        try {\n          try {\n            final Stat stat = getZkClient().setData(ALIASES, modAliasesJson, curAliases.getZNodeVersion(), true);\n            setIfNewer(Aliases.fromJSON(modAliasesJson, stat.getVersion()));\n            return;\n          } catch (KeeperException.BadVersionException e) {\n            LOG.debug(e.toString(), e);\n            LOG.warn(\"Couldn't save aliases due to race with another modification; will update and retry until timeout\");\n            // considered a backoff here, but we really do want to compete strongly since the normal case is\n            // that we will do one update and succeed. This is left as a hot loop for limited tries intentionally.\n            // More failures than that here probably indicate a bug or a very strange high write frequency usage for\n            // aliases.json, timeouts mean zk is being very slow to respond, or this node is being crushed\n            // by other processing and just can't find any cpu cycles at all.\n            update();\n            if (deadlineNanos < System.nanoTime()) {\n              throw new SolrException(ErrorCode.SERVER_ERROR, \"Timed out trying to update aliases! \" +\n                  \"Either zookeeper or this node may be overloaded.\");\n            }\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        } catch (KeeperException e) {\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        }\n      }\n      throw new SolrException(ErrorCode.SERVER_ERROR, \"Too many successive version failures trying to update aliases\");\n    }\n\n","sourceOld":"    /**\n     * Writes an updated {@link Aliases} to zk.\n     * It will retry if there are races with other modifications, giving up after 30 seconds with a SolrException.\n     * The caller should understand it's possible the aliases has further changed if it examines it.\n     */\n    public void applyModificationAndExportToZk(UnaryOperator<Aliases> op) {\n      final long deadlineNanos = System.nanoTime() + TimeUnit.SECONDS.toNanos(30);\n      // note: triesLeft tuning is based on ConcurrentCreateRoutedAliasTest\n      for (int triesLeft = 30; triesLeft > 0; triesLeft--) {\n        // we could synchronize on \"this\" but there doesn't seem to be a point; we have a retry loop.\n        Aliases curAliases = getAliases();\n        Aliases modAliases = op.apply(curAliases);\n        final byte[] modAliasesJson = modAliases.toJSON();\n        if (curAliases == modAliases) {\n          LOG.debug(\"Current aliases has the desired modification; no further ZK interaction needed.\");\n          return;\n        }\n\n        try {\n          try {\n            final Stat stat = getZkClient().setData(ALIASES, modAliasesJson, curAliases.getZNodeVersion(), true);\n            setIfNewer(Aliases.fromJSON(modAliasesJson, stat.getVersion()));\n            return;\n          } catch (KeeperException.BadVersionException e) {\n            LOG.debug(e.toString(), e);\n            LOG.warn(\"Couldn't save aliases due to race with another modification; will update and retry until timeout\");\n            // considered a backoff here, but we really do want to compete strongly since the normal case is\n            // that we will do one update and succeed. This is left as a hot loop for limited tries intentionally.\n            // More failures than that here probably indicate a bug or a very strange high write frequency usage for\n            // aliases.json, timeouts mean zk is being very slow to respond, or this node is being crushed\n            // by other processing and just can't find any cpu cycles at all.\n            update();\n            if (deadlineNanos < System.nanoTime()) {\n              throw new SolrException(ErrorCode.SERVER_ERROR, \"Timed out trying to update aliases! \" +\n                  \"Either zookeeper or this node may be overloaded.\");\n            }\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        } catch (KeeperException e) {\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        }\n      }\n      throw new SolrException(ErrorCode.SERVER_ERROR, \"Too many successive version failures trying to update aliases\");\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e9c81f7e703d7ccca5bc78beb61253f0a8a22afd","date":1534976797,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/common/cloud/ZkStateReader.AliasesManager#applyModificationAndExportToZk(UnaryOperator[Aliases]).mjava","pathOld":"solr/solrj/src/java/org/apache/solr/common/cloud/ZkStateReader.AliasesManager#applyModificationAndExportToZk(UnaryOperator[Aliases]).mjava","sourceNew":"    /**\n     * Writes an updated {@link Aliases} to zk.\n     * It will retry if there are races with other modifications, giving up after 30 seconds with a SolrException.\n     * The caller should understand it's possible the aliases has further changed if it examines it.\n     */\n    public void applyModificationAndExportToZk(UnaryOperator<Aliases> op) {\n      // The current aliases hasn't been update()'ed yet -- which is impossible?  Any way just update it first.\n      if (aliases.getZNodeVersion() == -1) {\n        try {\n          boolean updated = update();\n          assert updated;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        } catch (KeeperException e) {\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        }\n      }\n\n      final long deadlineNanos = System.nanoTime() + TimeUnit.SECONDS.toNanos(30);\n      // note: triesLeft tuning is based on ConcurrentCreateRoutedAliasTest\n      for (int triesLeft = 30; triesLeft > 0; triesLeft--) {\n        // we could synchronize on \"this\" but there doesn't seem to be a point; we have a retry loop.\n        Aliases curAliases = getAliases();\n        Aliases modAliases = op.apply(curAliases);\n        final byte[] modAliasesJson = modAliases.toJSON();\n        if (curAliases == modAliases) {\n          log.debug(\"Current aliases has the desired modification; no further ZK interaction needed.\");\n          return;\n        }\n\n        try {\n          try {\n            final Stat stat = getZkClient().setData(ALIASES, modAliasesJson, curAliases.getZNodeVersion(), true);\n            setIfNewer(Aliases.fromJSON(modAliasesJson, stat.getVersion()));\n            return;\n          } catch (KeeperException.BadVersionException e) {\n            log.debug(e.toString(), e);\n            log.warn(\"Couldn't save aliases due to race with another modification; will update and retry until timeout\");\n            // considered a backoff here, but we really do want to compete strongly since the normal case is\n            // that we will do one update and succeed. This is left as a hot loop for limited tries intentionally.\n            // More failures than that here probably indicate a bug or a very strange high write frequency usage for\n            // aliases.json, timeouts mean zk is being very slow to respond, or this node is being crushed\n            // by other processing and just can't find any cpu cycles at all.\n            update();\n            if (deadlineNanos < System.nanoTime()) {\n              throw new SolrException(ErrorCode.SERVER_ERROR, \"Timed out trying to update aliases! \" +\n                  \"Either zookeeper or this node may be overloaded.\");\n            }\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        } catch (KeeperException e) {\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        }\n      }\n      throw new SolrException(ErrorCode.SERVER_ERROR, \"Too many successive version failures trying to update aliases\");\n    }\n\n","sourceOld":"    /**\n     * Writes an updated {@link Aliases} to zk.\n     * It will retry if there are races with other modifications, giving up after 30 seconds with a SolrException.\n     * The caller should understand it's possible the aliases has further changed if it examines it.\n     */\n    public void applyModificationAndExportToZk(UnaryOperator<Aliases> op) {\n      // The current aliases hasn't been update()'ed yet -- which is impossible?  Any way just update it first.\n      if (aliases.getZNodeVersion() == -1) {\n        try {\n          boolean updated = update();\n          assert updated;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        } catch (KeeperException e) {\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        }\n      }\n\n      final long deadlineNanos = System.nanoTime() + TimeUnit.SECONDS.toNanos(30);\n      // note: triesLeft tuning is based on ConcurrentCreateRoutedAliasTest\n      for (int triesLeft = 30; triesLeft > 0; triesLeft--) {\n        // we could synchronize on \"this\" but there doesn't seem to be a point; we have a retry loop.\n        Aliases curAliases = getAliases();\n        Aliases modAliases = op.apply(curAliases);\n        final byte[] modAliasesJson = modAliases.toJSON();\n        if (curAliases == modAliases) {\n          LOG.debug(\"Current aliases has the desired modification; no further ZK interaction needed.\");\n          return;\n        }\n\n        try {\n          try {\n            final Stat stat = getZkClient().setData(ALIASES, modAliasesJson, curAliases.getZNodeVersion(), true);\n            setIfNewer(Aliases.fromJSON(modAliasesJson, stat.getVersion()));\n            return;\n          } catch (KeeperException.BadVersionException e) {\n            LOG.debug(e.toString(), e);\n            LOG.warn(\"Couldn't save aliases due to race with another modification; will update and retry until timeout\");\n            // considered a backoff here, but we really do want to compete strongly since the normal case is\n            // that we will do one update and succeed. This is left as a hot loop for limited tries intentionally.\n            // More failures than that here probably indicate a bug or a very strange high write frequency usage for\n            // aliases.json, timeouts mean zk is being very slow to respond, or this node is being crushed\n            // by other processing and just can't find any cpu cycles at all.\n            update();\n            if (deadlineNanos < System.nanoTime()) {\n              throw new SolrException(ErrorCode.SERVER_ERROR, \"Timed out trying to update aliases! \" +\n                  \"Either zookeeper or this node may be overloaded.\");\n            }\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        } catch (KeeperException e) {\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        }\n      }\n      throw new SolrException(ErrorCode.SERVER_ERROR, \"Too many successive version failures trying to update aliases\");\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6dc613ed6f75d1988140301ee8de8fdb056fa337","date":1588034757,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/common/cloud/ZkStateReader.AliasesManager#applyModificationAndExportToZk(UnaryOperator[Aliases]).mjava","pathOld":"solr/solrj/src/java/org/apache/solr/common/cloud/ZkStateReader.AliasesManager#applyModificationAndExportToZk(UnaryOperator[Aliases]).mjava","sourceNew":"    /**\n     * Writes an updated {@link Aliases} to zk.\n     * It will retry if there are races with other modifications, giving up after 30 seconds with a SolrException.\n     * The caller should understand it's possible the aliases has further changed if it examines it.\n     */\n    public void applyModificationAndExportToZk(UnaryOperator<Aliases> op) {\n      // The current aliases hasn't been update()'ed yet -- which is impossible?  Any way just update it first.\n      if (aliases.getZNodeVersion() == -1) {\n        try {\n          boolean updated = update();\n          assert updated;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        } catch (KeeperException e) {\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        }\n      }\n\n      final long deadlineNanos = System.nanoTime() + TimeUnit.SECONDS.toNanos(30);\n      // note: triesLeft tuning is based on ConcurrentCreateRoutedAliasTest\n      for (int triesLeft = 30; triesLeft > 0; triesLeft--) {\n        // we could synchronize on \"this\" but there doesn't seem to be a point; we have a retry loop.\n        Aliases curAliases = getAliases();\n        Aliases modAliases = op.apply(curAliases);\n        final byte[] modAliasesJson = modAliases.toJSON();\n        if (curAliases == modAliases) {\n          log.debug(\"Current aliases has the desired modification; no further ZK interaction needed.\");\n          return;\n        }\n\n        try {\n          try {\n            final Stat stat = getZkClient().setData(ALIASES, modAliasesJson, curAliases.getZNodeVersion(), true);\n            setIfNewer(Aliases.fromJSON(modAliasesJson, stat.getVersion()));\n            return;\n          } catch (KeeperException.BadVersionException e) {\n            log.debug(\"{}\", e.toString(), e); // logOk\n            log.warn(\"Couldn't save aliases due to race with another modification; will update and retry until timeout\");\n            // considered a backoff here, but we really do want to compete strongly since the normal case is\n            // that we will do one update and succeed. This is left as a hot loop for limited tries intentionally.\n            // More failures than that here probably indicate a bug or a very strange high write frequency usage for\n            // aliases.json, timeouts mean zk is being very slow to respond, or this node is being crushed\n            // by other processing and just can't find any cpu cycles at all.\n            update();\n            if (deadlineNanos < System.nanoTime()) {\n              throw new SolrException(ErrorCode.SERVER_ERROR, \"Timed out trying to update aliases! \" +\n                  \"Either zookeeper or this node may be overloaded.\");\n            }\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        } catch (KeeperException e) {\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        }\n      }\n      throw new SolrException(ErrorCode.SERVER_ERROR, \"Too many successive version failures trying to update aliases\");\n    }\n\n","sourceOld":"    /**\n     * Writes an updated {@link Aliases} to zk.\n     * It will retry if there are races with other modifications, giving up after 30 seconds with a SolrException.\n     * The caller should understand it's possible the aliases has further changed if it examines it.\n     */\n    public void applyModificationAndExportToZk(UnaryOperator<Aliases> op) {\n      // The current aliases hasn't been update()'ed yet -- which is impossible?  Any way just update it first.\n      if (aliases.getZNodeVersion() == -1) {\n        try {\n          boolean updated = update();\n          assert updated;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        } catch (KeeperException e) {\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        }\n      }\n\n      final long deadlineNanos = System.nanoTime() + TimeUnit.SECONDS.toNanos(30);\n      // note: triesLeft tuning is based on ConcurrentCreateRoutedAliasTest\n      for (int triesLeft = 30; triesLeft > 0; triesLeft--) {\n        // we could synchronize on \"this\" but there doesn't seem to be a point; we have a retry loop.\n        Aliases curAliases = getAliases();\n        Aliases modAliases = op.apply(curAliases);\n        final byte[] modAliasesJson = modAliases.toJSON();\n        if (curAliases == modAliases) {\n          log.debug(\"Current aliases has the desired modification; no further ZK interaction needed.\");\n          return;\n        }\n\n        try {\n          try {\n            final Stat stat = getZkClient().setData(ALIASES, modAliasesJson, curAliases.getZNodeVersion(), true);\n            setIfNewer(Aliases.fromJSON(modAliasesJson, stat.getVersion()));\n            return;\n          } catch (KeeperException.BadVersionException e) {\n            log.debug(e.toString(), e);\n            log.warn(\"Couldn't save aliases due to race with another modification; will update and retry until timeout\");\n            // considered a backoff here, but we really do want to compete strongly since the normal case is\n            // that we will do one update and succeed. This is left as a hot loop for limited tries intentionally.\n            // More failures than that here probably indicate a bug or a very strange high write frequency usage for\n            // aliases.json, timeouts mean zk is being very slow to respond, or this node is being crushed\n            // by other processing and just can't find any cpu cycles at all.\n            update();\n            if (deadlineNanos < System.nanoTime()) {\n              throw new SolrException(ErrorCode.SERVER_ERROR, \"Timed out trying to update aliases! \" +\n                  \"Either zookeeper or this node may be overloaded.\");\n            }\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        } catch (KeeperException e) {\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        }\n      }\n      throw new SolrException(ErrorCode.SERVER_ERROR, \"Too many successive version failures trying to update aliases\");\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe9f2c4a0d7ac164e4bdd4eee7f87131aec83fd4","date":1588172214,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/common/cloud/ZkStateReader.AliasesManager#applyModificationAndExportToZk(UnaryOperator[Aliases]).mjava","pathOld":"solr/solrj/src/java/org/apache/solr/common/cloud/ZkStateReader.AliasesManager#applyModificationAndExportToZk(UnaryOperator[Aliases]).mjava","sourceNew":"    /**\n     * Writes an updated {@link Aliases} to zk.\n     * It will retry if there are races with other modifications, giving up after 30 seconds with a SolrException.\n     * The caller should understand it's possible the aliases has further changed if it examines it.\n     */\n    public void applyModificationAndExportToZk(UnaryOperator<Aliases> op) {\n      // The current aliases hasn't been update()'ed yet -- which is impossible?  Any way just update it first.\n      if (aliases.getZNodeVersion() == -1) {\n        try {\n          boolean updated = update();\n          assert updated;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        } catch (KeeperException e) {\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        }\n      }\n\n      final long deadlineNanos = System.nanoTime() + TimeUnit.SECONDS.toNanos(30);\n      // note: triesLeft tuning is based on ConcurrentCreateRoutedAliasTest\n      for (int triesLeft = 30; triesLeft > 0; triesLeft--) {\n        // we could synchronize on \"this\" but there doesn't seem to be a point; we have a retry loop.\n        Aliases curAliases = getAliases();\n        Aliases modAliases = op.apply(curAliases);\n        final byte[] modAliasesJson = modAliases.toJSON();\n        if (curAliases == modAliases) {\n          log.debug(\"Current aliases has the desired modification; no further ZK interaction needed.\");\n          return;\n        }\n\n        try {\n          try {\n            final Stat stat = getZkClient().setData(ALIASES, modAliasesJson, curAliases.getZNodeVersion(), true);\n            setIfNewer(Aliases.fromJSON(modAliasesJson, stat.getVersion()));\n            return;\n          } catch (KeeperException.BadVersionException e) {\n            log.debug(\"{}\", e, e);\n            log.warn(\"Couldn't save aliases due to race with another modification; will update and retry until timeout\");\n            // considered a backoff here, but we really do want to compete strongly since the normal case is\n            // that we will do one update and succeed. This is left as a hot loop for limited tries intentionally.\n            // More failures than that here probably indicate a bug or a very strange high write frequency usage for\n            // aliases.json, timeouts mean zk is being very slow to respond, or this node is being crushed\n            // by other processing and just can't find any cpu cycles at all.\n            update();\n            if (deadlineNanos < System.nanoTime()) {\n              throw new SolrException(ErrorCode.SERVER_ERROR, \"Timed out trying to update aliases! \" +\n                  \"Either zookeeper or this node may be overloaded.\");\n            }\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        } catch (KeeperException e) {\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        }\n      }\n      throw new SolrException(ErrorCode.SERVER_ERROR, \"Too many successive version failures trying to update aliases\");\n    }\n\n","sourceOld":"    /**\n     * Writes an updated {@link Aliases} to zk.\n     * It will retry if there are races with other modifications, giving up after 30 seconds with a SolrException.\n     * The caller should understand it's possible the aliases has further changed if it examines it.\n     */\n    public void applyModificationAndExportToZk(UnaryOperator<Aliases> op) {\n      // The current aliases hasn't been update()'ed yet -- which is impossible?  Any way just update it first.\n      if (aliases.getZNodeVersion() == -1) {\n        try {\n          boolean updated = update();\n          assert updated;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        } catch (KeeperException e) {\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        }\n      }\n\n      final long deadlineNanos = System.nanoTime() + TimeUnit.SECONDS.toNanos(30);\n      // note: triesLeft tuning is based on ConcurrentCreateRoutedAliasTest\n      for (int triesLeft = 30; triesLeft > 0; triesLeft--) {\n        // we could synchronize on \"this\" but there doesn't seem to be a point; we have a retry loop.\n        Aliases curAliases = getAliases();\n        Aliases modAliases = op.apply(curAliases);\n        final byte[] modAliasesJson = modAliases.toJSON();\n        if (curAliases == modAliases) {\n          log.debug(\"Current aliases has the desired modification; no further ZK interaction needed.\");\n          return;\n        }\n\n        try {\n          try {\n            final Stat stat = getZkClient().setData(ALIASES, modAliasesJson, curAliases.getZNodeVersion(), true);\n            setIfNewer(Aliases.fromJSON(modAliasesJson, stat.getVersion()));\n            return;\n          } catch (KeeperException.BadVersionException e) {\n            log.debug(\"{}\", e.toString(), e); // logOk\n            log.warn(\"Couldn't save aliases due to race with another modification; will update and retry until timeout\");\n            // considered a backoff here, but we really do want to compete strongly since the normal case is\n            // that we will do one update and succeed. This is left as a hot loop for limited tries intentionally.\n            // More failures than that here probably indicate a bug or a very strange high write frequency usage for\n            // aliases.json, timeouts mean zk is being very slow to respond, or this node is being crushed\n            // by other processing and just can't find any cpu cycles at all.\n            update();\n            if (deadlineNanos < System.nanoTime()) {\n              throw new SolrException(ErrorCode.SERVER_ERROR, \"Timed out trying to update aliases! \" +\n                  \"Either zookeeper or this node may be overloaded.\");\n            }\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        } catch (KeeperException e) {\n          throw new ZooKeeperException(ErrorCode.SERVER_ERROR, e.toString(), e);\n        }\n      }\n      throw new SolrException(ErrorCode.SERVER_ERROR, \"Too many successive version failures trying to update aliases\");\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"fe736fc2dca4bb2af36fa478366c29db9e31a1f9":["db3a95645baea7e03cf8ae62147cba606639004e","d3de3780a0f74036499aad4df2a73159b14fbad2"],"d3de3780a0f74036499aad4df2a73159b14fbad2":["db3a95645baea7e03cf8ae62147cba606639004e"],"6dc613ed6f75d1988140301ee8de8fdb056fa337":["e9c81f7e703d7ccca5bc78beb61253f0a8a22afd"],"6857f8205f8c5b4ff39a54d8aebb4fdfb7cfb691":["215e40821821b2df2e69355e208532c05ef095a5"],"a980795a56f2fcbc94caeb3233071312d5684d59":["6857f8205f8c5b4ff39a54d8aebb4fdfb7cfb691"],"215e40821821b2df2e69355e208532c05ef095a5":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":["db3a95645baea7e03cf8ae62147cba606639004e","fe736fc2dca4bb2af36fa478366c29db9e31a1f9"],"fe9f2c4a0d7ac164e4bdd4eee7f87131aec83fd4":["6dc613ed6f75d1988140301ee8de8fdb056fa337"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"db3a95645baea7e03cf8ae62147cba606639004e":["a980795a56f2fcbc94caeb3233071312d5684d59"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["fe9f2c4a0d7ac164e4bdd4eee7f87131aec83fd4"],"e9c81f7e703d7ccca5bc78beb61253f0a8a22afd":["fe736fc2dca4bb2af36fa478366c29db9e31a1f9"],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":["db3a95645baea7e03cf8ae62147cba606639004e","fe736fc2dca4bb2af36fa478366c29db9e31a1f9"]},"commit2Childs":{"fe736fc2dca4bb2af36fa478366c29db9e31a1f9":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","e9c81f7e703d7ccca5bc78beb61253f0a8a22afd","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"d3de3780a0f74036499aad4df2a73159b14fbad2":["fe736fc2dca4bb2af36fa478366c29db9e31a1f9"],"6dc613ed6f75d1988140301ee8de8fdb056fa337":["fe9f2c4a0d7ac164e4bdd4eee7f87131aec83fd4"],"6857f8205f8c5b4ff39a54d8aebb4fdfb7cfb691":["a980795a56f2fcbc94caeb3233071312d5684d59"],"215e40821821b2df2e69355e208532c05ef095a5":["6857f8205f8c5b4ff39a54d8aebb4fdfb7cfb691"],"a980795a56f2fcbc94caeb3233071312d5684d59":["db3a95645baea7e03cf8ae62147cba606639004e"],"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["215e40821821b2df2e69355e208532c05ef095a5"],"fe9f2c4a0d7ac164e4bdd4eee7f87131aec83fd4":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"db3a95645baea7e03cf8ae62147cba606639004e":["fe736fc2dca4bb2af36fa478366c29db9e31a1f9","d3de3780a0f74036499aad4df2a73159b14fbad2","0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"e9c81f7e703d7ccca5bc78beb61253f0a8a22afd":["6dc613ed6f75d1988140301ee8de8fdb056fa337"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":[]},"heads":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","cd5edd1f2b162a5cfa08efd17851a07373a96817","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}