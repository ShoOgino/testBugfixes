{"path":"solr/core/src/test/org/apache/solr/store/blockcache/BlockDirectoryTest#assertInputsEquals(String,Directory,Directory).mjava","commits":[{"id":"849494cf2f3a96af5c8c84995108ddd8456fcd04","date":1372277913,"type":0,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/store/blockcache/BlockDirectoryTest#assertInputsEquals(String,Directory,Directory).mjava","pathOld":"/dev/null","sourceNew":"  private void assertInputsEquals(String name, Directory fsDir, Directory hdfs) throws IOException {\n    int reads = random.nextInt(MAX_NUMBER_OF_READS);\n    IndexInput fsInput = fsDir.openInput(name, new IOContext());\n    IndexInput hdfsInput = hdfs.openInput(name, new IOContext());\n    assertEquals(fsInput.length(), hdfsInput.length());\n    int fileLength = (int) fsInput.length();\n    for (int i = 0; i < reads; i++) {\n      byte[] fsBuf = new byte[random.nextInt(Math.min(MAX_BUFFER_SIZE - MIN_BUFFER_SIZE, fileLength)) + MIN_BUFFER_SIZE];\n      byte[] hdfsBuf = new byte[fsBuf.length];\n      int offset = random.nextInt(fsBuf.length);\n      int length = random.nextInt(fsBuf.length - offset);\n      int pos = random.nextInt(fileLength - length);\n      fsInput.seek(pos);\n      fsInput.readBytes(fsBuf, offset, length);\n      hdfsInput.seek(pos);\n      hdfsInput.readBytes(hdfsBuf, offset, length);\n      for (int f = offset; f < length; f++) {\n        if (fsBuf[f] != hdfsBuf[f]) {\n          fail(\"read [\" + i + \"]\");\n        }\n      }\n    }\n    fsInput.close();\n    hdfsInput.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cd6f10088c31a5eff04e076e43b08faf3642d77f","date":1373468752,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/store/blockcache/BlockDirectoryTest#assertInputsEquals(String,Directory,Directory).mjava","pathOld":"solr/core/src/test/org/apache/solr/store/blockcache/BlockDirectoryTest#assertInputsEquals(String,Directory,Directory).mjava","sourceNew":"  private void assertInputsEquals(String name, Directory fsDir, Directory hdfs) throws IOException {\n    int reads = random.nextInt(MAX_NUMBER_OF_READS);\n    IndexInput fsInput = fsDir.openInput(name, new IOContext());\n    IndexInput hdfsInput = hdfs.openInput(name, new IOContext());\n    assertEquals(fsInput.length(), hdfsInput.length());\n    int fileLength = (int) fsInput.length();\n    for (int i = 0; i < reads; i++) {\n      int rnd;\n      if (fileLength == 0) {\n        rnd = 0;\n      } else {\n        rnd = random.nextInt(Math.min(MAX_BUFFER_SIZE - MIN_BUFFER_SIZE, fileLength));\n      }\n\n      byte[] fsBuf = new byte[rnd + MIN_BUFFER_SIZE];\n      byte[] hdfsBuf = new byte[fsBuf.length];\n      int offset = random.nextInt(fsBuf.length);\n      int length = random.nextInt(fsBuf.length - offset);\n      \n      int pos;\n      if (fileLength == 0) {\n        pos = 0;\n      } else {\n        pos = random.nextInt(fileLength - length);\n      }\n    \n      fsInput.seek(pos);\n      fsInput.readBytes(fsBuf, offset, length);\n      hdfsInput.seek(pos);\n      hdfsInput.readBytes(hdfsBuf, offset, length);\n      for (int f = offset; f < length; f++) {\n        if (fsBuf[f] != hdfsBuf[f]) {\n          fail(\"read [\" + i + \"]\");\n        }\n      }\n    }\n    fsInput.close();\n    hdfsInput.close();\n  }\n\n","sourceOld":"  private void assertInputsEquals(String name, Directory fsDir, Directory hdfs) throws IOException {\n    int reads = random.nextInt(MAX_NUMBER_OF_READS);\n    IndexInput fsInput = fsDir.openInput(name, new IOContext());\n    IndexInput hdfsInput = hdfs.openInput(name, new IOContext());\n    assertEquals(fsInput.length(), hdfsInput.length());\n    int fileLength = (int) fsInput.length();\n    for (int i = 0; i < reads; i++) {\n      byte[] fsBuf = new byte[random.nextInt(Math.min(MAX_BUFFER_SIZE - MIN_BUFFER_SIZE, fileLength)) + MIN_BUFFER_SIZE];\n      byte[] hdfsBuf = new byte[fsBuf.length];\n      int offset = random.nextInt(fsBuf.length);\n      int length = random.nextInt(fsBuf.length - offset);\n      int pos = random.nextInt(fileLength - length);\n      fsInput.seek(pos);\n      fsInput.readBytes(fsBuf, offset, length);\n      hdfsInput.seek(pos);\n      hdfsInput.readBytes(hdfsBuf, offset, length);\n      for (int f = offset; f < length; f++) {\n        if (fsBuf[f] != hdfsBuf[f]) {\n          fail(\"read [\" + i + \"]\");\n        }\n      }\n    }\n    fsInput.close();\n    hdfsInput.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"37a0f60745e53927c4c876cfe5b5a58170f0646c","date":1373994005,"type":0,"author":"Han Jiang","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/store/blockcache/BlockDirectoryTest#assertInputsEquals(String,Directory,Directory).mjava","pathOld":"/dev/null","sourceNew":"  private void assertInputsEquals(String name, Directory fsDir, Directory hdfs) throws IOException {\n    int reads = random.nextInt(MAX_NUMBER_OF_READS);\n    IndexInput fsInput = fsDir.openInput(name, new IOContext());\n    IndexInput hdfsInput = hdfs.openInput(name, new IOContext());\n    assertEquals(fsInput.length(), hdfsInput.length());\n    int fileLength = (int) fsInput.length();\n    for (int i = 0; i < reads; i++) {\n      int rnd;\n      if (fileLength == 0) {\n        rnd = 0;\n      } else {\n        rnd = random.nextInt(Math.min(MAX_BUFFER_SIZE - MIN_BUFFER_SIZE, fileLength));\n      }\n\n      byte[] fsBuf = new byte[rnd + MIN_BUFFER_SIZE];\n      byte[] hdfsBuf = new byte[fsBuf.length];\n      int offset = random.nextInt(fsBuf.length);\n      int length = random.nextInt(fsBuf.length - offset);\n      \n      int pos;\n      if (fileLength == 0) {\n        pos = 0;\n      } else {\n        pos = random.nextInt(fileLength - length);\n      }\n    \n      fsInput.seek(pos);\n      fsInput.readBytes(fsBuf, offset, length);\n      hdfsInput.seek(pos);\n      hdfsInput.readBytes(hdfsBuf, offset, length);\n      for (int f = offset; f < length; f++) {\n        if (fsBuf[f] != hdfsBuf[f]) {\n          fail(\"read [\" + i + \"]\");\n        }\n      }\n    }\n    fsInput.close();\n    hdfsInput.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"849494cf2f3a96af5c8c84995108ddd8456fcd04":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","cd6f10088c31a5eff04e076e43b08faf3642d77f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd6f10088c31a5eff04e076e43b08faf3642d77f":["849494cf2f3a96af5c8c84995108ddd8456fcd04"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["cd6f10088c31a5eff04e076e43b08faf3642d77f"]},"commit2Childs":{"849494cf2f3a96af5c8c84995108ddd8456fcd04":["cd6f10088c31a5eff04e076e43b08faf3642d77f"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["849494cf2f3a96af5c8c84995108ddd8456fcd04","37a0f60745e53927c4c876cfe5b5a58170f0646c"],"cd6f10088c31a5eff04e076e43b08faf3642d77f":["37a0f60745e53927c4c876cfe5b5a58170f0646c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["37a0f60745e53927c4c876cfe5b5a58170f0646c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}