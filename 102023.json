{"path":"solr/core/src/java/org/apache/solr/cloud/autoscaling/IndexSizeTrigger#run().mjava","commits":[{"id":"ce71a9836bf1eba34a0ab31884e9eb8ad3f1cef1","date":1523453934,"type":0,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/autoscaling/IndexSizeTrigger#run().mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public void run() {\n    synchronized(this) {\n      if (isClosed) {\n        log.warn(getName() + \" ran but was already closed\");\n        return;\n      }\n    }\n    AutoScaling.TriggerEventProcessor processor = processorRef.get();\n    if (processor == null) {\n      return;\n    }\n\n    // replica name / info + size, retrieved from leaders only\n    Map<String, ReplicaInfo> currentSizes = new HashMap<>();\n\n    try {\n      ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n      for (String node : clusterState.getLiveNodes()) {\n        Map<String, ReplicaInfo> metricTags = new HashMap<>();\n        // coll, shard, replica\n        Map<String, Map<String, List<ReplicaInfo>>> infos = cloudManager.getNodeStateProvider().getReplicaInfo(node, Collections.emptyList());\n        infos.forEach((coll, shards) -> {\n          if (!collections.isEmpty() && !collections.contains(coll)) {\n            return;\n          }\n          DocCollection docCollection = clusterState.getCollection(coll);\n\n          shards.forEach((sh, replicas) -> {\n            // check only the leader of a replica in active shard\n            Slice s = docCollection.getSlice(sh);\n            if (s.getState() != Slice.State.ACTIVE) {\n              return;\n            }\n            Replica r = s.getLeader();\n            // no leader - don't do anything\n            if (r == null) {\n              return;\n            }\n            // find ReplicaInfo\n            ReplicaInfo info = null;\n            for (ReplicaInfo ri : replicas) {\n              if (r.getCoreName().equals(ri.getCore())) {\n                info = ri;\n                break;\n              }\n            }\n            if (info == null) {\n              // probably replica is not on this node?\n              return;\n            }\n            // we have to translate to the metrics registry name, which uses \"_replica_nN\" as suffix\n            String replicaName = Utils.parseMetricsReplicaName(coll, info.getCore());\n            if (replicaName == null) { // should never happen???\n              replicaName = info.getName(); // which is actually coreNode name...\n            }\n            String registry = SolrCoreMetricManager.createRegistryName(true, coll, sh, replicaName, null);\n            String tag = \"metrics:\" + registry + \":INDEX.sizeInBytes\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.numDocs\";\n            metricTags.put(tag, info);\n          });\n        });\n        if (metricTags.isEmpty()) {\n          continue;\n        }\n        Map<String, Object> sizes = cloudManager.getNodeStateProvider().getNodeValues(node, metricTags.keySet());\n        sizes.forEach((tag, size) -> {\n          final ReplicaInfo info = metricTags.get(tag);\n          if (info == null) {\n            log.warn(\"Missing replica info for response tag \" + tag);\n          } else {\n            // verify that it's a Number\n            if (!(size instanceof Number)) {\n              log.warn(\"invalid size value - not a number: '\" + size + \"' is \" + size.getClass().getName());\n              return;\n            }\n\n            ReplicaInfo currentInfo = currentSizes.computeIfAbsent(info.getCore(), k -> (ReplicaInfo)info.clone());\n            if (tag.contains(\"INDEX\")) {\n              currentInfo.getVariables().put(BYTES_SIZE_PROP, ((Number) size).longValue());\n            } else {\n              currentInfo.getVariables().put(DOCS_SIZE_PROP, ((Number) size).longValue());\n            }\n          }\n        });\n      }\n    } catch (IOException e) {\n      log.warn(\"Error running trigger \" + getName(), e);\n      return;\n    }\n\n    long now = cloudManager.getTimeSource().getTimeNs();\n\n    // now check thresholds\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> aboveSize = new HashMap<>();\n    currentSizes.entrySet().stream()\n        .filter(e -> (\n            (Long)e.getValue().getVariable(BYTES_SIZE_PROP) > aboveBytes ||\n            (Long)e.getValue().getVariable(DOCS_SIZE_PROP) > aboveDocs\n            ) && waitForElapsed(e.getKey(), now, lastEventMap))\n        .forEach(e -> {\n          ReplicaInfo info = e.getValue();\n          List<ReplicaInfo> infos = aboveSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)e.getValue().getVariable(BYTES_SIZE_PROP) > aboveBytes) {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_DOCS_PROP);\n            }\n            infos.add(info);\n          }\n        });\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> belowSize = new HashMap<>();\n    currentSizes.entrySet().stream()\n        .filter(e -> (\n            (Long)e.getValue().getVariable(BYTES_SIZE_PROP) < belowBytes ||\n            (Long)e.getValue().getVariable(DOCS_SIZE_PROP) < belowDocs\n            ) && waitForElapsed(e.getKey(), now, lastEventMap))\n        .forEach(e -> {\n          ReplicaInfo info = e.getValue();\n          List<ReplicaInfo> infos = belowSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)e.getValue().getVariable(BYTES_SIZE_PROP) < belowBytes) {\n              info.getVariables().put(VIOLATION_PROP, BELOW_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, BELOW_DOCS_PROP);\n            }\n            infos.add(info);\n          }\n        });\n\n    if (aboveSize.isEmpty() && belowSize.isEmpty()) {\n      return;\n    }\n\n    // find the earliest time when a condition was exceeded\n    final AtomicLong eventTime = new AtomicLong(now);\n\n    // calculate ops\n    final List<TriggerEvent.Op> ops = new ArrayList<>();\n    aboveSize.forEach((coll, replicas) -> {\n      replicas.forEach(r -> {\n        TriggerEvent.Op op = new TriggerEvent.Op(aboveOp);\n        op.addHint(Suggester.Hint.COLL_SHARD, new Pair<>(coll, r.getShard()));\n        ops.add(op);\n        Long time = lastEventMap.get(r.getCore());\n        if (time != null && eventTime.get() > time) {\n          eventTime.set(time);\n        }\n      });\n    });\n    belowSize.forEach((coll, replicas) -> {\n      if (replicas.size() < 2) {\n        return;\n      }\n      // sort by increasing size\n      replicas.sort((r1, r2) -> {\n        // XXX this is not quite correct - if BYTES_SIZE_PROP decided that replica got here\n        // then we should be sorting by BYTES_SIZE_PROP. However, since DOCS and BYTES are\n        // loosely correlated it's simpler to sort just by docs (which better reflects the \"too small\"\n        // condition than index size, due to possibly existing deleted docs that still occupy space)\n        long delta = (Long) r1.getVariable(DOCS_SIZE_PROP) - (Long) r2.getVariable(DOCS_SIZE_PROP);\n        if (delta > 0) {\n          return 1;\n        } else if (delta < 0) {\n          return -1;\n        } else {\n          return 0;\n        }\n      });\n\n      // TODO: MERGESHARDS is not implemented yet. For now take the top two smallest shards\n      // TODO: but in the future we probably need to get ones with adjacent ranges.\n\n      // TODO: generate as many MERGESHARDS as needed to consume all belowSize shards\n      TriggerEvent.Op op = new TriggerEvent.Op(belowOp);\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(0).getShard()));\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(1).getShard()));\n      ops.add(op);\n      Long time = lastEventMap.get(replicas.get(0).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n      time = lastEventMap.get(replicas.get(1).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n    });\n\n    if (ops.isEmpty()) {\n      return;\n    }\n    if (processor.process(new IndexSizeEvent(getName(), eventTime.get(), ops, aboveSize, belowSize))) {\n      // update last event times\n      aboveSize.forEach((coll, replicas) -> {\n        replicas.forEach(r -> lastEventMap.put(r.getCore(), now));\n      });\n      belowSize.forEach((coll, replicas) -> {\n        lastEventMap.put(replicas.get(0).getCore(), now);\n        lastEventMap.put(replicas.get(1).getCore(), now);\n      });\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["383671a9fe2f5147abf22eb1ce56e3ca3d6eb133","256b1da668dae74c726d5d71bce6153202d6b001"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"43345f1452f9510f8aaadae6156fe0c834e7d957","date":1523483670,"type":0,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/autoscaling/IndexSizeTrigger#run().mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public void run() {\n    synchronized(this) {\n      if (isClosed) {\n        log.warn(getName() + \" ran but was already closed\");\n        return;\n      }\n    }\n    AutoScaling.TriggerEventProcessor processor = processorRef.get();\n    if (processor == null) {\n      return;\n    }\n\n    // replica name / info + size, retrieved from leaders only\n    Map<String, ReplicaInfo> currentSizes = new HashMap<>();\n\n    try {\n      ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n      for (String node : clusterState.getLiveNodes()) {\n        Map<String, ReplicaInfo> metricTags = new HashMap<>();\n        // coll, shard, replica\n        Map<String, Map<String, List<ReplicaInfo>>> infos = cloudManager.getNodeStateProvider().getReplicaInfo(node, Collections.emptyList());\n        infos.forEach((coll, shards) -> {\n          if (!collections.isEmpty() && !collections.contains(coll)) {\n            return;\n          }\n          DocCollection docCollection = clusterState.getCollection(coll);\n\n          shards.forEach((sh, replicas) -> {\n            // check only the leader of a replica in active shard\n            Slice s = docCollection.getSlice(sh);\n            if (s.getState() != Slice.State.ACTIVE) {\n              return;\n            }\n            Replica r = s.getLeader();\n            // no leader - don't do anything\n            if (r == null) {\n              return;\n            }\n            // find ReplicaInfo\n            ReplicaInfo info = null;\n            for (ReplicaInfo ri : replicas) {\n              if (r.getCoreName().equals(ri.getCore())) {\n                info = ri;\n                break;\n              }\n            }\n            if (info == null) {\n              // probably replica is not on this node?\n              return;\n            }\n            // we have to translate to the metrics registry name, which uses \"_replica_nN\" as suffix\n            String replicaName = Utils.parseMetricsReplicaName(coll, info.getCore());\n            if (replicaName == null) { // should never happen???\n              replicaName = info.getName(); // which is actually coreNode name...\n            }\n            String registry = SolrCoreMetricManager.createRegistryName(true, coll, sh, replicaName, null);\n            String tag = \"metrics:\" + registry + \":INDEX.sizeInBytes\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.numDocs\";\n            metricTags.put(tag, info);\n          });\n        });\n        if (metricTags.isEmpty()) {\n          continue;\n        }\n        Map<String, Object> sizes = cloudManager.getNodeStateProvider().getNodeValues(node, metricTags.keySet());\n        sizes.forEach((tag, size) -> {\n          final ReplicaInfo info = metricTags.get(tag);\n          if (info == null) {\n            log.warn(\"Missing replica info for response tag \" + tag);\n          } else {\n            // verify that it's a Number\n            if (!(size instanceof Number)) {\n              log.warn(\"invalid size value - not a number: '\" + size + \"' is \" + size.getClass().getName());\n              return;\n            }\n\n            ReplicaInfo currentInfo = currentSizes.computeIfAbsent(info.getCore(), k -> (ReplicaInfo)info.clone());\n            if (tag.contains(\"INDEX\")) {\n              currentInfo.getVariables().put(BYTES_SIZE_PROP, ((Number) size).longValue());\n            } else {\n              currentInfo.getVariables().put(DOCS_SIZE_PROP, ((Number) size).longValue());\n            }\n          }\n        });\n      }\n    } catch (IOException e) {\n      log.warn(\"Error running trigger \" + getName(), e);\n      return;\n    }\n\n    long now = cloudManager.getTimeSource().getTimeNs();\n\n    // now check thresholds\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> aboveSize = new HashMap<>();\n    currentSizes.entrySet().stream()\n        .filter(e -> (\n            (Long)e.getValue().getVariable(BYTES_SIZE_PROP) > aboveBytes ||\n            (Long)e.getValue().getVariable(DOCS_SIZE_PROP) > aboveDocs\n            ) && waitForElapsed(e.getKey(), now, lastEventMap))\n        .forEach(e -> {\n          ReplicaInfo info = e.getValue();\n          List<ReplicaInfo> infos = aboveSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)e.getValue().getVariable(BYTES_SIZE_PROP) > aboveBytes) {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_DOCS_PROP);\n            }\n            infos.add(info);\n          }\n        });\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> belowSize = new HashMap<>();\n    currentSizes.entrySet().stream()\n        .filter(e -> (\n            (Long)e.getValue().getVariable(BYTES_SIZE_PROP) < belowBytes ||\n            (Long)e.getValue().getVariable(DOCS_SIZE_PROP) < belowDocs\n            ) && waitForElapsed(e.getKey(), now, lastEventMap))\n        .forEach(e -> {\n          ReplicaInfo info = e.getValue();\n          List<ReplicaInfo> infos = belowSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)e.getValue().getVariable(BYTES_SIZE_PROP) < belowBytes) {\n              info.getVariables().put(VIOLATION_PROP, BELOW_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, BELOW_DOCS_PROP);\n            }\n            infos.add(info);\n          }\n        });\n\n    if (aboveSize.isEmpty() && belowSize.isEmpty()) {\n      return;\n    }\n\n    // find the earliest time when a condition was exceeded\n    final AtomicLong eventTime = new AtomicLong(now);\n\n    // calculate ops\n    final List<TriggerEvent.Op> ops = new ArrayList<>();\n    aboveSize.forEach((coll, replicas) -> {\n      replicas.forEach(r -> {\n        TriggerEvent.Op op = new TriggerEvent.Op(aboveOp);\n        op.addHint(Suggester.Hint.COLL_SHARD, new Pair<>(coll, r.getShard()));\n        ops.add(op);\n        Long time = lastEventMap.get(r.getCore());\n        if (time != null && eventTime.get() > time) {\n          eventTime.set(time);\n        }\n      });\n    });\n    belowSize.forEach((coll, replicas) -> {\n      if (replicas.size() < 2) {\n        return;\n      }\n      // sort by increasing size\n      replicas.sort((r1, r2) -> {\n        // XXX this is not quite correct - if BYTES_SIZE_PROP decided that replica got here\n        // then we should be sorting by BYTES_SIZE_PROP. However, since DOCS and BYTES are\n        // loosely correlated it's simpler to sort just by docs (which better reflects the \"too small\"\n        // condition than index size, due to possibly existing deleted docs that still occupy space)\n        long delta = (Long) r1.getVariable(DOCS_SIZE_PROP) - (Long) r2.getVariable(DOCS_SIZE_PROP);\n        if (delta > 0) {\n          return 1;\n        } else if (delta < 0) {\n          return -1;\n        } else {\n          return 0;\n        }\n      });\n\n      // TODO: MERGESHARDS is not implemented yet. For now take the top two smallest shards\n      // TODO: but in the future we probably need to get ones with adjacent ranges.\n\n      // TODO: generate as many MERGESHARDS as needed to consume all belowSize shards\n      TriggerEvent.Op op = new TriggerEvent.Op(belowOp);\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(0).getShard()));\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(1).getShard()));\n      ops.add(op);\n      Long time = lastEventMap.get(replicas.get(0).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n      time = lastEventMap.get(replicas.get(1).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n    });\n\n    if (ops.isEmpty()) {\n      return;\n    }\n    if (processor.process(new IndexSizeEvent(getName(), eventTime.get(), ops, aboveSize, belowSize))) {\n      // update last event times\n      aboveSize.forEach((coll, replicas) -> {\n        replicas.forEach(r -> lastEventMap.put(r.getCore(), now));\n      });\n      belowSize.forEach((coll, replicas) -> {\n        lastEventMap.put(replicas.get(0).getCore(), now);\n        lastEventMap.put(replicas.get(1).getCore(), now);\n      });\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0532cd2ef583ac8047d77493ec05c81836f483c2","date":1527687859,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/autoscaling/IndexSizeTrigger#run().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/autoscaling/IndexSizeTrigger#run().mjava","sourceNew":"  @Override\n  public void run() {\n    synchronized(this) {\n      if (isClosed) {\n        log.warn(getName() + \" ran but was already closed\");\n        return;\n      }\n    }\n    AutoScaling.TriggerEventProcessor processor = processorRef.get();\n    if (processor == null) {\n      return;\n    }\n\n    // replica name / info + size, retrieved from leaders only\n    Map<String, ReplicaInfo> currentSizes = new HashMap<>();\n\n    try {\n      ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n      for (String node : clusterState.getLiveNodes()) {\n        Map<String, ReplicaInfo> metricTags = new HashMap<>();\n        // coll, shard, replica\n        Map<String, Map<String, List<ReplicaInfo>>> infos = cloudManager.getNodeStateProvider().getReplicaInfo(node, Collections.emptyList());\n        infos.forEach((coll, shards) -> {\n          if (!collections.isEmpty() && !collections.contains(coll)) {\n            return;\n          }\n          DocCollection docCollection = clusterState.getCollection(coll);\n\n          shards.forEach((sh, replicas) -> {\n            // check only the leader of a replica in active shard\n            Slice s = docCollection.getSlice(sh);\n            if (s.getState() != Slice.State.ACTIVE) {\n              return;\n            }\n            Replica r = s.getLeader();\n            // no leader - don't do anything\n            if (r == null) {\n              return;\n            }\n            // find ReplicaInfo\n            ReplicaInfo info = null;\n            for (ReplicaInfo ri : replicas) {\n              if (r.getCoreName().equals(ri.getCore())) {\n                info = ri;\n                break;\n              }\n            }\n            if (info == null) {\n              // probably replica is not on this node?\n              return;\n            }\n            // we have to translate to the metrics registry name, which uses \"_replica_nN\" as suffix\n            String replicaName = Utils.parseMetricsReplicaName(coll, info.getCore());\n            if (replicaName == null) { // should never happen???\n              replicaName = info.getName(); // which is actually coreNode name...\n            }\n            String registry = SolrCoreMetricManager.createRegistryName(true, coll, sh, replicaName, null);\n            String tag = \"metrics:\" + registry + \":INDEX.sizeInBytes\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.numDocs\";\n            metricTags.put(tag, info);\n          });\n        });\n        if (metricTags.isEmpty()) {\n          continue;\n        }\n        Map<String, Object> sizes = cloudManager.getNodeStateProvider().getNodeValues(node, metricTags.keySet());\n        sizes.forEach((tag, size) -> {\n          final ReplicaInfo info = metricTags.get(tag);\n          if (info == null) {\n            log.warn(\"Missing replica info for response tag \" + tag);\n          } else {\n            // verify that it's a Number\n            if (!(size instanceof Number)) {\n              log.warn(\"invalid size value - not a number: '\" + size + \"' is \" + size.getClass().getName());\n              return;\n            }\n\n            ReplicaInfo currentInfo = currentSizes.computeIfAbsent(info.getCore(), k -> (ReplicaInfo)info.clone());\n            if (tag.contains(\"INDEX\")) {\n              currentInfo.getVariables().put(BYTES_SIZE_PROP, ((Number) size).longValue());\n            } else {\n              currentInfo.getVariables().put(DOCS_SIZE_PROP, ((Number) size).longValue());\n            }\n          }\n        });\n      }\n    } catch (IOException e) {\n      log.warn(\"Error running trigger \" + getName(), e);\n      return;\n    }\n\n    long now = cloudManager.getTimeSource().getTimeNs();\n\n    // now check thresholds\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> aboveSize = new HashMap<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) > aboveDocs) {\n        if (waitForElapsed(coreName, now, lastAboveEventMap)) {\n          List<ReplicaInfo> infos = aboveSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes) {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_DOCS_PROP);\n            }\n            infos.add(info);\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastAboveEventMap.remove(coreName);\n      }\n    });\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> belowSize = new HashMap<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if ((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) < belowDocs) {\n        if (waitForElapsed(coreName, now, lastBelowEventMap)) {\n          List<ReplicaInfo> infos = belowSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes) {\n              info.getVariables().put(VIOLATION_PROP, BELOW_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, BELOW_DOCS_PROP);\n            }\n            infos.add(info);\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastBelowEventMap.remove(coreName);\n      }\n    });\n\n    if (aboveSize.isEmpty() && belowSize.isEmpty()) {\n      log.trace(\"NO VIOLATIONS: Now={}\", now);\n      log.trace(\"lastAbove={}\", lastAboveEventMap);\n      log.trace(\"lastBelow={}\", lastBelowEventMap);\n      return;\n    }\n\n    // find the earliest time when a condition was exceeded\n    final AtomicLong eventTime = new AtomicLong(now);\n\n    // calculate ops\n    final List<TriggerEvent.Op> ops = new ArrayList<>();\n    aboveSize.forEach((coll, replicas) -> {\n      replicas.forEach(r -> {\n        TriggerEvent.Op op = new TriggerEvent.Op(aboveOp);\n        op.addHint(Suggester.Hint.COLL_SHARD, new Pair<>(coll, r.getShard()));\n        ops.add(op);\n        Long time = lastAboveEventMap.get(r.getCore());\n        if (time != null && eventTime.get() > time) {\n          eventTime.set(time);\n        }\n      });\n    });\n    belowSize.forEach((coll, replicas) -> {\n      if (replicas.size() < 2) {\n        return;\n      }\n      // sort by increasing size\n      replicas.sort((r1, r2) -> {\n        // XXX this is not quite correct - if BYTES_SIZE_PROP decided that replica got here\n        // then we should be sorting by BYTES_SIZE_PROP. However, since DOCS and BYTES are\n        // loosely correlated it's simpler to sort just by docs (which better reflects the \"too small\"\n        // condition than index size, due to possibly existing deleted docs that still occupy space)\n        long delta = (Long) r1.getVariable(DOCS_SIZE_PROP) - (Long) r2.getVariable(DOCS_SIZE_PROP);\n        if (delta > 0) {\n          return 1;\n        } else if (delta < 0) {\n          return -1;\n        } else {\n          return 0;\n        }\n      });\n\n      // TODO: MERGESHARDS is not implemented yet. For now take the top two smallest shards\n      // TODO: but in the future we probably need to get ones with adjacent ranges.\n\n      // TODO: generate as many MERGESHARDS as needed to consume all belowSize shards\n      TriggerEvent.Op op = new TriggerEvent.Op(belowOp);\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(0).getShard()));\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(1).getShard()));\n      ops.add(op);\n      Long time = lastBelowEventMap.get(replicas.get(0).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n      time = lastBelowEventMap.get(replicas.get(1).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n    });\n\n    if (ops.isEmpty()) {\n      return;\n    }\n    if (processor.process(new IndexSizeEvent(getName(), eventTime.get(), ops, aboveSize, belowSize))) {\n      // update last event times\n      aboveSize.forEach((coll, replicas) -> {\n        replicas.forEach(r -> lastAboveEventMap.put(r.getCore(), now));\n      });\n      belowSize.forEach((coll, replicas) -> {\n        lastBelowEventMap.put(replicas.get(0).getCore(), now);\n        lastBelowEventMap.put(replicas.get(1).getCore(), now);\n      });\n    }\n  }\n\n","sourceOld":"  @Override\n  public void run() {\n    synchronized(this) {\n      if (isClosed) {\n        log.warn(getName() + \" ran but was already closed\");\n        return;\n      }\n    }\n    AutoScaling.TriggerEventProcessor processor = processorRef.get();\n    if (processor == null) {\n      return;\n    }\n\n    // replica name / info + size, retrieved from leaders only\n    Map<String, ReplicaInfo> currentSizes = new HashMap<>();\n\n    try {\n      ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n      for (String node : clusterState.getLiveNodes()) {\n        Map<String, ReplicaInfo> metricTags = new HashMap<>();\n        // coll, shard, replica\n        Map<String, Map<String, List<ReplicaInfo>>> infos = cloudManager.getNodeStateProvider().getReplicaInfo(node, Collections.emptyList());\n        infos.forEach((coll, shards) -> {\n          if (!collections.isEmpty() && !collections.contains(coll)) {\n            return;\n          }\n          DocCollection docCollection = clusterState.getCollection(coll);\n\n          shards.forEach((sh, replicas) -> {\n            // check only the leader of a replica in active shard\n            Slice s = docCollection.getSlice(sh);\n            if (s.getState() != Slice.State.ACTIVE) {\n              return;\n            }\n            Replica r = s.getLeader();\n            // no leader - don't do anything\n            if (r == null) {\n              return;\n            }\n            // find ReplicaInfo\n            ReplicaInfo info = null;\n            for (ReplicaInfo ri : replicas) {\n              if (r.getCoreName().equals(ri.getCore())) {\n                info = ri;\n                break;\n              }\n            }\n            if (info == null) {\n              // probably replica is not on this node?\n              return;\n            }\n            // we have to translate to the metrics registry name, which uses \"_replica_nN\" as suffix\n            String replicaName = Utils.parseMetricsReplicaName(coll, info.getCore());\n            if (replicaName == null) { // should never happen???\n              replicaName = info.getName(); // which is actually coreNode name...\n            }\n            String registry = SolrCoreMetricManager.createRegistryName(true, coll, sh, replicaName, null);\n            String tag = \"metrics:\" + registry + \":INDEX.sizeInBytes\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.numDocs\";\n            metricTags.put(tag, info);\n          });\n        });\n        if (metricTags.isEmpty()) {\n          continue;\n        }\n        Map<String, Object> sizes = cloudManager.getNodeStateProvider().getNodeValues(node, metricTags.keySet());\n        sizes.forEach((tag, size) -> {\n          final ReplicaInfo info = metricTags.get(tag);\n          if (info == null) {\n            log.warn(\"Missing replica info for response tag \" + tag);\n          } else {\n            // verify that it's a Number\n            if (!(size instanceof Number)) {\n              log.warn(\"invalid size value - not a number: '\" + size + \"' is \" + size.getClass().getName());\n              return;\n            }\n\n            ReplicaInfo currentInfo = currentSizes.computeIfAbsent(info.getCore(), k -> (ReplicaInfo)info.clone());\n            if (tag.contains(\"INDEX\")) {\n              currentInfo.getVariables().put(BYTES_SIZE_PROP, ((Number) size).longValue());\n            } else {\n              currentInfo.getVariables().put(DOCS_SIZE_PROP, ((Number) size).longValue());\n            }\n          }\n        });\n      }\n    } catch (IOException e) {\n      log.warn(\"Error running trigger \" + getName(), e);\n      return;\n    }\n\n    long now = cloudManager.getTimeSource().getTimeNs();\n\n    // now check thresholds\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> aboveSize = new HashMap<>();\n    currentSizes.entrySet().stream()\n        .filter(e -> (\n            (Long)e.getValue().getVariable(BYTES_SIZE_PROP) > aboveBytes ||\n            (Long)e.getValue().getVariable(DOCS_SIZE_PROP) > aboveDocs\n            ) && waitForElapsed(e.getKey(), now, lastEventMap))\n        .forEach(e -> {\n          ReplicaInfo info = e.getValue();\n          List<ReplicaInfo> infos = aboveSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)e.getValue().getVariable(BYTES_SIZE_PROP) > aboveBytes) {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_DOCS_PROP);\n            }\n            infos.add(info);\n          }\n        });\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> belowSize = new HashMap<>();\n    currentSizes.entrySet().stream()\n        .filter(e -> (\n            (Long)e.getValue().getVariable(BYTES_SIZE_PROP) < belowBytes ||\n            (Long)e.getValue().getVariable(DOCS_SIZE_PROP) < belowDocs\n            ) && waitForElapsed(e.getKey(), now, lastEventMap))\n        .forEach(e -> {\n          ReplicaInfo info = e.getValue();\n          List<ReplicaInfo> infos = belowSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)e.getValue().getVariable(BYTES_SIZE_PROP) < belowBytes) {\n              info.getVariables().put(VIOLATION_PROP, BELOW_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, BELOW_DOCS_PROP);\n            }\n            infos.add(info);\n          }\n        });\n\n    if (aboveSize.isEmpty() && belowSize.isEmpty()) {\n      return;\n    }\n\n    // find the earliest time when a condition was exceeded\n    final AtomicLong eventTime = new AtomicLong(now);\n\n    // calculate ops\n    final List<TriggerEvent.Op> ops = new ArrayList<>();\n    aboveSize.forEach((coll, replicas) -> {\n      replicas.forEach(r -> {\n        TriggerEvent.Op op = new TriggerEvent.Op(aboveOp);\n        op.addHint(Suggester.Hint.COLL_SHARD, new Pair<>(coll, r.getShard()));\n        ops.add(op);\n        Long time = lastEventMap.get(r.getCore());\n        if (time != null && eventTime.get() > time) {\n          eventTime.set(time);\n        }\n      });\n    });\n    belowSize.forEach((coll, replicas) -> {\n      if (replicas.size() < 2) {\n        return;\n      }\n      // sort by increasing size\n      replicas.sort((r1, r2) -> {\n        // XXX this is not quite correct - if BYTES_SIZE_PROP decided that replica got here\n        // then we should be sorting by BYTES_SIZE_PROP. However, since DOCS and BYTES are\n        // loosely correlated it's simpler to sort just by docs (which better reflects the \"too small\"\n        // condition than index size, due to possibly existing deleted docs that still occupy space)\n        long delta = (Long) r1.getVariable(DOCS_SIZE_PROP) - (Long) r2.getVariable(DOCS_SIZE_PROP);\n        if (delta > 0) {\n          return 1;\n        } else if (delta < 0) {\n          return -1;\n        } else {\n          return 0;\n        }\n      });\n\n      // TODO: MERGESHARDS is not implemented yet. For now take the top two smallest shards\n      // TODO: but in the future we probably need to get ones with adjacent ranges.\n\n      // TODO: generate as many MERGESHARDS as needed to consume all belowSize shards\n      TriggerEvent.Op op = new TriggerEvent.Op(belowOp);\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(0).getShard()));\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(1).getShard()));\n      ops.add(op);\n      Long time = lastEventMap.get(replicas.get(0).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n      time = lastEventMap.get(replicas.get(1).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n    });\n\n    if (ops.isEmpty()) {\n      return;\n    }\n    if (processor.process(new IndexSizeEvent(getName(), eventTime.get(), ops, aboveSize, belowSize))) {\n      // update last event times\n      aboveSize.forEach((coll, replicas) -> {\n        replicas.forEach(r -> lastEventMap.put(r.getCore(), now));\n      });\n      belowSize.forEach((coll, replicas) -> {\n        lastEventMap.put(replicas.get(0).getCore(), now);\n        lastEventMap.put(replicas.get(1).getCore(), now);\n      });\n    }\n  }\n\n","bugFix":null,"bugIntro":["256b1da668dae74c726d5d71bce6153202d6b001"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b8cde5442ed20c0c3ffd14ea2e2a64609367c193","date":1528792993,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/autoscaling/IndexSizeTrigger#run().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/autoscaling/IndexSizeTrigger#run().mjava","sourceNew":"  @Override\n  public void run() {\n    synchronized(this) {\n      if (isClosed) {\n        log.warn(getName() + \" ran but was already closed\");\n        return;\n      }\n    }\n    AutoScaling.TriggerEventProcessor processor = processorRef.get();\n    if (processor == null) {\n      return;\n    }\n\n    // replica name / info + size, retrieved from leaders only\n    Map<String, ReplicaInfo> currentSizes = new HashMap<>();\n\n    try {\n      ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n      for (String node : clusterState.getLiveNodes()) {\n        Map<String, ReplicaInfo> metricTags = new HashMap<>();\n        // coll, shard, replica\n        Map<String, Map<String, List<ReplicaInfo>>> infos = cloudManager.getNodeStateProvider().getReplicaInfo(node, Collections.emptyList());\n        infos.forEach((coll, shards) -> {\n          if (!collections.isEmpty() && !collections.contains(coll)) {\n            return;\n          }\n          DocCollection docCollection = clusterState.getCollection(coll);\n\n          shards.forEach((sh, replicas) -> {\n            // check only the leader of a replica in active shard\n            Slice s = docCollection.getSlice(sh);\n            if (s.getState() != Slice.State.ACTIVE) {\n              return;\n            }\n            Replica r = s.getLeader();\n            // no leader - don't do anything\n            if (r == null) {\n              return;\n            }\n            // find ReplicaInfo\n            ReplicaInfo info = null;\n            for (ReplicaInfo ri : replicas) {\n              if (r.getCoreName().equals(ri.getCore())) {\n                info = ri;\n                break;\n              }\n            }\n            if (info == null) {\n              // probably replica is not on this node?\n              return;\n            }\n            // we have to translate to the metrics registry name, which uses \"_replica_nN\" as suffix\n            String replicaName = Utils.parseMetricsReplicaName(coll, info.getCore());\n            if (replicaName == null) { // should never happen???\n              replicaName = info.getName(); // which is actually coreNode name...\n            }\n            String registry = SolrCoreMetricManager.createRegistryName(true, coll, sh, replicaName, null);\n            String tag = \"metrics:\" + registry + \":INDEX.sizeInBytes\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.numDocs\";\n            metricTags.put(tag, info);\n          });\n        });\n        if (metricTags.isEmpty()) {\n          continue;\n        }\n        Map<String, Object> sizes = cloudManager.getNodeStateProvider().getNodeValues(node, metricTags.keySet());\n        sizes.forEach((tag, size) -> {\n          final ReplicaInfo info = metricTags.get(tag);\n          if (info == null) {\n            log.warn(\"Missing replica info for response tag \" + tag);\n          } else {\n            // verify that it's a Number\n            if (!(size instanceof Number)) {\n              log.warn(\"invalid size value - not a number: '\" + size + \"' is \" + size.getClass().getName());\n              return;\n            }\n\n            ReplicaInfo currentInfo = currentSizes.computeIfAbsent(info.getCore(), k -> (ReplicaInfo)info.clone());\n            if (tag.contains(\"INDEX\")) {\n              currentInfo.getVariables().put(BYTES_SIZE_PROP, ((Number) size).longValue());\n            } else {\n              currentInfo.getVariables().put(DOCS_SIZE_PROP, ((Number) size).longValue());\n            }\n          }\n        });\n      }\n    } catch (IOException e) {\n      log.warn(\"Error running trigger \" + getName(), e);\n      return;\n    }\n\n    long now = cloudManager.getTimeSource().getTimeNs();\n\n    // now check thresholds\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> aboveSize = new HashMap<>();\n\n    Set<String> splittable = new HashSet<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) > aboveDocs) {\n        if (waitForElapsed(coreName, now, lastAboveEventMap)) {\n          List<ReplicaInfo> infos = aboveSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes) {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_DOCS_PROP);\n            }\n            infos.add(info);\n            splittable.add(info.getName());\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastAboveEventMap.remove(coreName);\n      }\n    });\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> belowSize = new HashMap<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if (((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) < belowDocs) &&\n          // make sure we don't produce conflicting ops\n          !splittable.contains(info.getName())) {\n        if (waitForElapsed(coreName, now, lastBelowEventMap)) {\n          List<ReplicaInfo> infos = belowSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes) {\n              info.getVariables().put(VIOLATION_PROP, BELOW_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, BELOW_DOCS_PROP);\n            }\n            infos.add(info);\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastBelowEventMap.remove(coreName);\n      }\n    });\n\n    if (aboveSize.isEmpty() && belowSize.isEmpty()) {\n      log.trace(\"NO VIOLATIONS: Now={}\", now);\n      log.trace(\"lastAbove={}\", lastAboveEventMap);\n      log.trace(\"lastBelow={}\", lastBelowEventMap);\n      return;\n    }\n\n    // find the earliest time when a condition was exceeded\n    final AtomicLong eventTime = new AtomicLong(now);\n\n    // calculate ops\n    final List<TriggerEvent.Op> ops = new ArrayList<>();\n    aboveSize.forEach((coll, replicas) -> {\n      replicas.forEach(r -> {\n        TriggerEvent.Op op = new TriggerEvent.Op(aboveOp);\n        op.addHint(Suggester.Hint.COLL_SHARD, new Pair<>(coll, r.getShard()));\n        ops.add(op);\n        Long time = lastAboveEventMap.get(r.getCore());\n        if (time != null && eventTime.get() > time) {\n          eventTime.set(time);\n        }\n      });\n    });\n    belowSize.forEach((coll, replicas) -> {\n      if (replicas.size() < 2) {\n        return;\n      }\n      // sort by increasing size\n      replicas.sort((r1, r2) -> {\n        // XXX this is not quite correct - if BYTES_SIZE_PROP decided that replica got here\n        // then we should be sorting by BYTES_SIZE_PROP. However, since DOCS and BYTES are\n        // loosely correlated it's simpler to sort just by docs (which better reflects the \"too small\"\n        // condition than index size, due to possibly existing deleted docs that still occupy space)\n        long delta = (Long) r1.getVariable(DOCS_SIZE_PROP) - (Long) r2.getVariable(DOCS_SIZE_PROP);\n        if (delta > 0) {\n          return 1;\n        } else if (delta < 0) {\n          return -1;\n        } else {\n          return 0;\n        }\n      });\n\n      // TODO: MERGESHARDS is not implemented yet. For now take the top two smallest shards\n      // TODO: but in the future we probably need to get ones with adjacent ranges.\n\n      // TODO: generate as many MERGESHARDS as needed to consume all belowSize shards\n      TriggerEvent.Op op = new TriggerEvent.Op(belowOp);\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(0).getShard()));\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(1).getShard()));\n      ops.add(op);\n      Long time = lastBelowEventMap.get(replicas.get(0).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n      time = lastBelowEventMap.get(replicas.get(1).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n    });\n\n    if (ops.isEmpty()) {\n      return;\n    }\n    if (processor.process(new IndexSizeEvent(getName(), eventTime.get(), ops, aboveSize, belowSize))) {\n      // update last event times\n      aboveSize.forEach((coll, replicas) -> {\n        replicas.forEach(r -> lastAboveEventMap.put(r.getCore(), now));\n      });\n      belowSize.forEach((coll, replicas) -> {\n        lastBelowEventMap.put(replicas.get(0).getCore(), now);\n        lastBelowEventMap.put(replicas.get(1).getCore(), now);\n      });\n    }\n  }\n\n","sourceOld":"  @Override\n  public void run() {\n    synchronized(this) {\n      if (isClosed) {\n        log.warn(getName() + \" ran but was already closed\");\n        return;\n      }\n    }\n    AutoScaling.TriggerEventProcessor processor = processorRef.get();\n    if (processor == null) {\n      return;\n    }\n\n    // replica name / info + size, retrieved from leaders only\n    Map<String, ReplicaInfo> currentSizes = new HashMap<>();\n\n    try {\n      ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n      for (String node : clusterState.getLiveNodes()) {\n        Map<String, ReplicaInfo> metricTags = new HashMap<>();\n        // coll, shard, replica\n        Map<String, Map<String, List<ReplicaInfo>>> infos = cloudManager.getNodeStateProvider().getReplicaInfo(node, Collections.emptyList());\n        infos.forEach((coll, shards) -> {\n          if (!collections.isEmpty() && !collections.contains(coll)) {\n            return;\n          }\n          DocCollection docCollection = clusterState.getCollection(coll);\n\n          shards.forEach((sh, replicas) -> {\n            // check only the leader of a replica in active shard\n            Slice s = docCollection.getSlice(sh);\n            if (s.getState() != Slice.State.ACTIVE) {\n              return;\n            }\n            Replica r = s.getLeader();\n            // no leader - don't do anything\n            if (r == null) {\n              return;\n            }\n            // find ReplicaInfo\n            ReplicaInfo info = null;\n            for (ReplicaInfo ri : replicas) {\n              if (r.getCoreName().equals(ri.getCore())) {\n                info = ri;\n                break;\n              }\n            }\n            if (info == null) {\n              // probably replica is not on this node?\n              return;\n            }\n            // we have to translate to the metrics registry name, which uses \"_replica_nN\" as suffix\n            String replicaName = Utils.parseMetricsReplicaName(coll, info.getCore());\n            if (replicaName == null) { // should never happen???\n              replicaName = info.getName(); // which is actually coreNode name...\n            }\n            String registry = SolrCoreMetricManager.createRegistryName(true, coll, sh, replicaName, null);\n            String tag = \"metrics:\" + registry + \":INDEX.sizeInBytes\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.numDocs\";\n            metricTags.put(tag, info);\n          });\n        });\n        if (metricTags.isEmpty()) {\n          continue;\n        }\n        Map<String, Object> sizes = cloudManager.getNodeStateProvider().getNodeValues(node, metricTags.keySet());\n        sizes.forEach((tag, size) -> {\n          final ReplicaInfo info = metricTags.get(tag);\n          if (info == null) {\n            log.warn(\"Missing replica info for response tag \" + tag);\n          } else {\n            // verify that it's a Number\n            if (!(size instanceof Number)) {\n              log.warn(\"invalid size value - not a number: '\" + size + \"' is \" + size.getClass().getName());\n              return;\n            }\n\n            ReplicaInfo currentInfo = currentSizes.computeIfAbsent(info.getCore(), k -> (ReplicaInfo)info.clone());\n            if (tag.contains(\"INDEX\")) {\n              currentInfo.getVariables().put(BYTES_SIZE_PROP, ((Number) size).longValue());\n            } else {\n              currentInfo.getVariables().put(DOCS_SIZE_PROP, ((Number) size).longValue());\n            }\n          }\n        });\n      }\n    } catch (IOException e) {\n      log.warn(\"Error running trigger \" + getName(), e);\n      return;\n    }\n\n    long now = cloudManager.getTimeSource().getTimeNs();\n\n    // now check thresholds\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> aboveSize = new HashMap<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) > aboveDocs) {\n        if (waitForElapsed(coreName, now, lastAboveEventMap)) {\n          List<ReplicaInfo> infos = aboveSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes) {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_DOCS_PROP);\n            }\n            infos.add(info);\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastAboveEventMap.remove(coreName);\n      }\n    });\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> belowSize = new HashMap<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if ((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) < belowDocs) {\n        if (waitForElapsed(coreName, now, lastBelowEventMap)) {\n          List<ReplicaInfo> infos = belowSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes) {\n              info.getVariables().put(VIOLATION_PROP, BELOW_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, BELOW_DOCS_PROP);\n            }\n            infos.add(info);\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastBelowEventMap.remove(coreName);\n      }\n    });\n\n    if (aboveSize.isEmpty() && belowSize.isEmpty()) {\n      log.trace(\"NO VIOLATIONS: Now={}\", now);\n      log.trace(\"lastAbove={}\", lastAboveEventMap);\n      log.trace(\"lastBelow={}\", lastBelowEventMap);\n      return;\n    }\n\n    // find the earliest time when a condition was exceeded\n    final AtomicLong eventTime = new AtomicLong(now);\n\n    // calculate ops\n    final List<TriggerEvent.Op> ops = new ArrayList<>();\n    aboveSize.forEach((coll, replicas) -> {\n      replicas.forEach(r -> {\n        TriggerEvent.Op op = new TriggerEvent.Op(aboveOp);\n        op.addHint(Suggester.Hint.COLL_SHARD, new Pair<>(coll, r.getShard()));\n        ops.add(op);\n        Long time = lastAboveEventMap.get(r.getCore());\n        if (time != null && eventTime.get() > time) {\n          eventTime.set(time);\n        }\n      });\n    });\n    belowSize.forEach((coll, replicas) -> {\n      if (replicas.size() < 2) {\n        return;\n      }\n      // sort by increasing size\n      replicas.sort((r1, r2) -> {\n        // XXX this is not quite correct - if BYTES_SIZE_PROP decided that replica got here\n        // then we should be sorting by BYTES_SIZE_PROP. However, since DOCS and BYTES are\n        // loosely correlated it's simpler to sort just by docs (which better reflects the \"too small\"\n        // condition than index size, due to possibly existing deleted docs that still occupy space)\n        long delta = (Long) r1.getVariable(DOCS_SIZE_PROP) - (Long) r2.getVariable(DOCS_SIZE_PROP);\n        if (delta > 0) {\n          return 1;\n        } else if (delta < 0) {\n          return -1;\n        } else {\n          return 0;\n        }\n      });\n\n      // TODO: MERGESHARDS is not implemented yet. For now take the top two smallest shards\n      // TODO: but in the future we probably need to get ones with adjacent ranges.\n\n      // TODO: generate as many MERGESHARDS as needed to consume all belowSize shards\n      TriggerEvent.Op op = new TriggerEvent.Op(belowOp);\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(0).getShard()));\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(1).getShard()));\n      ops.add(op);\n      Long time = lastBelowEventMap.get(replicas.get(0).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n      time = lastBelowEventMap.get(replicas.get(1).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n    });\n\n    if (ops.isEmpty()) {\n      return;\n    }\n    if (processor.process(new IndexSizeEvent(getName(), eventTime.get(), ops, aboveSize, belowSize))) {\n      // update last event times\n      aboveSize.forEach((coll, replicas) -> {\n        replicas.forEach(r -> lastAboveEventMap.put(r.getCore(), now));\n      });\n      belowSize.forEach((coll, replicas) -> {\n        lastBelowEventMap.put(replicas.get(0).getCore(), now);\n        lastBelowEventMap.put(replicas.get(1).getCore(), now);\n      });\n    }\n  }\n\n","bugFix":null,"bugIntro":["256b1da668dae74c726d5d71bce6153202d6b001"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26","date":1531589977,"type":3,"author":"Michael Braun","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/autoscaling/IndexSizeTrigger#run().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/autoscaling/IndexSizeTrigger#run().mjava","sourceNew":"  @Override\n  public void run() {\n    synchronized(this) {\n      if (isClosed) {\n        log.warn(getName() + \" ran but was already closed\");\n        return;\n      }\n    }\n    AutoScaling.TriggerEventProcessor processor = processorRef.get();\n    if (processor == null) {\n      return;\n    }\n\n    // replica name / info + size, retrieved from leaders only\n    Map<String, ReplicaInfo> currentSizes = new HashMap<>();\n\n    try {\n      ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n      for (String node : clusterState.getLiveNodes()) {\n        Map<String, ReplicaInfo> metricTags = new HashMap<>();\n        // coll, shard, replica\n        Map<String, Map<String, List<ReplicaInfo>>> infos = cloudManager.getNodeStateProvider().getReplicaInfo(node, Collections.emptyList());\n        infos.forEach((coll, shards) -> {\n          if (!collections.isEmpty() && !collections.contains(coll)) {\n            return;\n          }\n          DocCollection docCollection = clusterState.getCollection(coll);\n\n          shards.forEach((sh, replicas) -> {\n            // check only the leader of a replica in active shard\n            Slice s = docCollection.getSlice(sh);\n            if (s.getState() != Slice.State.ACTIVE) {\n              return;\n            }\n            Replica r = s.getLeader();\n            // no leader - don't do anything\n            if (r == null) {\n              return;\n            }\n            // find ReplicaInfo\n            ReplicaInfo info = null;\n            for (ReplicaInfo ri : replicas) {\n              if (r.getCoreName().equals(ri.getCore())) {\n                info = ri;\n                break;\n              }\n            }\n            if (info == null) {\n              // probably replica is not on this node?\n              return;\n            }\n            // we have to translate to the metrics registry name, which uses \"_replica_nN\" as suffix\n            String replicaName = Utils.parseMetricsReplicaName(coll, info.getCore());\n            if (replicaName == null) { // should never happen???\n              replicaName = info.getName(); // which is actually coreNode name...\n            }\n            String registry = SolrCoreMetricManager.createRegistryName(true, coll, sh, replicaName, null);\n            String tag = \"metrics:\" + registry + \":INDEX.sizeInBytes\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.numDocs\";\n            metricTags.put(tag, info);\n          });\n        });\n        if (metricTags.isEmpty()) {\n          continue;\n        }\n        Map<String, Object> sizes = cloudManager.getNodeStateProvider().getNodeValues(node, metricTags.keySet());\n        sizes.forEach((tag, size) -> {\n          final ReplicaInfo info = metricTags.get(tag);\n          if (info == null) {\n            log.warn(\"Missing replica info for response tag \" + tag);\n          } else {\n            // verify that it's a Number\n            if (!(size instanceof Number)) {\n              log.warn(\"invalid size value - not a number: '\" + size + \"' is \" + size.getClass().getName());\n              return;\n            }\n\n            ReplicaInfo currentInfo = currentSizes.computeIfAbsent(info.getCore(), k -> (ReplicaInfo)info.clone());\n            if (tag.contains(\"INDEX\")) {\n              currentInfo.getVariables().put(BYTES_SIZE_PROP, ((Number) size).longValue());\n            } else {\n              currentInfo.getVariables().put(DOCS_SIZE_PROP, ((Number) size).longValue());\n            }\n          }\n        });\n      }\n    } catch (IOException e) {\n      log.warn(\"Error running trigger \" + getName(), e);\n      return;\n    }\n\n    long now = cloudManager.getTimeSource().getTimeNs();\n\n    // now check thresholds\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> aboveSize = new HashMap<>();\n\n    Set<String> splittable = new HashSet<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) > aboveDocs) {\n        if (waitForElapsed(coreName, now, lastAboveEventMap)) {\n          List<ReplicaInfo> infos = aboveSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes) {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_DOCS_PROP);\n            }\n            infos.add(info);\n            splittable.add(info.getName());\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastAboveEventMap.remove(coreName);\n      }\n    });\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> belowSize = new HashMap<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if (((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) < belowDocs) &&\n          // make sure we don't produce conflicting ops\n          !splittable.contains(info.getName())) {\n        if (waitForElapsed(coreName, now, lastBelowEventMap)) {\n          List<ReplicaInfo> infos = belowSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes) {\n              info.getVariables().put(VIOLATION_PROP, BELOW_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, BELOW_DOCS_PROP);\n            }\n            infos.add(info);\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastBelowEventMap.remove(coreName);\n      }\n    });\n\n    if (aboveSize.isEmpty() && belowSize.isEmpty()) {\n      log.trace(\"NO VIOLATIONS: Now={}\", now);\n      log.trace(\"lastAbove={}\", lastAboveEventMap);\n      log.trace(\"lastBelow={}\", lastBelowEventMap);\n      return;\n    }\n\n    // find the earliest time when a condition was exceeded\n    final AtomicLong eventTime = new AtomicLong(now);\n\n    // calculate ops\n    final List<TriggerEvent.Op> ops = new ArrayList<>();\n    aboveSize.forEach((coll, replicas) -> {\n      replicas.forEach(r -> {\n        TriggerEvent.Op op = new TriggerEvent.Op(aboveOp);\n        op.addHint(Suggester.Hint.COLL_SHARD, new Pair<>(coll, r.getShard()));\n        ops.add(op);\n        Long time = lastAboveEventMap.get(r.getCore());\n        if (time != null && eventTime.get() > time) {\n          eventTime.set(time);\n        }\n      });\n    });\n    belowSize.forEach((coll, replicas) -> {\n      if (replicas.size() < 2) {\n        return;\n      }\n      // sort by increasing size\n      replicas.sort((r1, r2) -> {\n        // XXX this is not quite correct - if BYTES_SIZE_PROP decided that replica got here\n        // then we should be sorting by BYTES_SIZE_PROP. However, since DOCS and BYTES are\n        // loosely correlated it's simpler to sort just by docs (which better reflects the \"too small\"\n        // condition than index size, due to possibly existing deleted docs that still occupy space)\n        long delta = (Long) r1.getVariable(DOCS_SIZE_PROP) - (Long) r2.getVariable(DOCS_SIZE_PROP);\n        if (delta > 0) {\n          return 1;\n        } else if (delta < 0) {\n          return -1;\n        } else {\n          return 0;\n        }\n      });\n\n      // TODO: MERGESHARDS is not implemented yet. For now take the top two smallest shards\n      // TODO: but in the future we probably need to get ones with adjacent ranges.\n\n      // TODO: generate as many MERGESHARDS as needed to consume all belowSize shards\n      TriggerEvent.Op op = new TriggerEvent.Op(belowOp);\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(0).getShard()));\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(1).getShard()));\n      ops.add(op);\n      Long time = lastBelowEventMap.get(replicas.get(0).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n      time = lastBelowEventMap.get(replicas.get(1).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n    });\n\n    if (ops.isEmpty()) {\n      return;\n    }\n    if (processor.process(new IndexSizeEvent(getName(), eventTime.get(), ops, aboveSize, belowSize))) {\n      // update last event times\n      aboveSize.forEach((coll, replicas) -> {\n        replicas.forEach(r -> lastAboveEventMap.put(r.getCore(), now));\n      });\n      belowSize.forEach((coll, replicas) -> {\n        lastBelowEventMap.put(replicas.get(0).getCore(), now);\n        lastBelowEventMap.put(replicas.get(1).getCore(), now);\n      });\n    }\n  }\n\n","sourceOld":"  @Override\n  public void run() {\n    synchronized(this) {\n      if (isClosed) {\n        log.warn(getName() + \" ran but was already closed\");\n        return;\n      }\n    }\n    AutoScaling.TriggerEventProcessor processor = processorRef.get();\n    if (processor == null) {\n      return;\n    }\n\n    // replica name / info + size, retrieved from leaders only\n    Map<String, ReplicaInfo> currentSizes = new HashMap<>();\n\n    try {\n      ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n      for (String node : clusterState.getLiveNodes()) {\n        Map<String, ReplicaInfo> metricTags = new HashMap<>();\n        // coll, shard, replica\n        Map<String, Map<String, List<ReplicaInfo>>> infos = cloudManager.getNodeStateProvider().getReplicaInfo(node, Collections.emptyList());\n        infos.forEach((coll, shards) -> {\n          if (!collections.isEmpty() && !collections.contains(coll)) {\n            return;\n          }\n          DocCollection docCollection = clusterState.getCollection(coll);\n\n          shards.forEach((sh, replicas) -> {\n            // check only the leader of a replica in active shard\n            Slice s = docCollection.getSlice(sh);\n            if (s.getState() != Slice.State.ACTIVE) {\n              return;\n            }\n            Replica r = s.getLeader();\n            // no leader - don't do anything\n            if (r == null) {\n              return;\n            }\n            // find ReplicaInfo\n            ReplicaInfo info = null;\n            for (ReplicaInfo ri : replicas) {\n              if (r.getCoreName().equals(ri.getCore())) {\n                info = ri;\n                break;\n              }\n            }\n            if (info == null) {\n              // probably replica is not on this node?\n              return;\n            }\n            // we have to translate to the metrics registry name, which uses \"_replica_nN\" as suffix\n            String replicaName = Utils.parseMetricsReplicaName(coll, info.getCore());\n            if (replicaName == null) { // should never happen???\n              replicaName = info.getName(); // which is actually coreNode name...\n            }\n            String registry = SolrCoreMetricManager.createRegistryName(true, coll, sh, replicaName, null);\n            String tag = \"metrics:\" + registry + \":INDEX.sizeInBytes\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.numDocs\";\n            metricTags.put(tag, info);\n          });\n        });\n        if (metricTags.isEmpty()) {\n          continue;\n        }\n        Map<String, Object> sizes = cloudManager.getNodeStateProvider().getNodeValues(node, metricTags.keySet());\n        sizes.forEach((tag, size) -> {\n          final ReplicaInfo info = metricTags.get(tag);\n          if (info == null) {\n            log.warn(\"Missing replica info for response tag \" + tag);\n          } else {\n            // verify that it's a Number\n            if (!(size instanceof Number)) {\n              log.warn(\"invalid size value - not a number: '\" + size + \"' is \" + size.getClass().getName());\n              return;\n            }\n\n            ReplicaInfo currentInfo = currentSizes.computeIfAbsent(info.getCore(), k -> (ReplicaInfo)info.clone());\n            if (tag.contains(\"INDEX\")) {\n              currentInfo.getVariables().put(BYTES_SIZE_PROP, ((Number) size).longValue());\n            } else {\n              currentInfo.getVariables().put(DOCS_SIZE_PROP, ((Number) size).longValue());\n            }\n          }\n        });\n      }\n    } catch (IOException e) {\n      log.warn(\"Error running trigger \" + getName(), e);\n      return;\n    }\n\n    long now = cloudManager.getTimeSource().getTimeNs();\n\n    // now check thresholds\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> aboveSize = new HashMap<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) > aboveDocs) {\n        if (waitForElapsed(coreName, now, lastAboveEventMap)) {\n          List<ReplicaInfo> infos = aboveSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes) {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_DOCS_PROP);\n            }\n            infos.add(info);\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastAboveEventMap.remove(coreName);\n      }\n    });\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> belowSize = new HashMap<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if ((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) < belowDocs) {\n        if (waitForElapsed(coreName, now, lastBelowEventMap)) {\n          List<ReplicaInfo> infos = belowSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes) {\n              info.getVariables().put(VIOLATION_PROP, BELOW_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, BELOW_DOCS_PROP);\n            }\n            infos.add(info);\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastBelowEventMap.remove(coreName);\n      }\n    });\n\n    if (aboveSize.isEmpty() && belowSize.isEmpty()) {\n      log.trace(\"NO VIOLATIONS: Now={}\", now);\n      log.trace(\"lastAbove={}\", lastAboveEventMap);\n      log.trace(\"lastBelow={}\", lastBelowEventMap);\n      return;\n    }\n\n    // find the earliest time when a condition was exceeded\n    final AtomicLong eventTime = new AtomicLong(now);\n\n    // calculate ops\n    final List<TriggerEvent.Op> ops = new ArrayList<>();\n    aboveSize.forEach((coll, replicas) -> {\n      replicas.forEach(r -> {\n        TriggerEvent.Op op = new TriggerEvent.Op(aboveOp);\n        op.addHint(Suggester.Hint.COLL_SHARD, new Pair<>(coll, r.getShard()));\n        ops.add(op);\n        Long time = lastAboveEventMap.get(r.getCore());\n        if (time != null && eventTime.get() > time) {\n          eventTime.set(time);\n        }\n      });\n    });\n    belowSize.forEach((coll, replicas) -> {\n      if (replicas.size() < 2) {\n        return;\n      }\n      // sort by increasing size\n      replicas.sort((r1, r2) -> {\n        // XXX this is not quite correct - if BYTES_SIZE_PROP decided that replica got here\n        // then we should be sorting by BYTES_SIZE_PROP. However, since DOCS and BYTES are\n        // loosely correlated it's simpler to sort just by docs (which better reflects the \"too small\"\n        // condition than index size, due to possibly existing deleted docs that still occupy space)\n        long delta = (Long) r1.getVariable(DOCS_SIZE_PROP) - (Long) r2.getVariable(DOCS_SIZE_PROP);\n        if (delta > 0) {\n          return 1;\n        } else if (delta < 0) {\n          return -1;\n        } else {\n          return 0;\n        }\n      });\n\n      // TODO: MERGESHARDS is not implemented yet. For now take the top two smallest shards\n      // TODO: but in the future we probably need to get ones with adjacent ranges.\n\n      // TODO: generate as many MERGESHARDS as needed to consume all belowSize shards\n      TriggerEvent.Op op = new TriggerEvent.Op(belowOp);\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(0).getShard()));\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(1).getShard()));\n      ops.add(op);\n      Long time = lastBelowEventMap.get(replicas.get(0).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n      time = lastBelowEventMap.get(replicas.get(1).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n    });\n\n    if (ops.isEmpty()) {\n      return;\n    }\n    if (processor.process(new IndexSizeEvent(getName(), eventTime.get(), ops, aboveSize, belowSize))) {\n      // update last event times\n      aboveSize.forEach((coll, replicas) -> {\n        replicas.forEach(r -> lastAboveEventMap.put(r.getCore(), now));\n      });\n      belowSize.forEach((coll, replicas) -> {\n        lastBelowEventMap.put(replicas.get(0).getCore(), now);\n        lastBelowEventMap.put(replicas.get(1).getCore(), now);\n      });\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","date":1531905561,"type":3,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/autoscaling/IndexSizeTrigger#run().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/autoscaling/IndexSizeTrigger#run().mjava","sourceNew":"  @Override\n  public void run() {\n    synchronized(this) {\n      if (isClosed) {\n        log.warn(getName() + \" ran but was already closed\");\n        return;\n      }\n    }\n    AutoScaling.TriggerEventProcessor processor = processorRef.get();\n    if (processor == null) {\n      return;\n    }\n\n    // replica name / info + size, retrieved from leaders only\n    Map<String, ReplicaInfo> currentSizes = new HashMap<>();\n\n    try {\n      ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n      for (String node : clusterState.getLiveNodes()) {\n        Map<String, ReplicaInfo> metricTags = new HashMap<>();\n        // coll, shard, replica\n        Map<String, Map<String, List<ReplicaInfo>>> infos = cloudManager.getNodeStateProvider().getReplicaInfo(node, Collections.emptyList());\n        infos.forEach((coll, shards) -> {\n          if (!collections.isEmpty() && !collections.contains(coll)) {\n            return;\n          }\n          DocCollection docCollection = clusterState.getCollection(coll);\n\n          shards.forEach((sh, replicas) -> {\n            // check only the leader of a replica in active shard\n            Slice s = docCollection.getSlice(sh);\n            if (s.getState() != Slice.State.ACTIVE) {\n              return;\n            }\n            Replica r = s.getLeader();\n            // no leader - don't do anything\n            if (r == null) {\n              return;\n            }\n            // find ReplicaInfo\n            ReplicaInfo info = null;\n            for (ReplicaInfo ri : replicas) {\n              if (r.getCoreName().equals(ri.getCore())) {\n                info = ri;\n                break;\n              }\n            }\n            if (info == null) {\n              // probably replica is not on this node?\n              return;\n            }\n            // we have to translate to the metrics registry name, which uses \"_replica_nN\" as suffix\n            String replicaName = Utils.parseMetricsReplicaName(coll, info.getCore());\n            if (replicaName == null) { // should never happen???\n              replicaName = info.getName(); // which is actually coreNode name...\n            }\n            String registry = SolrCoreMetricManager.createRegistryName(true, coll, sh, replicaName, null);\n            String tag = \"metrics:\" + registry + \":INDEX.sizeInBytes\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.numDocs\";\n            metricTags.put(tag, info);\n          });\n        });\n        if (metricTags.isEmpty()) {\n          continue;\n        }\n        Map<String, Object> sizes = cloudManager.getNodeStateProvider().getNodeValues(node, metricTags.keySet());\n        sizes.forEach((tag, size) -> {\n          final ReplicaInfo info = metricTags.get(tag);\n          if (info == null) {\n            log.warn(\"Missing replica info for response tag \" + tag);\n          } else {\n            // verify that it's a Number\n            if (!(size instanceof Number)) {\n              log.warn(\"invalid size value - not a number: '\" + size + \"' is \" + size.getClass().getName());\n              return;\n            }\n\n            ReplicaInfo currentInfo = currentSizes.computeIfAbsent(info.getCore(), k -> (ReplicaInfo)info.clone());\n            if (tag.contains(\"INDEX\")) {\n              currentInfo.getVariables().put(BYTES_SIZE_PROP, ((Number) size).longValue());\n            } else {\n              currentInfo.getVariables().put(DOCS_SIZE_PROP, ((Number) size).longValue());\n            }\n          }\n        });\n      }\n    } catch (IOException e) {\n      log.warn(\"Error running trigger \" + getName(), e);\n      return;\n    }\n\n    long now = cloudManager.getTimeSource().getTimeNs();\n\n    // now check thresholds\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> aboveSize = new HashMap<>();\n\n    Set<String> splittable = new HashSet<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) > aboveDocs) {\n        if (waitForElapsed(coreName, now, lastAboveEventMap)) {\n          List<ReplicaInfo> infos = aboveSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes) {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_DOCS_PROP);\n            }\n            infos.add(info);\n            splittable.add(info.getName());\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastAboveEventMap.remove(coreName);\n      }\n    });\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> belowSize = new HashMap<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if (((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) < belowDocs) &&\n          // make sure we don't produce conflicting ops\n          !splittable.contains(info.getName())) {\n        if (waitForElapsed(coreName, now, lastBelowEventMap)) {\n          List<ReplicaInfo> infos = belowSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes) {\n              info.getVariables().put(VIOLATION_PROP, BELOW_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, BELOW_DOCS_PROP);\n            }\n            infos.add(info);\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastBelowEventMap.remove(coreName);\n      }\n    });\n\n    if (aboveSize.isEmpty() && belowSize.isEmpty()) {\n      log.trace(\"NO VIOLATIONS: Now={}\", now);\n      log.trace(\"lastAbove={}\", lastAboveEventMap);\n      log.trace(\"lastBelow={}\", lastBelowEventMap);\n      return;\n    }\n\n    // find the earliest time when a condition was exceeded\n    final AtomicLong eventTime = new AtomicLong(now);\n\n    // calculate ops\n    final List<TriggerEvent.Op> ops = new ArrayList<>();\n    aboveSize.forEach((coll, replicas) -> {\n      replicas.forEach(r -> {\n        TriggerEvent.Op op = new TriggerEvent.Op(aboveOp);\n        op.addHint(Suggester.Hint.COLL_SHARD, new Pair<>(coll, r.getShard()));\n        ops.add(op);\n        Long time = lastAboveEventMap.get(r.getCore());\n        if (time != null && eventTime.get() > time) {\n          eventTime.set(time);\n        }\n      });\n    });\n    belowSize.forEach((coll, replicas) -> {\n      if (replicas.size() < 2) {\n        return;\n      }\n      // sort by increasing size\n      replicas.sort((r1, r2) -> {\n        // XXX this is not quite correct - if BYTES_SIZE_PROP decided that replica got here\n        // then we should be sorting by BYTES_SIZE_PROP. However, since DOCS and BYTES are\n        // loosely correlated it's simpler to sort just by docs (which better reflects the \"too small\"\n        // condition than index size, due to possibly existing deleted docs that still occupy space)\n        long delta = (Long) r1.getVariable(DOCS_SIZE_PROP) - (Long) r2.getVariable(DOCS_SIZE_PROP);\n        if (delta > 0) {\n          return 1;\n        } else if (delta < 0) {\n          return -1;\n        } else {\n          return 0;\n        }\n      });\n\n      // TODO: MERGESHARDS is not implemented yet. For now take the top two smallest shards\n      // TODO: but in the future we probably need to get ones with adjacent ranges.\n\n      // TODO: generate as many MERGESHARDS as needed to consume all belowSize shards\n      TriggerEvent.Op op = new TriggerEvent.Op(belowOp);\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(0).getShard()));\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(1).getShard()));\n      ops.add(op);\n      Long time = lastBelowEventMap.get(replicas.get(0).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n      time = lastBelowEventMap.get(replicas.get(1).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n    });\n\n    if (ops.isEmpty()) {\n      return;\n    }\n    if (processor.process(new IndexSizeEvent(getName(), eventTime.get(), ops, aboveSize, belowSize))) {\n      // update last event times\n      aboveSize.forEach((coll, replicas) -> {\n        replicas.forEach(r -> lastAboveEventMap.put(r.getCore(), now));\n      });\n      belowSize.forEach((coll, replicas) -> {\n        lastBelowEventMap.put(replicas.get(0).getCore(), now);\n        lastBelowEventMap.put(replicas.get(1).getCore(), now);\n      });\n    }\n  }\n\n","sourceOld":"  @Override\n  public void run() {\n    synchronized(this) {\n      if (isClosed) {\n        log.warn(getName() + \" ran but was already closed\");\n        return;\n      }\n    }\n    AutoScaling.TriggerEventProcessor processor = processorRef.get();\n    if (processor == null) {\n      return;\n    }\n\n    // replica name / info + size, retrieved from leaders only\n    Map<String, ReplicaInfo> currentSizes = new HashMap<>();\n\n    try {\n      ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n      for (String node : clusterState.getLiveNodes()) {\n        Map<String, ReplicaInfo> metricTags = new HashMap<>();\n        // coll, shard, replica\n        Map<String, Map<String, List<ReplicaInfo>>> infos = cloudManager.getNodeStateProvider().getReplicaInfo(node, Collections.emptyList());\n        infos.forEach((coll, shards) -> {\n          if (!collections.isEmpty() && !collections.contains(coll)) {\n            return;\n          }\n          DocCollection docCollection = clusterState.getCollection(coll);\n\n          shards.forEach((sh, replicas) -> {\n            // check only the leader of a replica in active shard\n            Slice s = docCollection.getSlice(sh);\n            if (s.getState() != Slice.State.ACTIVE) {\n              return;\n            }\n            Replica r = s.getLeader();\n            // no leader - don't do anything\n            if (r == null) {\n              return;\n            }\n            // find ReplicaInfo\n            ReplicaInfo info = null;\n            for (ReplicaInfo ri : replicas) {\n              if (r.getCoreName().equals(ri.getCore())) {\n                info = ri;\n                break;\n              }\n            }\n            if (info == null) {\n              // probably replica is not on this node?\n              return;\n            }\n            // we have to translate to the metrics registry name, which uses \"_replica_nN\" as suffix\n            String replicaName = Utils.parseMetricsReplicaName(coll, info.getCore());\n            if (replicaName == null) { // should never happen???\n              replicaName = info.getName(); // which is actually coreNode name...\n            }\n            String registry = SolrCoreMetricManager.createRegistryName(true, coll, sh, replicaName, null);\n            String tag = \"metrics:\" + registry + \":INDEX.sizeInBytes\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.numDocs\";\n            metricTags.put(tag, info);\n          });\n        });\n        if (metricTags.isEmpty()) {\n          continue;\n        }\n        Map<String, Object> sizes = cloudManager.getNodeStateProvider().getNodeValues(node, metricTags.keySet());\n        sizes.forEach((tag, size) -> {\n          final ReplicaInfo info = metricTags.get(tag);\n          if (info == null) {\n            log.warn(\"Missing replica info for response tag \" + tag);\n          } else {\n            // verify that it's a Number\n            if (!(size instanceof Number)) {\n              log.warn(\"invalid size value - not a number: '\" + size + \"' is \" + size.getClass().getName());\n              return;\n            }\n\n            ReplicaInfo currentInfo = currentSizes.computeIfAbsent(info.getCore(), k -> (ReplicaInfo)info.clone());\n            if (tag.contains(\"INDEX\")) {\n              currentInfo.getVariables().put(BYTES_SIZE_PROP, ((Number) size).longValue());\n            } else {\n              currentInfo.getVariables().put(DOCS_SIZE_PROP, ((Number) size).longValue());\n            }\n          }\n        });\n      }\n    } catch (IOException e) {\n      log.warn(\"Error running trigger \" + getName(), e);\n      return;\n    }\n\n    long now = cloudManager.getTimeSource().getTimeNs();\n\n    // now check thresholds\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> aboveSize = new HashMap<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) > aboveDocs) {\n        if (waitForElapsed(coreName, now, lastAboveEventMap)) {\n          List<ReplicaInfo> infos = aboveSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes) {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_DOCS_PROP);\n            }\n            infos.add(info);\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastAboveEventMap.remove(coreName);\n      }\n    });\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> belowSize = new HashMap<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if ((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) < belowDocs) {\n        if (waitForElapsed(coreName, now, lastBelowEventMap)) {\n          List<ReplicaInfo> infos = belowSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes) {\n              info.getVariables().put(VIOLATION_PROP, BELOW_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, BELOW_DOCS_PROP);\n            }\n            infos.add(info);\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastBelowEventMap.remove(coreName);\n      }\n    });\n\n    if (aboveSize.isEmpty() && belowSize.isEmpty()) {\n      log.trace(\"NO VIOLATIONS: Now={}\", now);\n      log.trace(\"lastAbove={}\", lastAboveEventMap);\n      log.trace(\"lastBelow={}\", lastBelowEventMap);\n      return;\n    }\n\n    // find the earliest time when a condition was exceeded\n    final AtomicLong eventTime = new AtomicLong(now);\n\n    // calculate ops\n    final List<TriggerEvent.Op> ops = new ArrayList<>();\n    aboveSize.forEach((coll, replicas) -> {\n      replicas.forEach(r -> {\n        TriggerEvent.Op op = new TriggerEvent.Op(aboveOp);\n        op.addHint(Suggester.Hint.COLL_SHARD, new Pair<>(coll, r.getShard()));\n        ops.add(op);\n        Long time = lastAboveEventMap.get(r.getCore());\n        if (time != null && eventTime.get() > time) {\n          eventTime.set(time);\n        }\n      });\n    });\n    belowSize.forEach((coll, replicas) -> {\n      if (replicas.size() < 2) {\n        return;\n      }\n      // sort by increasing size\n      replicas.sort((r1, r2) -> {\n        // XXX this is not quite correct - if BYTES_SIZE_PROP decided that replica got here\n        // then we should be sorting by BYTES_SIZE_PROP. However, since DOCS and BYTES are\n        // loosely correlated it's simpler to sort just by docs (which better reflects the \"too small\"\n        // condition than index size, due to possibly existing deleted docs that still occupy space)\n        long delta = (Long) r1.getVariable(DOCS_SIZE_PROP) - (Long) r2.getVariable(DOCS_SIZE_PROP);\n        if (delta > 0) {\n          return 1;\n        } else if (delta < 0) {\n          return -1;\n        } else {\n          return 0;\n        }\n      });\n\n      // TODO: MERGESHARDS is not implemented yet. For now take the top two smallest shards\n      // TODO: but in the future we probably need to get ones with adjacent ranges.\n\n      // TODO: generate as many MERGESHARDS as needed to consume all belowSize shards\n      TriggerEvent.Op op = new TriggerEvent.Op(belowOp);\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(0).getShard()));\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(1).getShard()));\n      ops.add(op);\n      Long time = lastBelowEventMap.get(replicas.get(0).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n      time = lastBelowEventMap.get(replicas.get(1).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n    });\n\n    if (ops.isEmpty()) {\n      return;\n    }\n    if (processor.process(new IndexSizeEvent(getName(), eventTime.get(), ops, aboveSize, belowSize))) {\n      // update last event times\n      aboveSize.forEach((coll, replicas) -> {\n        replicas.forEach(r -> lastAboveEventMap.put(r.getCore(), now));\n      });\n      belowSize.forEach((coll, replicas) -> {\n        lastBelowEventMap.put(replicas.get(0).getCore(), now);\n        lastBelowEventMap.put(replicas.get(1).getCore(), now);\n      });\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"042b92cf48996255bedb0c3c4bf772d7e06e4dea","date":1534272102,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/autoscaling/IndexSizeTrigger#run().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/autoscaling/IndexSizeTrigger#run().mjava","sourceNew":"  @Override\n  public void run() {\n    synchronized(this) {\n      if (isClosed) {\n        log.warn(getName() + \" ran but was already closed\");\n        return;\n      }\n    }\n    AutoScaling.TriggerEventProcessor processor = processorRef.get();\n    if (processor == null) {\n      return;\n    }\n\n    // replica name / info + size, retrieved from leaders only\n    Map<String, ReplicaInfo> currentSizes = new HashMap<>();\n\n    try {\n      ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n      for (String node : clusterState.getLiveNodes()) {\n        Map<String, ReplicaInfo> metricTags = new HashMap<>();\n        // coll, shard, replica\n        Map<String, Map<String, List<ReplicaInfo>>> infos = cloudManager.getNodeStateProvider().getReplicaInfo(node, Collections.emptyList());\n        infos.forEach((coll, shards) -> {\n          if (!collections.isEmpty() && !collections.contains(coll)) {\n            return;\n          }\n          DocCollection docCollection = clusterState.getCollection(coll);\n\n          shards.forEach((sh, replicas) -> {\n            // check only the leader of a replica in active shard\n            Slice s = docCollection.getSlice(sh);\n            if (s.getState() != Slice.State.ACTIVE) {\n              return;\n            }\n            Replica r = s.getLeader();\n            // no leader - don't do anything\n            if (r == null) {\n              return;\n            }\n            // find ReplicaInfo\n            ReplicaInfo info = null;\n            for (ReplicaInfo ri : replicas) {\n              if (r.getCoreName().equals(ri.getCore())) {\n                info = ri;\n                break;\n              }\n            }\n            if (info == null) {\n              // probably replica is not on this node?\n              return;\n            }\n            // we have to translate to the metrics registry name, which uses \"_replica_nN\" as suffix\n            String replicaName = Utils.parseMetricsReplicaName(coll, info.getCore());\n            if (replicaName == null) { // should never happen???\n              replicaName = info.getName(); // which is actually coreNode name...\n            }\n            String registry = SolrCoreMetricManager.createRegistryName(true, coll, sh, replicaName, null);\n            String tag = \"metrics:\" + registry + \":INDEX.sizeInBytes\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.numDocs\";\n            metricTags.put(tag, info);\n          });\n        });\n        if (metricTags.isEmpty()) {\n          continue;\n        }\n        Map<String, Object> sizes = cloudManager.getNodeStateProvider().getNodeValues(node, metricTags.keySet());\n        sizes.forEach((tag, size) -> {\n          final ReplicaInfo info = metricTags.get(tag);\n          if (info == null) {\n            log.warn(\"Missing replica info for response tag \" + tag);\n          } else {\n            // verify that it's a Number\n            if (!(size instanceof Number)) {\n              log.warn(\"invalid size value - not a number: '\" + size + \"' is \" + size.getClass().getName());\n              return;\n            }\n\n            ReplicaInfo currentInfo = currentSizes.computeIfAbsent(info.getCore(), k -> (ReplicaInfo)info.clone());\n            if (tag.contains(\"INDEX\")) {\n              currentInfo.getVariables().put(BYTES_SIZE_PROP, ((Number) size).longValue());\n            } else {\n              currentInfo.getVariables().put(DOCS_SIZE_PROP, ((Number) size).longValue());\n            }\n          }\n        });\n      }\n    } catch (IOException e) {\n      log.warn(\"Error running trigger \" + getName(), e);\n      return;\n    }\n\n    long now = cloudManager.getTimeSource().getTimeNs();\n\n    // now check thresholds\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> aboveSize = new HashMap<>();\n\n    Set<String> splittable = new HashSet<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) > aboveDocs) {\n        if (waitForElapsed(coreName, now, lastAboveEventMap)) {\n          List<ReplicaInfo> infos = aboveSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes) {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_DOCS_PROP);\n            }\n            infos.add(info);\n            splittable.add(info.getName());\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastAboveEventMap.remove(coreName);\n      }\n    });\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> belowSize = new HashMap<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if (((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) < belowDocs) &&\n          // make sure we don't produce conflicting ops\n          !splittable.contains(info.getName())) {\n        if (waitForElapsed(coreName, now, lastBelowEventMap)) {\n          List<ReplicaInfo> infos = belowSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes) {\n              info.getVariables().put(VIOLATION_PROP, BELOW_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, BELOW_DOCS_PROP);\n            }\n            infos.add(info);\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastBelowEventMap.remove(coreName);\n      }\n    });\n\n    if (aboveSize.isEmpty() && belowSize.isEmpty()) {\n      log.trace(\"NO VIOLATIONS: Now={}\", now);\n      log.trace(\"lastAbove={}\", lastAboveEventMap);\n      log.trace(\"lastBelow={}\", lastBelowEventMap);\n      return;\n    }\n\n    // find the earliest time when a condition was exceeded\n    final AtomicLong eventTime = new AtomicLong(now);\n\n    // calculate ops\n    final List<TriggerEvent.Op> ops = new ArrayList<>();\n    aboveSize.forEach((coll, replicas) -> {\n      replicas.forEach(r -> {\n        TriggerEvent.Op op = new TriggerEvent.Op(aboveOp);\n        op.addHint(Suggester.Hint.COLL_SHARD, new Pair<>(coll, r.getShard()));\n        ops.add(op);\n        Long time = lastAboveEventMap.get(r.getCore());\n        if (time != null && eventTime.get() > time) {\n          eventTime.set(time);\n        }\n      });\n    });\n    belowSize.forEach((coll, replicas) -> {\n      if (replicas.size() < 2) {\n        return;\n      }\n      // sort by increasing size\n      replicas.sort((r1, r2) -> {\n        // XXX this is not quite correct - if BYTES_SIZE_PROP decided that replica got here\n        // then we should be sorting by BYTES_SIZE_PROP. However, since DOCS and BYTES are\n        // loosely correlated it's simpler to sort just by docs (which better reflects the \"too small\"\n        // condition than index size, due to possibly existing deleted docs that still occupy space)\n        long delta = (Long) r1.getVariable(DOCS_SIZE_PROP) - (Long) r2.getVariable(DOCS_SIZE_PROP);\n        if (delta > 0) {\n          return 1;\n        } else if (delta < 0) {\n          return -1;\n        } else {\n          return 0;\n        }\n      });\n\n      // TODO: MERGESHARDS is not implemented yet. For now take the top two smallest shards\n      // TODO: but in the future we probably need to get ones with adjacent ranges.\n\n      // TODO: generate as many MERGESHARDS as needed to consume all belowSize shards\n      TriggerEvent.Op op = new TriggerEvent.Op(belowOp);\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(0).getShard()));\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(1).getShard()));\n      ops.add(op);\n      Long time = lastBelowEventMap.get(replicas.get(0).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n      time = lastBelowEventMap.get(replicas.get(1).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n    });\n\n    if (ops.isEmpty()) {\n      return;\n    }\n    if (processor.process(new IndexSizeEvent(getName(), eventTime.get(), ops, aboveSize, belowSize))) {\n      // update last event times\n      aboveSize.forEach((coll, replicas) -> {\n        replicas.forEach(r -> lastAboveEventMap.put(r.getCore(), now));\n      });\n      belowSize.forEach((coll, replicas) -> {\n        if (replicas.size() < 2) {\n          return;\n        }\n        lastBelowEventMap.put(replicas.get(0).getCore(), now);\n        lastBelowEventMap.put(replicas.get(1).getCore(), now);\n      });\n    }\n  }\n\n","sourceOld":"  @Override\n  public void run() {\n    synchronized(this) {\n      if (isClosed) {\n        log.warn(getName() + \" ran but was already closed\");\n        return;\n      }\n    }\n    AutoScaling.TriggerEventProcessor processor = processorRef.get();\n    if (processor == null) {\n      return;\n    }\n\n    // replica name / info + size, retrieved from leaders only\n    Map<String, ReplicaInfo> currentSizes = new HashMap<>();\n\n    try {\n      ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n      for (String node : clusterState.getLiveNodes()) {\n        Map<String, ReplicaInfo> metricTags = new HashMap<>();\n        // coll, shard, replica\n        Map<String, Map<String, List<ReplicaInfo>>> infos = cloudManager.getNodeStateProvider().getReplicaInfo(node, Collections.emptyList());\n        infos.forEach((coll, shards) -> {\n          if (!collections.isEmpty() && !collections.contains(coll)) {\n            return;\n          }\n          DocCollection docCollection = clusterState.getCollection(coll);\n\n          shards.forEach((sh, replicas) -> {\n            // check only the leader of a replica in active shard\n            Slice s = docCollection.getSlice(sh);\n            if (s.getState() != Slice.State.ACTIVE) {\n              return;\n            }\n            Replica r = s.getLeader();\n            // no leader - don't do anything\n            if (r == null) {\n              return;\n            }\n            // find ReplicaInfo\n            ReplicaInfo info = null;\n            for (ReplicaInfo ri : replicas) {\n              if (r.getCoreName().equals(ri.getCore())) {\n                info = ri;\n                break;\n              }\n            }\n            if (info == null) {\n              // probably replica is not on this node?\n              return;\n            }\n            // we have to translate to the metrics registry name, which uses \"_replica_nN\" as suffix\n            String replicaName = Utils.parseMetricsReplicaName(coll, info.getCore());\n            if (replicaName == null) { // should never happen???\n              replicaName = info.getName(); // which is actually coreNode name...\n            }\n            String registry = SolrCoreMetricManager.createRegistryName(true, coll, sh, replicaName, null);\n            String tag = \"metrics:\" + registry + \":INDEX.sizeInBytes\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.numDocs\";\n            metricTags.put(tag, info);\n          });\n        });\n        if (metricTags.isEmpty()) {\n          continue;\n        }\n        Map<String, Object> sizes = cloudManager.getNodeStateProvider().getNodeValues(node, metricTags.keySet());\n        sizes.forEach((tag, size) -> {\n          final ReplicaInfo info = metricTags.get(tag);\n          if (info == null) {\n            log.warn(\"Missing replica info for response tag \" + tag);\n          } else {\n            // verify that it's a Number\n            if (!(size instanceof Number)) {\n              log.warn(\"invalid size value - not a number: '\" + size + \"' is \" + size.getClass().getName());\n              return;\n            }\n\n            ReplicaInfo currentInfo = currentSizes.computeIfAbsent(info.getCore(), k -> (ReplicaInfo)info.clone());\n            if (tag.contains(\"INDEX\")) {\n              currentInfo.getVariables().put(BYTES_SIZE_PROP, ((Number) size).longValue());\n            } else {\n              currentInfo.getVariables().put(DOCS_SIZE_PROP, ((Number) size).longValue());\n            }\n          }\n        });\n      }\n    } catch (IOException e) {\n      log.warn(\"Error running trigger \" + getName(), e);\n      return;\n    }\n\n    long now = cloudManager.getTimeSource().getTimeNs();\n\n    // now check thresholds\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> aboveSize = new HashMap<>();\n\n    Set<String> splittable = new HashSet<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) > aboveDocs) {\n        if (waitForElapsed(coreName, now, lastAboveEventMap)) {\n          List<ReplicaInfo> infos = aboveSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes) {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_DOCS_PROP);\n            }\n            infos.add(info);\n            splittable.add(info.getName());\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastAboveEventMap.remove(coreName);\n      }\n    });\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> belowSize = new HashMap<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if (((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) < belowDocs) &&\n          // make sure we don't produce conflicting ops\n          !splittable.contains(info.getName())) {\n        if (waitForElapsed(coreName, now, lastBelowEventMap)) {\n          List<ReplicaInfo> infos = belowSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes) {\n              info.getVariables().put(VIOLATION_PROP, BELOW_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, BELOW_DOCS_PROP);\n            }\n            infos.add(info);\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastBelowEventMap.remove(coreName);\n      }\n    });\n\n    if (aboveSize.isEmpty() && belowSize.isEmpty()) {\n      log.trace(\"NO VIOLATIONS: Now={}\", now);\n      log.trace(\"lastAbove={}\", lastAboveEventMap);\n      log.trace(\"lastBelow={}\", lastBelowEventMap);\n      return;\n    }\n\n    // find the earliest time when a condition was exceeded\n    final AtomicLong eventTime = new AtomicLong(now);\n\n    // calculate ops\n    final List<TriggerEvent.Op> ops = new ArrayList<>();\n    aboveSize.forEach((coll, replicas) -> {\n      replicas.forEach(r -> {\n        TriggerEvent.Op op = new TriggerEvent.Op(aboveOp);\n        op.addHint(Suggester.Hint.COLL_SHARD, new Pair<>(coll, r.getShard()));\n        ops.add(op);\n        Long time = lastAboveEventMap.get(r.getCore());\n        if (time != null && eventTime.get() > time) {\n          eventTime.set(time);\n        }\n      });\n    });\n    belowSize.forEach((coll, replicas) -> {\n      if (replicas.size() < 2) {\n        return;\n      }\n      // sort by increasing size\n      replicas.sort((r1, r2) -> {\n        // XXX this is not quite correct - if BYTES_SIZE_PROP decided that replica got here\n        // then we should be sorting by BYTES_SIZE_PROP. However, since DOCS and BYTES are\n        // loosely correlated it's simpler to sort just by docs (which better reflects the \"too small\"\n        // condition than index size, due to possibly existing deleted docs that still occupy space)\n        long delta = (Long) r1.getVariable(DOCS_SIZE_PROP) - (Long) r2.getVariable(DOCS_SIZE_PROP);\n        if (delta > 0) {\n          return 1;\n        } else if (delta < 0) {\n          return -1;\n        } else {\n          return 0;\n        }\n      });\n\n      // TODO: MERGESHARDS is not implemented yet. For now take the top two smallest shards\n      // TODO: but in the future we probably need to get ones with adjacent ranges.\n\n      // TODO: generate as many MERGESHARDS as needed to consume all belowSize shards\n      TriggerEvent.Op op = new TriggerEvent.Op(belowOp);\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(0).getShard()));\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(1).getShard()));\n      ops.add(op);\n      Long time = lastBelowEventMap.get(replicas.get(0).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n      time = lastBelowEventMap.get(replicas.get(1).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n    });\n\n    if (ops.isEmpty()) {\n      return;\n    }\n    if (processor.process(new IndexSizeEvent(getName(), eventTime.get(), ops, aboveSize, belowSize))) {\n      // update last event times\n      aboveSize.forEach((coll, replicas) -> {\n        replicas.forEach(r -> lastAboveEventMap.put(r.getCore(), now));\n      });\n      belowSize.forEach((coll, replicas) -> {\n        lastBelowEventMap.put(replicas.get(0).getCore(), now);\n        lastBelowEventMap.put(replicas.get(1).getCore(), now);\n      });\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9414d03a96422f6a92ced130e94f73f3fe1fc9a3","date":1538650105,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/autoscaling/IndexSizeTrigger#run().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/autoscaling/IndexSizeTrigger#run().mjava","sourceNew":"  @Override\n  public void run() {\n    synchronized(this) {\n      if (isClosed) {\n        log.warn(getName() + \" ran but was already closed\");\n        return;\n      }\n    }\n    AutoScaling.TriggerEventProcessor processor = processorRef.get();\n    if (processor == null) {\n      return;\n    }\n\n    // replica name / info + size, retrieved from leaders only\n    Map<String, ReplicaInfo> currentSizes = new HashMap<>();\n\n    try {\n      ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n      for (String node : clusterState.getLiveNodes()) {\n        Map<String, ReplicaInfo> metricTags = new HashMap<>();\n        // coll, shard, replica\n        Map<String, Map<String, List<ReplicaInfo>>> infos = cloudManager.getNodeStateProvider().getReplicaInfo(node, Collections.emptyList());\n        infos.forEach((coll, shards) -> {\n          if (!collections.isEmpty() && !collections.contains(coll)) {\n            return;\n          }\n          DocCollection docCollection = clusterState.getCollection(coll);\n\n          shards.forEach((sh, replicas) -> {\n            // check only the leader of a replica in active shard\n            Slice s = docCollection.getSlice(sh);\n            if (s.getState() != Slice.State.ACTIVE) {\n              return;\n            }\n            Replica r = s.getLeader();\n            // no leader - don't do anything\n            if (r == null) {\n              return;\n            }\n            // find ReplicaInfo\n            ReplicaInfo info = null;\n            for (ReplicaInfo ri : replicas) {\n              if (r.getCoreName().equals(ri.getCore())) {\n                info = ri;\n                break;\n              }\n            }\n            if (info == null) {\n              // probably replica is not on this node?\n              return;\n            }\n            // we have to translate to the metrics registry name, which uses \"_replica_nN\" as suffix\n            String replicaName = Utils.parseMetricsReplicaName(coll, info.getCore());\n            if (replicaName == null) { // should never happen???\n              replicaName = info.getName(); // which is actually coreNode name...\n            }\n            String registry = SolrCoreMetricManager.createRegistryName(true, coll, sh, replicaName, null);\n            String tag = \"metrics:\" + registry + \":INDEX.sizeInBytes\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.numDocs\";\n            metricTags.put(tag, info);\n          });\n        });\n        if (metricTags.isEmpty()) {\n          continue;\n        }\n        Map<String, Object> sizes = cloudManager.getNodeStateProvider().getNodeValues(node, metricTags.keySet());\n        sizes.forEach((tag, size) -> {\n          final ReplicaInfo info = metricTags.get(tag);\n          if (info == null) {\n            log.warn(\"Missing replica info for response tag \" + tag);\n          } else {\n            // verify that it's a Number\n            if (!(size instanceof Number)) {\n              log.warn(\"invalid size value - not a number: '\" + size + \"' is \" + size.getClass().getName());\n              return;\n            }\n\n            ReplicaInfo currentInfo = currentSizes.computeIfAbsent(info.getCore(), k -> (ReplicaInfo)info.clone());\n            if (tag.contains(\"INDEX\")) {\n              currentInfo.getVariables().put(BYTES_SIZE_PROP, ((Number) size).longValue());\n            } else {\n              currentInfo.getVariables().put(DOCS_SIZE_PROP, ((Number) size).longValue());\n            }\n          }\n        });\n      }\n    } catch (IOException e) {\n      log.warn(\"Error running trigger \" + getName(), e);\n      return;\n    }\n\n    long now = cloudManager.getTimeSource().getTimeNs();\n\n    // now check thresholds\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> aboveSize = new HashMap<>();\n\n    Set<String> splittable = new HashSet<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) > aboveDocs) {\n        if (waitForElapsed(coreName, now, lastAboveEventMap)) {\n          List<ReplicaInfo> infos = aboveSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes) {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_DOCS_PROP);\n            }\n            infos.add(info);\n            splittable.add(info.getName());\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastAboveEventMap.remove(coreName);\n      }\n    });\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> belowSize = new HashMap<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if (((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) < belowDocs) &&\n          // make sure we don't produce conflicting ops\n          !splittable.contains(info.getName())) {\n        if (waitForElapsed(coreName, now, lastBelowEventMap)) {\n          List<ReplicaInfo> infos = belowSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes) {\n              info.getVariables().put(VIOLATION_PROP, BELOW_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, BELOW_DOCS_PROP);\n            }\n            infos.add(info);\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastBelowEventMap.remove(coreName);\n      }\n    });\n\n    if (aboveSize.isEmpty() && belowSize.isEmpty()) {\n      log.trace(\"NO VIOLATIONS: Now={}\", now);\n      log.trace(\"lastAbove={}\", lastAboveEventMap);\n      log.trace(\"lastBelow={}\", lastBelowEventMap);\n      return;\n    }\n\n    // find the earliest time when a condition was exceeded\n    final AtomicLong eventTime = new AtomicLong(now);\n\n    // calculate ops\n    final List<TriggerEvent.Op> ops = new ArrayList<>();\n    aboveSize.forEach((coll, replicas) -> {\n      // sort by decreasing size to first split the largest ones\n      // XXX see the comment below about using DOCS_SIZE_PROP in lieu of BYTES_SIZE_PROP\n      replicas.sort((r1, r2) -> {\n        long delta = (Long) r1.getVariable(DOCS_SIZE_PROP) - (Long) r2.getVariable(DOCS_SIZE_PROP);\n        if (delta > 0) {\n          return -1;\n        } else if (delta < 0) {\n          return 1;\n        } else {\n          return 0;\n        }\n      });\n      replicas.forEach(r -> {\n        if (ops.size() >= maxOps) {\n          return;\n        }\n        TriggerEvent.Op op = new TriggerEvent.Op(aboveOp);\n        op.addHint(Suggester.Hint.COLL_SHARD, new Pair<>(coll, r.getShard()));\n        ops.add(op);\n        Long time = lastAboveEventMap.get(r.getCore());\n        if (time != null && eventTime.get() > time) {\n          eventTime.set(time);\n        }\n      });\n    });\n    belowSize.forEach((coll, replicas) -> {\n      if (replicas.size() < 2) {\n        return;\n      }\n      if (ops.size() >= maxOps) {\n        return;\n      }\n      // sort by increasing size\n      replicas.sort((r1, r2) -> {\n        // XXX this is not quite correct - if BYTES_SIZE_PROP decided that replica got here\n        // then we should be sorting by BYTES_SIZE_PROP. However, since DOCS and BYTES are\n        // loosely correlated it's simpler to sort just by docs (which better reflects the \"too small\"\n        // condition than index size, due to possibly existing deleted docs that still occupy space)\n        long delta = (Long) r1.getVariable(DOCS_SIZE_PROP) - (Long) r2.getVariable(DOCS_SIZE_PROP);\n        if (delta > 0) {\n          return 1;\n        } else if (delta < 0) {\n          return -1;\n        } else {\n          return 0;\n        }\n      });\n\n      // TODO: MERGESHARDS is not implemented yet. For now take the top two smallest shards\n      // TODO: but in the future we probably need to get ones with adjacent ranges.\n\n      // TODO: generate as many MERGESHARDS as needed to consume all belowSize shards\n      TriggerEvent.Op op = new TriggerEvent.Op(belowOp);\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(0).getShard()));\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(1).getShard()));\n      ops.add(op);\n      Long time = lastBelowEventMap.get(replicas.get(0).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n      time = lastBelowEventMap.get(replicas.get(1).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n    });\n\n    if (ops.isEmpty()) {\n      return;\n    }\n    if (processor.process(new IndexSizeEvent(getName(), eventTime.get(), ops, aboveSize, belowSize))) {\n      // update last event times\n      aboveSize.forEach((coll, replicas) -> {\n        replicas.forEach(r -> lastAboveEventMap.put(r.getCore(), now));\n      });\n      belowSize.forEach((coll, replicas) -> {\n        if (replicas.size() < 2) {\n          return;\n        }\n        lastBelowEventMap.put(replicas.get(0).getCore(), now);\n        lastBelowEventMap.put(replicas.get(1).getCore(), now);\n      });\n    }\n  }\n\n","sourceOld":"  @Override\n  public void run() {\n    synchronized(this) {\n      if (isClosed) {\n        log.warn(getName() + \" ran but was already closed\");\n        return;\n      }\n    }\n    AutoScaling.TriggerEventProcessor processor = processorRef.get();\n    if (processor == null) {\n      return;\n    }\n\n    // replica name / info + size, retrieved from leaders only\n    Map<String, ReplicaInfo> currentSizes = new HashMap<>();\n\n    try {\n      ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n      for (String node : clusterState.getLiveNodes()) {\n        Map<String, ReplicaInfo> metricTags = new HashMap<>();\n        // coll, shard, replica\n        Map<String, Map<String, List<ReplicaInfo>>> infos = cloudManager.getNodeStateProvider().getReplicaInfo(node, Collections.emptyList());\n        infos.forEach((coll, shards) -> {\n          if (!collections.isEmpty() && !collections.contains(coll)) {\n            return;\n          }\n          DocCollection docCollection = clusterState.getCollection(coll);\n\n          shards.forEach((sh, replicas) -> {\n            // check only the leader of a replica in active shard\n            Slice s = docCollection.getSlice(sh);\n            if (s.getState() != Slice.State.ACTIVE) {\n              return;\n            }\n            Replica r = s.getLeader();\n            // no leader - don't do anything\n            if (r == null) {\n              return;\n            }\n            // find ReplicaInfo\n            ReplicaInfo info = null;\n            for (ReplicaInfo ri : replicas) {\n              if (r.getCoreName().equals(ri.getCore())) {\n                info = ri;\n                break;\n              }\n            }\n            if (info == null) {\n              // probably replica is not on this node?\n              return;\n            }\n            // we have to translate to the metrics registry name, which uses \"_replica_nN\" as suffix\n            String replicaName = Utils.parseMetricsReplicaName(coll, info.getCore());\n            if (replicaName == null) { // should never happen???\n              replicaName = info.getName(); // which is actually coreNode name...\n            }\n            String registry = SolrCoreMetricManager.createRegistryName(true, coll, sh, replicaName, null);\n            String tag = \"metrics:\" + registry + \":INDEX.sizeInBytes\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.numDocs\";\n            metricTags.put(tag, info);\n          });\n        });\n        if (metricTags.isEmpty()) {\n          continue;\n        }\n        Map<String, Object> sizes = cloudManager.getNodeStateProvider().getNodeValues(node, metricTags.keySet());\n        sizes.forEach((tag, size) -> {\n          final ReplicaInfo info = metricTags.get(tag);\n          if (info == null) {\n            log.warn(\"Missing replica info for response tag \" + tag);\n          } else {\n            // verify that it's a Number\n            if (!(size instanceof Number)) {\n              log.warn(\"invalid size value - not a number: '\" + size + \"' is \" + size.getClass().getName());\n              return;\n            }\n\n            ReplicaInfo currentInfo = currentSizes.computeIfAbsent(info.getCore(), k -> (ReplicaInfo)info.clone());\n            if (tag.contains(\"INDEX\")) {\n              currentInfo.getVariables().put(BYTES_SIZE_PROP, ((Number) size).longValue());\n            } else {\n              currentInfo.getVariables().put(DOCS_SIZE_PROP, ((Number) size).longValue());\n            }\n          }\n        });\n      }\n    } catch (IOException e) {\n      log.warn(\"Error running trigger \" + getName(), e);\n      return;\n    }\n\n    long now = cloudManager.getTimeSource().getTimeNs();\n\n    // now check thresholds\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> aboveSize = new HashMap<>();\n\n    Set<String> splittable = new HashSet<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) > aboveDocs) {\n        if (waitForElapsed(coreName, now, lastAboveEventMap)) {\n          List<ReplicaInfo> infos = aboveSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes) {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_DOCS_PROP);\n            }\n            infos.add(info);\n            splittable.add(info.getName());\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastAboveEventMap.remove(coreName);\n      }\n    });\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> belowSize = new HashMap<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if (((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) < belowDocs) &&\n          // make sure we don't produce conflicting ops\n          !splittable.contains(info.getName())) {\n        if (waitForElapsed(coreName, now, lastBelowEventMap)) {\n          List<ReplicaInfo> infos = belowSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes) {\n              info.getVariables().put(VIOLATION_PROP, BELOW_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, BELOW_DOCS_PROP);\n            }\n            infos.add(info);\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastBelowEventMap.remove(coreName);\n      }\n    });\n\n    if (aboveSize.isEmpty() && belowSize.isEmpty()) {\n      log.trace(\"NO VIOLATIONS: Now={}\", now);\n      log.trace(\"lastAbove={}\", lastAboveEventMap);\n      log.trace(\"lastBelow={}\", lastBelowEventMap);\n      return;\n    }\n\n    // find the earliest time when a condition was exceeded\n    final AtomicLong eventTime = new AtomicLong(now);\n\n    // calculate ops\n    final List<TriggerEvent.Op> ops = new ArrayList<>();\n    aboveSize.forEach((coll, replicas) -> {\n      replicas.forEach(r -> {\n        TriggerEvent.Op op = new TriggerEvent.Op(aboveOp);\n        op.addHint(Suggester.Hint.COLL_SHARD, new Pair<>(coll, r.getShard()));\n        ops.add(op);\n        Long time = lastAboveEventMap.get(r.getCore());\n        if (time != null && eventTime.get() > time) {\n          eventTime.set(time);\n        }\n      });\n    });\n    belowSize.forEach((coll, replicas) -> {\n      if (replicas.size() < 2) {\n        return;\n      }\n      // sort by increasing size\n      replicas.sort((r1, r2) -> {\n        // XXX this is not quite correct - if BYTES_SIZE_PROP decided that replica got here\n        // then we should be sorting by BYTES_SIZE_PROP. However, since DOCS and BYTES are\n        // loosely correlated it's simpler to sort just by docs (which better reflects the \"too small\"\n        // condition than index size, due to possibly existing deleted docs that still occupy space)\n        long delta = (Long) r1.getVariable(DOCS_SIZE_PROP) - (Long) r2.getVariable(DOCS_SIZE_PROP);\n        if (delta > 0) {\n          return 1;\n        } else if (delta < 0) {\n          return -1;\n        } else {\n          return 0;\n        }\n      });\n\n      // TODO: MERGESHARDS is not implemented yet. For now take the top two smallest shards\n      // TODO: but in the future we probably need to get ones with adjacent ranges.\n\n      // TODO: generate as many MERGESHARDS as needed to consume all belowSize shards\n      TriggerEvent.Op op = new TriggerEvent.Op(belowOp);\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(0).getShard()));\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(1).getShard()));\n      ops.add(op);\n      Long time = lastBelowEventMap.get(replicas.get(0).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n      time = lastBelowEventMap.get(replicas.get(1).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n    });\n\n    if (ops.isEmpty()) {\n      return;\n    }\n    if (processor.process(new IndexSizeEvent(getName(), eventTime.get(), ops, aboveSize, belowSize))) {\n      // update last event times\n      aboveSize.forEach((coll, replicas) -> {\n        replicas.forEach(r -> lastAboveEventMap.put(r.getCore(), now));\n      });\n      belowSize.forEach((coll, replicas) -> {\n        if (replicas.size() < 2) {\n          return;\n        }\n        lastBelowEventMap.put(replicas.get(0).getCore(), now);\n        lastBelowEventMap.put(replicas.get(1).getCore(), now);\n      });\n    }\n  }\n\n","bugFix":null,"bugIntro":["256b1da668dae74c726d5d71bce6153202d6b001"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"383671a9fe2f5147abf22eb1ce56e3ca3d6eb133","date":1541008054,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/autoscaling/IndexSizeTrigger#run().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/autoscaling/IndexSizeTrigger#run().mjava","sourceNew":"  @Override\n  public void run() {\n    synchronized(this) {\n      if (isClosed) {\n        log.warn(getName() + \" ran but was already closed\");\n        return;\n      }\n    }\n    AutoScaling.TriggerEventProcessor processor = processorRef.get();\n    if (processor == null) {\n      return;\n    }\n\n    // replica name / info + size, retrieved from leaders only\n    Map<String, ReplicaInfo> currentSizes = new HashMap<>();\n\n    try {\n      ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n      for (String node : clusterState.getLiveNodes()) {\n        Map<String, ReplicaInfo> metricTags = new HashMap<>();\n        // coll, shard, replica\n        Map<String, Map<String, List<ReplicaInfo>>> infos = cloudManager.getNodeStateProvider().getReplicaInfo(node, Collections.emptyList());\n        infos.forEach((coll, shards) -> {\n          if (!collections.isEmpty() && !collections.contains(coll)) {\n            return;\n          }\n          DocCollection docCollection = clusterState.getCollection(coll);\n\n          shards.forEach((sh, replicas) -> {\n            // check only the leader replica in an active shard\n            Slice s = docCollection.getSlice(sh);\n            if (s.getState() != Slice.State.ACTIVE) {\n              return;\n            }\n            Replica r = s.getLeader();\n            // no leader - don't do anything\n            if (r == null) {\n              return;\n            }\n            // not on this node\n            if (!r.getNodeName().equals(node)) {\n              return;\n            }\n            // find ReplicaInfo\n            ReplicaInfo info = null;\n            for (ReplicaInfo ri : replicas) {\n              if (r.getCoreName().equals(ri.getCore())) {\n                info = ri;\n                break;\n              }\n            }\n            if (info == null) {\n              // probably replica is not on this node?\n              return;\n            }\n            // we have to translate to the metrics registry name, which uses \"_replica_nN\" as suffix\n            String replicaName = Utils.parseMetricsReplicaName(coll, info.getCore());\n            if (replicaName == null) { // should never happen???\n              replicaName = info.getName(); // which is actually coreNode name...\n            }\n            String registry = SolrCoreMetricManager.createRegistryName(true, coll, sh, replicaName, null);\n            String tag = \"metrics:\" + registry + \":INDEX.sizeInBytes\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.numDocs\";\n            metricTags.put(tag, info);\n          });\n        });\n        if (metricTags.isEmpty()) {\n          continue;\n        }\n        Map<String, Object> sizes = cloudManager.getNodeStateProvider().getNodeValues(node, metricTags.keySet());\n        sizes.forEach((tag, size) -> {\n          final ReplicaInfo info = metricTags.get(tag);\n          if (info == null) {\n            log.warn(\"Missing replica info for response tag \" + tag);\n          } else {\n            // verify that it's a Number\n            if (!(size instanceof Number)) {\n              log.warn(\"invalid size value for tag \" + tag + \" - not a number: '\" + size + \"' is \" + size.getClass().getName());\n              return;\n            }\n\n            ReplicaInfo currentInfo = currentSizes.computeIfAbsent(info.getCore(), k -> (ReplicaInfo)info.clone());\n            if (tag.contains(\"INDEX\")) {\n              currentInfo.getVariables().put(BYTES_SIZE_PROP, ((Number) size).longValue());\n            } else if (tag.contains(\"SEARCHER\")) {\n              currentInfo.getVariables().put(DOCS_SIZE_PROP, ((Number) size).longValue());\n            }\n          }\n        });\n      }\n    } catch (IOException e) {\n      log.warn(\"Error running trigger \" + getName(), e);\n      return;\n    }\n\n    long now = cloudManager.getTimeSource().getTimeNs();\n\n    // now check thresholds\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> aboveSize = new HashMap<>();\n\n    Set<String> splittable = new HashSet<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) > aboveDocs) {\n        if (waitForElapsed(coreName, now, lastAboveEventMap)) {\n          List<ReplicaInfo> infos = aboveSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes) {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_DOCS_PROP);\n            }\n            infos.add(info);\n            splittable.add(info.getName());\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastAboveEventMap.remove(coreName);\n      }\n    });\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> belowSize = new HashMap<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if (((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) < belowDocs) &&\n          // make sure we don't produce conflicting ops\n          !splittable.contains(info.getName())) {\n        if (waitForElapsed(coreName, now, lastBelowEventMap)) {\n          List<ReplicaInfo> infos = belowSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes) {\n              info.getVariables().put(VIOLATION_PROP, BELOW_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, BELOW_DOCS_PROP);\n            }\n            infos.add(info);\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastBelowEventMap.remove(coreName);\n      }\n    });\n\n    if (aboveSize.isEmpty() && belowSize.isEmpty()) {\n      log.trace(\"NO VIOLATIONS: Now={}\", now);\n      log.trace(\"lastAbove={}\", lastAboveEventMap);\n      log.trace(\"lastBelow={}\", lastBelowEventMap);\n      return;\n    }\n\n    // find the earliest time when a condition was exceeded\n    final AtomicLong eventTime = new AtomicLong(now);\n\n    // calculate ops\n    final List<TriggerEvent.Op> ops = new ArrayList<>();\n    aboveSize.forEach((coll, replicas) -> {\n      // sort by decreasing size to first split the largest ones\n      // XXX see the comment below about using DOCS_SIZE_PROP in lieu of BYTES_SIZE_PROP\n      replicas.sort((r1, r2) -> {\n        long delta = (Long) r1.getVariable(DOCS_SIZE_PROP) - (Long) r2.getVariable(DOCS_SIZE_PROP);\n        if (delta > 0) {\n          return -1;\n        } else if (delta < 0) {\n          return 1;\n        } else {\n          return 0;\n        }\n      });\n      replicas.forEach(r -> {\n        if (ops.size() >= maxOps) {\n          return;\n        }\n        TriggerEvent.Op op = new TriggerEvent.Op(aboveOp);\n        op.addHint(Suggester.Hint.COLL_SHARD, new Pair<>(coll, r.getShard()));\n        Map<String, Object> params = new HashMap<>();\n        params.put(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        op.addHint(Suggester.Hint.PARAMS, params);\n        ops.add(op);\n        Long time = lastAboveEventMap.get(r.getCore());\n        if (time != null && eventTime.get() > time) {\n          eventTime.set(time);\n        }\n      });\n    });\n    belowSize.forEach((coll, replicas) -> {\n      if (replicas.size() < 2) {\n        return;\n      }\n      if (ops.size() >= maxOps) {\n        return;\n      }\n      // sort by increasing size\n      replicas.sort((r1, r2) -> {\n        // XXX this is not quite correct - if BYTES_SIZE_PROP decided that replica got here\n        // then we should be sorting by BYTES_SIZE_PROP. However, since DOCS and BYTES are\n        // loosely correlated it's simpler to sort just by docs (which better reflects the \"too small\"\n        // condition than index size, due to possibly existing deleted docs that still occupy space)\n        long delta = (Long) r1.getVariable(DOCS_SIZE_PROP) - (Long) r2.getVariable(DOCS_SIZE_PROP);\n        if (delta > 0) {\n          return 1;\n        } else if (delta < 0) {\n          return -1;\n        } else {\n          return 0;\n        }\n      });\n\n      // TODO: MERGESHARDS is not implemented yet. For now take the top two smallest shards\n      // TODO: but in the future we probably need to get ones with adjacent ranges.\n\n      // TODO: generate as many MERGESHARDS as needed to consume all belowSize shards\n      TriggerEvent.Op op = new TriggerEvent.Op(belowOp);\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(0).getShard()));\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(1).getShard()));\n      ops.add(op);\n      Long time = lastBelowEventMap.get(replicas.get(0).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n      time = lastBelowEventMap.get(replicas.get(1).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n    });\n\n    if (ops.isEmpty()) {\n      return;\n    }\n    if (processor.process(new IndexSizeEvent(getName(), eventTime.get(), ops, aboveSize, belowSize))) {\n      // update last event times\n      aboveSize.forEach((coll, replicas) -> {\n        replicas.forEach(r -> lastAboveEventMap.put(r.getCore(), now));\n      });\n      belowSize.forEach((coll, replicas) -> {\n        if (replicas.size() < 2) {\n          return;\n        }\n        lastBelowEventMap.put(replicas.get(0).getCore(), now);\n        lastBelowEventMap.put(replicas.get(1).getCore(), now);\n      });\n    }\n  }\n\n","sourceOld":"  @Override\n  public void run() {\n    synchronized(this) {\n      if (isClosed) {\n        log.warn(getName() + \" ran but was already closed\");\n        return;\n      }\n    }\n    AutoScaling.TriggerEventProcessor processor = processorRef.get();\n    if (processor == null) {\n      return;\n    }\n\n    // replica name / info + size, retrieved from leaders only\n    Map<String, ReplicaInfo> currentSizes = new HashMap<>();\n\n    try {\n      ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n      for (String node : clusterState.getLiveNodes()) {\n        Map<String, ReplicaInfo> metricTags = new HashMap<>();\n        // coll, shard, replica\n        Map<String, Map<String, List<ReplicaInfo>>> infos = cloudManager.getNodeStateProvider().getReplicaInfo(node, Collections.emptyList());\n        infos.forEach((coll, shards) -> {\n          if (!collections.isEmpty() && !collections.contains(coll)) {\n            return;\n          }\n          DocCollection docCollection = clusterState.getCollection(coll);\n\n          shards.forEach((sh, replicas) -> {\n            // check only the leader of a replica in active shard\n            Slice s = docCollection.getSlice(sh);\n            if (s.getState() != Slice.State.ACTIVE) {\n              return;\n            }\n            Replica r = s.getLeader();\n            // no leader - don't do anything\n            if (r == null) {\n              return;\n            }\n            // find ReplicaInfo\n            ReplicaInfo info = null;\n            for (ReplicaInfo ri : replicas) {\n              if (r.getCoreName().equals(ri.getCore())) {\n                info = ri;\n                break;\n              }\n            }\n            if (info == null) {\n              // probably replica is not on this node?\n              return;\n            }\n            // we have to translate to the metrics registry name, which uses \"_replica_nN\" as suffix\n            String replicaName = Utils.parseMetricsReplicaName(coll, info.getCore());\n            if (replicaName == null) { // should never happen???\n              replicaName = info.getName(); // which is actually coreNode name...\n            }\n            String registry = SolrCoreMetricManager.createRegistryName(true, coll, sh, replicaName, null);\n            String tag = \"metrics:\" + registry + \":INDEX.sizeInBytes\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.numDocs\";\n            metricTags.put(tag, info);\n          });\n        });\n        if (metricTags.isEmpty()) {\n          continue;\n        }\n        Map<String, Object> sizes = cloudManager.getNodeStateProvider().getNodeValues(node, metricTags.keySet());\n        sizes.forEach((tag, size) -> {\n          final ReplicaInfo info = metricTags.get(tag);\n          if (info == null) {\n            log.warn(\"Missing replica info for response tag \" + tag);\n          } else {\n            // verify that it's a Number\n            if (!(size instanceof Number)) {\n              log.warn(\"invalid size value - not a number: '\" + size + \"' is \" + size.getClass().getName());\n              return;\n            }\n\n            ReplicaInfo currentInfo = currentSizes.computeIfAbsent(info.getCore(), k -> (ReplicaInfo)info.clone());\n            if (tag.contains(\"INDEX\")) {\n              currentInfo.getVariables().put(BYTES_SIZE_PROP, ((Number) size).longValue());\n            } else {\n              currentInfo.getVariables().put(DOCS_SIZE_PROP, ((Number) size).longValue());\n            }\n          }\n        });\n      }\n    } catch (IOException e) {\n      log.warn(\"Error running trigger \" + getName(), e);\n      return;\n    }\n\n    long now = cloudManager.getTimeSource().getTimeNs();\n\n    // now check thresholds\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> aboveSize = new HashMap<>();\n\n    Set<String> splittable = new HashSet<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) > aboveDocs) {\n        if (waitForElapsed(coreName, now, lastAboveEventMap)) {\n          List<ReplicaInfo> infos = aboveSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes) {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_DOCS_PROP);\n            }\n            infos.add(info);\n            splittable.add(info.getName());\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastAboveEventMap.remove(coreName);\n      }\n    });\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> belowSize = new HashMap<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if (((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) < belowDocs) &&\n          // make sure we don't produce conflicting ops\n          !splittable.contains(info.getName())) {\n        if (waitForElapsed(coreName, now, lastBelowEventMap)) {\n          List<ReplicaInfo> infos = belowSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes) {\n              info.getVariables().put(VIOLATION_PROP, BELOW_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, BELOW_DOCS_PROP);\n            }\n            infos.add(info);\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastBelowEventMap.remove(coreName);\n      }\n    });\n\n    if (aboveSize.isEmpty() && belowSize.isEmpty()) {\n      log.trace(\"NO VIOLATIONS: Now={}\", now);\n      log.trace(\"lastAbove={}\", lastAboveEventMap);\n      log.trace(\"lastBelow={}\", lastBelowEventMap);\n      return;\n    }\n\n    // find the earliest time when a condition was exceeded\n    final AtomicLong eventTime = new AtomicLong(now);\n\n    // calculate ops\n    final List<TriggerEvent.Op> ops = new ArrayList<>();\n    aboveSize.forEach((coll, replicas) -> {\n      // sort by decreasing size to first split the largest ones\n      // XXX see the comment below about using DOCS_SIZE_PROP in lieu of BYTES_SIZE_PROP\n      replicas.sort((r1, r2) -> {\n        long delta = (Long) r1.getVariable(DOCS_SIZE_PROP) - (Long) r2.getVariable(DOCS_SIZE_PROP);\n        if (delta > 0) {\n          return -1;\n        } else if (delta < 0) {\n          return 1;\n        } else {\n          return 0;\n        }\n      });\n      replicas.forEach(r -> {\n        if (ops.size() >= maxOps) {\n          return;\n        }\n        TriggerEvent.Op op = new TriggerEvent.Op(aboveOp);\n        op.addHint(Suggester.Hint.COLL_SHARD, new Pair<>(coll, r.getShard()));\n        ops.add(op);\n        Long time = lastAboveEventMap.get(r.getCore());\n        if (time != null && eventTime.get() > time) {\n          eventTime.set(time);\n        }\n      });\n    });\n    belowSize.forEach((coll, replicas) -> {\n      if (replicas.size() < 2) {\n        return;\n      }\n      if (ops.size() >= maxOps) {\n        return;\n      }\n      // sort by increasing size\n      replicas.sort((r1, r2) -> {\n        // XXX this is not quite correct - if BYTES_SIZE_PROP decided that replica got here\n        // then we should be sorting by BYTES_SIZE_PROP. However, since DOCS and BYTES are\n        // loosely correlated it's simpler to sort just by docs (which better reflects the \"too small\"\n        // condition than index size, due to possibly existing deleted docs that still occupy space)\n        long delta = (Long) r1.getVariable(DOCS_SIZE_PROP) - (Long) r2.getVariable(DOCS_SIZE_PROP);\n        if (delta > 0) {\n          return 1;\n        } else if (delta < 0) {\n          return -1;\n        } else {\n          return 0;\n        }\n      });\n\n      // TODO: MERGESHARDS is not implemented yet. For now take the top two smallest shards\n      // TODO: but in the future we probably need to get ones with adjacent ranges.\n\n      // TODO: generate as many MERGESHARDS as needed to consume all belowSize shards\n      TriggerEvent.Op op = new TriggerEvent.Op(belowOp);\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(0).getShard()));\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(1).getShard()));\n      ops.add(op);\n      Long time = lastBelowEventMap.get(replicas.get(0).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n      time = lastBelowEventMap.get(replicas.get(1).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n    });\n\n    if (ops.isEmpty()) {\n      return;\n    }\n    if (processor.process(new IndexSizeEvent(getName(), eventTime.get(), ops, aboveSize, belowSize))) {\n      // update last event times\n      aboveSize.forEach((coll, replicas) -> {\n        replicas.forEach(r -> lastAboveEventMap.put(r.getCore(), now));\n      });\n      belowSize.forEach((coll, replicas) -> {\n        if (replicas.size() < 2) {\n          return;\n        }\n        lastBelowEventMap.put(replicas.get(0).getCore(), now);\n        lastBelowEventMap.put(replicas.get(1).getCore(), now);\n      });\n    }\n  }\n\n","bugFix":["ce71a9836bf1eba34a0ab31884e9eb8ad3f1cef1"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7cac1f2920f8057198f04505797cbabf74dd9a97","date":1546884894,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/autoscaling/IndexSizeTrigger#run().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/autoscaling/IndexSizeTrigger#run().mjava","sourceNew":"  @Override\n  public void run() {\n    synchronized(this) {\n      if (isClosed) {\n        log.warn(getName() + \" ran but was already closed\");\n        return;\n      }\n    }\n    AutoScaling.TriggerEventProcessor processor = processorRef.get();\n    if (processor == null) {\n      return;\n    }\n\n    // replica name / info + size, retrieved from leaders only\n    Map<String, ReplicaInfo> currentSizes = new HashMap<>();\n\n    try {\n      ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n      for (String node : clusterState.getLiveNodes()) {\n        Map<String, ReplicaInfo> metricTags = new HashMap<>();\n        // coll, shard, replica\n        Map<String, Map<String, List<ReplicaInfo>>> infos = cloudManager.getNodeStateProvider().getReplicaInfo(node, Collections.emptyList());\n        infos.forEach((coll, shards) -> {\n          if (!collections.isEmpty() && !collections.contains(coll)) {\n            return;\n          }\n          DocCollection docCollection = clusterState.getCollection(coll);\n\n          shards.forEach((sh, replicas) -> {\n            // check only the leader replica in an active shard\n            Slice s = docCollection.getSlice(sh);\n            if (s.getState() != Slice.State.ACTIVE) {\n              return;\n            }\n            Replica r = s.getLeader();\n            // no leader - don't do anything\n            if (r == null) {\n              return;\n            }\n            // not on this node\n            if (!r.getNodeName().equals(node)) {\n              return;\n            }\n            // find ReplicaInfo\n            ReplicaInfo info = null;\n            for (ReplicaInfo ri : replicas) {\n              if (r.getCoreName().equals(ri.getCore())) {\n                info = ri;\n                break;\n              }\n            }\n            if (info == null) {\n              // probably replica is not on this node?\n              return;\n            }\n            // we have to translate to the metrics registry name, which uses \"_replica_nN\" as suffix\n            String replicaName = Utils.parseMetricsReplicaName(coll, info.getCore());\n            if (replicaName == null) { // should never happen???\n              replicaName = info.getName(); // which is actually coreNode name...\n            }\n            String registry = SolrCoreMetricManager.createRegistryName(true, coll, sh, replicaName, null);\n            String tag = \"metrics:\" + registry + \":INDEX.sizeInBytes\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.numDocs\";\n            metricTags.put(tag, info);\n          });\n        });\n        if (metricTags.isEmpty()) {\n          continue;\n        }\n        Map<String, Object> sizes = cloudManager.getNodeStateProvider().getNodeValues(node, metricTags.keySet());\n        sizes.forEach((tag, size) -> {\n          final ReplicaInfo info = metricTags.get(tag);\n          if (info == null) {\n            log.warn(\"Missing replica info for response tag \" + tag);\n          } else {\n            // verify that it's a Number\n            if (!(size instanceof Number)) {\n              log.warn(\"invalid size value for tag \" + tag + \" - not a number: '\" + size + \"' is \" + size.getClass().getName());\n              return;\n            }\n\n            ReplicaInfo currentInfo = currentSizes.computeIfAbsent(info.getCore(), k -> (ReplicaInfo)info.clone());\n            if (tag.contains(\"INDEX\")) {\n              currentInfo.getVariables().put(BYTES_SIZE_PROP, ((Number) size).longValue());\n            } else if (tag.contains(\"SEARCHER\")) {\n              currentInfo.getVariables().put(DOCS_SIZE_PROP, ((Number) size).longValue());\n            }\n          }\n        });\n      }\n    } catch (IOException e) {\n      log.warn(\"Error running trigger \" + getName(), e);\n      return;\n    }\n\n    long now = cloudManager.getTimeSource().getTimeNs();\n\n    // now check thresholds\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> aboveSize = new HashMap<>();\n\n    Set<String> splittable = new HashSet<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) > aboveDocs) {\n        if (waitForElapsed(coreName, now, lastAboveEventMap)) {\n          List<ReplicaInfo> infos = aboveSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes) {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_DOCS_PROP);\n            }\n            infos.add(info);\n            splittable.add(info.getName());\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastAboveEventMap.remove(coreName);\n      }\n    });\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> belowSize = new HashMap<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if (((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) < belowDocs) &&\n          // make sure we don't produce conflicting ops\n          !splittable.contains(info.getName())) {\n        if (waitForElapsed(coreName, now, lastBelowEventMap)) {\n          List<ReplicaInfo> infos = belowSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes) {\n              info.getVariables().put(VIOLATION_PROP, BELOW_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, BELOW_DOCS_PROP);\n            }\n            infos.add(info);\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastBelowEventMap.remove(coreName);\n      }\n    });\n\n    if (aboveSize.isEmpty() && belowSize.isEmpty()) {\n      log.trace(\"NO VIOLATIONS: Now={}\", now);\n      log.trace(\"lastAbove={}\", lastAboveEventMap);\n      log.trace(\"lastBelow={}\", lastBelowEventMap);\n      return;\n    }\n\n    // find the earliest time when a condition was exceeded\n    final AtomicLong eventTime = new AtomicLong(now);\n\n    // calculate ops\n    final List<TriggerEvent.Op> ops = new ArrayList<>();\n    aboveSize.forEach((coll, replicas) -> {\n      // sort by decreasing size to first split the largest ones\n      // XXX see the comment below about using DOCS_SIZE_PROP in lieu of BYTES_SIZE_PROP\n      replicas.sort((r1, r2) -> {\n        long delta = (Long) r1.getVariable(DOCS_SIZE_PROP) - (Long) r2.getVariable(DOCS_SIZE_PROP);\n        if (delta > 0) {\n          return -1;\n        } else if (delta < 0) {\n          return 1;\n        } else {\n          return 0;\n        }\n      });\n      replicas.forEach(r -> {\n        if (ops.size() >= maxOps) {\n          return;\n        }\n        TriggerEvent.Op op = new TriggerEvent.Op(aboveOp);\n        op.addHint(Suggester.Hint.COLL_SHARD, new Pair<>(coll, r.getShard()));\n        Map<String, Object> params = new HashMap<>();\n        params.put(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        if (splitFuzz > 0) {\n          params.put(CommonAdminParams.SPLIT_FUZZ, splitFuzz);\n        }\n        op.addHint(Suggester.Hint.PARAMS, params);\n        ops.add(op);\n        Long time = lastAboveEventMap.get(r.getCore());\n        if (time != null && eventTime.get() > time) {\n          eventTime.set(time);\n        }\n      });\n    });\n    belowSize.forEach((coll, replicas) -> {\n      if (replicas.size() < 2) {\n        return;\n      }\n      if (ops.size() >= maxOps) {\n        return;\n      }\n      // sort by increasing size\n      replicas.sort((r1, r2) -> {\n        // XXX this is not quite correct - if BYTES_SIZE_PROP decided that replica got here\n        // then we should be sorting by BYTES_SIZE_PROP. However, since DOCS and BYTES are\n        // loosely correlated it's simpler to sort just by docs (which better reflects the \"too small\"\n        // condition than index size, due to possibly existing deleted docs that still occupy space)\n        long delta = (Long) r1.getVariable(DOCS_SIZE_PROP) - (Long) r2.getVariable(DOCS_SIZE_PROP);\n        if (delta > 0) {\n          return 1;\n        } else if (delta < 0) {\n          return -1;\n        } else {\n          return 0;\n        }\n      });\n\n      // TODO: MERGESHARDS is not implemented yet. For now take the top two smallest shards\n      // TODO: but in the future we probably need to get ones with adjacent ranges.\n\n      // TODO: generate as many MERGESHARDS as needed to consume all belowSize shards\n      TriggerEvent.Op op = new TriggerEvent.Op(belowOp);\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(0).getShard()));\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(1).getShard()));\n      ops.add(op);\n      Long time = lastBelowEventMap.get(replicas.get(0).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n      time = lastBelowEventMap.get(replicas.get(1).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n    });\n\n    if (ops.isEmpty()) {\n      return;\n    }\n    if (processor.process(new IndexSizeEvent(getName(), eventTime.get(), ops, aboveSize, belowSize))) {\n      // update last event times\n      aboveSize.forEach((coll, replicas) -> {\n        replicas.forEach(r -> lastAboveEventMap.put(r.getCore(), now));\n      });\n      belowSize.forEach((coll, replicas) -> {\n        if (replicas.size() < 2) {\n          return;\n        }\n        lastBelowEventMap.put(replicas.get(0).getCore(), now);\n        lastBelowEventMap.put(replicas.get(1).getCore(), now);\n      });\n    }\n  }\n\n","sourceOld":"  @Override\n  public void run() {\n    synchronized(this) {\n      if (isClosed) {\n        log.warn(getName() + \" ran but was already closed\");\n        return;\n      }\n    }\n    AutoScaling.TriggerEventProcessor processor = processorRef.get();\n    if (processor == null) {\n      return;\n    }\n\n    // replica name / info + size, retrieved from leaders only\n    Map<String, ReplicaInfo> currentSizes = new HashMap<>();\n\n    try {\n      ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n      for (String node : clusterState.getLiveNodes()) {\n        Map<String, ReplicaInfo> metricTags = new HashMap<>();\n        // coll, shard, replica\n        Map<String, Map<String, List<ReplicaInfo>>> infos = cloudManager.getNodeStateProvider().getReplicaInfo(node, Collections.emptyList());\n        infos.forEach((coll, shards) -> {\n          if (!collections.isEmpty() && !collections.contains(coll)) {\n            return;\n          }\n          DocCollection docCollection = clusterState.getCollection(coll);\n\n          shards.forEach((sh, replicas) -> {\n            // check only the leader replica in an active shard\n            Slice s = docCollection.getSlice(sh);\n            if (s.getState() != Slice.State.ACTIVE) {\n              return;\n            }\n            Replica r = s.getLeader();\n            // no leader - don't do anything\n            if (r == null) {\n              return;\n            }\n            // not on this node\n            if (!r.getNodeName().equals(node)) {\n              return;\n            }\n            // find ReplicaInfo\n            ReplicaInfo info = null;\n            for (ReplicaInfo ri : replicas) {\n              if (r.getCoreName().equals(ri.getCore())) {\n                info = ri;\n                break;\n              }\n            }\n            if (info == null) {\n              // probably replica is not on this node?\n              return;\n            }\n            // we have to translate to the metrics registry name, which uses \"_replica_nN\" as suffix\n            String replicaName = Utils.parseMetricsReplicaName(coll, info.getCore());\n            if (replicaName == null) { // should never happen???\n              replicaName = info.getName(); // which is actually coreNode name...\n            }\n            String registry = SolrCoreMetricManager.createRegistryName(true, coll, sh, replicaName, null);\n            String tag = \"metrics:\" + registry + \":INDEX.sizeInBytes\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.numDocs\";\n            metricTags.put(tag, info);\n          });\n        });\n        if (metricTags.isEmpty()) {\n          continue;\n        }\n        Map<String, Object> sizes = cloudManager.getNodeStateProvider().getNodeValues(node, metricTags.keySet());\n        sizes.forEach((tag, size) -> {\n          final ReplicaInfo info = metricTags.get(tag);\n          if (info == null) {\n            log.warn(\"Missing replica info for response tag \" + tag);\n          } else {\n            // verify that it's a Number\n            if (!(size instanceof Number)) {\n              log.warn(\"invalid size value for tag \" + tag + \" - not a number: '\" + size + \"' is \" + size.getClass().getName());\n              return;\n            }\n\n            ReplicaInfo currentInfo = currentSizes.computeIfAbsent(info.getCore(), k -> (ReplicaInfo)info.clone());\n            if (tag.contains(\"INDEX\")) {\n              currentInfo.getVariables().put(BYTES_SIZE_PROP, ((Number) size).longValue());\n            } else if (tag.contains(\"SEARCHER\")) {\n              currentInfo.getVariables().put(DOCS_SIZE_PROP, ((Number) size).longValue());\n            }\n          }\n        });\n      }\n    } catch (IOException e) {\n      log.warn(\"Error running trigger \" + getName(), e);\n      return;\n    }\n\n    long now = cloudManager.getTimeSource().getTimeNs();\n\n    // now check thresholds\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> aboveSize = new HashMap<>();\n\n    Set<String> splittable = new HashSet<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) > aboveDocs) {\n        if (waitForElapsed(coreName, now, lastAboveEventMap)) {\n          List<ReplicaInfo> infos = aboveSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes) {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_DOCS_PROP);\n            }\n            infos.add(info);\n            splittable.add(info.getName());\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastAboveEventMap.remove(coreName);\n      }\n    });\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> belowSize = new HashMap<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if (((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) < belowDocs) &&\n          // make sure we don't produce conflicting ops\n          !splittable.contains(info.getName())) {\n        if (waitForElapsed(coreName, now, lastBelowEventMap)) {\n          List<ReplicaInfo> infos = belowSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes) {\n              info.getVariables().put(VIOLATION_PROP, BELOW_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, BELOW_DOCS_PROP);\n            }\n            infos.add(info);\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastBelowEventMap.remove(coreName);\n      }\n    });\n\n    if (aboveSize.isEmpty() && belowSize.isEmpty()) {\n      log.trace(\"NO VIOLATIONS: Now={}\", now);\n      log.trace(\"lastAbove={}\", lastAboveEventMap);\n      log.trace(\"lastBelow={}\", lastBelowEventMap);\n      return;\n    }\n\n    // find the earliest time when a condition was exceeded\n    final AtomicLong eventTime = new AtomicLong(now);\n\n    // calculate ops\n    final List<TriggerEvent.Op> ops = new ArrayList<>();\n    aboveSize.forEach((coll, replicas) -> {\n      // sort by decreasing size to first split the largest ones\n      // XXX see the comment below about using DOCS_SIZE_PROP in lieu of BYTES_SIZE_PROP\n      replicas.sort((r1, r2) -> {\n        long delta = (Long) r1.getVariable(DOCS_SIZE_PROP) - (Long) r2.getVariable(DOCS_SIZE_PROP);\n        if (delta > 0) {\n          return -1;\n        } else if (delta < 0) {\n          return 1;\n        } else {\n          return 0;\n        }\n      });\n      replicas.forEach(r -> {\n        if (ops.size() >= maxOps) {\n          return;\n        }\n        TriggerEvent.Op op = new TriggerEvent.Op(aboveOp);\n        op.addHint(Suggester.Hint.COLL_SHARD, new Pair<>(coll, r.getShard()));\n        Map<String, Object> params = new HashMap<>();\n        params.put(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        op.addHint(Suggester.Hint.PARAMS, params);\n        ops.add(op);\n        Long time = lastAboveEventMap.get(r.getCore());\n        if (time != null && eventTime.get() > time) {\n          eventTime.set(time);\n        }\n      });\n    });\n    belowSize.forEach((coll, replicas) -> {\n      if (replicas.size() < 2) {\n        return;\n      }\n      if (ops.size() >= maxOps) {\n        return;\n      }\n      // sort by increasing size\n      replicas.sort((r1, r2) -> {\n        // XXX this is not quite correct - if BYTES_SIZE_PROP decided that replica got here\n        // then we should be sorting by BYTES_SIZE_PROP. However, since DOCS and BYTES are\n        // loosely correlated it's simpler to sort just by docs (which better reflects the \"too small\"\n        // condition than index size, due to possibly existing deleted docs that still occupy space)\n        long delta = (Long) r1.getVariable(DOCS_SIZE_PROP) - (Long) r2.getVariable(DOCS_SIZE_PROP);\n        if (delta > 0) {\n          return 1;\n        } else if (delta < 0) {\n          return -1;\n        } else {\n          return 0;\n        }\n      });\n\n      // TODO: MERGESHARDS is not implemented yet. For now take the top two smallest shards\n      // TODO: but in the future we probably need to get ones with adjacent ranges.\n\n      // TODO: generate as many MERGESHARDS as needed to consume all belowSize shards\n      TriggerEvent.Op op = new TriggerEvent.Op(belowOp);\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(0).getShard()));\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(1).getShard()));\n      ops.add(op);\n      Long time = lastBelowEventMap.get(replicas.get(0).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n      time = lastBelowEventMap.get(replicas.get(1).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n    });\n\n    if (ops.isEmpty()) {\n      return;\n    }\n    if (processor.process(new IndexSizeEvent(getName(), eventTime.get(), ops, aboveSize, belowSize))) {\n      // update last event times\n      aboveSize.forEach((coll, replicas) -> {\n        replicas.forEach(r -> lastAboveEventMap.put(r.getCore(), now));\n      });\n      belowSize.forEach((coll, replicas) -> {\n        if (replicas.size() < 2) {\n          return;\n        }\n        lastBelowEventMap.put(replicas.get(0).getCore(), now);\n        lastBelowEventMap.put(replicas.get(1).getCore(), now);\n      });\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9ef8d00dbfbeb534eba8a219a5df9d99b2de6ab5","date":1556572478,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/autoscaling/IndexSizeTrigger#run().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/autoscaling/IndexSizeTrigger#run().mjava","sourceNew":"  @Override\n  public void run() {\n    synchronized(this) {\n      if (isClosed) {\n        log.warn(getName() + \" ran but was already closed\");\n        return;\n      }\n    }\n    AutoScaling.TriggerEventProcessor processor = processorRef.get();\n    if (processor == null) {\n      return;\n    }\n\n    // replica name / info + size, retrieved from leaders only\n    Map<String, ReplicaInfo> currentSizes = new HashMap<>();\n\n    try {\n      ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n      for (String node : clusterState.getLiveNodes()) {\n        Map<String, ReplicaInfo> metricTags = new HashMap<>();\n        // coll, shard, replica\n        Map<String, Map<String, List<ReplicaInfo>>> infos = cloudManager.getNodeStateProvider().getReplicaInfo(node, Collections.emptyList());\n        infos.forEach((coll, shards) -> {\n          if (!collections.isEmpty() && !collections.contains(coll)) {\n            return;\n          }\n          DocCollection docCollection = clusterState.getCollection(coll);\n\n          shards.forEach((sh, replicas) -> {\n            // check only the leader replica in an active shard\n            Slice s = docCollection.getSlice(sh);\n            if (s.getState() != Slice.State.ACTIVE) {\n              return;\n            }\n            Replica r = s.getLeader();\n            // no leader - don't do anything\n            if (r == null) {\n              return;\n            }\n            // not on this node\n            if (!r.getNodeName().equals(node)) {\n              return;\n            }\n            // find ReplicaInfo\n            ReplicaInfo info = null;\n            for (ReplicaInfo ri : replicas) {\n              if (r.getCoreName().equals(ri.getCore())) {\n                info = ri;\n                break;\n              }\n            }\n            if (info == null) {\n              // probably replica is not on this node?\n              return;\n            }\n            // we have to translate to the metrics registry name, which uses \"_replica_nN\" as suffix\n            String replicaName = Utils.parseMetricsReplicaName(coll, info.getCore());\n            if (replicaName == null) { // should never happen???\n              replicaName = info.getName(); // which is actually coreNode name...\n            }\n            String registry = SolrCoreMetricManager.createRegistryName(true, coll, sh, replicaName, null);\n            String tag = \"metrics:\" + registry + \":\" + CORE_IDX.metricsAttribute;\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.numDocs\";\n            metricTags.put(tag, info);\n          });\n        });\n        if (metricTags.isEmpty()) {\n          continue;\n        }\n        Map<String, Object> sizes = cloudManager.getNodeStateProvider().getNodeValues(node, metricTags.keySet());\n        sizes.forEach((tag, size) -> {\n          final ReplicaInfo info = metricTags.get(tag);\n          if (info == null) {\n            log.warn(\"Missing replica info for response tag \" + tag);\n          } else {\n            // verify that it's a Number\n            if (!(size instanceof Number)) {\n              log.warn(\"invalid size value for tag \" + tag + \" - not a number: '\" + size + \"' is \" + size.getClass().getName());\n              return;\n            }\n\n            ReplicaInfo currentInfo = currentSizes.computeIfAbsent(info.getCore(), k -> (ReplicaInfo)info.clone());\n            if (tag.contains(\"INDEX\")) {\n              currentInfo.getVariables().put(BYTES_SIZE_PROP, ((Number) size).longValue());\n            } else if (tag.contains(\"SEARCHER\")) {\n              currentInfo.getVariables().put(DOCS_SIZE_PROP, ((Number) size).longValue());\n            }\n          }\n        });\n      }\n    } catch (IOException e) {\n      log.warn(\"Error running trigger \" + getName(), e);\n      return;\n    }\n\n    long now = cloudManager.getTimeSource().getTimeNs();\n\n    // now check thresholds\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> aboveSize = new HashMap<>();\n\n    Set<String> splittable = new HashSet<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) > aboveDocs) {\n        if (waitForElapsed(coreName, now, lastAboveEventMap)) {\n          List<ReplicaInfo> infos = aboveSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes) {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_DOCS_PROP);\n            }\n            infos.add(info);\n            splittable.add(info.getName());\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastAboveEventMap.remove(coreName);\n      }\n    });\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> belowSize = new HashMap<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if (((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) < belowDocs) &&\n          // make sure we don't produce conflicting ops\n          !splittable.contains(info.getName())) {\n        if (waitForElapsed(coreName, now, lastBelowEventMap)) {\n          List<ReplicaInfo> infos = belowSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes) {\n              info.getVariables().put(VIOLATION_PROP, BELOW_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, BELOW_DOCS_PROP);\n            }\n            infos.add(info);\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastBelowEventMap.remove(coreName);\n      }\n    });\n\n    if (aboveSize.isEmpty() && belowSize.isEmpty()) {\n      log.trace(\"NO VIOLATIONS: Now={}\", now);\n      log.trace(\"lastAbove={}\", lastAboveEventMap);\n      log.trace(\"lastBelow={}\", lastBelowEventMap);\n      return;\n    }\n\n    // find the earliest time when a condition was exceeded\n    final AtomicLong eventTime = new AtomicLong(now);\n\n    // calculate ops\n    final List<TriggerEvent.Op> ops = new ArrayList<>();\n    aboveSize.forEach((coll, replicas) -> {\n      // sort by decreasing size to first split the largest ones\n      // XXX see the comment below about using DOCS_SIZE_PROP in lieu of BYTES_SIZE_PROP\n      replicas.sort((r1, r2) -> {\n        long delta = (Long) r1.getVariable(DOCS_SIZE_PROP) - (Long) r2.getVariable(DOCS_SIZE_PROP);\n        if (delta > 0) {\n          return -1;\n        } else if (delta < 0) {\n          return 1;\n        } else {\n          return 0;\n        }\n      });\n      replicas.forEach(r -> {\n        if (ops.size() >= maxOps) {\n          return;\n        }\n        TriggerEvent.Op op = new TriggerEvent.Op(aboveOp);\n        op.addHint(Suggester.Hint.COLL_SHARD, new Pair<>(coll, r.getShard()));\n        Map<String, Object> params = new HashMap<>();\n        params.put(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        if (splitFuzz > 0) {\n          params.put(CommonAdminParams.SPLIT_FUZZ, splitFuzz);\n        }\n        op.addHint(Suggester.Hint.PARAMS, params);\n        ops.add(op);\n        Long time = lastAboveEventMap.get(r.getCore());\n        if (time != null && eventTime.get() > time) {\n          eventTime.set(time);\n        }\n      });\n    });\n    belowSize.forEach((coll, replicas) -> {\n      if (replicas.size() < 2) {\n        return;\n      }\n      if (ops.size() >= maxOps) {\n        return;\n      }\n      // sort by increasing size\n      replicas.sort((r1, r2) -> {\n        // XXX this is not quite correct - if BYTES_SIZE_PROP decided that replica got here\n        // then we should be sorting by BYTES_SIZE_PROP. However, since DOCS and BYTES are\n        // loosely correlated it's simpler to sort just by docs (which better reflects the \"too small\"\n        // condition than index size, due to possibly existing deleted docs that still occupy space)\n        long delta = (Long) r1.getVariable(DOCS_SIZE_PROP) - (Long) r2.getVariable(DOCS_SIZE_PROP);\n        if (delta > 0) {\n          return 1;\n        } else if (delta < 0) {\n          return -1;\n        } else {\n          return 0;\n        }\n      });\n\n      // TODO: MERGESHARDS is not implemented yet. For now take the top two smallest shards\n      // TODO: but in the future we probably need to get ones with adjacent ranges.\n\n      // TODO: generate as many MERGESHARDS as needed to consume all belowSize shards\n      TriggerEvent.Op op = new TriggerEvent.Op(belowOp);\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(0).getShard()));\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(1).getShard()));\n      ops.add(op);\n      Long time = lastBelowEventMap.get(replicas.get(0).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n      time = lastBelowEventMap.get(replicas.get(1).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n    });\n\n    if (ops.isEmpty()) {\n      return;\n    }\n    if (processor.process(new IndexSizeEvent(getName(), eventTime.get(), ops, aboveSize, belowSize))) {\n      // update last event times\n      aboveSize.forEach((coll, replicas) -> {\n        replicas.forEach(r -> lastAboveEventMap.put(r.getCore(), now));\n      });\n      belowSize.forEach((coll, replicas) -> {\n        if (replicas.size() < 2) {\n          return;\n        }\n        lastBelowEventMap.put(replicas.get(0).getCore(), now);\n        lastBelowEventMap.put(replicas.get(1).getCore(), now);\n      });\n    }\n  }\n\n","sourceOld":"  @Override\n  public void run() {\n    synchronized(this) {\n      if (isClosed) {\n        log.warn(getName() + \" ran but was already closed\");\n        return;\n      }\n    }\n    AutoScaling.TriggerEventProcessor processor = processorRef.get();\n    if (processor == null) {\n      return;\n    }\n\n    // replica name / info + size, retrieved from leaders only\n    Map<String, ReplicaInfo> currentSizes = new HashMap<>();\n\n    try {\n      ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n      for (String node : clusterState.getLiveNodes()) {\n        Map<String, ReplicaInfo> metricTags = new HashMap<>();\n        // coll, shard, replica\n        Map<String, Map<String, List<ReplicaInfo>>> infos = cloudManager.getNodeStateProvider().getReplicaInfo(node, Collections.emptyList());\n        infos.forEach((coll, shards) -> {\n          if (!collections.isEmpty() && !collections.contains(coll)) {\n            return;\n          }\n          DocCollection docCollection = clusterState.getCollection(coll);\n\n          shards.forEach((sh, replicas) -> {\n            // check only the leader replica in an active shard\n            Slice s = docCollection.getSlice(sh);\n            if (s.getState() != Slice.State.ACTIVE) {\n              return;\n            }\n            Replica r = s.getLeader();\n            // no leader - don't do anything\n            if (r == null) {\n              return;\n            }\n            // not on this node\n            if (!r.getNodeName().equals(node)) {\n              return;\n            }\n            // find ReplicaInfo\n            ReplicaInfo info = null;\n            for (ReplicaInfo ri : replicas) {\n              if (r.getCoreName().equals(ri.getCore())) {\n                info = ri;\n                break;\n              }\n            }\n            if (info == null) {\n              // probably replica is not on this node?\n              return;\n            }\n            // we have to translate to the metrics registry name, which uses \"_replica_nN\" as suffix\n            String replicaName = Utils.parseMetricsReplicaName(coll, info.getCore());\n            if (replicaName == null) { // should never happen???\n              replicaName = info.getName(); // which is actually coreNode name...\n            }\n            String registry = SolrCoreMetricManager.createRegistryName(true, coll, sh, replicaName, null);\n            String tag = \"metrics:\" + registry + \":INDEX.sizeInBytes\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.numDocs\";\n            metricTags.put(tag, info);\n          });\n        });\n        if (metricTags.isEmpty()) {\n          continue;\n        }\n        Map<String, Object> sizes = cloudManager.getNodeStateProvider().getNodeValues(node, metricTags.keySet());\n        sizes.forEach((tag, size) -> {\n          final ReplicaInfo info = metricTags.get(tag);\n          if (info == null) {\n            log.warn(\"Missing replica info for response tag \" + tag);\n          } else {\n            // verify that it's a Number\n            if (!(size instanceof Number)) {\n              log.warn(\"invalid size value for tag \" + tag + \" - not a number: '\" + size + \"' is \" + size.getClass().getName());\n              return;\n            }\n\n            ReplicaInfo currentInfo = currentSizes.computeIfAbsent(info.getCore(), k -> (ReplicaInfo)info.clone());\n            if (tag.contains(\"INDEX\")) {\n              currentInfo.getVariables().put(BYTES_SIZE_PROP, ((Number) size).longValue());\n            } else if (tag.contains(\"SEARCHER\")) {\n              currentInfo.getVariables().put(DOCS_SIZE_PROP, ((Number) size).longValue());\n            }\n          }\n        });\n      }\n    } catch (IOException e) {\n      log.warn(\"Error running trigger \" + getName(), e);\n      return;\n    }\n\n    long now = cloudManager.getTimeSource().getTimeNs();\n\n    // now check thresholds\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> aboveSize = new HashMap<>();\n\n    Set<String> splittable = new HashSet<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) > aboveDocs) {\n        if (waitForElapsed(coreName, now, lastAboveEventMap)) {\n          List<ReplicaInfo> infos = aboveSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes) {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_DOCS_PROP);\n            }\n            infos.add(info);\n            splittable.add(info.getName());\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastAboveEventMap.remove(coreName);\n      }\n    });\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> belowSize = new HashMap<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if (((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) < belowDocs) &&\n          // make sure we don't produce conflicting ops\n          !splittable.contains(info.getName())) {\n        if (waitForElapsed(coreName, now, lastBelowEventMap)) {\n          List<ReplicaInfo> infos = belowSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes) {\n              info.getVariables().put(VIOLATION_PROP, BELOW_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, BELOW_DOCS_PROP);\n            }\n            infos.add(info);\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastBelowEventMap.remove(coreName);\n      }\n    });\n\n    if (aboveSize.isEmpty() && belowSize.isEmpty()) {\n      log.trace(\"NO VIOLATIONS: Now={}\", now);\n      log.trace(\"lastAbove={}\", lastAboveEventMap);\n      log.trace(\"lastBelow={}\", lastBelowEventMap);\n      return;\n    }\n\n    // find the earliest time when a condition was exceeded\n    final AtomicLong eventTime = new AtomicLong(now);\n\n    // calculate ops\n    final List<TriggerEvent.Op> ops = new ArrayList<>();\n    aboveSize.forEach((coll, replicas) -> {\n      // sort by decreasing size to first split the largest ones\n      // XXX see the comment below about using DOCS_SIZE_PROP in lieu of BYTES_SIZE_PROP\n      replicas.sort((r1, r2) -> {\n        long delta = (Long) r1.getVariable(DOCS_SIZE_PROP) - (Long) r2.getVariable(DOCS_SIZE_PROP);\n        if (delta > 0) {\n          return -1;\n        } else if (delta < 0) {\n          return 1;\n        } else {\n          return 0;\n        }\n      });\n      replicas.forEach(r -> {\n        if (ops.size() >= maxOps) {\n          return;\n        }\n        TriggerEvent.Op op = new TriggerEvent.Op(aboveOp);\n        op.addHint(Suggester.Hint.COLL_SHARD, new Pair<>(coll, r.getShard()));\n        Map<String, Object> params = new HashMap<>();\n        params.put(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        if (splitFuzz > 0) {\n          params.put(CommonAdminParams.SPLIT_FUZZ, splitFuzz);\n        }\n        op.addHint(Suggester.Hint.PARAMS, params);\n        ops.add(op);\n        Long time = lastAboveEventMap.get(r.getCore());\n        if (time != null && eventTime.get() > time) {\n          eventTime.set(time);\n        }\n      });\n    });\n    belowSize.forEach((coll, replicas) -> {\n      if (replicas.size() < 2) {\n        return;\n      }\n      if (ops.size() >= maxOps) {\n        return;\n      }\n      // sort by increasing size\n      replicas.sort((r1, r2) -> {\n        // XXX this is not quite correct - if BYTES_SIZE_PROP decided that replica got here\n        // then we should be sorting by BYTES_SIZE_PROP. However, since DOCS and BYTES are\n        // loosely correlated it's simpler to sort just by docs (which better reflects the \"too small\"\n        // condition than index size, due to possibly existing deleted docs that still occupy space)\n        long delta = (Long) r1.getVariable(DOCS_SIZE_PROP) - (Long) r2.getVariable(DOCS_SIZE_PROP);\n        if (delta > 0) {\n          return 1;\n        } else if (delta < 0) {\n          return -1;\n        } else {\n          return 0;\n        }\n      });\n\n      // TODO: MERGESHARDS is not implemented yet. For now take the top two smallest shards\n      // TODO: but in the future we probably need to get ones with adjacent ranges.\n\n      // TODO: generate as many MERGESHARDS as needed to consume all belowSize shards\n      TriggerEvent.Op op = new TriggerEvent.Op(belowOp);\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(0).getShard()));\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(1).getShard()));\n      ops.add(op);\n      Long time = lastBelowEventMap.get(replicas.get(0).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n      time = lastBelowEventMap.get(replicas.get(1).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n    });\n\n    if (ops.isEmpty()) {\n      return;\n    }\n    if (processor.process(new IndexSizeEvent(getName(), eventTime.get(), ops, aboveSize, belowSize))) {\n      // update last event times\n      aboveSize.forEach((coll, replicas) -> {\n        replicas.forEach(r -> lastAboveEventMap.put(r.getCore(), now));\n      });\n      belowSize.forEach((coll, replicas) -> {\n        if (replicas.size() < 2) {\n          return;\n        }\n        lastBelowEventMap.put(replicas.get(0).getCore(), now);\n        lastBelowEventMap.put(replicas.get(1).getCore(), now);\n      });\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2d80c1ad9241ae005a167d7ee8ac473601b0e57c","date":1559036097,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/autoscaling/IndexSizeTrigger#run().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/autoscaling/IndexSizeTrigger#run().mjava","sourceNew":"  @Override\n  public void run() {\n    synchronized(this) {\n      if (isClosed) {\n        log.warn(getName() + \" ran but was already closed\");\n        return;\n      }\n    }\n    AutoScaling.TriggerEventProcessor processor = processorRef.get();\n    if (processor == null) {\n      return;\n    }\n\n    // replica name / info + size, retrieved from leaders only\n    Map<String, ReplicaInfo> currentSizes = new HashMap<>();\n\n    try {\n      ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n      for (String node : clusterState.getLiveNodes()) {\n        Map<String, ReplicaInfo> metricTags = new HashMap<>();\n        // coll, shard, replica\n        Map<String, Map<String, List<ReplicaInfo>>> infos = cloudManager.getNodeStateProvider().getReplicaInfo(node, Collections.emptyList());\n        infos.forEach((coll, shards) -> {\n          if (!collections.isEmpty() && !collections.contains(coll)) {\n            return;\n          }\n          DocCollection docCollection = clusterState.getCollection(coll);\n\n          shards.forEach((sh, replicas) -> {\n            // check only the leader replica in an active shard\n            Slice s = docCollection.getSlice(sh);\n            if (s.getState() != Slice.State.ACTIVE) {\n              return;\n            }\n            Replica r = s.getLeader();\n            // no leader - don't do anything\n            if (r == null) {\n              return;\n            }\n            // not on this node\n            if (!r.getNodeName().equals(node)) {\n              return;\n            }\n            // find ReplicaInfo\n            ReplicaInfo info = null;\n            for (ReplicaInfo ri : replicas) {\n              if (r.getCoreName().equals(ri.getCore())) {\n                info = ri;\n                break;\n              }\n            }\n            if (info == null) {\n              // probably replica is not on this node?\n              return;\n            }\n            // we have to translate to the metrics registry name, which uses \"_replica_nN\" as suffix\n            String replicaName = Utils.parseMetricsReplicaName(coll, info.getCore());\n            if (replicaName == null) { // should never happen???\n              replicaName = info.getName(); // which is actually coreNode name...\n            }\n            String registry = SolrCoreMetricManager.createRegistryName(true, coll, sh, replicaName, null);\n            String tag = \"metrics:\" + registry + \":\" + CORE_IDX.metricsAttribute;\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.numDocs\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.maxDoc\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.indexCommitSize\";\n            metricTags.put(tag, info);\n          });\n        });\n        if (metricTags.isEmpty()) {\n          continue;\n        }\n        Map<String, Object> sizes = cloudManager.getNodeStateProvider().getNodeValues(node, metricTags.keySet());\n        sizes.forEach((tag, size) -> {\n          final ReplicaInfo info = metricTags.get(tag);\n          if (info == null) {\n            log.warn(\"Missing replica info for response tag \" + tag);\n          } else {\n            // verify that it's a Number\n            if (!(size instanceof Number)) {\n              log.warn(\"invalid size value for tag \" + tag + \" - not a number: '\" + size + \"' is \" + size.getClass().getName());\n              return;\n            }\n\n            ReplicaInfo currentInfo = currentSizes.computeIfAbsent(info.getCore(), k -> (ReplicaInfo)info.clone());\n            if (tag.contains(\"INDEX\")) {\n              currentInfo.getVariables().put(TOTAL_BYTES_SIZE_PROP, ((Number) size).longValue());\n            } else if (tag.endsWith(\"SEARCHER.searcher.numDocs\")) {\n              currentInfo.getVariables().put(DOCS_SIZE_PROP, ((Number) size).longValue());\n            } else if (tag.endsWith(\"SEARCHER.searcher.maxDoc\")) {\n              currentInfo.getVariables().put(MAX_DOC_PROP, ((Number) size).longValue());\n            } else if (tag.endsWith(\"SEARCHER.searcher.indexCommitSize\")) {\n              currentInfo.getVariables().put(COMMIT_SIZE_PROP, ((Number) size).longValue());\n            }\n          }\n        });\n      }\n    } catch (IOException e) {\n      log.warn(\"Error running trigger \" + getName(), e);\n      return;\n    }\n\n    long now = cloudManager.getTimeSource().getTimeNs();\n\n    // now check thresholds\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> aboveSize = new HashMap<>();\n\n    Set<String> splittable = new HashSet<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      // calculate estimated bytes\n      long maxDoc = (Long)info.getVariable(MAX_DOC_PROP);\n      long numDocs = (Long)info.getVariable(DOCS_SIZE_PROP);\n      long commitSize = (Long)info.getVariable(COMMIT_SIZE_PROP, 0L);\n      if (commitSize <= 0) {\n        commitSize = (Long)info.getVariable(TOTAL_BYTES_SIZE_PROP);\n      }\n      // calculate estimated size as a side-effect\n      commitSize = estimatedSize(maxDoc, numDocs, commitSize);\n      info.getVariables().put(BYTES_SIZE_PROP, commitSize);\n\n      if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) > aboveDocs) {\n        if (waitForElapsed(coreName, now, lastAboveEventMap)) {\n          List<ReplicaInfo> infos = aboveSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes) {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_DOCS_PROP);\n            }\n            infos.add(info);\n            splittable.add(info.getName());\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastAboveEventMap.remove(coreName);\n      }\n    });\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> belowSize = new HashMap<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if (((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) < belowDocs) &&\n          // make sure we don't produce conflicting ops\n          !splittable.contains(info.getName())) {\n        if (waitForElapsed(coreName, now, lastBelowEventMap)) {\n          List<ReplicaInfo> infos = belowSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes) {\n              info.getVariables().put(VIOLATION_PROP, BELOW_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, BELOW_DOCS_PROP);\n            }\n            infos.add(info);\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastBelowEventMap.remove(coreName);\n      }\n    });\n\n    if (aboveSize.isEmpty() && belowSize.isEmpty()) {\n      log.trace(\"NO VIOLATIONS: Now={}\", now);\n      log.trace(\"lastAbove={}\", lastAboveEventMap);\n      log.trace(\"lastBelow={}\", lastBelowEventMap);\n      return;\n    }\n\n    // find the earliest time when a condition was exceeded\n    final AtomicLong eventTime = new AtomicLong(now);\n\n    // calculate ops\n    final List<TriggerEvent.Op> ops = new ArrayList<>();\n    aboveSize.forEach((coll, replicas) -> {\n      // sort by decreasing size to first split the largest ones\n      // XXX see the comment below about using DOCS_SIZE_PROP in lieu of BYTES_SIZE_PROP\n      replicas.sort((r1, r2) -> {\n        long delta = (Long) r1.getVariable(DOCS_SIZE_PROP) - (Long) r2.getVariable(DOCS_SIZE_PROP);\n        if (delta > 0) {\n          return -1;\n        } else if (delta < 0) {\n          return 1;\n        } else {\n          return 0;\n        }\n      });\n      replicas.forEach(r -> {\n        if (ops.size() >= maxOps) {\n          return;\n        }\n        TriggerEvent.Op op = new TriggerEvent.Op(aboveOp);\n        op.addHint(Suggester.Hint.COLL_SHARD, new Pair<>(coll, r.getShard()));\n        Map<String, Object> params = new HashMap<>();\n        params.put(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        if (splitFuzz > 0) {\n          params.put(CommonAdminParams.SPLIT_FUZZ, splitFuzz);\n        }\n        op.addHint(Suggester.Hint.PARAMS, params);\n        ops.add(op);\n        Long time = lastAboveEventMap.get(r.getCore());\n        if (time != null && eventTime.get() > time) {\n          eventTime.set(time);\n        }\n      });\n    });\n    belowSize.forEach((coll, replicas) -> {\n      if (replicas.size() < 2) {\n        return;\n      }\n      if (ops.size() >= maxOps) {\n        return;\n      }\n      // sort by increasing size\n      replicas.sort((r1, r2) -> {\n        // XXX this is not quite correct - if BYTES_SIZE_PROP decided that replica got here\n        // then we should be sorting by BYTES_SIZE_PROP. However, since DOCS and BYTES are\n        // loosely correlated it's simpler to sort just by docs (which better reflects the \"too small\"\n        // condition than index size, due to possibly existing deleted docs that still occupy space)\n        long delta = (Long) r1.getVariable(DOCS_SIZE_PROP) - (Long) r2.getVariable(DOCS_SIZE_PROP);\n        if (delta > 0) {\n          return 1;\n        } else if (delta < 0) {\n          return -1;\n        } else {\n          return 0;\n        }\n      });\n\n      // TODO: MERGESHARDS is not implemented yet. For now take the top two smallest shards\n      // TODO: but in the future we probably need to get ones with adjacent ranges.\n\n      // TODO: generate as many MERGESHARDS as needed to consume all belowSize shards\n      TriggerEvent.Op op = new TriggerEvent.Op(belowOp);\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(0).getShard()));\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(1).getShard()));\n      ops.add(op);\n      Long time = lastBelowEventMap.get(replicas.get(0).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n      time = lastBelowEventMap.get(replicas.get(1).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n    });\n\n    if (ops.isEmpty()) {\n      return;\n    }\n    if (processor.process(new IndexSizeEvent(getName(), eventTime.get(), ops, aboveSize, belowSize))) {\n      // update last event times\n      aboveSize.forEach((coll, replicas) -> {\n        replicas.forEach(r -> lastAboveEventMap.put(r.getCore(), now));\n      });\n      belowSize.forEach((coll, replicas) -> {\n        if (replicas.size() < 2) {\n          return;\n        }\n        lastBelowEventMap.put(replicas.get(0).getCore(), now);\n        lastBelowEventMap.put(replicas.get(1).getCore(), now);\n      });\n    }\n  }\n\n","sourceOld":"  @Override\n  public void run() {\n    synchronized(this) {\n      if (isClosed) {\n        log.warn(getName() + \" ran but was already closed\");\n        return;\n      }\n    }\n    AutoScaling.TriggerEventProcessor processor = processorRef.get();\n    if (processor == null) {\n      return;\n    }\n\n    // replica name / info + size, retrieved from leaders only\n    Map<String, ReplicaInfo> currentSizes = new HashMap<>();\n\n    try {\n      ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n      for (String node : clusterState.getLiveNodes()) {\n        Map<String, ReplicaInfo> metricTags = new HashMap<>();\n        // coll, shard, replica\n        Map<String, Map<String, List<ReplicaInfo>>> infos = cloudManager.getNodeStateProvider().getReplicaInfo(node, Collections.emptyList());\n        infos.forEach((coll, shards) -> {\n          if (!collections.isEmpty() && !collections.contains(coll)) {\n            return;\n          }\n          DocCollection docCollection = clusterState.getCollection(coll);\n\n          shards.forEach((sh, replicas) -> {\n            // check only the leader replica in an active shard\n            Slice s = docCollection.getSlice(sh);\n            if (s.getState() != Slice.State.ACTIVE) {\n              return;\n            }\n            Replica r = s.getLeader();\n            // no leader - don't do anything\n            if (r == null) {\n              return;\n            }\n            // not on this node\n            if (!r.getNodeName().equals(node)) {\n              return;\n            }\n            // find ReplicaInfo\n            ReplicaInfo info = null;\n            for (ReplicaInfo ri : replicas) {\n              if (r.getCoreName().equals(ri.getCore())) {\n                info = ri;\n                break;\n              }\n            }\n            if (info == null) {\n              // probably replica is not on this node?\n              return;\n            }\n            // we have to translate to the metrics registry name, which uses \"_replica_nN\" as suffix\n            String replicaName = Utils.parseMetricsReplicaName(coll, info.getCore());\n            if (replicaName == null) { // should never happen???\n              replicaName = info.getName(); // which is actually coreNode name...\n            }\n            String registry = SolrCoreMetricManager.createRegistryName(true, coll, sh, replicaName, null);\n            String tag = \"metrics:\" + registry + \":\" + CORE_IDX.metricsAttribute;\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.numDocs\";\n            metricTags.put(tag, info);\n          });\n        });\n        if (metricTags.isEmpty()) {\n          continue;\n        }\n        Map<String, Object> sizes = cloudManager.getNodeStateProvider().getNodeValues(node, metricTags.keySet());\n        sizes.forEach((tag, size) -> {\n          final ReplicaInfo info = metricTags.get(tag);\n          if (info == null) {\n            log.warn(\"Missing replica info for response tag \" + tag);\n          } else {\n            // verify that it's a Number\n            if (!(size instanceof Number)) {\n              log.warn(\"invalid size value for tag \" + tag + \" - not a number: '\" + size + \"' is \" + size.getClass().getName());\n              return;\n            }\n\n            ReplicaInfo currentInfo = currentSizes.computeIfAbsent(info.getCore(), k -> (ReplicaInfo)info.clone());\n            if (tag.contains(\"INDEX\")) {\n              currentInfo.getVariables().put(BYTES_SIZE_PROP, ((Number) size).longValue());\n            } else if (tag.contains(\"SEARCHER\")) {\n              currentInfo.getVariables().put(DOCS_SIZE_PROP, ((Number) size).longValue());\n            }\n          }\n        });\n      }\n    } catch (IOException e) {\n      log.warn(\"Error running trigger \" + getName(), e);\n      return;\n    }\n\n    long now = cloudManager.getTimeSource().getTimeNs();\n\n    // now check thresholds\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> aboveSize = new HashMap<>();\n\n    Set<String> splittable = new HashSet<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) > aboveDocs) {\n        if (waitForElapsed(coreName, now, lastAboveEventMap)) {\n          List<ReplicaInfo> infos = aboveSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes) {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_DOCS_PROP);\n            }\n            infos.add(info);\n            splittable.add(info.getName());\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastAboveEventMap.remove(coreName);\n      }\n    });\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> belowSize = new HashMap<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if (((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) < belowDocs) &&\n          // make sure we don't produce conflicting ops\n          !splittable.contains(info.getName())) {\n        if (waitForElapsed(coreName, now, lastBelowEventMap)) {\n          List<ReplicaInfo> infos = belowSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes) {\n              info.getVariables().put(VIOLATION_PROP, BELOW_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, BELOW_DOCS_PROP);\n            }\n            infos.add(info);\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastBelowEventMap.remove(coreName);\n      }\n    });\n\n    if (aboveSize.isEmpty() && belowSize.isEmpty()) {\n      log.trace(\"NO VIOLATIONS: Now={}\", now);\n      log.trace(\"lastAbove={}\", lastAboveEventMap);\n      log.trace(\"lastBelow={}\", lastBelowEventMap);\n      return;\n    }\n\n    // find the earliest time when a condition was exceeded\n    final AtomicLong eventTime = new AtomicLong(now);\n\n    // calculate ops\n    final List<TriggerEvent.Op> ops = new ArrayList<>();\n    aboveSize.forEach((coll, replicas) -> {\n      // sort by decreasing size to first split the largest ones\n      // XXX see the comment below about using DOCS_SIZE_PROP in lieu of BYTES_SIZE_PROP\n      replicas.sort((r1, r2) -> {\n        long delta = (Long) r1.getVariable(DOCS_SIZE_PROP) - (Long) r2.getVariable(DOCS_SIZE_PROP);\n        if (delta > 0) {\n          return -1;\n        } else if (delta < 0) {\n          return 1;\n        } else {\n          return 0;\n        }\n      });\n      replicas.forEach(r -> {\n        if (ops.size() >= maxOps) {\n          return;\n        }\n        TriggerEvent.Op op = new TriggerEvent.Op(aboveOp);\n        op.addHint(Suggester.Hint.COLL_SHARD, new Pair<>(coll, r.getShard()));\n        Map<String, Object> params = new HashMap<>();\n        params.put(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        if (splitFuzz > 0) {\n          params.put(CommonAdminParams.SPLIT_FUZZ, splitFuzz);\n        }\n        op.addHint(Suggester.Hint.PARAMS, params);\n        ops.add(op);\n        Long time = lastAboveEventMap.get(r.getCore());\n        if (time != null && eventTime.get() > time) {\n          eventTime.set(time);\n        }\n      });\n    });\n    belowSize.forEach((coll, replicas) -> {\n      if (replicas.size() < 2) {\n        return;\n      }\n      if (ops.size() >= maxOps) {\n        return;\n      }\n      // sort by increasing size\n      replicas.sort((r1, r2) -> {\n        // XXX this is not quite correct - if BYTES_SIZE_PROP decided that replica got here\n        // then we should be sorting by BYTES_SIZE_PROP. However, since DOCS and BYTES are\n        // loosely correlated it's simpler to sort just by docs (which better reflects the \"too small\"\n        // condition than index size, due to possibly existing deleted docs that still occupy space)\n        long delta = (Long) r1.getVariable(DOCS_SIZE_PROP) - (Long) r2.getVariable(DOCS_SIZE_PROP);\n        if (delta > 0) {\n          return 1;\n        } else if (delta < 0) {\n          return -1;\n        } else {\n          return 0;\n        }\n      });\n\n      // TODO: MERGESHARDS is not implemented yet. For now take the top two smallest shards\n      // TODO: but in the future we probably need to get ones with adjacent ranges.\n\n      // TODO: generate as many MERGESHARDS as needed to consume all belowSize shards\n      TriggerEvent.Op op = new TriggerEvent.Op(belowOp);\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(0).getShard()));\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(1).getShard()));\n      ops.add(op);\n      Long time = lastBelowEventMap.get(replicas.get(0).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n      time = lastBelowEventMap.get(replicas.get(1).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n    });\n\n    if (ops.isEmpty()) {\n      return;\n    }\n    if (processor.process(new IndexSizeEvent(getName(), eventTime.get(), ops, aboveSize, belowSize))) {\n      // update last event times\n      aboveSize.forEach((coll, replicas) -> {\n        replicas.forEach(r -> lastAboveEventMap.put(r.getCore(), now));\n      });\n      belowSize.forEach((coll, replicas) -> {\n        if (replicas.size() < 2) {\n          return;\n        }\n        lastBelowEventMap.put(replicas.get(0).getCore(), now);\n        lastBelowEventMap.put(replicas.get(1).getCore(), now);\n      });\n    }\n  }\n\n","bugFix":null,"bugIntro":["256b1da668dae74c726d5d71bce6153202d6b001"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a97a72dc16d01fda8ca5c9e0264b3604e30ab539","date":1565639985,"type":3,"author":"Megan Carey","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/autoscaling/IndexSizeTrigger#run().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/autoscaling/IndexSizeTrigger#run().mjava","sourceNew":"  @Override\n  public void run() {\n    synchronized(this) {\n      if (isClosed) {\n        log.warn(getName() + \" ran but was already closed\");\n        return;\n      }\n    }\n    AutoScaling.TriggerEventProcessor processor = processorRef.get();\n    if (processor == null) {\n      return;\n    }\n\n    // replica name / info + size, retrieved from leaders only\n    Map<String, ReplicaInfo> currentSizes = new HashMap<>();\n\n    try {\n      ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n      for (String node : clusterState.getLiveNodes()) {\n        Map<String, ReplicaInfo> metricTags = new HashMap<>();\n        // coll, shard, replica\n        Map<String, Map<String, List<ReplicaInfo>>> infos = cloudManager.getNodeStateProvider().getReplicaInfo(node, Collections.emptyList());\n        infos.forEach((coll, shards) -> {\n          if (!collections.isEmpty() && !collections.contains(coll)) {\n            return;\n          }\n          DocCollection docCollection = clusterState.getCollection(coll);\n\n          shards.forEach((sh, replicas) -> {\n            // check only the leader replica in an active shard\n            Slice s = docCollection.getSlice(sh);\n            if (s.getState() != Slice.State.ACTIVE) {\n              return;\n            }\n            Replica r = s.getLeader();\n            // no leader - don't do anything\n            if (r == null) {\n              return;\n            }\n            // not on this node\n            if (!r.getNodeName().equals(node)) {\n              return;\n            }\n            // find ReplicaInfo\n            ReplicaInfo info = null;\n            for (ReplicaInfo ri : replicas) {\n              if (r.getCoreName().equals(ri.getCore())) {\n                info = ri;\n                break;\n              }\n            }\n            if (info == null) {\n              // probably replica is not on this node?\n              return;\n            }\n            // we have to translate to the metrics registry name, which uses \"_replica_nN\" as suffix\n            String replicaName = Utils.parseMetricsReplicaName(coll, info.getCore());\n            if (replicaName == null) { // should never happen???\n              replicaName = info.getName(); // which is actually coreNode name...\n            }\n            String registry = SolrCoreMetricManager.createRegistryName(true, coll, sh, replicaName, null);\n            String tag = \"metrics:\" + registry + \":\" + CORE_IDX.metricsAttribute;\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.numDocs\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.maxDoc\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.indexCommitSize\";\n            metricTags.put(tag, info);\n          });\n        });\n        if (metricTags.isEmpty()) {\n          continue;\n        }\n        Map<String, Object> sizes = cloudManager.getNodeStateProvider().getNodeValues(node, metricTags.keySet());\n        sizes.forEach((tag, size) -> {\n          final ReplicaInfo info = metricTags.get(tag);\n          if (info == null) {\n            log.warn(\"Missing replica info for response tag \" + tag);\n          } else {\n            // verify that it's a Number\n            if (!(size instanceof Number)) {\n              log.warn(\"invalid size value for tag \" + tag + \" - not a number: '\" + size + \"' is \" + size.getClass().getName());\n              return;\n            }\n\n            ReplicaInfo currentInfo = currentSizes.computeIfAbsent(info.getCore(), k -> (ReplicaInfo)info.clone());\n            if (tag.contains(\"INDEX\")) {\n              currentInfo.getVariables().put(TOTAL_BYTES_SIZE_PROP, ((Number) size).longValue());\n            } else if (tag.endsWith(\"SEARCHER.searcher.numDocs\")) {\n              currentInfo.getVariables().put(DOCS_SIZE_PROP, ((Number) size).longValue());\n            } else if (tag.endsWith(\"SEARCHER.searcher.maxDoc\")) {\n              currentInfo.getVariables().put(MAX_DOC_PROP, ((Number) size).longValue());\n            } else if (tag.endsWith(\"SEARCHER.searcher.indexCommitSize\")) {\n              currentInfo.getVariables().put(COMMIT_SIZE_PROP, ((Number) size).longValue());\n            }\n          }\n        });\n      }\n    } catch (IOException e) {\n      log.warn(\"Error running trigger \" + getName(), e);\n      return;\n    }\n\n    long now = cloudManager.getTimeSource().getTimeNs();\n\n    // now check thresholds\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> aboveSize = new HashMap<>();\n\n    Set<String> splittable = new HashSet<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      // calculate estimated bytes\n      long maxDoc = (Long)info.getVariable(MAX_DOC_PROP);\n      long numDocs = (Long)info.getVariable(DOCS_SIZE_PROP);\n      long commitSize = (Long)info.getVariable(COMMIT_SIZE_PROP, 0L);\n      if (commitSize <= 0) {\n        commitSize = (Long)info.getVariable(TOTAL_BYTES_SIZE_PROP);\n      }\n      // calculate estimated size as a side-effect\n      commitSize = estimatedSize(maxDoc, numDocs, commitSize);\n      info.getVariables().put(BYTES_SIZE_PROP, commitSize);\n\n      if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) > aboveDocs) {\n        if (waitForElapsed(coreName, now, lastAboveEventMap)) {\n          List<ReplicaInfo> infos = aboveSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes) {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_DOCS_PROP);\n            }\n            infos.add(info);\n            splittable.add(info.getName());\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastAboveEventMap.remove(coreName);\n      }\n    });\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> belowSize = new HashMap<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if (((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) < belowDocs) &&\n          // make sure we don't produce conflicting ops\n          !splittable.contains(info.getName())) {\n        if (waitForElapsed(coreName, now, lastBelowEventMap)) {\n          List<ReplicaInfo> infos = belowSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes) {\n              info.getVariables().put(VIOLATION_PROP, BELOW_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, BELOW_DOCS_PROP);\n            }\n            infos.add(info);\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastBelowEventMap.remove(coreName);\n      }\n    });\n\n    if (aboveSize.isEmpty() && belowSize.isEmpty()) {\n      log.trace(\"NO VIOLATIONS: Now={}\", now);\n      log.trace(\"lastAbove={}\", lastAboveEventMap);\n      log.trace(\"lastBelow={}\", lastBelowEventMap);\n      return;\n    }\n\n    // find the earliest time when a condition was exceeded\n    final AtomicLong eventTime = new AtomicLong(now);\n\n    // calculate ops\n    final List<TriggerEvent.Op> ops = new ArrayList<>();\n    aboveSize.forEach((coll, replicas) -> {\n      // sort by decreasing size to first split the largest ones\n      // XXX see the comment below about using DOCS_SIZE_PROP in lieu of BYTES_SIZE_PROP\n      replicas.sort((r1, r2) -> {\n        long delta = (Long) r1.getVariable(DOCS_SIZE_PROP) - (Long) r2.getVariable(DOCS_SIZE_PROP);\n        if (delta > 0) {\n          return -1;\n        } else if (delta < 0) {\n          return 1;\n        } else {\n          return 0;\n        }\n      });\n      replicas.forEach(r -> {\n        if (ops.size() >= maxOps) {\n          return;\n        }\n        TriggerEvent.Op op = new TriggerEvent.Op(aboveOp);\n        op.addHint(Suggester.Hint.COLL_SHARD, new Pair<>(coll, r.getShard()));\n        Map<String, Object> params = new HashMap<>();\n        params.put(SPLIT_METHOD_PROP, splitMethod.toLower());\n        if (splitFuzz > 0) {\n          params.put(SPLIT_FUZZ_PROP, splitFuzz);\n        }\n        params.put(SPLIT_BY_PREFIX, splitByPrefix);\n        op.addHint(Suggester.Hint.PARAMS, params);\n        ops.add(op);\n        Long time = lastAboveEventMap.get(r.getCore());\n        if (time != null && eventTime.get() > time) {\n          eventTime.set(time);\n        }\n      });\n    });\n    belowSize.forEach((coll, replicas) -> {\n      if (replicas.size() < 2) {\n        return;\n      }\n      if (ops.size() >= maxOps) {\n        return;\n      }\n      // sort by increasing size\n      replicas.sort((r1, r2) -> {\n        // XXX this is not quite correct - if BYTES_SIZE_PROP decided that replica got here\n        // then we should be sorting by BYTES_SIZE_PROP. However, since DOCS and BYTES are\n        // loosely correlated it's simpler to sort just by docs (which better reflects the \"too small\"\n        // condition than index size, due to possibly existing deleted docs that still occupy space)\n        long delta = (Long) r1.getVariable(DOCS_SIZE_PROP) - (Long) r2.getVariable(DOCS_SIZE_PROP);\n        if (delta > 0) {\n          return 1;\n        } else if (delta < 0) {\n          return -1;\n        } else {\n          return 0;\n        }\n      });\n\n      // TODO: MERGESHARDS is not implemented yet. For now take the top two smallest shards\n      // TODO: but in the future we probably need to get ones with adjacent ranges.\n\n      // TODO: generate as many MERGESHARDS as needed to consume all belowSize shards\n      TriggerEvent.Op op = new TriggerEvent.Op(belowOp);\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(0).getShard()));\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(1).getShard()));\n      ops.add(op);\n      Long time = lastBelowEventMap.get(replicas.get(0).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n      time = lastBelowEventMap.get(replicas.get(1).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n    });\n\n    if (ops.isEmpty()) {\n      return;\n    }\n    if (processor.process(new IndexSizeEvent(getName(), eventTime.get(), ops, aboveSize, belowSize))) {\n      // update last event times\n      aboveSize.forEach((coll, replicas) -> {\n        replicas.forEach(r -> lastAboveEventMap.put(r.getCore(), now));\n      });\n      belowSize.forEach((coll, replicas) -> {\n        if (replicas.size() < 2) {\n          return;\n        }\n        lastBelowEventMap.put(replicas.get(0).getCore(), now);\n        lastBelowEventMap.put(replicas.get(1).getCore(), now);\n      });\n    }\n  }\n\n","sourceOld":"  @Override\n  public void run() {\n    synchronized(this) {\n      if (isClosed) {\n        log.warn(getName() + \" ran but was already closed\");\n        return;\n      }\n    }\n    AutoScaling.TriggerEventProcessor processor = processorRef.get();\n    if (processor == null) {\n      return;\n    }\n\n    // replica name / info + size, retrieved from leaders only\n    Map<String, ReplicaInfo> currentSizes = new HashMap<>();\n\n    try {\n      ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n      for (String node : clusterState.getLiveNodes()) {\n        Map<String, ReplicaInfo> metricTags = new HashMap<>();\n        // coll, shard, replica\n        Map<String, Map<String, List<ReplicaInfo>>> infos = cloudManager.getNodeStateProvider().getReplicaInfo(node, Collections.emptyList());\n        infos.forEach((coll, shards) -> {\n          if (!collections.isEmpty() && !collections.contains(coll)) {\n            return;\n          }\n          DocCollection docCollection = clusterState.getCollection(coll);\n\n          shards.forEach((sh, replicas) -> {\n            // check only the leader replica in an active shard\n            Slice s = docCollection.getSlice(sh);\n            if (s.getState() != Slice.State.ACTIVE) {\n              return;\n            }\n            Replica r = s.getLeader();\n            // no leader - don't do anything\n            if (r == null) {\n              return;\n            }\n            // not on this node\n            if (!r.getNodeName().equals(node)) {\n              return;\n            }\n            // find ReplicaInfo\n            ReplicaInfo info = null;\n            for (ReplicaInfo ri : replicas) {\n              if (r.getCoreName().equals(ri.getCore())) {\n                info = ri;\n                break;\n              }\n            }\n            if (info == null) {\n              // probably replica is not on this node?\n              return;\n            }\n            // we have to translate to the metrics registry name, which uses \"_replica_nN\" as suffix\n            String replicaName = Utils.parseMetricsReplicaName(coll, info.getCore());\n            if (replicaName == null) { // should never happen???\n              replicaName = info.getName(); // which is actually coreNode name...\n            }\n            String registry = SolrCoreMetricManager.createRegistryName(true, coll, sh, replicaName, null);\n            String tag = \"metrics:\" + registry + \":\" + CORE_IDX.metricsAttribute;\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.numDocs\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.maxDoc\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.indexCommitSize\";\n            metricTags.put(tag, info);\n          });\n        });\n        if (metricTags.isEmpty()) {\n          continue;\n        }\n        Map<String, Object> sizes = cloudManager.getNodeStateProvider().getNodeValues(node, metricTags.keySet());\n        sizes.forEach((tag, size) -> {\n          final ReplicaInfo info = metricTags.get(tag);\n          if (info == null) {\n            log.warn(\"Missing replica info for response tag \" + tag);\n          } else {\n            // verify that it's a Number\n            if (!(size instanceof Number)) {\n              log.warn(\"invalid size value for tag \" + tag + \" - not a number: '\" + size + \"' is \" + size.getClass().getName());\n              return;\n            }\n\n            ReplicaInfo currentInfo = currentSizes.computeIfAbsent(info.getCore(), k -> (ReplicaInfo)info.clone());\n            if (tag.contains(\"INDEX\")) {\n              currentInfo.getVariables().put(TOTAL_BYTES_SIZE_PROP, ((Number) size).longValue());\n            } else if (tag.endsWith(\"SEARCHER.searcher.numDocs\")) {\n              currentInfo.getVariables().put(DOCS_SIZE_PROP, ((Number) size).longValue());\n            } else if (tag.endsWith(\"SEARCHER.searcher.maxDoc\")) {\n              currentInfo.getVariables().put(MAX_DOC_PROP, ((Number) size).longValue());\n            } else if (tag.endsWith(\"SEARCHER.searcher.indexCommitSize\")) {\n              currentInfo.getVariables().put(COMMIT_SIZE_PROP, ((Number) size).longValue());\n            }\n          }\n        });\n      }\n    } catch (IOException e) {\n      log.warn(\"Error running trigger \" + getName(), e);\n      return;\n    }\n\n    long now = cloudManager.getTimeSource().getTimeNs();\n\n    // now check thresholds\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> aboveSize = new HashMap<>();\n\n    Set<String> splittable = new HashSet<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      // calculate estimated bytes\n      long maxDoc = (Long)info.getVariable(MAX_DOC_PROP);\n      long numDocs = (Long)info.getVariable(DOCS_SIZE_PROP);\n      long commitSize = (Long)info.getVariable(COMMIT_SIZE_PROP, 0L);\n      if (commitSize <= 0) {\n        commitSize = (Long)info.getVariable(TOTAL_BYTES_SIZE_PROP);\n      }\n      // calculate estimated size as a side-effect\n      commitSize = estimatedSize(maxDoc, numDocs, commitSize);\n      info.getVariables().put(BYTES_SIZE_PROP, commitSize);\n\n      if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) > aboveDocs) {\n        if (waitForElapsed(coreName, now, lastAboveEventMap)) {\n          List<ReplicaInfo> infos = aboveSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes) {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_DOCS_PROP);\n            }\n            infos.add(info);\n            splittable.add(info.getName());\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastAboveEventMap.remove(coreName);\n      }\n    });\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> belowSize = new HashMap<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if (((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) < belowDocs) &&\n          // make sure we don't produce conflicting ops\n          !splittable.contains(info.getName())) {\n        if (waitForElapsed(coreName, now, lastBelowEventMap)) {\n          List<ReplicaInfo> infos = belowSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes) {\n              info.getVariables().put(VIOLATION_PROP, BELOW_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, BELOW_DOCS_PROP);\n            }\n            infos.add(info);\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastBelowEventMap.remove(coreName);\n      }\n    });\n\n    if (aboveSize.isEmpty() && belowSize.isEmpty()) {\n      log.trace(\"NO VIOLATIONS: Now={}\", now);\n      log.trace(\"lastAbove={}\", lastAboveEventMap);\n      log.trace(\"lastBelow={}\", lastBelowEventMap);\n      return;\n    }\n\n    // find the earliest time when a condition was exceeded\n    final AtomicLong eventTime = new AtomicLong(now);\n\n    // calculate ops\n    final List<TriggerEvent.Op> ops = new ArrayList<>();\n    aboveSize.forEach((coll, replicas) -> {\n      // sort by decreasing size to first split the largest ones\n      // XXX see the comment below about using DOCS_SIZE_PROP in lieu of BYTES_SIZE_PROP\n      replicas.sort((r1, r2) -> {\n        long delta = (Long) r1.getVariable(DOCS_SIZE_PROP) - (Long) r2.getVariable(DOCS_SIZE_PROP);\n        if (delta > 0) {\n          return -1;\n        } else if (delta < 0) {\n          return 1;\n        } else {\n          return 0;\n        }\n      });\n      replicas.forEach(r -> {\n        if (ops.size() >= maxOps) {\n          return;\n        }\n        TriggerEvent.Op op = new TriggerEvent.Op(aboveOp);\n        op.addHint(Suggester.Hint.COLL_SHARD, new Pair<>(coll, r.getShard()));\n        Map<String, Object> params = new HashMap<>();\n        params.put(CommonAdminParams.SPLIT_METHOD, splitMethod.toLower());\n        if (splitFuzz > 0) {\n          params.put(CommonAdminParams.SPLIT_FUZZ, splitFuzz);\n        }\n        op.addHint(Suggester.Hint.PARAMS, params);\n        ops.add(op);\n        Long time = lastAboveEventMap.get(r.getCore());\n        if (time != null && eventTime.get() > time) {\n          eventTime.set(time);\n        }\n      });\n    });\n    belowSize.forEach((coll, replicas) -> {\n      if (replicas.size() < 2) {\n        return;\n      }\n      if (ops.size() >= maxOps) {\n        return;\n      }\n      // sort by increasing size\n      replicas.sort((r1, r2) -> {\n        // XXX this is not quite correct - if BYTES_SIZE_PROP decided that replica got here\n        // then we should be sorting by BYTES_SIZE_PROP. However, since DOCS and BYTES are\n        // loosely correlated it's simpler to sort just by docs (which better reflects the \"too small\"\n        // condition than index size, due to possibly existing deleted docs that still occupy space)\n        long delta = (Long) r1.getVariable(DOCS_SIZE_PROP) - (Long) r2.getVariable(DOCS_SIZE_PROP);\n        if (delta > 0) {\n          return 1;\n        } else if (delta < 0) {\n          return -1;\n        } else {\n          return 0;\n        }\n      });\n\n      // TODO: MERGESHARDS is not implemented yet. For now take the top two smallest shards\n      // TODO: but in the future we probably need to get ones with adjacent ranges.\n\n      // TODO: generate as many MERGESHARDS as needed to consume all belowSize shards\n      TriggerEvent.Op op = new TriggerEvent.Op(belowOp);\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(0).getShard()));\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(1).getShard()));\n      ops.add(op);\n      Long time = lastBelowEventMap.get(replicas.get(0).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n      time = lastBelowEventMap.get(replicas.get(1).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n    });\n\n    if (ops.isEmpty()) {\n      return;\n    }\n    if (processor.process(new IndexSizeEvent(getName(), eventTime.get(), ops, aboveSize, belowSize))) {\n      // update last event times\n      aboveSize.forEach((coll, replicas) -> {\n        replicas.forEach(r -> lastAboveEventMap.put(r.getCore(), now));\n      });\n      belowSize.forEach((coll, replicas) -> {\n        if (replicas.size() < 2) {\n          return;\n        }\n        lastBelowEventMap.put(replicas.get(0).getCore(), now);\n        lastBelowEventMap.put(replicas.get(1).getCore(), now);\n      });\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"256b1da668dae74c726d5d71bce6153202d6b001","date":1584099017,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/autoscaling/IndexSizeTrigger#run().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/autoscaling/IndexSizeTrigger#run().mjava","sourceNew":"  @Override\n  public void run() {\n    synchronized(this) {\n      if (isClosed) {\n        log.warn(getName() + \" ran but was already closed\");\n        return;\n      }\n    }\n    AutoScaling.TriggerEventProcessor processor = processorRef.get();\n    if (processor == null) {\n      return;\n    }\n\n    // replica name / info + size, retrieved from leaders only\n    Map<String, ReplicaInfo> currentSizes = new HashMap<>();\n\n    try {\n      ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n      for (String node : clusterState.getLiveNodes()) {\n        Map<String, ReplicaInfo> metricTags = new HashMap<>();\n        // coll, shard, replica\n        Map<String, Map<String, List<ReplicaInfo>>> infos = cloudManager.getNodeStateProvider().getReplicaInfo(node, Collections.emptyList());\n        infos.forEach((coll, shards) -> {\n          if (!collections.isEmpty() && !collections.contains(coll)) {\n            return;\n          }\n          DocCollection docCollection = clusterState.getCollection(coll);\n\n          shards.forEach((sh, replicas) -> {\n            // check only the leader replica in an active shard\n            Slice s = docCollection.getSlice(sh);\n            if (s.getState() != Slice.State.ACTIVE) {\n              return;\n            }\n            Replica r = s.getLeader();\n            // no leader - don't do anything\n            if (r == null) {\n              return;\n            }\n            // not on this node\n            if (!r.getNodeName().equals(node)) {\n              return;\n            }\n            // find ReplicaInfo\n            ReplicaInfo info = null;\n            for (ReplicaInfo ri : replicas) {\n              if (r.getCoreName().equals(ri.getCore())) {\n                info = ri;\n                break;\n              }\n            }\n            if (info == null) {\n              // probably replica is not on this node?\n              return;\n            }\n            // we have to translate to the metrics registry name, which uses \"_replica_nN\" as suffix\n            String replicaName = Utils.parseMetricsReplicaName(coll, info.getCore());\n            if (replicaName == null) { // should never happen???\n              replicaName = info.getName(); // which is actually coreNode name...\n            }\n            String registry = SolrCoreMetricManager.createRegistryName(true, coll, sh, replicaName, null);\n            String tag = \"metrics:\" + registry + \":\" + CORE_IDX.metricsAttribute;\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.numDocs\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.maxDoc\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.indexCommitSize\";\n            metricTags.put(tag, info);\n          });\n        });\n        if (metricTags.isEmpty()) {\n          continue;\n        }\n        Map<String, Object> sizes = cloudManager.getNodeStateProvider().getNodeValues(node, metricTags.keySet());\n        sizes.forEach((tag, size) -> {\n          final ReplicaInfo info = metricTags.get(tag);\n          if (info == null) {\n            log.warn(\"Missing replica info for response tag \" + tag);\n          } else {\n            // verify that it's a Number\n            if (!(size instanceof Number)) {\n              log.warn(\"invalid size value for tag \" + tag + \" - not a number: '\" + size + \"' is \" + size.getClass().getName());\n              return;\n            }\n\n            ReplicaInfo currentInfo = currentSizes.computeIfAbsent(info.getCore(), k -> (ReplicaInfo)info.clone());\n            if (tag.contains(\"INDEX\")) {\n              currentInfo.getVariables().put(TOTAL_BYTES_SIZE_KEY, ((Number) size).longValue());\n            } else if (tag.endsWith(\"SEARCHER.searcher.numDocs\")) {\n              currentInfo.getVariables().put(DOCS_SIZE_KEY, ((Number) size).longValue());\n            } else if (tag.endsWith(\"SEARCHER.searcher.maxDoc\")) {\n              currentInfo.getVariables().put(MAX_DOC_KEY, ((Number) size).longValue());\n            } else if (tag.endsWith(\"SEARCHER.searcher.indexCommitSize\")) {\n              currentInfo.getVariables().put(COMMIT_SIZE_KEY, ((Number) size).longValue());\n            }\n          }\n        });\n      }\n    } catch (IOException e) {\n      log.warn(\"Error running trigger \" + getName(), e);\n      return;\n    }\n\n    long now = cloudManager.getTimeSource().getTimeNs();\n\n    // now check thresholds\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> aboveSize = new HashMap<>();\n\n    Set<String> splittable = new HashSet<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      // calculate estimated bytes\n      long maxDoc = (Long)info.getVariable(MAX_DOC_KEY);\n      long numDocs = (Long)info.getVariable(DOCS_SIZE_KEY);\n      long commitSize = (Long)info.getVariable(COMMIT_SIZE_KEY, 0L);\n      if (commitSize <= 0) {\n        commitSize = (Long)info.getVariable(TOTAL_BYTES_SIZE_KEY);\n      }\n      // calculate estimated size as a side-effect\n      commitSize = estimatedSize(maxDoc, numDocs, commitSize);\n      info.getVariables().put(BYTES_SIZE_KEY, commitSize);\n\n      if ((Long)info.getVariable(BYTES_SIZE_KEY) > aboveBytes ||\n          (Long)info.getVariable(DOCS_SIZE_KEY) > aboveDocs) {\n        if (waitForElapsed(coreName, now, lastAboveEventMap)) {\n          List<ReplicaInfo> infos = aboveSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_KEY) > aboveBytes) {\n              info.getVariables().put(VIOLATION_KEY, ABOVE_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_KEY, ABOVE_DOCS_PROP);\n            }\n            infos.add(info);\n            splittable.add(info.getName());\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastAboveEventMap.remove(coreName);\n      }\n    });\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> belowSize = new HashMap<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if (((Long)info.getVariable(BYTES_SIZE_KEY) < belowBytes ||\n          (Long)info.getVariable(DOCS_SIZE_KEY) < belowDocs) &&\n          // make sure we don't produce conflicting ops\n          !splittable.contains(info.getName())) {\n        if (waitForElapsed(coreName, now, lastBelowEventMap)) {\n          List<ReplicaInfo> infos = belowSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_KEY) < belowBytes) {\n              info.getVariables().put(VIOLATION_KEY, BELOW_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_KEY, BELOW_DOCS_PROP);\n            }\n            infos.add(info);\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastBelowEventMap.remove(coreName);\n      }\n    });\n\n    if (aboveSize.isEmpty() && belowSize.isEmpty()) {\n      log.trace(\"NO VIOLATIONS: Now={}\", now);\n      log.trace(\"lastAbove={}\", lastAboveEventMap);\n      log.trace(\"lastBelow={}\", lastBelowEventMap);\n      return;\n    }\n\n    // find the earliest time when a condition was exceeded\n    final AtomicLong eventTime = new AtomicLong(now);\n\n    // calculate ops\n    final List<TriggerEvent.Op> ops = new ArrayList<>();\n    aboveSize.forEach((coll, replicas) -> {\n      // sort by decreasing size to first split the largest ones\n      // XXX see the comment below about using DOCS_SIZE_PROP in lieu of BYTES_SIZE_PROP\n      replicas.sort((r1, r2) -> {\n        long delta = (Long) r1.getVariable(DOCS_SIZE_KEY) - (Long) r2.getVariable(DOCS_SIZE_KEY);\n        if (delta > 0) {\n          return -1;\n        } else if (delta < 0) {\n          return 1;\n        } else {\n          return 0;\n        }\n      });\n      replicas.forEach(r -> {\n        if (ops.size() >= maxOps) {\n          return;\n        }\n        TriggerEvent.Op op = new TriggerEvent.Op(aboveOp);\n        op.addHint(Suggester.Hint.COLL_SHARD, new Pair<>(coll, r.getShard()));\n        Map<String, Object> params = new HashMap<>();\n        params.put(SPLIT_METHOD_PROP, splitMethod.toLower());\n        if (splitFuzz > 0) {\n          params.put(SPLIT_FUZZ_PROP, splitFuzz);\n        }\n        params.put(SPLIT_BY_PREFIX, splitByPrefix);\n        op.addHint(Suggester.Hint.PARAMS, params);\n        ops.add(op);\n        Long time = lastAboveEventMap.get(r.getCore());\n        if (time != null && eventTime.get() > time) {\n          eventTime.set(time);\n        }\n      });\n    });\n    belowSize.forEach((coll, replicas) -> {\n      if (replicas.size() < 2) {\n        return;\n      }\n      if (ops.size() >= maxOps) {\n        return;\n      }\n      // sort by increasing size\n      replicas.sort((r1, r2) -> {\n        // XXX this is not quite correct - if BYTES_SIZE_PROP decided that replica got here\n        // then we should be sorting by BYTES_SIZE_PROP. However, since DOCS and BYTES are\n        // loosely correlated it's simpler to sort just by docs (which better reflects the \"too small\"\n        // condition than index size, due to possibly existing deleted docs that still occupy space)\n        long delta = (Long) r1.getVariable(DOCS_SIZE_KEY) - (Long) r2.getVariable(DOCS_SIZE_KEY);\n        if (delta > 0) {\n          return 1;\n        } else if (delta < 0) {\n          return -1;\n        } else {\n          return 0;\n        }\n      });\n\n      // TODO: MERGESHARDS is not implemented yet. For now take the top two smallest shards\n      // TODO: but in the future we probably need to get ones with adjacent ranges.\n\n      // TODO: generate as many MERGESHARDS as needed to consume all belowSize shards\n      TriggerEvent.Op op = new TriggerEvent.Op(belowOp);\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(0).getShard()));\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(1).getShard()));\n      ops.add(op);\n      Long time = lastBelowEventMap.get(replicas.get(0).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n      time = lastBelowEventMap.get(replicas.get(1).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n    });\n\n    if (ops.isEmpty()) {\n      return;\n    }\n    if (processor.process(new IndexSizeEvent(getName(), eventTime.get(), ops, aboveSize, belowSize))) {\n      // update last event times\n      aboveSize.forEach((coll, replicas) -> {\n        replicas.forEach(r -> lastAboveEventMap.put(r.getCore(), now));\n      });\n      belowSize.forEach((coll, replicas) -> {\n        if (replicas.size() < 2) {\n          return;\n        }\n        lastBelowEventMap.put(replicas.get(0).getCore(), now);\n        lastBelowEventMap.put(replicas.get(1).getCore(), now);\n      });\n    }\n  }\n\n","sourceOld":"  @Override\n  public void run() {\n    synchronized(this) {\n      if (isClosed) {\n        log.warn(getName() + \" ran but was already closed\");\n        return;\n      }\n    }\n    AutoScaling.TriggerEventProcessor processor = processorRef.get();\n    if (processor == null) {\n      return;\n    }\n\n    // replica name / info + size, retrieved from leaders only\n    Map<String, ReplicaInfo> currentSizes = new HashMap<>();\n\n    try {\n      ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n      for (String node : clusterState.getLiveNodes()) {\n        Map<String, ReplicaInfo> metricTags = new HashMap<>();\n        // coll, shard, replica\n        Map<String, Map<String, List<ReplicaInfo>>> infos = cloudManager.getNodeStateProvider().getReplicaInfo(node, Collections.emptyList());\n        infos.forEach((coll, shards) -> {\n          if (!collections.isEmpty() && !collections.contains(coll)) {\n            return;\n          }\n          DocCollection docCollection = clusterState.getCollection(coll);\n\n          shards.forEach((sh, replicas) -> {\n            // check only the leader replica in an active shard\n            Slice s = docCollection.getSlice(sh);\n            if (s.getState() != Slice.State.ACTIVE) {\n              return;\n            }\n            Replica r = s.getLeader();\n            // no leader - don't do anything\n            if (r == null) {\n              return;\n            }\n            // not on this node\n            if (!r.getNodeName().equals(node)) {\n              return;\n            }\n            // find ReplicaInfo\n            ReplicaInfo info = null;\n            for (ReplicaInfo ri : replicas) {\n              if (r.getCoreName().equals(ri.getCore())) {\n                info = ri;\n                break;\n              }\n            }\n            if (info == null) {\n              // probably replica is not on this node?\n              return;\n            }\n            // we have to translate to the metrics registry name, which uses \"_replica_nN\" as suffix\n            String replicaName = Utils.parseMetricsReplicaName(coll, info.getCore());\n            if (replicaName == null) { // should never happen???\n              replicaName = info.getName(); // which is actually coreNode name...\n            }\n            String registry = SolrCoreMetricManager.createRegistryName(true, coll, sh, replicaName, null);\n            String tag = \"metrics:\" + registry + \":\" + CORE_IDX.metricsAttribute;\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.numDocs\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.maxDoc\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.indexCommitSize\";\n            metricTags.put(tag, info);\n          });\n        });\n        if (metricTags.isEmpty()) {\n          continue;\n        }\n        Map<String, Object> sizes = cloudManager.getNodeStateProvider().getNodeValues(node, metricTags.keySet());\n        sizes.forEach((tag, size) -> {\n          final ReplicaInfo info = metricTags.get(tag);\n          if (info == null) {\n            log.warn(\"Missing replica info for response tag \" + tag);\n          } else {\n            // verify that it's a Number\n            if (!(size instanceof Number)) {\n              log.warn(\"invalid size value for tag \" + tag + \" - not a number: '\" + size + \"' is \" + size.getClass().getName());\n              return;\n            }\n\n            ReplicaInfo currentInfo = currentSizes.computeIfAbsent(info.getCore(), k -> (ReplicaInfo)info.clone());\n            if (tag.contains(\"INDEX\")) {\n              currentInfo.getVariables().put(TOTAL_BYTES_SIZE_PROP, ((Number) size).longValue());\n            } else if (tag.endsWith(\"SEARCHER.searcher.numDocs\")) {\n              currentInfo.getVariables().put(DOCS_SIZE_PROP, ((Number) size).longValue());\n            } else if (tag.endsWith(\"SEARCHER.searcher.maxDoc\")) {\n              currentInfo.getVariables().put(MAX_DOC_PROP, ((Number) size).longValue());\n            } else if (tag.endsWith(\"SEARCHER.searcher.indexCommitSize\")) {\n              currentInfo.getVariables().put(COMMIT_SIZE_PROP, ((Number) size).longValue());\n            }\n          }\n        });\n      }\n    } catch (IOException e) {\n      log.warn(\"Error running trigger \" + getName(), e);\n      return;\n    }\n\n    long now = cloudManager.getTimeSource().getTimeNs();\n\n    // now check thresholds\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> aboveSize = new HashMap<>();\n\n    Set<String> splittable = new HashSet<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      // calculate estimated bytes\n      long maxDoc = (Long)info.getVariable(MAX_DOC_PROP);\n      long numDocs = (Long)info.getVariable(DOCS_SIZE_PROP);\n      long commitSize = (Long)info.getVariable(COMMIT_SIZE_PROP, 0L);\n      if (commitSize <= 0) {\n        commitSize = (Long)info.getVariable(TOTAL_BYTES_SIZE_PROP);\n      }\n      // calculate estimated size as a side-effect\n      commitSize = estimatedSize(maxDoc, numDocs, commitSize);\n      info.getVariables().put(BYTES_SIZE_PROP, commitSize);\n\n      if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) > aboveDocs) {\n        if (waitForElapsed(coreName, now, lastAboveEventMap)) {\n          List<ReplicaInfo> infos = aboveSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) > aboveBytes) {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, ABOVE_DOCS_PROP);\n            }\n            infos.add(info);\n            splittable.add(info.getName());\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastAboveEventMap.remove(coreName);\n      }\n    });\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> belowSize = new HashMap<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if (((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes ||\n          (Long)info.getVariable(DOCS_SIZE_PROP) < belowDocs) &&\n          // make sure we don't produce conflicting ops\n          !splittable.contains(info.getName())) {\n        if (waitForElapsed(coreName, now, lastBelowEventMap)) {\n          List<ReplicaInfo> infos = belowSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_PROP) < belowBytes) {\n              info.getVariables().put(VIOLATION_PROP, BELOW_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_PROP, BELOW_DOCS_PROP);\n            }\n            infos.add(info);\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastBelowEventMap.remove(coreName);\n      }\n    });\n\n    if (aboveSize.isEmpty() && belowSize.isEmpty()) {\n      log.trace(\"NO VIOLATIONS: Now={}\", now);\n      log.trace(\"lastAbove={}\", lastAboveEventMap);\n      log.trace(\"lastBelow={}\", lastBelowEventMap);\n      return;\n    }\n\n    // find the earliest time when a condition was exceeded\n    final AtomicLong eventTime = new AtomicLong(now);\n\n    // calculate ops\n    final List<TriggerEvent.Op> ops = new ArrayList<>();\n    aboveSize.forEach((coll, replicas) -> {\n      // sort by decreasing size to first split the largest ones\n      // XXX see the comment below about using DOCS_SIZE_PROP in lieu of BYTES_SIZE_PROP\n      replicas.sort((r1, r2) -> {\n        long delta = (Long) r1.getVariable(DOCS_SIZE_PROP) - (Long) r2.getVariable(DOCS_SIZE_PROP);\n        if (delta > 0) {\n          return -1;\n        } else if (delta < 0) {\n          return 1;\n        } else {\n          return 0;\n        }\n      });\n      replicas.forEach(r -> {\n        if (ops.size() >= maxOps) {\n          return;\n        }\n        TriggerEvent.Op op = new TriggerEvent.Op(aboveOp);\n        op.addHint(Suggester.Hint.COLL_SHARD, new Pair<>(coll, r.getShard()));\n        Map<String, Object> params = new HashMap<>();\n        params.put(SPLIT_METHOD_PROP, splitMethod.toLower());\n        if (splitFuzz > 0) {\n          params.put(SPLIT_FUZZ_PROP, splitFuzz);\n        }\n        params.put(SPLIT_BY_PREFIX, splitByPrefix);\n        op.addHint(Suggester.Hint.PARAMS, params);\n        ops.add(op);\n        Long time = lastAboveEventMap.get(r.getCore());\n        if (time != null && eventTime.get() > time) {\n          eventTime.set(time);\n        }\n      });\n    });\n    belowSize.forEach((coll, replicas) -> {\n      if (replicas.size() < 2) {\n        return;\n      }\n      if (ops.size() >= maxOps) {\n        return;\n      }\n      // sort by increasing size\n      replicas.sort((r1, r2) -> {\n        // XXX this is not quite correct - if BYTES_SIZE_PROP decided that replica got here\n        // then we should be sorting by BYTES_SIZE_PROP. However, since DOCS and BYTES are\n        // loosely correlated it's simpler to sort just by docs (which better reflects the \"too small\"\n        // condition than index size, due to possibly existing deleted docs that still occupy space)\n        long delta = (Long) r1.getVariable(DOCS_SIZE_PROP) - (Long) r2.getVariable(DOCS_SIZE_PROP);\n        if (delta > 0) {\n          return 1;\n        } else if (delta < 0) {\n          return -1;\n        } else {\n          return 0;\n        }\n      });\n\n      // TODO: MERGESHARDS is not implemented yet. For now take the top two smallest shards\n      // TODO: but in the future we probably need to get ones with adjacent ranges.\n\n      // TODO: generate as many MERGESHARDS as needed to consume all belowSize shards\n      TriggerEvent.Op op = new TriggerEvent.Op(belowOp);\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(0).getShard()));\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(1).getShard()));\n      ops.add(op);\n      Long time = lastBelowEventMap.get(replicas.get(0).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n      time = lastBelowEventMap.get(replicas.get(1).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n    });\n\n    if (ops.isEmpty()) {\n      return;\n    }\n    if (processor.process(new IndexSizeEvent(getName(), eventTime.get(), ops, aboveSize, belowSize))) {\n      // update last event times\n      aboveSize.forEach((coll, replicas) -> {\n        replicas.forEach(r -> lastAboveEventMap.put(r.getCore(), now));\n      });\n      belowSize.forEach((coll, replicas) -> {\n        if (replicas.size() < 2) {\n          return;\n        }\n        lastBelowEventMap.put(replicas.get(0).getCore(), now);\n        lastBelowEventMap.put(replicas.get(1).getCore(), now);\n      });\n    }\n  }\n\n","bugFix":["2d80c1ad9241ae005a167d7ee8ac473601b0e57c","ce71a9836bf1eba34a0ab31884e9eb8ad3f1cef1","0532cd2ef583ac8047d77493ec05c81836f483c2","9414d03a96422f6a92ced130e94f73f3fe1fc9a3","b8cde5442ed20c0c3ffd14ea2e2a64609367c193"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e35f2dde06b35aa9904949a3a93fabd090371077","date":1587906921,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/autoscaling/IndexSizeTrigger#run().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/autoscaling/IndexSizeTrigger#run().mjava","sourceNew":"  @Override\n  public void run() {\n    synchronized(this) {\n      if (isClosed) {\n        log.warn(\"{} ran but was already closed\", getName());\n        return;\n      }\n    }\n    AutoScaling.TriggerEventProcessor processor = processorRef.get();\n    if (processor == null) {\n      return;\n    }\n\n    // replica name / info + size, retrieved from leaders only\n    Map<String, ReplicaInfo> currentSizes = new HashMap<>();\n\n    try {\n      ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n      for (String node : clusterState.getLiveNodes()) {\n        Map<String, ReplicaInfo> metricTags = new HashMap<>();\n        // coll, shard, replica\n        Map<String, Map<String, List<ReplicaInfo>>> infos = cloudManager.getNodeStateProvider().getReplicaInfo(node, Collections.emptyList());\n        infos.forEach((coll, shards) -> {\n          if (!collections.isEmpty() && !collections.contains(coll)) {\n            return;\n          }\n          DocCollection docCollection = clusterState.getCollection(coll);\n\n          shards.forEach((sh, replicas) -> {\n            // check only the leader replica in an active shard\n            Slice s = docCollection.getSlice(sh);\n            if (s.getState() != Slice.State.ACTIVE) {\n              return;\n            }\n            Replica r = s.getLeader();\n            // no leader - don't do anything\n            if (r == null) {\n              return;\n            }\n            // not on this node\n            if (!r.getNodeName().equals(node)) {\n              return;\n            }\n            // find ReplicaInfo\n            ReplicaInfo info = null;\n            for (ReplicaInfo ri : replicas) {\n              if (r.getCoreName().equals(ri.getCore())) {\n                info = ri;\n                break;\n              }\n            }\n            if (info == null) {\n              // probably replica is not on this node?\n              return;\n            }\n            // we have to translate to the metrics registry name, which uses \"_replica_nN\" as suffix\n            String replicaName = Utils.parseMetricsReplicaName(coll, info.getCore());\n            if (replicaName == null) { // should never happen???\n              replicaName = info.getName(); // which is actually coreNode name...\n            }\n            String registry = SolrCoreMetricManager.createRegistryName(true, coll, sh, replicaName, null);\n            String tag = \"metrics:\" + registry + \":\" + CORE_IDX.metricsAttribute;\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.numDocs\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.maxDoc\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.indexCommitSize\";\n            metricTags.put(tag, info);\n          });\n        });\n        if (metricTags.isEmpty()) {\n          continue;\n        }\n        Map<String, Object> sizes = cloudManager.getNodeStateProvider().getNodeValues(node, metricTags.keySet());\n        sizes.forEach((tag, size) -> {\n          final ReplicaInfo info = metricTags.get(tag);\n          if (info == null) {\n            log.warn(\"Missing replica info for response tag {}\", tag);\n          } else {\n            // verify that it's a Number\n            if (!(size instanceof Number)) {\n              log.warn(\"invalid size value for tag {} - not a number: '{}' is {}\", tag, size, size.getClass().getName());\n              return;\n            }\n\n            ReplicaInfo currentInfo = currentSizes.computeIfAbsent(info.getCore(), k -> (ReplicaInfo)info.clone());\n            if (tag.contains(\"INDEX\")) {\n              currentInfo.getVariables().put(TOTAL_BYTES_SIZE_KEY, ((Number) size).longValue());\n            } else if (tag.endsWith(\"SEARCHER.searcher.numDocs\")) {\n              currentInfo.getVariables().put(DOCS_SIZE_KEY, ((Number) size).longValue());\n            } else if (tag.endsWith(\"SEARCHER.searcher.maxDoc\")) {\n              currentInfo.getVariables().put(MAX_DOC_KEY, ((Number) size).longValue());\n            } else if (tag.endsWith(\"SEARCHER.searcher.indexCommitSize\")) {\n              currentInfo.getVariables().put(COMMIT_SIZE_KEY, ((Number) size).longValue());\n            }\n          }\n        });\n      }\n    } catch (IOException e) {\n      log.warn(\"Error running trigger {}\", getName(), e);\n      return;\n    }\n\n    long now = cloudManager.getTimeSource().getTimeNs();\n\n    // now check thresholds\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> aboveSize = new HashMap<>();\n\n    Set<String> splittable = new HashSet<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      // calculate estimated bytes\n      long maxDoc = (Long)info.getVariable(MAX_DOC_KEY);\n      long numDocs = (Long)info.getVariable(DOCS_SIZE_KEY);\n      long commitSize = (Long)info.getVariable(COMMIT_SIZE_KEY, 0L);\n      if (commitSize <= 0) {\n        commitSize = (Long)info.getVariable(TOTAL_BYTES_SIZE_KEY);\n      }\n      // calculate estimated size as a side-effect\n      commitSize = estimatedSize(maxDoc, numDocs, commitSize);\n      info.getVariables().put(BYTES_SIZE_KEY, commitSize);\n\n      if ((Long)info.getVariable(BYTES_SIZE_KEY) > aboveBytes ||\n          (Long)info.getVariable(DOCS_SIZE_KEY) > aboveDocs) {\n        if (waitForElapsed(coreName, now, lastAboveEventMap)) {\n          List<ReplicaInfo> infos = aboveSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_KEY) > aboveBytes) {\n              info.getVariables().put(VIOLATION_KEY, ABOVE_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_KEY, ABOVE_DOCS_PROP);\n            }\n            infos.add(info);\n            splittable.add(info.getName());\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastAboveEventMap.remove(coreName);\n      }\n    });\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> belowSize = new HashMap<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if (((Long)info.getVariable(BYTES_SIZE_KEY) < belowBytes ||\n          (Long)info.getVariable(DOCS_SIZE_KEY) < belowDocs) &&\n          // make sure we don't produce conflicting ops\n          !splittable.contains(info.getName())) {\n        if (waitForElapsed(coreName, now, lastBelowEventMap)) {\n          List<ReplicaInfo> infos = belowSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_KEY) < belowBytes) {\n              info.getVariables().put(VIOLATION_KEY, BELOW_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_KEY, BELOW_DOCS_PROP);\n            }\n            infos.add(info);\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastBelowEventMap.remove(coreName);\n      }\n    });\n\n    if (aboveSize.isEmpty() && belowSize.isEmpty()) {\n      log.trace(\"NO VIOLATIONS: Now={}\", now);\n      log.trace(\"lastAbove={}\", lastAboveEventMap);\n      log.trace(\"lastBelow={}\", lastBelowEventMap);\n      return;\n    }\n\n    // find the earliest time when a condition was exceeded\n    final AtomicLong eventTime = new AtomicLong(now);\n\n    // calculate ops\n    final List<TriggerEvent.Op> ops = new ArrayList<>();\n    aboveSize.forEach((coll, replicas) -> {\n      // sort by decreasing size to first split the largest ones\n      // XXX see the comment below about using DOCS_SIZE_PROP in lieu of BYTES_SIZE_PROP\n      replicas.sort((r1, r2) -> {\n        long delta = (Long) r1.getVariable(DOCS_SIZE_KEY) - (Long) r2.getVariable(DOCS_SIZE_KEY);\n        if (delta > 0) {\n          return -1;\n        } else if (delta < 0) {\n          return 1;\n        } else {\n          return 0;\n        }\n      });\n      replicas.forEach(r -> {\n        if (ops.size() >= maxOps) {\n          return;\n        }\n        TriggerEvent.Op op = new TriggerEvent.Op(aboveOp);\n        op.addHint(Suggester.Hint.COLL_SHARD, new Pair<>(coll, r.getShard()));\n        Map<String, Object> params = new HashMap<>();\n        params.put(SPLIT_METHOD_PROP, splitMethod.toLower());\n        if (splitFuzz > 0) {\n          params.put(SPLIT_FUZZ_PROP, splitFuzz);\n        }\n        params.put(SPLIT_BY_PREFIX, splitByPrefix);\n        op.addHint(Suggester.Hint.PARAMS, params);\n        ops.add(op);\n        Long time = lastAboveEventMap.get(r.getCore());\n        if (time != null && eventTime.get() > time) {\n          eventTime.set(time);\n        }\n      });\n    });\n    belowSize.forEach((coll, replicas) -> {\n      if (replicas.size() < 2) {\n        return;\n      }\n      if (ops.size() >= maxOps) {\n        return;\n      }\n      // sort by increasing size\n      replicas.sort((r1, r2) -> {\n        // XXX this is not quite correct - if BYTES_SIZE_PROP decided that replica got here\n        // then we should be sorting by BYTES_SIZE_PROP. However, since DOCS and BYTES are\n        // loosely correlated it's simpler to sort just by docs (which better reflects the \"too small\"\n        // condition than index size, due to possibly existing deleted docs that still occupy space)\n        long delta = (Long) r1.getVariable(DOCS_SIZE_KEY) - (Long) r2.getVariable(DOCS_SIZE_KEY);\n        if (delta > 0) {\n          return 1;\n        } else if (delta < 0) {\n          return -1;\n        } else {\n          return 0;\n        }\n      });\n\n      // TODO: MERGESHARDS is not implemented yet. For now take the top two smallest shards\n      // TODO: but in the future we probably need to get ones with adjacent ranges.\n\n      // TODO: generate as many MERGESHARDS as needed to consume all belowSize shards\n      TriggerEvent.Op op = new TriggerEvent.Op(belowOp);\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(0).getShard()));\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(1).getShard()));\n      ops.add(op);\n      Long time = lastBelowEventMap.get(replicas.get(0).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n      time = lastBelowEventMap.get(replicas.get(1).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n    });\n\n    if (ops.isEmpty()) {\n      return;\n    }\n    if (processor.process(new IndexSizeEvent(getName(), eventTime.get(), ops, aboveSize, belowSize))) {\n      // update last event times\n      aboveSize.forEach((coll, replicas) -> {\n        replicas.forEach(r -> lastAboveEventMap.put(r.getCore(), now));\n      });\n      belowSize.forEach((coll, replicas) -> {\n        if (replicas.size() < 2) {\n          return;\n        }\n        lastBelowEventMap.put(replicas.get(0).getCore(), now);\n        lastBelowEventMap.put(replicas.get(1).getCore(), now);\n      });\n    }\n  }\n\n","sourceOld":"  @Override\n  public void run() {\n    synchronized(this) {\n      if (isClosed) {\n        log.warn(getName() + \" ran but was already closed\");\n        return;\n      }\n    }\n    AutoScaling.TriggerEventProcessor processor = processorRef.get();\n    if (processor == null) {\n      return;\n    }\n\n    // replica name / info + size, retrieved from leaders only\n    Map<String, ReplicaInfo> currentSizes = new HashMap<>();\n\n    try {\n      ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n      for (String node : clusterState.getLiveNodes()) {\n        Map<String, ReplicaInfo> metricTags = new HashMap<>();\n        // coll, shard, replica\n        Map<String, Map<String, List<ReplicaInfo>>> infos = cloudManager.getNodeStateProvider().getReplicaInfo(node, Collections.emptyList());\n        infos.forEach((coll, shards) -> {\n          if (!collections.isEmpty() && !collections.contains(coll)) {\n            return;\n          }\n          DocCollection docCollection = clusterState.getCollection(coll);\n\n          shards.forEach((sh, replicas) -> {\n            // check only the leader replica in an active shard\n            Slice s = docCollection.getSlice(sh);\n            if (s.getState() != Slice.State.ACTIVE) {\n              return;\n            }\n            Replica r = s.getLeader();\n            // no leader - don't do anything\n            if (r == null) {\n              return;\n            }\n            // not on this node\n            if (!r.getNodeName().equals(node)) {\n              return;\n            }\n            // find ReplicaInfo\n            ReplicaInfo info = null;\n            for (ReplicaInfo ri : replicas) {\n              if (r.getCoreName().equals(ri.getCore())) {\n                info = ri;\n                break;\n              }\n            }\n            if (info == null) {\n              // probably replica is not on this node?\n              return;\n            }\n            // we have to translate to the metrics registry name, which uses \"_replica_nN\" as suffix\n            String replicaName = Utils.parseMetricsReplicaName(coll, info.getCore());\n            if (replicaName == null) { // should never happen???\n              replicaName = info.getName(); // which is actually coreNode name...\n            }\n            String registry = SolrCoreMetricManager.createRegistryName(true, coll, sh, replicaName, null);\n            String tag = \"metrics:\" + registry + \":\" + CORE_IDX.metricsAttribute;\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.numDocs\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.maxDoc\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.indexCommitSize\";\n            metricTags.put(tag, info);\n          });\n        });\n        if (metricTags.isEmpty()) {\n          continue;\n        }\n        Map<String, Object> sizes = cloudManager.getNodeStateProvider().getNodeValues(node, metricTags.keySet());\n        sizes.forEach((tag, size) -> {\n          final ReplicaInfo info = metricTags.get(tag);\n          if (info == null) {\n            log.warn(\"Missing replica info for response tag \" + tag);\n          } else {\n            // verify that it's a Number\n            if (!(size instanceof Number)) {\n              log.warn(\"invalid size value for tag \" + tag + \" - not a number: '\" + size + \"' is \" + size.getClass().getName());\n              return;\n            }\n\n            ReplicaInfo currentInfo = currentSizes.computeIfAbsent(info.getCore(), k -> (ReplicaInfo)info.clone());\n            if (tag.contains(\"INDEX\")) {\n              currentInfo.getVariables().put(TOTAL_BYTES_SIZE_KEY, ((Number) size).longValue());\n            } else if (tag.endsWith(\"SEARCHER.searcher.numDocs\")) {\n              currentInfo.getVariables().put(DOCS_SIZE_KEY, ((Number) size).longValue());\n            } else if (tag.endsWith(\"SEARCHER.searcher.maxDoc\")) {\n              currentInfo.getVariables().put(MAX_DOC_KEY, ((Number) size).longValue());\n            } else if (tag.endsWith(\"SEARCHER.searcher.indexCommitSize\")) {\n              currentInfo.getVariables().put(COMMIT_SIZE_KEY, ((Number) size).longValue());\n            }\n          }\n        });\n      }\n    } catch (IOException e) {\n      log.warn(\"Error running trigger \" + getName(), e);\n      return;\n    }\n\n    long now = cloudManager.getTimeSource().getTimeNs();\n\n    // now check thresholds\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> aboveSize = new HashMap<>();\n\n    Set<String> splittable = new HashSet<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      // calculate estimated bytes\n      long maxDoc = (Long)info.getVariable(MAX_DOC_KEY);\n      long numDocs = (Long)info.getVariable(DOCS_SIZE_KEY);\n      long commitSize = (Long)info.getVariable(COMMIT_SIZE_KEY, 0L);\n      if (commitSize <= 0) {\n        commitSize = (Long)info.getVariable(TOTAL_BYTES_SIZE_KEY);\n      }\n      // calculate estimated size as a side-effect\n      commitSize = estimatedSize(maxDoc, numDocs, commitSize);\n      info.getVariables().put(BYTES_SIZE_KEY, commitSize);\n\n      if ((Long)info.getVariable(BYTES_SIZE_KEY) > aboveBytes ||\n          (Long)info.getVariable(DOCS_SIZE_KEY) > aboveDocs) {\n        if (waitForElapsed(coreName, now, lastAboveEventMap)) {\n          List<ReplicaInfo> infos = aboveSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_KEY) > aboveBytes) {\n              info.getVariables().put(VIOLATION_KEY, ABOVE_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_KEY, ABOVE_DOCS_PROP);\n            }\n            infos.add(info);\n            splittable.add(info.getName());\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastAboveEventMap.remove(coreName);\n      }\n    });\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> belowSize = new HashMap<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if (((Long)info.getVariable(BYTES_SIZE_KEY) < belowBytes ||\n          (Long)info.getVariable(DOCS_SIZE_KEY) < belowDocs) &&\n          // make sure we don't produce conflicting ops\n          !splittable.contains(info.getName())) {\n        if (waitForElapsed(coreName, now, lastBelowEventMap)) {\n          List<ReplicaInfo> infos = belowSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_KEY) < belowBytes) {\n              info.getVariables().put(VIOLATION_KEY, BELOW_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_KEY, BELOW_DOCS_PROP);\n            }\n            infos.add(info);\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastBelowEventMap.remove(coreName);\n      }\n    });\n\n    if (aboveSize.isEmpty() && belowSize.isEmpty()) {\n      log.trace(\"NO VIOLATIONS: Now={}\", now);\n      log.trace(\"lastAbove={}\", lastAboveEventMap);\n      log.trace(\"lastBelow={}\", lastBelowEventMap);\n      return;\n    }\n\n    // find the earliest time when a condition was exceeded\n    final AtomicLong eventTime = new AtomicLong(now);\n\n    // calculate ops\n    final List<TriggerEvent.Op> ops = new ArrayList<>();\n    aboveSize.forEach((coll, replicas) -> {\n      // sort by decreasing size to first split the largest ones\n      // XXX see the comment below about using DOCS_SIZE_PROP in lieu of BYTES_SIZE_PROP\n      replicas.sort((r1, r2) -> {\n        long delta = (Long) r1.getVariable(DOCS_SIZE_KEY) - (Long) r2.getVariable(DOCS_SIZE_KEY);\n        if (delta > 0) {\n          return -1;\n        } else if (delta < 0) {\n          return 1;\n        } else {\n          return 0;\n        }\n      });\n      replicas.forEach(r -> {\n        if (ops.size() >= maxOps) {\n          return;\n        }\n        TriggerEvent.Op op = new TriggerEvent.Op(aboveOp);\n        op.addHint(Suggester.Hint.COLL_SHARD, new Pair<>(coll, r.getShard()));\n        Map<String, Object> params = new HashMap<>();\n        params.put(SPLIT_METHOD_PROP, splitMethod.toLower());\n        if (splitFuzz > 0) {\n          params.put(SPLIT_FUZZ_PROP, splitFuzz);\n        }\n        params.put(SPLIT_BY_PREFIX, splitByPrefix);\n        op.addHint(Suggester.Hint.PARAMS, params);\n        ops.add(op);\n        Long time = lastAboveEventMap.get(r.getCore());\n        if (time != null && eventTime.get() > time) {\n          eventTime.set(time);\n        }\n      });\n    });\n    belowSize.forEach((coll, replicas) -> {\n      if (replicas.size() < 2) {\n        return;\n      }\n      if (ops.size() >= maxOps) {\n        return;\n      }\n      // sort by increasing size\n      replicas.sort((r1, r2) -> {\n        // XXX this is not quite correct - if BYTES_SIZE_PROP decided that replica got here\n        // then we should be sorting by BYTES_SIZE_PROP. However, since DOCS and BYTES are\n        // loosely correlated it's simpler to sort just by docs (which better reflects the \"too small\"\n        // condition than index size, due to possibly existing deleted docs that still occupy space)\n        long delta = (Long) r1.getVariable(DOCS_SIZE_KEY) - (Long) r2.getVariable(DOCS_SIZE_KEY);\n        if (delta > 0) {\n          return 1;\n        } else if (delta < 0) {\n          return -1;\n        } else {\n          return 0;\n        }\n      });\n\n      // TODO: MERGESHARDS is not implemented yet. For now take the top two smallest shards\n      // TODO: but in the future we probably need to get ones with adjacent ranges.\n\n      // TODO: generate as many MERGESHARDS as needed to consume all belowSize shards\n      TriggerEvent.Op op = new TriggerEvent.Op(belowOp);\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(0).getShard()));\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(1).getShard()));\n      ops.add(op);\n      Long time = lastBelowEventMap.get(replicas.get(0).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n      time = lastBelowEventMap.get(replicas.get(1).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n    });\n\n    if (ops.isEmpty()) {\n      return;\n    }\n    if (processor.process(new IndexSizeEvent(getName(), eventTime.get(), ops, aboveSize, belowSize))) {\n      // update last event times\n      aboveSize.forEach((coll, replicas) -> {\n        replicas.forEach(r -> lastAboveEventMap.put(r.getCore(), now));\n      });\n      belowSize.forEach((coll, replicas) -> {\n        if (replicas.size() < 2) {\n          return;\n        }\n        lastBelowEventMap.put(replicas.get(0).getCore(), now);\n        lastBelowEventMap.put(replicas.get(1).getCore(), now);\n      });\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"daa0f21a44e235a2299ea1fa913898b182dd7cce","date":1590952026,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/autoscaling/IndexSizeTrigger#run().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/autoscaling/IndexSizeTrigger#run().mjava","sourceNew":"  @Override\n  @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n  public void run() {\n    synchronized(this) {\n      if (isClosed) {\n        log.warn(\"{} ran but was already closed\", getName());\n        return;\n      }\n    }\n    AutoScaling.TriggerEventProcessor processor = processorRef.get();\n    if (processor == null) {\n      return;\n    }\n\n    // replica name / info + size, retrieved from leaders only\n    Map<String, ReplicaInfo> currentSizes = new HashMap<>();\n\n    try {\n      ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n      for (String node : clusterState.getLiveNodes()) {\n        Map<String, ReplicaInfo> metricTags = new HashMap<>();\n        // coll, shard, replica\n        Map<String, Map<String, List<ReplicaInfo>>> infos = cloudManager.getNodeStateProvider().getReplicaInfo(node, Collections.emptyList());\n        infos.forEach((coll, shards) -> {\n          if (!collections.isEmpty() && !collections.contains(coll)) {\n            return;\n          }\n          DocCollection docCollection = clusterState.getCollection(coll);\n\n          shards.forEach((sh, replicas) -> {\n            // check only the leader replica in an active shard\n            Slice s = docCollection.getSlice(sh);\n            if (s.getState() != Slice.State.ACTIVE) {\n              return;\n            }\n            Replica r = s.getLeader();\n            // no leader - don't do anything\n            if (r == null) {\n              return;\n            }\n            // not on this node\n            if (!r.getNodeName().equals(node)) {\n              return;\n            }\n            // find ReplicaInfo\n            ReplicaInfo info = null;\n            for (ReplicaInfo ri : replicas) {\n              if (r.getCoreName().equals(ri.getCore())) {\n                info = ri;\n                break;\n              }\n            }\n            if (info == null) {\n              // probably replica is not on this node?\n              return;\n            }\n            // we have to translate to the metrics registry name, which uses \"_replica_nN\" as suffix\n            String replicaName = Utils.parseMetricsReplicaName(coll, info.getCore());\n            if (replicaName == null) { // should never happen???\n              replicaName = info.getName(); // which is actually coreNode name...\n            }\n            String registry = SolrCoreMetricManager.createRegistryName(true, coll, sh, replicaName, null);\n            String tag = \"metrics:\" + registry + \":\" + CORE_IDX.metricsAttribute;\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.numDocs\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.maxDoc\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.indexCommitSize\";\n            metricTags.put(tag, info);\n          });\n        });\n        if (metricTags.isEmpty()) {\n          continue;\n        }\n        Map<String, Object> sizes = cloudManager.getNodeStateProvider().getNodeValues(node, metricTags.keySet());\n        sizes.forEach((tag, size) -> {\n          final ReplicaInfo info = metricTags.get(tag);\n          if (info == null) {\n            log.warn(\"Missing replica info for response tag {}\", tag);\n          } else {\n            // verify that it's a Number\n            if (!(size instanceof Number)) {\n              log.warn(\"invalid size value for tag {} - not a number: '{}' is {}\", tag, size, size.getClass().getName());\n              return;\n            }\n\n            ReplicaInfo currentInfo = currentSizes.computeIfAbsent(info.getCore(), k -> (ReplicaInfo)info.clone());\n            if (tag.contains(\"INDEX\")) {\n              currentInfo.getVariables().put(TOTAL_BYTES_SIZE_KEY, ((Number) size).longValue());\n            } else if (tag.endsWith(\"SEARCHER.searcher.numDocs\")) {\n              currentInfo.getVariables().put(DOCS_SIZE_KEY, ((Number) size).longValue());\n            } else if (tag.endsWith(\"SEARCHER.searcher.maxDoc\")) {\n              currentInfo.getVariables().put(MAX_DOC_KEY, ((Number) size).longValue());\n            } else if (tag.endsWith(\"SEARCHER.searcher.indexCommitSize\")) {\n              currentInfo.getVariables().put(COMMIT_SIZE_KEY, ((Number) size).longValue());\n            }\n          }\n        });\n      }\n    } catch (IOException e) {\n      log.warn(\"Error running trigger {}\", getName(), e);\n      return;\n    }\n\n    long now = cloudManager.getTimeSource().getTimeNs();\n\n    // now check thresholds\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> aboveSize = new HashMap<>();\n\n    Set<String> splittable = new HashSet<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      // calculate estimated bytes\n      long maxDoc = (Long)info.getVariable(MAX_DOC_KEY);\n      long numDocs = (Long)info.getVariable(DOCS_SIZE_KEY);\n      long commitSize = (Long)info.getVariable(COMMIT_SIZE_KEY, 0L);\n      if (commitSize <= 0) {\n        commitSize = (Long)info.getVariable(TOTAL_BYTES_SIZE_KEY);\n      }\n      // calculate estimated size as a side-effect\n      commitSize = estimatedSize(maxDoc, numDocs, commitSize);\n      info.getVariables().put(BYTES_SIZE_KEY, commitSize);\n\n      if ((Long)info.getVariable(BYTES_SIZE_KEY) > aboveBytes ||\n          (Long)info.getVariable(DOCS_SIZE_KEY) > aboveDocs) {\n        if (waitForElapsed(coreName, now, lastAboveEventMap)) {\n          List<ReplicaInfo> infos = aboveSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_KEY) > aboveBytes) {\n              info.getVariables().put(VIOLATION_KEY, ABOVE_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_KEY, ABOVE_DOCS_PROP);\n            }\n            infos.add(info);\n            splittable.add(info.getName());\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastAboveEventMap.remove(coreName);\n      }\n    });\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> belowSize = new HashMap<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if (((Long)info.getVariable(BYTES_SIZE_KEY) < belowBytes ||\n          (Long)info.getVariable(DOCS_SIZE_KEY) < belowDocs) &&\n          // make sure we don't produce conflicting ops\n          !splittable.contains(info.getName())) {\n        if (waitForElapsed(coreName, now, lastBelowEventMap)) {\n          List<ReplicaInfo> infos = belowSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_KEY) < belowBytes) {\n              info.getVariables().put(VIOLATION_KEY, BELOW_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_KEY, BELOW_DOCS_PROP);\n            }\n            infos.add(info);\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastBelowEventMap.remove(coreName);\n      }\n    });\n\n    if (aboveSize.isEmpty() && belowSize.isEmpty()) {\n      log.trace(\"NO VIOLATIONS: Now={}\", now);\n      log.trace(\"lastAbove={}\", lastAboveEventMap);\n      log.trace(\"lastBelow={}\", lastBelowEventMap);\n      return;\n    }\n\n    // find the earliest time when a condition was exceeded\n    final AtomicLong eventTime = new AtomicLong(now);\n\n    // calculate ops\n    final List<TriggerEvent.Op> ops = new ArrayList<>();\n    aboveSize.forEach((coll, replicas) -> {\n      // sort by decreasing size to first split the largest ones\n      // XXX see the comment below about using DOCS_SIZE_PROP in lieu of BYTES_SIZE_PROP\n      replicas.sort((r1, r2) -> {\n        long delta = (Long) r1.getVariable(DOCS_SIZE_KEY) - (Long) r2.getVariable(DOCS_SIZE_KEY);\n        if (delta > 0) {\n          return -1;\n        } else if (delta < 0) {\n          return 1;\n        } else {\n          return 0;\n        }\n      });\n      replicas.forEach(r -> {\n        if (ops.size() >= maxOps) {\n          return;\n        }\n        TriggerEvent.Op op = new TriggerEvent.Op(aboveOp);\n        op.addHint(Suggester.Hint.COLL_SHARD, new Pair<>(coll, r.getShard()));\n        Map<String, Object> params = new HashMap<>();\n        params.put(SPLIT_METHOD_PROP, splitMethod.toLower());\n        if (splitFuzz > 0) {\n          params.put(SPLIT_FUZZ_PROP, splitFuzz);\n        }\n        params.put(SPLIT_BY_PREFIX, splitByPrefix);\n        op.addHint(Suggester.Hint.PARAMS, params);\n        ops.add(op);\n        Long time = lastAboveEventMap.get(r.getCore());\n        if (time != null && eventTime.get() > time) {\n          eventTime.set(time);\n        }\n      });\n    });\n    belowSize.forEach((coll, replicas) -> {\n      if (replicas.size() < 2) {\n        return;\n      }\n      if (ops.size() >= maxOps) {\n        return;\n      }\n      // sort by increasing size\n      replicas.sort((r1, r2) -> {\n        // XXX this is not quite correct - if BYTES_SIZE_PROP decided that replica got here\n        // then we should be sorting by BYTES_SIZE_PROP. However, since DOCS and BYTES are\n        // loosely correlated it's simpler to sort just by docs (which better reflects the \"too small\"\n        // condition than index size, due to possibly existing deleted docs that still occupy space)\n        long delta = (Long) r1.getVariable(DOCS_SIZE_KEY) - (Long) r2.getVariable(DOCS_SIZE_KEY);\n        if (delta > 0) {\n          return 1;\n        } else if (delta < 0) {\n          return -1;\n        } else {\n          return 0;\n        }\n      });\n\n      // TODO: MERGESHARDS is not implemented yet. For now take the top two smallest shards\n      // TODO: but in the future we probably need to get ones with adjacent ranges.\n\n      // TODO: generate as many MERGESHARDS as needed to consume all belowSize shards\n      TriggerEvent.Op op = new TriggerEvent.Op(belowOp);\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(0).getShard()));\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(1).getShard()));\n      ops.add(op);\n      Long time = lastBelowEventMap.get(replicas.get(0).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n      time = lastBelowEventMap.get(replicas.get(1).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n    });\n\n    if (ops.isEmpty()) {\n      return;\n    }\n    if (processor.process(new IndexSizeEvent(getName(), eventTime.get(), ops, aboveSize, belowSize))) {\n      // update last event times\n      aboveSize.forEach((coll, replicas) -> {\n        replicas.forEach(r -> lastAboveEventMap.put(r.getCore(), now));\n      });\n      belowSize.forEach((coll, replicas) -> {\n        if (replicas.size() < 2) {\n          return;\n        }\n        lastBelowEventMap.put(replicas.get(0).getCore(), now);\n        lastBelowEventMap.put(replicas.get(1).getCore(), now);\n      });\n    }\n  }\n\n","sourceOld":"  @Override\n  public void run() {\n    synchronized(this) {\n      if (isClosed) {\n        log.warn(\"{} ran but was already closed\", getName());\n        return;\n      }\n    }\n    AutoScaling.TriggerEventProcessor processor = processorRef.get();\n    if (processor == null) {\n      return;\n    }\n\n    // replica name / info + size, retrieved from leaders only\n    Map<String, ReplicaInfo> currentSizes = new HashMap<>();\n\n    try {\n      ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n      for (String node : clusterState.getLiveNodes()) {\n        Map<String, ReplicaInfo> metricTags = new HashMap<>();\n        // coll, shard, replica\n        Map<String, Map<String, List<ReplicaInfo>>> infos = cloudManager.getNodeStateProvider().getReplicaInfo(node, Collections.emptyList());\n        infos.forEach((coll, shards) -> {\n          if (!collections.isEmpty() && !collections.contains(coll)) {\n            return;\n          }\n          DocCollection docCollection = clusterState.getCollection(coll);\n\n          shards.forEach((sh, replicas) -> {\n            // check only the leader replica in an active shard\n            Slice s = docCollection.getSlice(sh);\n            if (s.getState() != Slice.State.ACTIVE) {\n              return;\n            }\n            Replica r = s.getLeader();\n            // no leader - don't do anything\n            if (r == null) {\n              return;\n            }\n            // not on this node\n            if (!r.getNodeName().equals(node)) {\n              return;\n            }\n            // find ReplicaInfo\n            ReplicaInfo info = null;\n            for (ReplicaInfo ri : replicas) {\n              if (r.getCoreName().equals(ri.getCore())) {\n                info = ri;\n                break;\n              }\n            }\n            if (info == null) {\n              // probably replica is not on this node?\n              return;\n            }\n            // we have to translate to the metrics registry name, which uses \"_replica_nN\" as suffix\n            String replicaName = Utils.parseMetricsReplicaName(coll, info.getCore());\n            if (replicaName == null) { // should never happen???\n              replicaName = info.getName(); // which is actually coreNode name...\n            }\n            String registry = SolrCoreMetricManager.createRegistryName(true, coll, sh, replicaName, null);\n            String tag = \"metrics:\" + registry + \":\" + CORE_IDX.metricsAttribute;\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.numDocs\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.maxDoc\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.indexCommitSize\";\n            metricTags.put(tag, info);\n          });\n        });\n        if (metricTags.isEmpty()) {\n          continue;\n        }\n        Map<String, Object> sizes = cloudManager.getNodeStateProvider().getNodeValues(node, metricTags.keySet());\n        sizes.forEach((tag, size) -> {\n          final ReplicaInfo info = metricTags.get(tag);\n          if (info == null) {\n            log.warn(\"Missing replica info for response tag {}\", tag);\n          } else {\n            // verify that it's a Number\n            if (!(size instanceof Number)) {\n              log.warn(\"invalid size value for tag {} - not a number: '{}' is {}\", tag, size, size.getClass().getName());\n              return;\n            }\n\n            ReplicaInfo currentInfo = currentSizes.computeIfAbsent(info.getCore(), k -> (ReplicaInfo)info.clone());\n            if (tag.contains(\"INDEX\")) {\n              currentInfo.getVariables().put(TOTAL_BYTES_SIZE_KEY, ((Number) size).longValue());\n            } else if (tag.endsWith(\"SEARCHER.searcher.numDocs\")) {\n              currentInfo.getVariables().put(DOCS_SIZE_KEY, ((Number) size).longValue());\n            } else if (tag.endsWith(\"SEARCHER.searcher.maxDoc\")) {\n              currentInfo.getVariables().put(MAX_DOC_KEY, ((Number) size).longValue());\n            } else if (tag.endsWith(\"SEARCHER.searcher.indexCommitSize\")) {\n              currentInfo.getVariables().put(COMMIT_SIZE_KEY, ((Number) size).longValue());\n            }\n          }\n        });\n      }\n    } catch (IOException e) {\n      log.warn(\"Error running trigger {}\", getName(), e);\n      return;\n    }\n\n    long now = cloudManager.getTimeSource().getTimeNs();\n\n    // now check thresholds\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> aboveSize = new HashMap<>();\n\n    Set<String> splittable = new HashSet<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      // calculate estimated bytes\n      long maxDoc = (Long)info.getVariable(MAX_DOC_KEY);\n      long numDocs = (Long)info.getVariable(DOCS_SIZE_KEY);\n      long commitSize = (Long)info.getVariable(COMMIT_SIZE_KEY, 0L);\n      if (commitSize <= 0) {\n        commitSize = (Long)info.getVariable(TOTAL_BYTES_SIZE_KEY);\n      }\n      // calculate estimated size as a side-effect\n      commitSize = estimatedSize(maxDoc, numDocs, commitSize);\n      info.getVariables().put(BYTES_SIZE_KEY, commitSize);\n\n      if ((Long)info.getVariable(BYTES_SIZE_KEY) > aboveBytes ||\n          (Long)info.getVariable(DOCS_SIZE_KEY) > aboveDocs) {\n        if (waitForElapsed(coreName, now, lastAboveEventMap)) {\n          List<ReplicaInfo> infos = aboveSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_KEY) > aboveBytes) {\n              info.getVariables().put(VIOLATION_KEY, ABOVE_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_KEY, ABOVE_DOCS_PROP);\n            }\n            infos.add(info);\n            splittable.add(info.getName());\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastAboveEventMap.remove(coreName);\n      }\n    });\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> belowSize = new HashMap<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if (((Long)info.getVariable(BYTES_SIZE_KEY) < belowBytes ||\n          (Long)info.getVariable(DOCS_SIZE_KEY) < belowDocs) &&\n          // make sure we don't produce conflicting ops\n          !splittable.contains(info.getName())) {\n        if (waitForElapsed(coreName, now, lastBelowEventMap)) {\n          List<ReplicaInfo> infos = belowSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_KEY) < belowBytes) {\n              info.getVariables().put(VIOLATION_KEY, BELOW_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_KEY, BELOW_DOCS_PROP);\n            }\n            infos.add(info);\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastBelowEventMap.remove(coreName);\n      }\n    });\n\n    if (aboveSize.isEmpty() && belowSize.isEmpty()) {\n      log.trace(\"NO VIOLATIONS: Now={}\", now);\n      log.trace(\"lastAbove={}\", lastAboveEventMap);\n      log.trace(\"lastBelow={}\", lastBelowEventMap);\n      return;\n    }\n\n    // find the earliest time when a condition was exceeded\n    final AtomicLong eventTime = new AtomicLong(now);\n\n    // calculate ops\n    final List<TriggerEvent.Op> ops = new ArrayList<>();\n    aboveSize.forEach((coll, replicas) -> {\n      // sort by decreasing size to first split the largest ones\n      // XXX see the comment below about using DOCS_SIZE_PROP in lieu of BYTES_SIZE_PROP\n      replicas.sort((r1, r2) -> {\n        long delta = (Long) r1.getVariable(DOCS_SIZE_KEY) - (Long) r2.getVariable(DOCS_SIZE_KEY);\n        if (delta > 0) {\n          return -1;\n        } else if (delta < 0) {\n          return 1;\n        } else {\n          return 0;\n        }\n      });\n      replicas.forEach(r -> {\n        if (ops.size() >= maxOps) {\n          return;\n        }\n        TriggerEvent.Op op = new TriggerEvent.Op(aboveOp);\n        op.addHint(Suggester.Hint.COLL_SHARD, new Pair<>(coll, r.getShard()));\n        Map<String, Object> params = new HashMap<>();\n        params.put(SPLIT_METHOD_PROP, splitMethod.toLower());\n        if (splitFuzz > 0) {\n          params.put(SPLIT_FUZZ_PROP, splitFuzz);\n        }\n        params.put(SPLIT_BY_PREFIX, splitByPrefix);\n        op.addHint(Suggester.Hint.PARAMS, params);\n        ops.add(op);\n        Long time = lastAboveEventMap.get(r.getCore());\n        if (time != null && eventTime.get() > time) {\n          eventTime.set(time);\n        }\n      });\n    });\n    belowSize.forEach((coll, replicas) -> {\n      if (replicas.size() < 2) {\n        return;\n      }\n      if (ops.size() >= maxOps) {\n        return;\n      }\n      // sort by increasing size\n      replicas.sort((r1, r2) -> {\n        // XXX this is not quite correct - if BYTES_SIZE_PROP decided that replica got here\n        // then we should be sorting by BYTES_SIZE_PROP. However, since DOCS and BYTES are\n        // loosely correlated it's simpler to sort just by docs (which better reflects the \"too small\"\n        // condition than index size, due to possibly existing deleted docs that still occupy space)\n        long delta = (Long) r1.getVariable(DOCS_SIZE_KEY) - (Long) r2.getVariable(DOCS_SIZE_KEY);\n        if (delta > 0) {\n          return 1;\n        } else if (delta < 0) {\n          return -1;\n        } else {\n          return 0;\n        }\n      });\n\n      // TODO: MERGESHARDS is not implemented yet. For now take the top two smallest shards\n      // TODO: but in the future we probably need to get ones with adjacent ranges.\n\n      // TODO: generate as many MERGESHARDS as needed to consume all belowSize shards\n      TriggerEvent.Op op = new TriggerEvent.Op(belowOp);\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(0).getShard()));\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(1).getShard()));\n      ops.add(op);\n      Long time = lastBelowEventMap.get(replicas.get(0).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n      time = lastBelowEventMap.get(replicas.get(1).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n    });\n\n    if (ops.isEmpty()) {\n      return;\n    }\n    if (processor.process(new IndexSizeEvent(getName(), eventTime.get(), ops, aboveSize, belowSize))) {\n      // update last event times\n      aboveSize.forEach((coll, replicas) -> {\n        replicas.forEach(r -> lastAboveEventMap.put(r.getCore(), now));\n      });\n      belowSize.forEach((coll, replicas) -> {\n        if (replicas.size() < 2) {\n          return;\n        }\n        lastBelowEventMap.put(replicas.get(0).getCore(), now);\n        lastBelowEventMap.put(replicas.get(1).getCore(), now);\n      });\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7e8ce2f9d2ddfcf5cfa7e73b8b2af287a2a276fd","date":1594731683,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/autoscaling/IndexSizeTrigger#run().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/autoscaling/IndexSizeTrigger#run().mjava","sourceNew":"  @Override\n  @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n  public void run() {\n    synchronized(this) {\n      if (isClosed) {\n        log.warn(\"{} ran but was already closed\", getName());\n        return;\n      }\n    }\n    AutoScaling.TriggerEventProcessor processor = processorRef.get();\n    if (processor == null) {\n      return;\n    }\n\n    // replica name / info + size, retrieved from leaders only\n    Map<String, Replica> currentSizes = new HashMap<>();\n\n    try {\n      ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n      for (String node : clusterState.getLiveNodes()) {\n        Map<String, Replica> metricTags = new HashMap<>();\n        // coll, shard, replica\n        Map<String, Map<String, List<Replica>>> infos = cloudManager.getNodeStateProvider().getReplicaInfo(node, Collections.emptyList());\n        infos.forEach((coll, shards) -> {\n          if (!collections.isEmpty() && !collections.contains(coll)) {\n            return;\n          }\n          DocCollection docCollection = clusterState.getCollection(coll);\n\n          shards.forEach((sh, replicas) -> {\n            // check only the leader replica in an active shard\n            Slice s = docCollection.getSlice(sh);\n            if (s.getState() != Slice.State.ACTIVE) {\n              return;\n            }\n            Replica r = s.getLeader();\n            // no leader - don't do anything\n            if (r == null) {\n              return;\n            }\n            // not on this node\n            if (!r.getNodeName().equals(node)) {\n              return;\n            }\n            // find ReplicaInfo\n            Replica info = null;\n            for (Replica ri : replicas) {\n              if (r.getCoreName().equals(ri.getCoreName())) {\n                info = ri;\n                break;\n              }\n            }\n            if (info == null) {\n              // probably replica is not on this node?\n              return;\n            }\n            // we have to translate to the metrics registry name, which uses \"_replica_nN\" as suffix\n            String replicaName = Utils.parseMetricsReplicaName(coll, info.getCoreName());\n            if (replicaName == null) { // should never happen???\n              replicaName = info.getName(); // which is actually coreNode name...\n            }\n            String registry = SolrCoreMetricManager.createRegistryName(true, coll, sh, replicaName, null);\n            String tag = \"metrics:\" + registry + \":\" + CORE_IDX.metricsAttribute;\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.numDocs\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.maxDoc\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.indexCommitSize\";\n            metricTags.put(tag, info);\n          });\n        });\n        if (metricTags.isEmpty()) {\n          continue;\n        }\n        Map<String, Object> sizes = cloudManager.getNodeStateProvider().getNodeValues(node, metricTags.keySet());\n        sizes.forEach((tag, size) -> {\n          final Replica info = metricTags.get(tag);\n          if (info == null) {\n            log.warn(\"Missing replica info for response tag {}\", tag);\n          } else {\n            // verify that it's a Number\n            if (!(size instanceof Number)) {\n              log.warn(\"invalid size value for tag {} - not a number: '{}' is {}\", tag, size, size.getClass().getName());\n              return;\n            }\n\n            Replica currentInfo = currentSizes.computeIfAbsent(info.getCoreName(), k -> (Replica) info.clone());\n            if (tag.contains(\"INDEX\")) {\n              currentInfo.getProperties().put(TOTAL_BYTES_SIZE_KEY, ((Number) size).longValue());\n            } else if (tag.endsWith(\"SEARCHER.searcher.numDocs\")) {\n              currentInfo.getProperties().put(DOCS_SIZE_KEY, ((Number) size).longValue());\n            } else if (tag.endsWith(\"SEARCHER.searcher.maxDoc\")) {\n              currentInfo.getProperties().put(MAX_DOC_KEY, ((Number) size).longValue());\n            } else if (tag.endsWith(\"SEARCHER.searcher.indexCommitSize\")) {\n              currentInfo.getProperties().put(COMMIT_SIZE_KEY, ((Number) size).longValue());\n            }\n          }\n        });\n      }\n    } catch (IOException e) {\n      log.warn(\"Error running trigger {}\", getName(), e);\n      return;\n    }\n\n    long now = cloudManager.getTimeSource().getTimeNs();\n\n    // now check thresholds\n\n    // collection / list(info)\n    Map<String, List<Replica>> aboveSize = new HashMap<>();\n\n    Set<String> splittable = new HashSet<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      // calculate estimated bytes\n      long maxDoc = (Long)info.get(MAX_DOC_KEY);\n      long numDocs = (Long)info.get(DOCS_SIZE_KEY);\n      long commitSize = (Long)info.get(COMMIT_SIZE_KEY, 0L);\n      if (commitSize <= 0) {\n        commitSize = (Long)info.get(TOTAL_BYTES_SIZE_KEY);\n      }\n      // calculate estimated size as a side-effect\n      commitSize = estimatedSize(maxDoc, numDocs, commitSize);\n      info.getProperties().put(BYTES_SIZE_KEY, commitSize);\n\n      if ((Long)info.get(BYTES_SIZE_KEY) > aboveBytes ||\n          (Long)info.get(DOCS_SIZE_KEY) > aboveDocs) {\n        if (waitForElapsed(coreName, now, lastAboveEventMap)) {\n          List<Replica> infos = aboveSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.get(BYTES_SIZE_KEY) > aboveBytes) {\n              info.getProperties().put(VIOLATION_KEY, ABOVE_BYTES_PROP);\n            } else {\n              info.getProperties().put(VIOLATION_KEY, ABOVE_DOCS_PROP);\n            }\n            infos.add(info);\n            splittable.add(info.getName());\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastAboveEventMap.remove(coreName);\n      }\n    });\n\n    // collection / list(info)\n    Map<String, List<Replica>> belowSize = new HashMap<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if (((Long)info.get(BYTES_SIZE_KEY) < belowBytes ||\n          (Long)info.get(DOCS_SIZE_KEY) < belowDocs) &&\n          // make sure we don't produce conflicting ops\n          !splittable.contains(info.getName())) {\n        if (waitForElapsed(coreName, now, lastBelowEventMap)) {\n          List<Replica> infos = belowSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.get(BYTES_SIZE_KEY) < belowBytes) {\n              info.getProperties().put(VIOLATION_KEY, BELOW_BYTES_PROP);\n            } else {\n              info.getProperties().put(VIOLATION_KEY, BELOW_DOCS_PROP);\n            }\n            infos.add(info);\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastBelowEventMap.remove(coreName);\n      }\n    });\n\n    if (aboveSize.isEmpty() && belowSize.isEmpty()) {\n      log.trace(\"NO VIOLATIONS: Now={}\", now);\n      log.trace(\"lastAbove={}\", lastAboveEventMap);\n      log.trace(\"lastBelow={}\", lastBelowEventMap);\n      return;\n    }\n\n    // find the earliest time when a condition was exceeded\n    final AtomicLong eventTime = new AtomicLong(now);\n\n    // calculate ops\n    final List<TriggerEvent.Op> ops = new ArrayList<>();\n    aboveSize.forEach((coll, replicas) -> {\n      // sort by decreasing size to first split the largest ones\n      // XXX see the comment below about using DOCS_SIZE_PROP in lieu of BYTES_SIZE_PROP\n      replicas.sort((r1, r2) -> {\n        long delta = (Long) r1.get(DOCS_SIZE_KEY) - (Long) r2.get(DOCS_SIZE_KEY);\n        if (delta > 0) {\n          return -1;\n        } else if (delta < 0) {\n          return 1;\n        } else {\n          return 0;\n        }\n      });\n      replicas.forEach(r -> {\n        if (ops.size() >= maxOps) {\n          return;\n        }\n        TriggerEvent.Op op = new TriggerEvent.Op(aboveOp);\n        op.addHint(Suggester.Hint.COLL_SHARD, new Pair<>(coll, r.getShard()));\n        Map<String, Object> params = new HashMap<>();\n        params.put(SPLIT_METHOD_PROP, splitMethod.toLower());\n        if (splitFuzz > 0) {\n          params.put(SPLIT_FUZZ_PROP, splitFuzz);\n        }\n        params.put(SPLIT_BY_PREFIX, splitByPrefix);\n        op.addHint(Suggester.Hint.PARAMS, params);\n        ops.add(op);\n        Long time = lastAboveEventMap.get(r.getCoreName());\n        if (time != null && eventTime.get() > time) {\n          eventTime.set(time);\n        }\n      });\n    });\n    belowSize.forEach((coll, replicas) -> {\n      if (replicas.size() < 2) {\n        return;\n      }\n      if (ops.size() >= maxOps) {\n        return;\n      }\n      // sort by increasing size\n      replicas.sort((r1, r2) -> {\n        // XXX this is not quite correct - if BYTES_SIZE_PROP decided that replica got here\n        // then we should be sorting by BYTES_SIZE_PROP. However, since DOCS and BYTES are\n        // loosely correlated it's simpler to sort just by docs (which better reflects the \"too small\"\n        // condition than index size, due to possibly existing deleted docs that still occupy space)\n        long delta = (Long) r1.get(DOCS_SIZE_KEY) - (Long) r2.get(DOCS_SIZE_KEY);\n        if (delta > 0) {\n          return 1;\n        } else if (delta < 0) {\n          return -1;\n        } else {\n          return 0;\n        }\n      });\n\n      // TODO: MERGESHARDS is not implemented yet. For now take the top two smallest shards\n      // TODO: but in the future we probably need to get ones with adjacent ranges.\n\n      // TODO: generate as many MERGESHARDS as needed to consume all belowSize shards\n      TriggerEvent.Op op = new TriggerEvent.Op(belowOp);\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(0).getShard()));\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(1).getShard()));\n      ops.add(op);\n      Long time = lastBelowEventMap.get(replicas.get(0).getCoreName());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n      time = lastBelowEventMap.get(replicas.get(1).getCoreName());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n    });\n\n    if (ops.isEmpty()) {\n      return;\n    }\n    if (processor.process(new IndexSizeEvent(getName(), eventTime.get(), ops, aboveSize, belowSize))) {\n      // update last event times\n      aboveSize.forEach((coll, replicas) -> {\n        replicas.forEach(r -> lastAboveEventMap.put(r.getCoreName(), now));\n      });\n      belowSize.forEach((coll, replicas) -> {\n        if (replicas.size() < 2) {\n          return;\n        }\n        lastBelowEventMap.put(replicas.get(0).getCoreName(), now);\n        lastBelowEventMap.put(replicas.get(1).getCoreName(), now);\n      });\n    }\n  }\n\n","sourceOld":"  @Override\n  @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n  public void run() {\n    synchronized(this) {\n      if (isClosed) {\n        log.warn(\"{} ran but was already closed\", getName());\n        return;\n      }\n    }\n    AutoScaling.TriggerEventProcessor processor = processorRef.get();\n    if (processor == null) {\n      return;\n    }\n\n    // replica name / info + size, retrieved from leaders only\n    Map<String, ReplicaInfo> currentSizes = new HashMap<>();\n\n    try {\n      ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n      for (String node : clusterState.getLiveNodes()) {\n        Map<String, ReplicaInfo> metricTags = new HashMap<>();\n        // coll, shard, replica\n        Map<String, Map<String, List<ReplicaInfo>>> infos = cloudManager.getNodeStateProvider().getReplicaInfo(node, Collections.emptyList());\n        infos.forEach((coll, shards) -> {\n          if (!collections.isEmpty() && !collections.contains(coll)) {\n            return;\n          }\n          DocCollection docCollection = clusterState.getCollection(coll);\n\n          shards.forEach((sh, replicas) -> {\n            // check only the leader replica in an active shard\n            Slice s = docCollection.getSlice(sh);\n            if (s.getState() != Slice.State.ACTIVE) {\n              return;\n            }\n            Replica r = s.getLeader();\n            // no leader - don't do anything\n            if (r == null) {\n              return;\n            }\n            // not on this node\n            if (!r.getNodeName().equals(node)) {\n              return;\n            }\n            // find ReplicaInfo\n            ReplicaInfo info = null;\n            for (ReplicaInfo ri : replicas) {\n              if (r.getCoreName().equals(ri.getCore())) {\n                info = ri;\n                break;\n              }\n            }\n            if (info == null) {\n              // probably replica is not on this node?\n              return;\n            }\n            // we have to translate to the metrics registry name, which uses \"_replica_nN\" as suffix\n            String replicaName = Utils.parseMetricsReplicaName(coll, info.getCore());\n            if (replicaName == null) { // should never happen???\n              replicaName = info.getName(); // which is actually coreNode name...\n            }\n            String registry = SolrCoreMetricManager.createRegistryName(true, coll, sh, replicaName, null);\n            String tag = \"metrics:\" + registry + \":\" + CORE_IDX.metricsAttribute;\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.numDocs\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.maxDoc\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.indexCommitSize\";\n            metricTags.put(tag, info);\n          });\n        });\n        if (metricTags.isEmpty()) {\n          continue;\n        }\n        Map<String, Object> sizes = cloudManager.getNodeStateProvider().getNodeValues(node, metricTags.keySet());\n        sizes.forEach((tag, size) -> {\n          final ReplicaInfo info = metricTags.get(tag);\n          if (info == null) {\n            log.warn(\"Missing replica info for response tag {}\", tag);\n          } else {\n            // verify that it's a Number\n            if (!(size instanceof Number)) {\n              log.warn(\"invalid size value for tag {} - not a number: '{}' is {}\", tag, size, size.getClass().getName());\n              return;\n            }\n\n            ReplicaInfo currentInfo = currentSizes.computeIfAbsent(info.getCore(), k -> (ReplicaInfo)info.clone());\n            if (tag.contains(\"INDEX\")) {\n              currentInfo.getVariables().put(TOTAL_BYTES_SIZE_KEY, ((Number) size).longValue());\n            } else if (tag.endsWith(\"SEARCHER.searcher.numDocs\")) {\n              currentInfo.getVariables().put(DOCS_SIZE_KEY, ((Number) size).longValue());\n            } else if (tag.endsWith(\"SEARCHER.searcher.maxDoc\")) {\n              currentInfo.getVariables().put(MAX_DOC_KEY, ((Number) size).longValue());\n            } else if (tag.endsWith(\"SEARCHER.searcher.indexCommitSize\")) {\n              currentInfo.getVariables().put(COMMIT_SIZE_KEY, ((Number) size).longValue());\n            }\n          }\n        });\n      }\n    } catch (IOException e) {\n      log.warn(\"Error running trigger {}\", getName(), e);\n      return;\n    }\n\n    long now = cloudManager.getTimeSource().getTimeNs();\n\n    // now check thresholds\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> aboveSize = new HashMap<>();\n\n    Set<String> splittable = new HashSet<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      // calculate estimated bytes\n      long maxDoc = (Long)info.getVariable(MAX_DOC_KEY);\n      long numDocs = (Long)info.getVariable(DOCS_SIZE_KEY);\n      long commitSize = (Long)info.getVariable(COMMIT_SIZE_KEY, 0L);\n      if (commitSize <= 0) {\n        commitSize = (Long)info.getVariable(TOTAL_BYTES_SIZE_KEY);\n      }\n      // calculate estimated size as a side-effect\n      commitSize = estimatedSize(maxDoc, numDocs, commitSize);\n      info.getVariables().put(BYTES_SIZE_KEY, commitSize);\n\n      if ((Long)info.getVariable(BYTES_SIZE_KEY) > aboveBytes ||\n          (Long)info.getVariable(DOCS_SIZE_KEY) > aboveDocs) {\n        if (waitForElapsed(coreName, now, lastAboveEventMap)) {\n          List<ReplicaInfo> infos = aboveSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_KEY) > aboveBytes) {\n              info.getVariables().put(VIOLATION_KEY, ABOVE_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_KEY, ABOVE_DOCS_PROP);\n            }\n            infos.add(info);\n            splittable.add(info.getName());\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastAboveEventMap.remove(coreName);\n      }\n    });\n\n    // collection / list(info)\n    Map<String, List<ReplicaInfo>> belowSize = new HashMap<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if (((Long)info.getVariable(BYTES_SIZE_KEY) < belowBytes ||\n          (Long)info.getVariable(DOCS_SIZE_KEY) < belowDocs) &&\n          // make sure we don't produce conflicting ops\n          !splittable.contains(info.getName())) {\n        if (waitForElapsed(coreName, now, lastBelowEventMap)) {\n          List<ReplicaInfo> infos = belowSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.getVariable(BYTES_SIZE_KEY) < belowBytes) {\n              info.getVariables().put(VIOLATION_KEY, BELOW_BYTES_PROP);\n            } else {\n              info.getVariables().put(VIOLATION_KEY, BELOW_DOCS_PROP);\n            }\n            infos.add(info);\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastBelowEventMap.remove(coreName);\n      }\n    });\n\n    if (aboveSize.isEmpty() && belowSize.isEmpty()) {\n      log.trace(\"NO VIOLATIONS: Now={}\", now);\n      log.trace(\"lastAbove={}\", lastAboveEventMap);\n      log.trace(\"lastBelow={}\", lastBelowEventMap);\n      return;\n    }\n\n    // find the earliest time when a condition was exceeded\n    final AtomicLong eventTime = new AtomicLong(now);\n\n    // calculate ops\n    final List<TriggerEvent.Op> ops = new ArrayList<>();\n    aboveSize.forEach((coll, replicas) -> {\n      // sort by decreasing size to first split the largest ones\n      // XXX see the comment below about using DOCS_SIZE_PROP in lieu of BYTES_SIZE_PROP\n      replicas.sort((r1, r2) -> {\n        long delta = (Long) r1.getVariable(DOCS_SIZE_KEY) - (Long) r2.getVariable(DOCS_SIZE_KEY);\n        if (delta > 0) {\n          return -1;\n        } else if (delta < 0) {\n          return 1;\n        } else {\n          return 0;\n        }\n      });\n      replicas.forEach(r -> {\n        if (ops.size() >= maxOps) {\n          return;\n        }\n        TriggerEvent.Op op = new TriggerEvent.Op(aboveOp);\n        op.addHint(Suggester.Hint.COLL_SHARD, new Pair<>(coll, r.getShard()));\n        Map<String, Object> params = new HashMap<>();\n        params.put(SPLIT_METHOD_PROP, splitMethod.toLower());\n        if (splitFuzz > 0) {\n          params.put(SPLIT_FUZZ_PROP, splitFuzz);\n        }\n        params.put(SPLIT_BY_PREFIX, splitByPrefix);\n        op.addHint(Suggester.Hint.PARAMS, params);\n        ops.add(op);\n        Long time = lastAboveEventMap.get(r.getCore());\n        if (time != null && eventTime.get() > time) {\n          eventTime.set(time);\n        }\n      });\n    });\n    belowSize.forEach((coll, replicas) -> {\n      if (replicas.size() < 2) {\n        return;\n      }\n      if (ops.size() >= maxOps) {\n        return;\n      }\n      // sort by increasing size\n      replicas.sort((r1, r2) -> {\n        // XXX this is not quite correct - if BYTES_SIZE_PROP decided that replica got here\n        // then we should be sorting by BYTES_SIZE_PROP. However, since DOCS and BYTES are\n        // loosely correlated it's simpler to sort just by docs (which better reflects the \"too small\"\n        // condition than index size, due to possibly existing deleted docs that still occupy space)\n        long delta = (Long) r1.getVariable(DOCS_SIZE_KEY) - (Long) r2.getVariable(DOCS_SIZE_KEY);\n        if (delta > 0) {\n          return 1;\n        } else if (delta < 0) {\n          return -1;\n        } else {\n          return 0;\n        }\n      });\n\n      // TODO: MERGESHARDS is not implemented yet. For now take the top two smallest shards\n      // TODO: but in the future we probably need to get ones with adjacent ranges.\n\n      // TODO: generate as many MERGESHARDS as needed to consume all belowSize shards\n      TriggerEvent.Op op = new TriggerEvent.Op(belowOp);\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(0).getShard()));\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(1).getShard()));\n      ops.add(op);\n      Long time = lastBelowEventMap.get(replicas.get(0).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n      time = lastBelowEventMap.get(replicas.get(1).getCore());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n    });\n\n    if (ops.isEmpty()) {\n      return;\n    }\n    if (processor.process(new IndexSizeEvent(getName(), eventTime.get(), ops, aboveSize, belowSize))) {\n      // update last event times\n      aboveSize.forEach((coll, replicas) -> {\n        replicas.forEach(r -> lastAboveEventMap.put(r.getCore(), now));\n      });\n      belowSize.forEach((coll, replicas) -> {\n        if (replicas.size() < 2) {\n          return;\n        }\n        lastBelowEventMap.put(replicas.get(0).getCore(), now);\n        lastBelowEventMap.put(replicas.get(1).getCore(), now);\n      });\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3f504512a03d978990cbff30db0522b354e846db","date":1595247421,"type":4,"author":"Ishan Chattopadhyaya","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/cloud/autoscaling/IndexSizeTrigger#run().mjava","sourceNew":null,"sourceOld":"  @Override\n  @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n  public void run() {\n    synchronized(this) {\n      if (isClosed) {\n        log.warn(\"{} ran but was already closed\", getName());\n        return;\n      }\n    }\n    AutoScaling.TriggerEventProcessor processor = processorRef.get();\n    if (processor == null) {\n      return;\n    }\n\n    // replica name / info + size, retrieved from leaders only\n    Map<String, Replica> currentSizes = new HashMap<>();\n\n    try {\n      ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n      for (String node : clusterState.getLiveNodes()) {\n        Map<String, Replica> metricTags = new HashMap<>();\n        // coll, shard, replica\n        Map<String, Map<String, List<Replica>>> infos = cloudManager.getNodeStateProvider().getReplicaInfo(node, Collections.emptyList());\n        infos.forEach((coll, shards) -> {\n          if (!collections.isEmpty() && !collections.contains(coll)) {\n            return;\n          }\n          DocCollection docCollection = clusterState.getCollection(coll);\n\n          shards.forEach((sh, replicas) -> {\n            // check only the leader replica in an active shard\n            Slice s = docCollection.getSlice(sh);\n            if (s.getState() != Slice.State.ACTIVE) {\n              return;\n            }\n            Replica r = s.getLeader();\n            // no leader - don't do anything\n            if (r == null) {\n              return;\n            }\n            // not on this node\n            if (!r.getNodeName().equals(node)) {\n              return;\n            }\n            // find ReplicaInfo\n            Replica info = null;\n            for (Replica ri : replicas) {\n              if (r.getCoreName().equals(ri.getCoreName())) {\n                info = ri;\n                break;\n              }\n            }\n            if (info == null) {\n              // probably replica is not on this node?\n              return;\n            }\n            // we have to translate to the metrics registry name, which uses \"_replica_nN\" as suffix\n            String replicaName = Utils.parseMetricsReplicaName(coll, info.getCoreName());\n            if (replicaName == null) { // should never happen???\n              replicaName = info.getName(); // which is actually coreNode name...\n            }\n            String registry = SolrCoreMetricManager.createRegistryName(true, coll, sh, replicaName, null);\n            String tag = \"metrics:\" + registry + \":\" + CORE_IDX.metricsAttribute;\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.numDocs\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.maxDoc\";\n            metricTags.put(tag, info);\n            tag = \"metrics:\" + registry + \":SEARCHER.searcher.indexCommitSize\";\n            metricTags.put(tag, info);\n          });\n        });\n        if (metricTags.isEmpty()) {\n          continue;\n        }\n        Map<String, Object> sizes = cloudManager.getNodeStateProvider().getNodeValues(node, metricTags.keySet());\n        sizes.forEach((tag, size) -> {\n          final Replica info = metricTags.get(tag);\n          if (info == null) {\n            log.warn(\"Missing replica info for response tag {}\", tag);\n          } else {\n            // verify that it's a Number\n            if (!(size instanceof Number)) {\n              log.warn(\"invalid size value for tag {} - not a number: '{}' is {}\", tag, size, size.getClass().getName());\n              return;\n            }\n\n            Replica currentInfo = currentSizes.computeIfAbsent(info.getCoreName(), k -> (Replica) info.clone());\n            if (tag.contains(\"INDEX\")) {\n              currentInfo.getProperties().put(TOTAL_BYTES_SIZE_KEY, ((Number) size).longValue());\n            } else if (tag.endsWith(\"SEARCHER.searcher.numDocs\")) {\n              currentInfo.getProperties().put(DOCS_SIZE_KEY, ((Number) size).longValue());\n            } else if (tag.endsWith(\"SEARCHER.searcher.maxDoc\")) {\n              currentInfo.getProperties().put(MAX_DOC_KEY, ((Number) size).longValue());\n            } else if (tag.endsWith(\"SEARCHER.searcher.indexCommitSize\")) {\n              currentInfo.getProperties().put(COMMIT_SIZE_KEY, ((Number) size).longValue());\n            }\n          }\n        });\n      }\n    } catch (IOException e) {\n      log.warn(\"Error running trigger {}\", getName(), e);\n      return;\n    }\n\n    long now = cloudManager.getTimeSource().getTimeNs();\n\n    // now check thresholds\n\n    // collection / list(info)\n    Map<String, List<Replica>> aboveSize = new HashMap<>();\n\n    Set<String> splittable = new HashSet<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      // calculate estimated bytes\n      long maxDoc = (Long)info.get(MAX_DOC_KEY);\n      long numDocs = (Long)info.get(DOCS_SIZE_KEY);\n      long commitSize = (Long)info.get(COMMIT_SIZE_KEY, 0L);\n      if (commitSize <= 0) {\n        commitSize = (Long)info.get(TOTAL_BYTES_SIZE_KEY);\n      }\n      // calculate estimated size as a side-effect\n      commitSize = estimatedSize(maxDoc, numDocs, commitSize);\n      info.getProperties().put(BYTES_SIZE_KEY, commitSize);\n\n      if ((Long)info.get(BYTES_SIZE_KEY) > aboveBytes ||\n          (Long)info.get(DOCS_SIZE_KEY) > aboveDocs) {\n        if (waitForElapsed(coreName, now, lastAboveEventMap)) {\n          List<Replica> infos = aboveSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.get(BYTES_SIZE_KEY) > aboveBytes) {\n              info.getProperties().put(VIOLATION_KEY, ABOVE_BYTES_PROP);\n            } else {\n              info.getProperties().put(VIOLATION_KEY, ABOVE_DOCS_PROP);\n            }\n            infos.add(info);\n            splittable.add(info.getName());\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastAboveEventMap.remove(coreName);\n      }\n    });\n\n    // collection / list(info)\n    Map<String, List<Replica>> belowSize = new HashMap<>();\n\n    currentSizes.forEach((coreName, info) -> {\n      if (((Long)info.get(BYTES_SIZE_KEY) < belowBytes ||\n          (Long)info.get(DOCS_SIZE_KEY) < belowDocs) &&\n          // make sure we don't produce conflicting ops\n          !splittable.contains(info.getName())) {\n        if (waitForElapsed(coreName, now, lastBelowEventMap)) {\n          List<Replica> infos = belowSize.computeIfAbsent(info.getCollection(), c -> new ArrayList<>());\n          if (!infos.contains(info)) {\n            if ((Long)info.get(BYTES_SIZE_KEY) < belowBytes) {\n              info.getProperties().put(VIOLATION_KEY, BELOW_BYTES_PROP);\n            } else {\n              info.getProperties().put(VIOLATION_KEY, BELOW_DOCS_PROP);\n            }\n            infos.add(info);\n          }\n        }\n      } else {\n        // no violation - clear waitForElapsed\n        lastBelowEventMap.remove(coreName);\n      }\n    });\n\n    if (aboveSize.isEmpty() && belowSize.isEmpty()) {\n      log.trace(\"NO VIOLATIONS: Now={}\", now);\n      log.trace(\"lastAbove={}\", lastAboveEventMap);\n      log.trace(\"lastBelow={}\", lastBelowEventMap);\n      return;\n    }\n\n    // find the earliest time when a condition was exceeded\n    final AtomicLong eventTime = new AtomicLong(now);\n\n    // calculate ops\n    final List<TriggerEvent.Op> ops = new ArrayList<>();\n    aboveSize.forEach((coll, replicas) -> {\n      // sort by decreasing size to first split the largest ones\n      // XXX see the comment below about using DOCS_SIZE_PROP in lieu of BYTES_SIZE_PROP\n      replicas.sort((r1, r2) -> {\n        long delta = (Long) r1.get(DOCS_SIZE_KEY) - (Long) r2.get(DOCS_SIZE_KEY);\n        if (delta > 0) {\n          return -1;\n        } else if (delta < 0) {\n          return 1;\n        } else {\n          return 0;\n        }\n      });\n      replicas.forEach(r -> {\n        if (ops.size() >= maxOps) {\n          return;\n        }\n        TriggerEvent.Op op = new TriggerEvent.Op(aboveOp);\n        op.addHint(Suggester.Hint.COLL_SHARD, new Pair<>(coll, r.getShard()));\n        Map<String, Object> params = new HashMap<>();\n        params.put(SPLIT_METHOD_PROP, splitMethod.toLower());\n        if (splitFuzz > 0) {\n          params.put(SPLIT_FUZZ_PROP, splitFuzz);\n        }\n        params.put(SPLIT_BY_PREFIX, splitByPrefix);\n        op.addHint(Suggester.Hint.PARAMS, params);\n        ops.add(op);\n        Long time = lastAboveEventMap.get(r.getCoreName());\n        if (time != null && eventTime.get() > time) {\n          eventTime.set(time);\n        }\n      });\n    });\n    belowSize.forEach((coll, replicas) -> {\n      if (replicas.size() < 2) {\n        return;\n      }\n      if (ops.size() >= maxOps) {\n        return;\n      }\n      // sort by increasing size\n      replicas.sort((r1, r2) -> {\n        // XXX this is not quite correct - if BYTES_SIZE_PROP decided that replica got here\n        // then we should be sorting by BYTES_SIZE_PROP. However, since DOCS and BYTES are\n        // loosely correlated it's simpler to sort just by docs (which better reflects the \"too small\"\n        // condition than index size, due to possibly existing deleted docs that still occupy space)\n        long delta = (Long) r1.get(DOCS_SIZE_KEY) - (Long) r2.get(DOCS_SIZE_KEY);\n        if (delta > 0) {\n          return 1;\n        } else if (delta < 0) {\n          return -1;\n        } else {\n          return 0;\n        }\n      });\n\n      // TODO: MERGESHARDS is not implemented yet. For now take the top two smallest shards\n      // TODO: but in the future we probably need to get ones with adjacent ranges.\n\n      // TODO: generate as many MERGESHARDS as needed to consume all belowSize shards\n      TriggerEvent.Op op = new TriggerEvent.Op(belowOp);\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(0).getShard()));\n      op.addHint(Suggester.Hint.COLL_SHARD, new Pair(coll, replicas.get(1).getShard()));\n      ops.add(op);\n      Long time = lastBelowEventMap.get(replicas.get(0).getCoreName());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n      time = lastBelowEventMap.get(replicas.get(1).getCoreName());\n      if (time != null && eventTime.get() > time) {\n        eventTime.set(time);\n      }\n    });\n\n    if (ops.isEmpty()) {\n      return;\n    }\n    if (processor.process(new IndexSizeEvent(getName(), eventTime.get(), ops, aboveSize, belowSize))) {\n      // update last event times\n      aboveSize.forEach((coll, replicas) -> {\n        replicas.forEach(r -> lastAboveEventMap.put(r.getCoreName(), now));\n      });\n      belowSize.forEach((coll, replicas) -> {\n        if (replicas.size() < 2) {\n          return;\n        }\n        lastBelowEventMap.put(replicas.get(0).getCoreName(), now);\n        lastBelowEventMap.put(replicas.get(1).getCoreName(), now);\n      });\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"383671a9fe2f5147abf22eb1ce56e3ca3d6eb133":["9414d03a96422f6a92ced130e94f73f3fe1fc9a3"],"7cac1f2920f8057198f04505797cbabf74dd9a97":["383671a9fe2f5147abf22eb1ce56e3ca3d6eb133"],"3f504512a03d978990cbff30db0522b354e846db":["7e8ce2f9d2ddfcf5cfa7e73b8b2af287a2a276fd"],"9414d03a96422f6a92ced130e94f73f3fe1fc9a3":["042b92cf48996255bedb0c3c4bf772d7e06e4dea"],"43345f1452f9510f8aaadae6156fe0c834e7d957":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","ce71a9836bf1eba34a0ab31884e9eb8ad3f1cef1"],"a97a72dc16d01fda8ca5c9e0264b3604e30ab539":["2d80c1ad9241ae005a167d7ee8ac473601b0e57c"],"042b92cf48996255bedb0c3c4bf772d7e06e4dea":["b8cde5442ed20c0c3ffd14ea2e2a64609367c193"],"b8cde5442ed20c0c3ffd14ea2e2a64609367c193":["0532cd2ef583ac8047d77493ec05c81836f483c2"],"0532cd2ef583ac8047d77493ec05c81836f483c2":["43345f1452f9510f8aaadae6156fe0c834e7d957"],"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":["0532cd2ef583ac8047d77493ec05c81836f483c2","b8cde5442ed20c0c3ffd14ea2e2a64609367c193"],"7e8ce2f9d2ddfcf5cfa7e73b8b2af287a2a276fd":["daa0f21a44e235a2299ea1fa913898b182dd7cce"],"ce71a9836bf1eba34a0ab31884e9eb8ad3f1cef1":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"daa0f21a44e235a2299ea1fa913898b182dd7cce":["e35f2dde06b35aa9904949a3a93fabd090371077"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"e35f2dde06b35aa9904949a3a93fabd090371077":["256b1da668dae74c726d5d71bce6153202d6b001"],"2d80c1ad9241ae005a167d7ee8ac473601b0e57c":["9ef8d00dbfbeb534eba8a219a5df9d99b2de6ab5"],"9ef8d00dbfbeb534eba8a219a5df9d99b2de6ab5":["7cac1f2920f8057198f04505797cbabf74dd9a97"],"256b1da668dae74c726d5d71bce6153202d6b001":["a97a72dc16d01fda8ca5c9e0264b3604e30ab539"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3f504512a03d978990cbff30db0522b354e846db"],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":["0532cd2ef583ac8047d77493ec05c81836f483c2","b8cde5442ed20c0c3ffd14ea2e2a64609367c193"]},"commit2Childs":{"383671a9fe2f5147abf22eb1ce56e3ca3d6eb133":["7cac1f2920f8057198f04505797cbabf74dd9a97"],"7cac1f2920f8057198f04505797cbabf74dd9a97":["9ef8d00dbfbeb534eba8a219a5df9d99b2de6ab5"],"3f504512a03d978990cbff30db0522b354e846db":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"9414d03a96422f6a92ced130e94f73f3fe1fc9a3":["383671a9fe2f5147abf22eb1ce56e3ca3d6eb133"],"43345f1452f9510f8aaadae6156fe0c834e7d957":["0532cd2ef583ac8047d77493ec05c81836f483c2"],"a97a72dc16d01fda8ca5c9e0264b3604e30ab539":["256b1da668dae74c726d5d71bce6153202d6b001"],"042b92cf48996255bedb0c3c4bf772d7e06e4dea":["9414d03a96422f6a92ced130e94f73f3fe1fc9a3"],"b8cde5442ed20c0c3ffd14ea2e2a64609367c193":["042b92cf48996255bedb0c3c4bf772d7e06e4dea","0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"0532cd2ef583ac8047d77493ec05c81836f483c2":["b8cde5442ed20c0c3ffd14ea2e2a64609367c193","0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":[],"7e8ce2f9d2ddfcf5cfa7e73b8b2af287a2a276fd":["3f504512a03d978990cbff30db0522b354e846db"],"ce71a9836bf1eba34a0ab31884e9eb8ad3f1cef1":["43345f1452f9510f8aaadae6156fe0c834e7d957"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["43345f1452f9510f8aaadae6156fe0c834e7d957","ce71a9836bf1eba34a0ab31884e9eb8ad3f1cef1"],"daa0f21a44e235a2299ea1fa913898b182dd7cce":["7e8ce2f9d2ddfcf5cfa7e73b8b2af287a2a276fd"],"2d80c1ad9241ae005a167d7ee8ac473601b0e57c":["a97a72dc16d01fda8ca5c9e0264b3604e30ab539"],"e35f2dde06b35aa9904949a3a93fabd090371077":["daa0f21a44e235a2299ea1fa913898b182dd7cce"],"9ef8d00dbfbeb534eba8a219a5df9d99b2de6ab5":["2d80c1ad9241ae005a167d7ee8ac473601b0e57c"],"256b1da668dae74c726d5d71bce6153202d6b001":["e35f2dde06b35aa9904949a3a93fabd090371077"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":[]},"heads":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","cd5edd1f2b162a5cfa08efd17851a07373a96817","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}