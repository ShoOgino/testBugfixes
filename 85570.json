{"path":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean,boolean).mjava","commits":[{"id":"3def6e0e7b7566dd7f04a3514e77ee97a40fc78a","date":1429888091,"type":0,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean,boolean).mjava","pathOld":"/dev/null","sourceNew":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting, boolean haTesting) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));  \n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    if (!HA_TESTING_ENABLED) haTesting = false;\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n    \n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    \n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n    \n    final MiniDFSCluster dfsCluster;\n    \n\n    if (!haTesting) {\n      dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n      System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    } else {\n      \n      dfsCluster = new MiniDFSCluster.Builder(conf)\n          .nnTopology(MiniDFSNNTopology.simpleHATopology()).numDataNodes(dataNodes)\n          .build();\n      \n      Configuration haConfig = getClientConfiguration(dfsCluster);\n\n      HdfsUtil.TEST_CONF = haConfig;\n      System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    }\n    \n    dfsCluster.waitActive();\n    \n    if (haTesting) dfsCluster.transitionToActive(0);\n    \n    int rndMode = LuceneTestCase.random().nextInt(3);\n    if (safeModeTesting && rndMode == 1) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n      \n      int rnd = LuceneTestCase.random().nextInt(10000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n        \n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n\n    } else if (haTesting && rndMode == 2) {\n      int rnd = LuceneTestCase.random().nextInt(30000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n        \n        @Override\n        public void run() {\n          // TODO: randomly transition to standby\n//          try {\n//            dfsCluster.transitionToStandby(0);\n//            dfsCluster.transitionToActive(1);\n//          } catch (IOException e) {\n//            throw new RuntimeException();\n//          }\n\n        }\n      }, rnd);\n    }  else {\n    \n      // TODO: we could do much better at testing this\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      badTlogOutStreamFs = FileSystem.get(hdfsDirPath.toUri(), conf);\n      badTlogOutStream = badTlogOutStreamFs.create(hdfsDirPath);\n    }\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["add53de9835b2cd1a7a80b4e0036afee171c9fdf"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e50357e583524185222c1c691f5c333b34f7cbb2","date":1452268776,"type":3,"author":"Christine Poerschke","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean,boolean).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean,boolean).mjava","sourceNew":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting, boolean haTesting) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));  \n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    if (!HA_TESTING_ENABLED) haTesting = false;\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n    \n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", DirectoryFactory.LOCK_TYPE_HDFS);\n    \n    \n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n    \n    final MiniDFSCluster dfsCluster;\n    \n\n    if (!haTesting) {\n      dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n      System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    } else {\n      \n      dfsCluster = new MiniDFSCluster.Builder(conf)\n          .nnTopology(MiniDFSNNTopology.simpleHATopology()).numDataNodes(dataNodes)\n          .build();\n      \n      Configuration haConfig = getClientConfiguration(dfsCluster);\n\n      HdfsUtil.TEST_CONF = haConfig;\n      System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    }\n    \n    dfsCluster.waitActive();\n    \n    if (haTesting) dfsCluster.transitionToActive(0);\n    \n    int rndMode = LuceneTestCase.random().nextInt(3);\n    if (safeModeTesting && rndMode == 1) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n      \n      int rnd = LuceneTestCase.random().nextInt(10000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n        \n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n\n    } else if (haTesting && rndMode == 2) {\n      int rnd = LuceneTestCase.random().nextInt(30000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n        \n        @Override\n        public void run() {\n          // TODO: randomly transition to standby\n//          try {\n//            dfsCluster.transitionToStandby(0);\n//            dfsCluster.transitionToActive(1);\n//          } catch (IOException e) {\n//            throw new RuntimeException();\n//          }\n\n        }\n      }, rnd);\n    }  else {\n    \n      // TODO: we could do much better at testing this\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      badTlogOutStreamFs = FileSystem.get(hdfsDirPath.toUri(), conf);\n      badTlogOutStream = badTlogOutStreamFs.create(hdfsDirPath);\n    }\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","sourceOld":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting, boolean haTesting) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));  \n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    if (!HA_TESTING_ENABLED) haTesting = false;\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n    \n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    \n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n    \n    final MiniDFSCluster dfsCluster;\n    \n\n    if (!haTesting) {\n      dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n      System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    } else {\n      \n      dfsCluster = new MiniDFSCluster.Builder(conf)\n          .nnTopology(MiniDFSNNTopology.simpleHATopology()).numDataNodes(dataNodes)\n          .build();\n      \n      Configuration haConfig = getClientConfiguration(dfsCluster);\n\n      HdfsUtil.TEST_CONF = haConfig;\n      System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    }\n    \n    dfsCluster.waitActive();\n    \n    if (haTesting) dfsCluster.transitionToActive(0);\n    \n    int rndMode = LuceneTestCase.random().nextInt(3);\n    if (safeModeTesting && rndMode == 1) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n      \n      int rnd = LuceneTestCase.random().nextInt(10000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n        \n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n\n    } else if (haTesting && rndMode == 2) {\n      int rnd = LuceneTestCase.random().nextInt(30000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n        \n        @Override\n        public void run() {\n          // TODO: randomly transition to standby\n//          try {\n//            dfsCluster.transitionToStandby(0);\n//            dfsCluster.transitionToActive(1);\n//          } catch (IOException e) {\n//            throw new RuntimeException();\n//          }\n\n        }\n      }, rnd);\n    }  else {\n    \n      // TODO: we could do much better at testing this\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      badTlogOutStreamFs = FileSystem.get(hdfsDirPath.toUri(), conf);\n      badTlogOutStream = badTlogOutStreamFs.create(hdfsDirPath);\n    }\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"54d917b2424232699b135ef134cbd0f8add290c9","date":1485967228,"type":3,"author":"markrmiller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean,boolean).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean,boolean).mjava","sourceNew":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting, boolean haTesting) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));  \n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    if (!HA_TESTING_ENABLED) haTesting = false;\n    \n    int dataNodes = Integer.getInteger(\"tests.hdfs.numdatanodes\", 2);\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n    \n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", DirectoryFactory.LOCK_TYPE_HDFS);\n    \n    \n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n    \n    final MiniDFSCluster dfsCluster;\n    \n\n    if (!haTesting) {\n      dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n      System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    } else {\n      \n      dfsCluster = new MiniDFSCluster.Builder(conf)\n          .nnTopology(MiniDFSNNTopology.simpleHATopology()).numDataNodes(dataNodes)\n          .build();\n      \n      Configuration haConfig = getClientConfiguration(dfsCluster);\n\n      HdfsUtil.TEST_CONF = haConfig;\n      System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    }\n    \n    dfsCluster.waitActive();\n    \n    if (haTesting) dfsCluster.transitionToActive(0);\n    \n    int rndMode = LuceneTestCase.random().nextInt(3);\n    if (safeModeTesting && rndMode == 1) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n      \n      int rnd = LuceneTestCase.random().nextInt(10000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n        \n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n\n    } else if (haTesting && rndMode == 2) {\n      int rnd = LuceneTestCase.random().nextInt(30000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n        \n        @Override\n        public void run() {\n          // TODO: randomly transition to standby\n//          try {\n//            dfsCluster.transitionToStandby(0);\n//            dfsCluster.transitionToActive(1);\n//          } catch (IOException e) {\n//            throw new RuntimeException();\n//          }\n\n        }\n      }, rnd);\n    }  else {\n    \n      // TODO: we could do much better at testing this\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      badTlogOutStreamFs = FileSystem.get(hdfsDirPath.toUri(), conf);\n      badTlogOutStream = badTlogOutStreamFs.create(hdfsDirPath);\n    }\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","sourceOld":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting, boolean haTesting) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));  \n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    if (!HA_TESTING_ENABLED) haTesting = false;\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n    \n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", DirectoryFactory.LOCK_TYPE_HDFS);\n    \n    \n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n    \n    final MiniDFSCluster dfsCluster;\n    \n\n    if (!haTesting) {\n      dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n      System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    } else {\n      \n      dfsCluster = new MiniDFSCluster.Builder(conf)\n          .nnTopology(MiniDFSNNTopology.simpleHATopology()).numDataNodes(dataNodes)\n          .build();\n      \n      Configuration haConfig = getClientConfiguration(dfsCluster);\n\n      HdfsUtil.TEST_CONF = haConfig;\n      System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    }\n    \n    dfsCluster.waitActive();\n    \n    if (haTesting) dfsCluster.transitionToActive(0);\n    \n    int rndMode = LuceneTestCase.random().nextInt(3);\n    if (safeModeTesting && rndMode == 1) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n      \n      int rnd = LuceneTestCase.random().nextInt(10000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n        \n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n\n    } else if (haTesting && rndMode == 2) {\n      int rnd = LuceneTestCase.random().nextInt(30000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n        \n        @Override\n        public void run() {\n          // TODO: randomly transition to standby\n//          try {\n//            dfsCluster.transitionToStandby(0);\n//            dfsCluster.transitionToActive(1);\n//          } catch (IOException e) {\n//            throw new RuntimeException();\n//          }\n\n        }\n      }, rnd);\n    }  else {\n    \n      // TODO: we could do much better at testing this\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      badTlogOutStreamFs = FileSystem.get(hdfsDirPath.toUri(), conf);\n      badTlogOutStream = badTlogOutStreamFs.create(hdfsDirPath);\n    }\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","bugFix":null,"bugIntro":["add53de9835b2cd1a7a80b4e0036afee171c9fdf"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7c3523a0ab04c3002eee3896c75ea5f10f388bcc","date":1485968422,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean,boolean).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean,boolean).mjava","sourceNew":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting, boolean haTesting) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));  \n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    if (!HA_TESTING_ENABLED) haTesting = false;\n    \n    int dataNodes = Integer.getInteger(\"tests.hdfs.numdatanodes\", 2);\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n    \n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", DirectoryFactory.LOCK_TYPE_HDFS);\n    \n    \n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n    \n    final MiniDFSCluster dfsCluster;\n    \n\n    if (!haTesting) {\n      dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n      System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    } else {\n      \n      dfsCluster = new MiniDFSCluster.Builder(conf)\n          .nnTopology(MiniDFSNNTopology.simpleHATopology()).numDataNodes(dataNodes)\n          .build();\n      \n      Configuration haConfig = getClientConfiguration(dfsCluster);\n\n      HdfsUtil.TEST_CONF = haConfig;\n      System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    }\n    \n    dfsCluster.waitActive();\n    \n    if (haTesting) dfsCluster.transitionToActive(0);\n    \n    int rndMode = LuceneTestCase.random().nextInt(3);\n    if (safeModeTesting && rndMode == 1) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n      \n      int rnd = LuceneTestCase.random().nextInt(10000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n        \n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n\n    } else if (haTesting && rndMode == 2) {\n      int rnd = LuceneTestCase.random().nextInt(30000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n        \n        @Override\n        public void run() {\n          // TODO: randomly transition to standby\n//          try {\n//            dfsCluster.transitionToStandby(0);\n//            dfsCluster.transitionToActive(1);\n//          } catch (IOException e) {\n//            throw new RuntimeException();\n//          }\n\n        }\n      }, rnd);\n    }  else {\n    \n      // TODO: we could do much better at testing this\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      badTlogOutStreamFs = FileSystem.get(hdfsDirPath.toUri(), conf);\n      badTlogOutStream = badTlogOutStreamFs.create(hdfsDirPath);\n    }\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","sourceOld":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting, boolean haTesting) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));  \n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    if (!HA_TESTING_ENABLED) haTesting = false;\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n    \n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", DirectoryFactory.LOCK_TYPE_HDFS);\n    \n    \n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n    \n    final MiniDFSCluster dfsCluster;\n    \n\n    if (!haTesting) {\n      dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n      System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    } else {\n      \n      dfsCluster = new MiniDFSCluster.Builder(conf)\n          .nnTopology(MiniDFSNNTopology.simpleHATopology()).numDataNodes(dataNodes)\n          .build();\n      \n      Configuration haConfig = getClientConfiguration(dfsCluster);\n\n      HdfsUtil.TEST_CONF = haConfig;\n      System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    }\n    \n    dfsCluster.waitActive();\n    \n    if (haTesting) dfsCluster.transitionToActive(0);\n    \n    int rndMode = LuceneTestCase.random().nextInt(3);\n    if (safeModeTesting && rndMode == 1) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n      \n      int rnd = LuceneTestCase.random().nextInt(10000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n        \n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n\n    } else if (haTesting && rndMode == 2) {\n      int rnd = LuceneTestCase.random().nextInt(30000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n        \n        @Override\n        public void run() {\n          // TODO: randomly transition to standby\n//          try {\n//            dfsCluster.transitionToStandby(0);\n//            dfsCluster.transitionToActive(1);\n//          } catch (IOException e) {\n//            throw new RuntimeException();\n//          }\n\n        }\n      }, rnd);\n    }  else {\n    \n      // TODO: we could do much better at testing this\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      badTlogOutStreamFs = FileSystem.get(hdfsDirPath.toUri(), conf);\n      badTlogOutStream = badTlogOutStreamFs.create(hdfsDirPath);\n    }\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ac0d6b5d90b3c22d588d4e59ec2e9fa6562ebd7f","date":1486579387,"type":3,"author":"markrmiller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean,boolean).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean,boolean).mjava","sourceNew":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting, boolean haTesting) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));  \n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    if (!HA_TESTING_ENABLED) haTesting = false;\n    \n    \n    // keep netty from using secure random on startup: SOLR-10098\n    ThreadLocalRandom.setInitialSeedUniquifier(1L);\n    \n    int dataNodes = Integer.getInteger(\"tests.hdfs.numdatanodes\", 2);\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n    \n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", DirectoryFactory.LOCK_TYPE_HDFS);\n    \n    \n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n    \n    final MiniDFSCluster dfsCluster;\n    \n\n    if (!haTesting) {\n      dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n      System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    } else {\n      \n      dfsCluster = new MiniDFSCluster.Builder(conf)\n          .nnTopology(MiniDFSNNTopology.simpleHATopology()).numDataNodes(dataNodes)\n          .build();\n      \n      Configuration haConfig = getClientConfiguration(dfsCluster);\n\n      HdfsUtil.TEST_CONF = haConfig;\n      System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    }\n    \n    dfsCluster.waitActive();\n    \n    if (haTesting) dfsCluster.transitionToActive(0);\n    \n    int rndMode = LuceneTestCase.random().nextInt(3);\n    if (safeModeTesting && rndMode == 1) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n      \n      int rnd = LuceneTestCase.random().nextInt(10000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n        \n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n\n    } else if (haTesting && rndMode == 2) {\n      int rnd = LuceneTestCase.random().nextInt(30000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n        \n        @Override\n        public void run() {\n          // TODO: randomly transition to standby\n//          try {\n//            dfsCluster.transitionToStandby(0);\n//            dfsCluster.transitionToActive(1);\n//          } catch (IOException e) {\n//            throw new RuntimeException();\n//          }\n\n        }\n      }, rnd);\n    }  else {\n    \n      // TODO: we could do much better at testing this\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      badTlogOutStreamFs = FileSystem.get(hdfsDirPath.toUri(), conf);\n      badTlogOutStream = badTlogOutStreamFs.create(hdfsDirPath);\n    }\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","sourceOld":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting, boolean haTesting) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));  \n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    if (!HA_TESTING_ENABLED) haTesting = false;\n    \n    int dataNodes = Integer.getInteger(\"tests.hdfs.numdatanodes\", 2);\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n    \n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", DirectoryFactory.LOCK_TYPE_HDFS);\n    \n    \n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n    \n    final MiniDFSCluster dfsCluster;\n    \n\n    if (!haTesting) {\n      dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n      System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    } else {\n      \n      dfsCluster = new MiniDFSCluster.Builder(conf)\n          .nnTopology(MiniDFSNNTopology.simpleHATopology()).numDataNodes(dataNodes)\n          .build();\n      \n      Configuration haConfig = getClientConfiguration(dfsCluster);\n\n      HdfsUtil.TEST_CONF = haConfig;\n      System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    }\n    \n    dfsCluster.waitActive();\n    \n    if (haTesting) dfsCluster.transitionToActive(0);\n    \n    int rndMode = LuceneTestCase.random().nextInt(3);\n    if (safeModeTesting && rndMode == 1) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n      \n      int rnd = LuceneTestCase.random().nextInt(10000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n        \n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n\n    } else if (haTesting && rndMode == 2) {\n      int rnd = LuceneTestCase.random().nextInt(30000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n        \n        @Override\n        public void run() {\n          // TODO: randomly transition to standby\n//          try {\n//            dfsCluster.transitionToStandby(0);\n//            dfsCluster.transitionToActive(1);\n//          } catch (IOException e) {\n//            throw new RuntimeException();\n//          }\n\n        }\n      }, rnd);\n    }  else {\n    \n      // TODO: we could do much better at testing this\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      badTlogOutStreamFs = FileSystem.get(hdfsDirPath.toUri(), conf);\n      badTlogOutStream = badTlogOutStreamFs.create(hdfsDirPath);\n    }\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"99b97f030e743d8dd4d7685202f71718d7e5af22","date":1548957620,"type":3,"author":"markrmiller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean,boolean).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean,boolean).mjava","sourceNew":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting, boolean haTesting) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n\n    checkFastDateFormat();\n\n    if (!HA_TESTING_ENABLED) haTesting = false;\n\n    int dataNodes = Integer.getInteger(\"tests.hdfs.numdatanodes\", 2);\n\n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n\n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", DirectoryFactory.LOCK_TYPE_HDFS);\n\n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n\n    final MiniDFSCluster dfsCluster;\n\n    if (!haTesting) {\n      dfsCluster = new MiniDFSCluster.Builder(conf).numDataNodes(dataNodes).format(true).build();\n\n      System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    } else {\n      dfsCluster = new MiniDFSCluster.Builder(conf)\n          .nnTopology(MiniDFSNNTopology.simpleHATopology()).numDataNodes(dataNodes)\n          .build();\n\n      Configuration haConfig = getClientConfiguration(dfsCluster);\n\n      HdfsUtil.TEST_CONF = haConfig;\n      System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    }\n\n    dfsCluster.waitActive();\n\n    if (haTesting) dfsCluster.transitionToActive(0);\n\n    int rndMode = LuceneTestCase.random().nextInt(3);\n    if (safeModeTesting && rndMode == 1) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n\n      int rnd = LuceneTestCase.random().nextInt(10000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n\n    } else if (haTesting && rndMode == 2) {\n      int rnd = LuceneTestCase.random().nextInt(30000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          // TODO: randomly transition to standby\n//          try {\n//            dfsCluster.transitionToStandby(0);\n//            dfsCluster.transitionToActive(1);\n//          } catch (IOException e) {\n//            throw new RuntimeException();\n//          }\n\n        }\n      }, rnd);\n    }  else {\n      // TODO: we could do much better at testing this\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      badTlogOutStreamFs = FileSystem.get(hdfsDirPath.toUri(), conf);\n      badTlogOutStream = badTlogOutStreamFs.create(hdfsDirPath);\n    }\n\n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n\n    return dfsCluster;\n  }\n\n","sourceOld":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting, boolean haTesting) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));  \n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    if (!HA_TESTING_ENABLED) haTesting = false;\n    \n    \n    // keep netty from using secure random on startup: SOLR-10098\n    ThreadLocalRandom.setInitialSeedUniquifier(1L);\n    \n    int dataNodes = Integer.getInteger(\"tests.hdfs.numdatanodes\", 2);\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n    \n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", DirectoryFactory.LOCK_TYPE_HDFS);\n    \n    \n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n    \n    final MiniDFSCluster dfsCluster;\n    \n\n    if (!haTesting) {\n      dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n      System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    } else {\n      \n      dfsCluster = new MiniDFSCluster.Builder(conf)\n          .nnTopology(MiniDFSNNTopology.simpleHATopology()).numDataNodes(dataNodes)\n          .build();\n      \n      Configuration haConfig = getClientConfiguration(dfsCluster);\n\n      HdfsUtil.TEST_CONF = haConfig;\n      System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    }\n    \n    dfsCluster.waitActive();\n    \n    if (haTesting) dfsCluster.transitionToActive(0);\n    \n    int rndMode = LuceneTestCase.random().nextInt(3);\n    if (safeModeTesting && rndMode == 1) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n      \n      int rnd = LuceneTestCase.random().nextInt(10000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n        \n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n\n    } else if (haTesting && rndMode == 2) {\n      int rnd = LuceneTestCase.random().nextInt(30000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n        \n        @Override\n        public void run() {\n          // TODO: randomly transition to standby\n//          try {\n//            dfsCluster.transitionToStandby(0);\n//            dfsCluster.transitionToActive(1);\n//          } catch (IOException e) {\n//            throw new RuntimeException();\n//          }\n\n        }\n      }, rnd);\n    }  else {\n    \n      // TODO: we could do much better at testing this\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      badTlogOutStreamFs = FileSystem.get(hdfsDirPath.toUri(), conf);\n      badTlogOutStream = badTlogOutStreamFs.create(hdfsDirPath);\n    }\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"34b74c124d68d8e306d6ef09624f3cd053b8a2fd","date":1548981962,"type":3,"author":"Kevin Risden","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean,boolean).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean,boolean).mjava","sourceNew":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting, boolean haTesting) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));  \n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    if (!HA_TESTING_ENABLED) haTesting = false;\n    \n    \n    // keep netty from using secure random on startup: SOLR-10098\n    ThreadLocalRandom.setInitialSeedUniquifier(1L);\n    \n    int dataNodes = Integer.getInteger(\"tests.hdfs.numdatanodes\", 2);\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n    \n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", DirectoryFactory.LOCK_TYPE_HDFS);\n    \n    \n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n    \n    final MiniDFSCluster dfsCluster;\n    \n\n    if (!haTesting) {\n      dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n      System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    } else {\n      \n      dfsCluster = new MiniDFSCluster.Builder(conf)\n          .nnTopology(MiniDFSNNTopology.simpleHATopology()).numDataNodes(dataNodes)\n          .build();\n      \n      Configuration haConfig = getClientConfiguration(dfsCluster);\n\n      HdfsUtil.TEST_CONF = haConfig;\n      System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    }\n    \n    dfsCluster.waitActive();\n    \n    if (haTesting) dfsCluster.transitionToActive(0);\n    \n    int rndMode = LuceneTestCase.random().nextInt(3);\n    if (safeModeTesting && rndMode == 1) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n      \n      int rnd = LuceneTestCase.random().nextInt(10000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n        \n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n\n    } else if (haTesting && rndMode == 2) {\n      int rnd = LuceneTestCase.random().nextInt(30000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n        \n        @Override\n        public void run() {\n          // TODO: randomly transition to standby\n//          try {\n//            dfsCluster.transitionToStandby(0);\n//            dfsCluster.transitionToActive(1);\n//          } catch (IOException e) {\n//            throw new RuntimeException();\n//          }\n\n        }\n      }, rnd);\n    }  else {\n    \n      // TODO: we could do much better at testing this\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      badTlogOutStreamFs = FileSystem.get(hdfsDirPath.toUri(), conf);\n      badTlogOutStream = badTlogOutStreamFs.create(hdfsDirPath);\n    }\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","sourceOld":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting, boolean haTesting) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n\n    checkFastDateFormat();\n\n    if (!HA_TESTING_ENABLED) haTesting = false;\n\n    int dataNodes = Integer.getInteger(\"tests.hdfs.numdatanodes\", 2);\n\n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n\n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", DirectoryFactory.LOCK_TYPE_HDFS);\n\n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n\n    final MiniDFSCluster dfsCluster;\n\n    if (!haTesting) {\n      dfsCluster = new MiniDFSCluster.Builder(conf).numDataNodes(dataNodes).format(true).build();\n\n      System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    } else {\n      dfsCluster = new MiniDFSCluster.Builder(conf)\n          .nnTopology(MiniDFSNNTopology.simpleHATopology()).numDataNodes(dataNodes)\n          .build();\n\n      Configuration haConfig = getClientConfiguration(dfsCluster);\n\n      HdfsUtil.TEST_CONF = haConfig;\n      System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    }\n\n    dfsCluster.waitActive();\n\n    if (haTesting) dfsCluster.transitionToActive(0);\n\n    int rndMode = LuceneTestCase.random().nextInt(3);\n    if (safeModeTesting && rndMode == 1) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n\n      int rnd = LuceneTestCase.random().nextInt(10000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n\n    } else if (haTesting && rndMode == 2) {\n      int rnd = LuceneTestCase.random().nextInt(30000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          // TODO: randomly transition to standby\n//          try {\n//            dfsCluster.transitionToStandby(0);\n//            dfsCluster.transitionToActive(1);\n//          } catch (IOException e) {\n//            throw new RuntimeException();\n//          }\n\n        }\n      }, rnd);\n    }  else {\n      // TODO: we could do much better at testing this\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      badTlogOutStreamFs = FileSystem.get(hdfsDirPath.toUri(), conf);\n      badTlogOutStream = badTlogOutStreamFs.create(hdfsDirPath);\n    }\n\n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n\n    return dfsCluster;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"44ca189138a5b6e1989d12ab992fab60e235ddc7","date":1549051496,"type":3,"author":"markrmiller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean,boolean).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean,boolean).mjava","sourceNew":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting, boolean haTesting) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n\n    checkFastDateFormat();\n    checkGeneratedIdMatches();\n\n    if (!HA_TESTING_ENABLED) haTesting = false;\n\n    int dataNodes = Integer.getInteger(\"tests.hdfs.numdatanodes\", 2);\n\n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n\n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", DirectoryFactory.LOCK_TYPE_HDFS);\n\n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n\n    final MiniDFSCluster dfsCluster;\n\n    if (!haTesting) {\n      dfsCluster = new MiniDFSCluster.Builder(conf).numDataNodes(dataNodes).format(true).build();\n\n      System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    } else {\n      dfsCluster = new MiniDFSCluster.Builder(conf)\n          .nnTopology(MiniDFSNNTopology.simpleHATopology()).numDataNodes(dataNodes)\n          .build();\n\n      Configuration haConfig = getClientConfiguration(dfsCluster);\n\n      HdfsUtil.TEST_CONF = haConfig;\n      System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    }\n\n    dfsCluster.waitActive();\n\n    if (haTesting) dfsCluster.transitionToActive(0);\n\n    int rndMode = LuceneTestCase.random().nextInt(3);\n    if (safeModeTesting && rndMode == 1) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n\n      int rnd = LuceneTestCase.random().nextInt(10000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n\n    } else if (haTesting && rndMode == 2) {\n      int rnd = LuceneTestCase.random().nextInt(30000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          // TODO: randomly transition to standby\n//          try {\n//            dfsCluster.transitionToStandby(0);\n//            dfsCluster.transitionToActive(1);\n//          } catch (IOException e) {\n//            throw new RuntimeException();\n//          }\n\n        }\n      }, rnd);\n    }  else {\n      // TODO: we could do much better at testing this\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      badTlogOutStreamFs = FileSystem.get(hdfsDirPath.toUri(), conf);\n      badTlogOutStream = badTlogOutStreamFs.create(hdfsDirPath);\n    }\n\n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n\n    return dfsCluster;\n  }\n\n","sourceOld":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting, boolean haTesting) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));  \n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    if (!HA_TESTING_ENABLED) haTesting = false;\n    \n    \n    // keep netty from using secure random on startup: SOLR-10098\n    ThreadLocalRandom.setInitialSeedUniquifier(1L);\n    \n    int dataNodes = Integer.getInteger(\"tests.hdfs.numdatanodes\", 2);\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n    \n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", DirectoryFactory.LOCK_TYPE_HDFS);\n    \n    \n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n    \n    final MiniDFSCluster dfsCluster;\n    \n\n    if (!haTesting) {\n      dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n      System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    } else {\n      \n      dfsCluster = new MiniDFSCluster.Builder(conf)\n          .nnTopology(MiniDFSNNTopology.simpleHATopology()).numDataNodes(dataNodes)\n          .build();\n      \n      Configuration haConfig = getClientConfiguration(dfsCluster);\n\n      HdfsUtil.TEST_CONF = haConfig;\n      System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    }\n    \n    dfsCluster.waitActive();\n    \n    if (haTesting) dfsCluster.transitionToActive(0);\n    \n    int rndMode = LuceneTestCase.random().nextInt(3);\n    if (safeModeTesting && rndMode == 1) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n      \n      int rnd = LuceneTestCase.random().nextInt(10000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n        \n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n\n    } else if (haTesting && rndMode == 2) {\n      int rnd = LuceneTestCase.random().nextInt(30000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n        \n        @Override\n        public void run() {\n          // TODO: randomly transition to standby\n//          try {\n//            dfsCluster.transitionToStandby(0);\n//            dfsCluster.transitionToActive(1);\n//          } catch (IOException e) {\n//            throw new RuntimeException();\n//          }\n\n        }\n      }, rnd);\n    }  else {\n    \n      // TODO: we could do much better at testing this\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      badTlogOutStreamFs = FileSystem.get(hdfsDirPath.toUri(), conf);\n      badTlogOutStream = badTlogOutStreamFs.create(hdfsDirPath);\n    }\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","bugFix":null,"bugIntro":["add53de9835b2cd1a7a80b4e0036afee171c9fdf"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2c801a37c38aedbd2ddbd27f2aaeb30cd5c7af0f","date":1552317217,"type":3,"author":"Kevin Risden","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean,boolean).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean,boolean).mjava","sourceNew":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting, boolean haTesting) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n\n    checkFastDateFormat();\n    checkGeneratedIdMatches();\n\n    if (!HA_TESTING_ENABLED) haTesting = false;\n\n    int dataNodes = Integer.getInteger(\"tests.hdfs.numdatanodes\", 2);\n\n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n\n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", DirectoryFactory.LOCK_TYPE_HDFS);\n\n    System.setProperty(\"solr.hdfs.blockcache.global\",\n        System.getProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean())));\n\n    final MiniDFSCluster dfsCluster;\n\n    if (!haTesting) {\n      dfsCluster = new MiniDFSCluster.Builder(conf).numDataNodes(dataNodes).format(true).build();\n\n      System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    } else {\n      dfsCluster = new MiniDFSCluster.Builder(conf)\n          .nnTopology(MiniDFSNNTopology.simpleHATopology()).numDataNodes(dataNodes)\n          .build();\n\n      Configuration haConfig = getClientConfiguration(dfsCluster);\n\n      HdfsUtil.TEST_CONF = haConfig;\n      System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    }\n\n    dfsCluster.waitActive();\n\n    if (haTesting) dfsCluster.transitionToActive(0);\n\n    int rndMode = LuceneTestCase.random().nextInt(3);\n    if (safeModeTesting && rndMode == 1) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n\n      int rnd = LuceneTestCase.random().nextInt(10000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n\n    } else if (haTesting && rndMode == 2) {\n      int rnd = LuceneTestCase.random().nextInt(30000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          // TODO: randomly transition to standby\n//          try {\n//            dfsCluster.transitionToStandby(0);\n//            dfsCluster.transitionToActive(1);\n//          } catch (IOException e) {\n//            throw new RuntimeException();\n//          }\n\n        }\n      }, rnd);\n    }  else {\n      // TODO: we could do much better at testing this\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      badTlogOutStreamFs = FileSystem.get(hdfsDirPath.toUri(), conf);\n      badTlogOutStream = badTlogOutStreamFs.create(hdfsDirPath);\n    }\n\n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n\n    return dfsCluster;\n  }\n\n","sourceOld":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting, boolean haTesting) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n\n    checkFastDateFormat();\n    checkGeneratedIdMatches();\n\n    if (!HA_TESTING_ENABLED) haTesting = false;\n\n    int dataNodes = Integer.getInteger(\"tests.hdfs.numdatanodes\", 2);\n\n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n\n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", DirectoryFactory.LOCK_TYPE_HDFS);\n\n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n\n    final MiniDFSCluster dfsCluster;\n\n    if (!haTesting) {\n      dfsCluster = new MiniDFSCluster.Builder(conf).numDataNodes(dataNodes).format(true).build();\n\n      System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    } else {\n      dfsCluster = new MiniDFSCluster.Builder(conf)\n          .nnTopology(MiniDFSNNTopology.simpleHATopology()).numDataNodes(dataNodes)\n          .build();\n\n      Configuration haConfig = getClientConfiguration(dfsCluster);\n\n      HdfsUtil.TEST_CONF = haConfig;\n      System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    }\n\n    dfsCluster.waitActive();\n\n    if (haTesting) dfsCluster.transitionToActive(0);\n\n    int rndMode = LuceneTestCase.random().nextInt(3);\n    if (safeModeTesting && rndMode == 1) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n\n      int rnd = LuceneTestCase.random().nextInt(10000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n\n    } else if (haTesting && rndMode == 2) {\n      int rnd = LuceneTestCase.random().nextInt(30000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          // TODO: randomly transition to standby\n//          try {\n//            dfsCluster.transitionToStandby(0);\n//            dfsCluster.transitionToActive(1);\n//          } catch (IOException e) {\n//            throw new RuntimeException();\n//          }\n\n        }\n      }, rnd);\n    }  else {\n      // TODO: we could do much better at testing this\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      badTlogOutStreamFs = FileSystem.get(hdfsDirPath.toUri(), conf);\n      badTlogOutStream = badTlogOutStreamFs.create(hdfsDirPath);\n    }\n\n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n\n    return dfsCluster;\n  }\n\n","bugFix":null,"bugIntro":["add53de9835b2cd1a7a80b4e0036afee171c9fdf"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"add53de9835b2cd1a7a80b4e0036afee171c9fdf","date":1552937136,"type":3,"author":"Kevin Risden","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean,boolean).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean,boolean).mjava","sourceNew":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting, boolean haTesting) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n\n    checkFastDateFormat();\n    checkGeneratedIdMatches();\n\n    if (!HA_TESTING_ENABLED) haTesting = false;\n\n    Configuration conf = getBasicConfiguration(new Configuration());\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    // Disable metrics logging for HDFS\n    conf.setInt(\"dfs.namenode.metrics.logger.period.seconds\", 0);\n    conf.setInt(\"dfs.datanode.metrics.logger.period.seconds\", 0);\n\n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", DirectoryFactory.LOCK_TYPE_HDFS);\n\n    // test-files/solr/solr.xml sets this to be 15000. This isn't long enough for HDFS in some cases.\n    System.setProperty(\"socketTimeout\", \"90000\");\n\n    String blockcacheGlobal = System.getProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(random().nextBoolean()));\n    System.setProperty(\"solr.hdfs.blockcache.global\", blockcacheGlobal);\n    // Limit memory usage for HDFS tests\n    if(Boolean.parseBoolean(blockcacheGlobal)) {\n      System.setProperty(\"solr.hdfs.blockcache.blocksperbank\", \"4096\");\n    } else {\n      System.setProperty(\"solr.hdfs.blockcache.blocksperbank\", \"512\");\n      System.setProperty(\"tests.hdfs.numdatanodes\", \"1\");\n    }\n\n    int dataNodes = Integer.getInteger(\"tests.hdfs.numdatanodes\", 2);\n    final MiniDFSCluster.Builder dfsClusterBuilder = new MiniDFSCluster.Builder(conf)\n        .numDataNodes(dataNodes).format(true);\n    if (haTesting) {\n      dfsClusterBuilder.nnTopology(MiniDFSNNTopology.simpleHATopology());\n    }\n    MiniDFSCluster dfsCluster = dfsClusterBuilder.build();\n    HdfsUtil.TEST_CONF = getClientConfiguration(dfsCluster);\n    System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n\n    dfsCluster.waitActive();\n\n    if (haTesting) dfsCluster.transitionToActive(0);\n\n    int rndMode = random().nextInt(3);\n    if (safeModeTesting && rndMode == 1) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n\n      int rnd = random().nextInt(10000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n    } else if (haTesting && rndMode == 2) {\n      int rnd = random().nextInt(30000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          // TODO: randomly transition to standby\n//          try {\n//            dfsCluster.transitionToStandby(0);\n//            dfsCluster.transitionToActive(1);\n//          } catch (IOException e) {\n//            throw new RuntimeException();\n//          }\n\n        }\n      }, rnd);\n    }  else {\n      // TODO: we could do much better at testing this\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      badTlogOutStreamFs = FileSystem.get(hdfsDirPath.toUri(), getClientConfiguration(dfsCluster));\n      badTlogOutStream = badTlogOutStreamFs.create(hdfsDirPath);\n    }\n\n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n\n    return dfsCluster;\n  }\n\n","sourceOld":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting, boolean haTesting) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n\n    checkFastDateFormat();\n    checkGeneratedIdMatches();\n\n    if (!HA_TESTING_ENABLED) haTesting = false;\n\n    int dataNodes = Integer.getInteger(\"tests.hdfs.numdatanodes\", 2);\n\n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n\n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", DirectoryFactory.LOCK_TYPE_HDFS);\n\n    System.setProperty(\"solr.hdfs.blockcache.global\",\n        System.getProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean())));\n\n    final MiniDFSCluster dfsCluster;\n\n    if (!haTesting) {\n      dfsCluster = new MiniDFSCluster.Builder(conf).numDataNodes(dataNodes).format(true).build();\n\n      System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    } else {\n      dfsCluster = new MiniDFSCluster.Builder(conf)\n          .nnTopology(MiniDFSNNTopology.simpleHATopology()).numDataNodes(dataNodes)\n          .build();\n\n      Configuration haConfig = getClientConfiguration(dfsCluster);\n\n      HdfsUtil.TEST_CONF = haConfig;\n      System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    }\n\n    dfsCluster.waitActive();\n\n    if (haTesting) dfsCluster.transitionToActive(0);\n\n    int rndMode = LuceneTestCase.random().nextInt(3);\n    if (safeModeTesting && rndMode == 1) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n\n      int rnd = LuceneTestCase.random().nextInt(10000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n\n    } else if (haTesting && rndMode == 2) {\n      int rnd = LuceneTestCase.random().nextInt(30000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          // TODO: randomly transition to standby\n//          try {\n//            dfsCluster.transitionToStandby(0);\n//            dfsCluster.transitionToActive(1);\n//          } catch (IOException e) {\n//            throw new RuntimeException();\n//          }\n\n        }\n      }, rnd);\n    }  else {\n      // TODO: we could do much better at testing this\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      badTlogOutStreamFs = FileSystem.get(hdfsDirPath.toUri(), conf);\n      badTlogOutStream = badTlogOutStreamFs.create(hdfsDirPath);\n    }\n\n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n\n    return dfsCluster;\n  }\n\n","bugFix":["44ca189138a5b6e1989d12ab992fab60e235ddc7","2c801a37c38aedbd2ddbd27f2aaeb30cd5c7af0f","3def6e0e7b7566dd7f04a3514e77ee97a40fc78a","54d917b2424232699b135ef134cbd0f8add290c9"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6bdf107cf16be0f22504ae184fed81596665a244","date":1576012524,"type":3,"author":"Kevin Risden","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean,boolean).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean,boolean).mjava","sourceNew":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting, boolean haTesting) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n\n    checkFastDateFormat();\n    checkGeneratedIdMatches();\n\n    if (!HA_TESTING_ENABLED) haTesting = false;\n\n    Configuration conf = getBasicConfiguration(new Configuration());\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    // Disable metrics logging for HDFS\n    conf.setInt(\"dfs.namenode.metrics.logger.period.seconds\", 0);\n    conf.setInt(\"dfs.datanode.metrics.logger.period.seconds\", 0);\n\n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", DirectoryFactory.LOCK_TYPE_HDFS);\n\n    // test-files/solr/solr.xml sets this to be 15000. This isn't long enough for HDFS in some cases.\n    System.setProperty(\"socketTimeout\", \"90000\");\n\n    String blockcacheGlobal = System.getProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(random().nextBoolean()));\n    System.setProperty(\"solr.hdfs.blockcache.global\", blockcacheGlobal);\n    // Limit memory usage for HDFS tests\n    if(Boolean.parseBoolean(blockcacheGlobal)) {\n      System.setProperty(\"solr.hdfs.blockcache.blocksperbank\", \"4096\");\n    } else {\n      System.setProperty(\"solr.hdfs.blockcache.blocksperbank\", \"512\");\n      System.setProperty(\"tests.hdfs.numdatanodes\", \"1\");\n    }\n\n    int dataNodes = Integer.getInteger(\"tests.hdfs.numdatanodes\", 2);\n    final MiniDFSCluster.Builder dfsClusterBuilder = new MiniDFSCluster.Builder(conf)\n        .numDataNodes(dataNodes).format(true);\n    if (haTesting) {\n      dfsClusterBuilder.nnTopology(MiniDFSNNTopology.simpleHATopology());\n    }\n    MiniDFSCluster dfsCluster = dfsClusterBuilder.build();\n    HdfsUtil.TEST_CONF = getClientConfiguration(dfsCluster);\n    System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n\n    dfsCluster.waitActive();\n\n    if (haTesting) dfsCluster.transitionToActive(0);\n\n    int rndMode = random().nextInt(3);\n    if (safeModeTesting && rndMode == 1) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n\n      int rnd = random().nextInt(10000);\n      Timer timer = new Timer();\n      synchronized (TIMERS_LOCK) {\n        if (timers == null) {\n          timers = new HashMap<>();\n        }\n        timers.put(dfsCluster, timer);\n      }\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n    } else if (haTesting && rndMode == 2) {\n      int rnd = random().nextInt(30000);\n      Timer timer = new Timer();\n      synchronized (TIMERS_LOCK) {\n        if (timers == null) {\n          timers = new HashMap<>();\n        }\n        timers.put(dfsCluster, timer);\n      }\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          // TODO: randomly transition to standby\n//          try {\n//            dfsCluster.transitionToStandby(0);\n//            dfsCluster.transitionToActive(1);\n//          } catch (IOException e) {\n//            throw new RuntimeException();\n//          }\n\n        }\n      }, rnd);\n    }  else {\n      // TODO: we could do much better at testing this\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      badTlogOutStreamFs = FileSystem.get(hdfsDirPath.toUri(), getClientConfiguration(dfsCluster));\n      badTlogOutStream = badTlogOutStreamFs.create(hdfsDirPath);\n    }\n\n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n\n    return dfsCluster;\n  }\n\n","sourceOld":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting, boolean haTesting) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n\n    checkFastDateFormat();\n    checkGeneratedIdMatches();\n\n    if (!HA_TESTING_ENABLED) haTesting = false;\n\n    Configuration conf = getBasicConfiguration(new Configuration());\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    // Disable metrics logging for HDFS\n    conf.setInt(\"dfs.namenode.metrics.logger.period.seconds\", 0);\n    conf.setInt(\"dfs.datanode.metrics.logger.period.seconds\", 0);\n\n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", DirectoryFactory.LOCK_TYPE_HDFS);\n\n    // test-files/solr/solr.xml sets this to be 15000. This isn't long enough for HDFS in some cases.\n    System.setProperty(\"socketTimeout\", \"90000\");\n\n    String blockcacheGlobal = System.getProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(random().nextBoolean()));\n    System.setProperty(\"solr.hdfs.blockcache.global\", blockcacheGlobal);\n    // Limit memory usage for HDFS tests\n    if(Boolean.parseBoolean(blockcacheGlobal)) {\n      System.setProperty(\"solr.hdfs.blockcache.blocksperbank\", \"4096\");\n    } else {\n      System.setProperty(\"solr.hdfs.blockcache.blocksperbank\", \"512\");\n      System.setProperty(\"tests.hdfs.numdatanodes\", \"1\");\n    }\n\n    int dataNodes = Integer.getInteger(\"tests.hdfs.numdatanodes\", 2);\n    final MiniDFSCluster.Builder dfsClusterBuilder = new MiniDFSCluster.Builder(conf)\n        .numDataNodes(dataNodes).format(true);\n    if (haTesting) {\n      dfsClusterBuilder.nnTopology(MiniDFSNNTopology.simpleHATopology());\n    }\n    MiniDFSCluster dfsCluster = dfsClusterBuilder.build();\n    HdfsUtil.TEST_CONF = getClientConfiguration(dfsCluster);\n    System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n\n    dfsCluster.waitActive();\n\n    if (haTesting) dfsCluster.transitionToActive(0);\n\n    int rndMode = random().nextInt(3);\n    if (safeModeTesting && rndMode == 1) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n\n      int rnd = random().nextInt(10000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n    } else if (haTesting && rndMode == 2) {\n      int rnd = random().nextInt(30000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          // TODO: randomly transition to standby\n//          try {\n//            dfsCluster.transitionToStandby(0);\n//            dfsCluster.transitionToActive(1);\n//          } catch (IOException e) {\n//            throw new RuntimeException();\n//          }\n\n        }\n      }, rnd);\n    }  else {\n      // TODO: we could do much better at testing this\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      badTlogOutStreamFs = FileSystem.get(hdfsDirPath.toUri(), getClientConfiguration(dfsCluster));\n      badTlogOutStream = badTlogOutStreamFs.create(hdfsDirPath);\n    }\n\n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n\n    return dfsCluster;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a229cb50768e988c50a2106bdae3a92154f428bf","date":1576051038,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean,boolean).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean,boolean).mjava","sourceNew":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting, boolean haTesting) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n\n    checkFastDateFormat();\n    checkGeneratedIdMatches();\n\n    if (!HA_TESTING_ENABLED) haTesting = false;\n\n    Configuration conf = getBasicConfiguration(new Configuration());\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    // Disable metrics logging for HDFS\n    conf.setInt(\"dfs.namenode.metrics.logger.period.seconds\", 0);\n    conf.setInt(\"dfs.datanode.metrics.logger.period.seconds\", 0);\n\n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", DirectoryFactory.LOCK_TYPE_HDFS);\n\n    // test-files/solr/solr.xml sets this to be 15000. This isn't long enough for HDFS in some cases.\n    System.setProperty(\"socketTimeout\", \"90000\");\n\n    String blockcacheGlobal = System.getProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(random().nextBoolean()));\n    System.setProperty(\"solr.hdfs.blockcache.global\", blockcacheGlobal);\n    // Limit memory usage for HDFS tests\n    if(Boolean.parseBoolean(blockcacheGlobal)) {\n      System.setProperty(\"solr.hdfs.blockcache.blocksperbank\", \"4096\");\n    } else {\n      System.setProperty(\"solr.hdfs.blockcache.blocksperbank\", \"512\");\n      System.setProperty(\"tests.hdfs.numdatanodes\", \"1\");\n    }\n\n    int dataNodes = Integer.getInteger(\"tests.hdfs.numdatanodes\", 2);\n    final MiniDFSCluster.Builder dfsClusterBuilder = new MiniDFSCluster.Builder(conf)\n        .numDataNodes(dataNodes).format(true);\n    if (haTesting) {\n      dfsClusterBuilder.nnTopology(MiniDFSNNTopology.simpleHATopology());\n    }\n    MiniDFSCluster dfsCluster = dfsClusterBuilder.build();\n    HdfsUtil.TEST_CONF = getClientConfiguration(dfsCluster);\n    System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n\n    dfsCluster.waitActive();\n\n    if (haTesting) dfsCluster.transitionToActive(0);\n\n    int rndMode = random().nextInt(3);\n    if (safeModeTesting && rndMode == 1) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n\n      int rnd = random().nextInt(10000);\n      Timer timer = new Timer();\n      synchronized (TIMERS_LOCK) {\n        if (timers == null) {\n          timers = new HashMap<>();\n        }\n        timers.put(dfsCluster, timer);\n      }\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n    } else if (haTesting && rndMode == 2) {\n      int rnd = random().nextInt(30000);\n      Timer timer = new Timer();\n      synchronized (TIMERS_LOCK) {\n        if (timers == null) {\n          timers = new HashMap<>();\n        }\n        timers.put(dfsCluster, timer);\n      }\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          // TODO: randomly transition to standby\n//          try {\n//            dfsCluster.transitionToStandby(0);\n//            dfsCluster.transitionToActive(1);\n//          } catch (IOException e) {\n//            throw new RuntimeException();\n//          }\n\n        }\n      }, rnd);\n    }  else {\n      // TODO: we could do much better at testing this\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      badTlogOutStreamFs = FileSystem.get(hdfsDirPath.toUri(), getClientConfiguration(dfsCluster));\n      badTlogOutStream = badTlogOutStreamFs.create(hdfsDirPath);\n    }\n\n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n\n    return dfsCluster;\n  }\n\n","sourceOld":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting, boolean haTesting) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n\n    checkFastDateFormat();\n    checkGeneratedIdMatches();\n\n    if (!HA_TESTING_ENABLED) haTesting = false;\n\n    Configuration conf = getBasicConfiguration(new Configuration());\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    // Disable metrics logging for HDFS\n    conf.setInt(\"dfs.namenode.metrics.logger.period.seconds\", 0);\n    conf.setInt(\"dfs.datanode.metrics.logger.period.seconds\", 0);\n\n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", DirectoryFactory.LOCK_TYPE_HDFS);\n\n    // test-files/solr/solr.xml sets this to be 15000. This isn't long enough for HDFS in some cases.\n    System.setProperty(\"socketTimeout\", \"90000\");\n\n    String blockcacheGlobal = System.getProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(random().nextBoolean()));\n    System.setProperty(\"solr.hdfs.blockcache.global\", blockcacheGlobal);\n    // Limit memory usage for HDFS tests\n    if(Boolean.parseBoolean(blockcacheGlobal)) {\n      System.setProperty(\"solr.hdfs.blockcache.blocksperbank\", \"4096\");\n    } else {\n      System.setProperty(\"solr.hdfs.blockcache.blocksperbank\", \"512\");\n      System.setProperty(\"tests.hdfs.numdatanodes\", \"1\");\n    }\n\n    int dataNodes = Integer.getInteger(\"tests.hdfs.numdatanodes\", 2);\n    final MiniDFSCluster.Builder dfsClusterBuilder = new MiniDFSCluster.Builder(conf)\n        .numDataNodes(dataNodes).format(true);\n    if (haTesting) {\n      dfsClusterBuilder.nnTopology(MiniDFSNNTopology.simpleHATopology());\n    }\n    MiniDFSCluster dfsCluster = dfsClusterBuilder.build();\n    HdfsUtil.TEST_CONF = getClientConfiguration(dfsCluster);\n    System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n\n    dfsCluster.waitActive();\n\n    if (haTesting) dfsCluster.transitionToActive(0);\n\n    int rndMode = random().nextInt(3);\n    if (safeModeTesting && rndMode == 1) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n\n      int rnd = random().nextInt(10000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n    } else if (haTesting && rndMode == 2) {\n      int rnd = random().nextInt(30000);\n      Timer timer = new Timer();\n      timers.put(dfsCluster, timer);\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          // TODO: randomly transition to standby\n//          try {\n//            dfsCluster.transitionToStandby(0);\n//            dfsCluster.transitionToActive(1);\n//          } catch (IOException e) {\n//            throw new RuntimeException();\n//          }\n\n        }\n      }, rnd);\n    }  else {\n      // TODO: we could do much better at testing this\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      badTlogOutStreamFs = FileSystem.get(hdfsDirPath.toUri(), getClientConfiguration(dfsCluster));\n      badTlogOutStream = badTlogOutStreamFs.create(hdfsDirPath);\n    }\n\n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n\n    return dfsCluster;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cc0ebd2dff6817d143057f270f7b9b3076c2fedf","date":1576072884,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean,boolean).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean,boolean).mjava","sourceNew":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting, boolean haTesting) throws Exception {\n    checkFastDateFormat();\n    checkGeneratedIdMatches();\n\n    if (!HA_TESTING_ENABLED) haTesting = false;\n\n    Configuration conf = getBasicConfiguration(new Configuration());\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    // Disable metrics logging for HDFS\n    conf.setInt(\"dfs.namenode.metrics.logger.period.seconds\", 0);\n    conf.setInt(\"dfs.datanode.metrics.logger.period.seconds\", 0);\n\n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", DirectoryFactory.LOCK_TYPE_HDFS);\n\n    // test-files/solr/solr.xml sets this to be 15000. This isn't long enough for HDFS in some cases.\n    System.setProperty(\"socketTimeout\", \"90000\");\n\n    String blockcacheGlobal = System.getProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(random().nextBoolean()));\n    System.setProperty(\"solr.hdfs.blockcache.global\", blockcacheGlobal);\n    // Limit memory usage for HDFS tests\n    if(Boolean.parseBoolean(blockcacheGlobal)) {\n      System.setProperty(\"solr.hdfs.blockcache.blocksperbank\", \"4096\");\n    } else {\n      System.setProperty(\"solr.hdfs.blockcache.blocksperbank\", \"512\");\n      System.setProperty(\"tests.hdfs.numdatanodes\", \"1\");\n    }\n\n    int dataNodes = Integer.getInteger(\"tests.hdfs.numdatanodes\", 2);\n    final MiniDFSCluster.Builder dfsClusterBuilder = new MiniDFSCluster.Builder(conf)\n        .numDataNodes(dataNodes).format(true);\n    if (haTesting) {\n      dfsClusterBuilder.nnTopology(MiniDFSNNTopology.simpleHATopology());\n    }\n\n    if (!NativeIO.isAvailable()) {\n      throw new AssumptionViolatedException(\"NativeIO not available for HDFS.\");\n    }\n\n    MiniDFSCluster dfsCluster = dfsClusterBuilder.build();\n    HdfsUtil.TEST_CONF = getClientConfiguration(dfsCluster);\n    System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n\n    dfsCluster.waitActive();\n\n    if (haTesting) dfsCluster.transitionToActive(0);\n\n    int rndMode = random().nextInt(3);\n    if (safeModeTesting && rndMode == 1) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n\n      int rnd = random().nextInt(10000);\n      Timer timer = new Timer();\n      synchronized (TIMERS_LOCK) {\n        if (timers == null) {\n          timers = new HashMap<>();\n        }\n        timers.put(dfsCluster, timer);\n      }\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n    } else if (haTesting && rndMode == 2) {\n      int rnd = random().nextInt(30000);\n      Timer timer = new Timer();\n      synchronized (TIMERS_LOCK) {\n        if (timers == null) {\n          timers = new HashMap<>();\n        }\n        timers.put(dfsCluster, timer);\n      }\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          // TODO: randomly transition to standby\n//          try {\n//            dfsCluster.transitionToStandby(0);\n//            dfsCluster.transitionToActive(1);\n//          } catch (IOException e) {\n//            throw new RuntimeException();\n//          }\n\n        }\n      }, rnd);\n    }  else {\n      // TODO: we could do much better at testing this\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      badTlogOutStreamFs = FileSystem.get(hdfsDirPath.toUri(), getClientConfiguration(dfsCluster));\n      badTlogOutStream = badTlogOutStreamFs.create(hdfsDirPath);\n    }\n\n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n\n    return dfsCluster;\n  }\n\n","sourceOld":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting, boolean haTesting) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n\n    checkFastDateFormat();\n    checkGeneratedIdMatches();\n\n    if (!HA_TESTING_ENABLED) haTesting = false;\n\n    Configuration conf = getBasicConfiguration(new Configuration());\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    // Disable metrics logging for HDFS\n    conf.setInt(\"dfs.namenode.metrics.logger.period.seconds\", 0);\n    conf.setInt(\"dfs.datanode.metrics.logger.period.seconds\", 0);\n\n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", DirectoryFactory.LOCK_TYPE_HDFS);\n\n    // test-files/solr/solr.xml sets this to be 15000. This isn't long enough for HDFS in some cases.\n    System.setProperty(\"socketTimeout\", \"90000\");\n\n    String blockcacheGlobal = System.getProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(random().nextBoolean()));\n    System.setProperty(\"solr.hdfs.blockcache.global\", blockcacheGlobal);\n    // Limit memory usage for HDFS tests\n    if(Boolean.parseBoolean(blockcacheGlobal)) {\n      System.setProperty(\"solr.hdfs.blockcache.blocksperbank\", \"4096\");\n    } else {\n      System.setProperty(\"solr.hdfs.blockcache.blocksperbank\", \"512\");\n      System.setProperty(\"tests.hdfs.numdatanodes\", \"1\");\n    }\n\n    int dataNodes = Integer.getInteger(\"tests.hdfs.numdatanodes\", 2);\n    final MiniDFSCluster.Builder dfsClusterBuilder = new MiniDFSCluster.Builder(conf)\n        .numDataNodes(dataNodes).format(true);\n    if (haTesting) {\n      dfsClusterBuilder.nnTopology(MiniDFSNNTopology.simpleHATopology());\n    }\n    MiniDFSCluster dfsCluster = dfsClusterBuilder.build();\n    HdfsUtil.TEST_CONF = getClientConfiguration(dfsCluster);\n    System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n\n    dfsCluster.waitActive();\n\n    if (haTesting) dfsCluster.transitionToActive(0);\n\n    int rndMode = random().nextInt(3);\n    if (safeModeTesting && rndMode == 1) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n\n      int rnd = random().nextInt(10000);\n      Timer timer = new Timer();\n      synchronized (TIMERS_LOCK) {\n        if (timers == null) {\n          timers = new HashMap<>();\n        }\n        timers.put(dfsCluster, timer);\n      }\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n    } else if (haTesting && rndMode == 2) {\n      int rnd = random().nextInt(30000);\n      Timer timer = new Timer();\n      synchronized (TIMERS_LOCK) {\n        if (timers == null) {\n          timers = new HashMap<>();\n        }\n        timers.put(dfsCluster, timer);\n      }\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          // TODO: randomly transition to standby\n//          try {\n//            dfsCluster.transitionToStandby(0);\n//            dfsCluster.transitionToActive(1);\n//          } catch (IOException e) {\n//            throw new RuntimeException();\n//          }\n\n        }\n      }, rnd);\n    }  else {\n      // TODO: we could do much better at testing this\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      badTlogOutStreamFs = FileSystem.get(hdfsDirPath.toUri(), getClientConfiguration(dfsCluster));\n      badTlogOutStream = badTlogOutStreamFs.create(hdfsDirPath);\n    }\n\n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n\n    return dfsCluster;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6c6b7e01cb749c3b01e226e06085dfb1d9ed8eab","date":1576073026,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean,boolean).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean,boolean).mjava","sourceNew":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting, boolean haTesting) throws Exception {\n    checkFastDateFormat();\n    checkGeneratedIdMatches();\n\n    if (!HA_TESTING_ENABLED) haTesting = false;\n\n    Configuration conf = getBasicConfiguration(new Configuration());\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    // Disable metrics logging for HDFS\n    conf.setInt(\"dfs.namenode.metrics.logger.period.seconds\", 0);\n    conf.setInt(\"dfs.datanode.metrics.logger.period.seconds\", 0);\n\n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", DirectoryFactory.LOCK_TYPE_HDFS);\n\n    // test-files/solr/solr.xml sets this to be 15000. This isn't long enough for HDFS in some cases.\n    System.setProperty(\"socketTimeout\", \"90000\");\n\n    String blockcacheGlobal = System.getProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(random().nextBoolean()));\n    System.setProperty(\"solr.hdfs.blockcache.global\", blockcacheGlobal);\n    // Limit memory usage for HDFS tests\n    if(Boolean.parseBoolean(blockcacheGlobal)) {\n      System.setProperty(\"solr.hdfs.blockcache.blocksperbank\", \"4096\");\n    } else {\n      System.setProperty(\"solr.hdfs.blockcache.blocksperbank\", \"512\");\n      System.setProperty(\"tests.hdfs.numdatanodes\", \"1\");\n    }\n\n    int dataNodes = Integer.getInteger(\"tests.hdfs.numdatanodes\", 2);\n    final MiniDFSCluster.Builder dfsClusterBuilder = new MiniDFSCluster.Builder(conf)\n        .numDataNodes(dataNodes).format(true);\n    if (haTesting) {\n      dfsClusterBuilder.nnTopology(MiniDFSNNTopology.simpleHATopology());\n    }\n\n    if (!NativeIO.isAvailable()) {\n      throw new AssumptionViolatedException(\"NativeIO not available for HDFS.\");\n    }\n\n    MiniDFSCluster dfsCluster = dfsClusterBuilder.build();\n    HdfsUtil.TEST_CONF = getClientConfiguration(dfsCluster);\n    System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n\n    dfsCluster.waitActive();\n\n    if (haTesting) dfsCluster.transitionToActive(0);\n\n    int rndMode = random().nextInt(3);\n    if (safeModeTesting && rndMode == 1) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n\n      int rnd = random().nextInt(10000);\n      Timer timer = new Timer();\n      synchronized (TIMERS_LOCK) {\n        if (timers == null) {\n          timers = new HashMap<>();\n        }\n        timers.put(dfsCluster, timer);\n      }\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n    } else if (haTesting && rndMode == 2) {\n      int rnd = random().nextInt(30000);\n      Timer timer = new Timer();\n      synchronized (TIMERS_LOCK) {\n        if (timers == null) {\n          timers = new HashMap<>();\n        }\n        timers.put(dfsCluster, timer);\n      }\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          // TODO: randomly transition to standby\n//          try {\n//            dfsCluster.transitionToStandby(0);\n//            dfsCluster.transitionToActive(1);\n//          } catch (IOException e) {\n//            throw new RuntimeException();\n//          }\n\n        }\n      }, rnd);\n    }  else {\n      // TODO: we could do much better at testing this\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      badTlogOutStreamFs = FileSystem.get(hdfsDirPath.toUri(), getClientConfiguration(dfsCluster));\n      badTlogOutStream = badTlogOutStreamFs.create(hdfsDirPath);\n    }\n\n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n\n    return dfsCluster;\n  }\n\n","sourceOld":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting, boolean haTesting) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n\n    checkFastDateFormat();\n    checkGeneratedIdMatches();\n\n    if (!HA_TESTING_ENABLED) haTesting = false;\n\n    Configuration conf = getBasicConfiguration(new Configuration());\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    // Disable metrics logging for HDFS\n    conf.setInt(\"dfs.namenode.metrics.logger.period.seconds\", 0);\n    conf.setInt(\"dfs.datanode.metrics.logger.period.seconds\", 0);\n\n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", DirectoryFactory.LOCK_TYPE_HDFS);\n\n    // test-files/solr/solr.xml sets this to be 15000. This isn't long enough for HDFS in some cases.\n    System.setProperty(\"socketTimeout\", \"90000\");\n\n    String blockcacheGlobal = System.getProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(random().nextBoolean()));\n    System.setProperty(\"solr.hdfs.blockcache.global\", blockcacheGlobal);\n    // Limit memory usage for HDFS tests\n    if(Boolean.parseBoolean(blockcacheGlobal)) {\n      System.setProperty(\"solr.hdfs.blockcache.blocksperbank\", \"4096\");\n    } else {\n      System.setProperty(\"solr.hdfs.blockcache.blocksperbank\", \"512\");\n      System.setProperty(\"tests.hdfs.numdatanodes\", \"1\");\n    }\n\n    int dataNodes = Integer.getInteger(\"tests.hdfs.numdatanodes\", 2);\n    final MiniDFSCluster.Builder dfsClusterBuilder = new MiniDFSCluster.Builder(conf)\n        .numDataNodes(dataNodes).format(true);\n    if (haTesting) {\n      dfsClusterBuilder.nnTopology(MiniDFSNNTopology.simpleHATopology());\n    }\n    MiniDFSCluster dfsCluster = dfsClusterBuilder.build();\n    HdfsUtil.TEST_CONF = getClientConfiguration(dfsCluster);\n    System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n\n    dfsCluster.waitActive();\n\n    if (haTesting) dfsCluster.transitionToActive(0);\n\n    int rndMode = random().nextInt(3);\n    if (safeModeTesting && rndMode == 1) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n\n      int rnd = random().nextInt(10000);\n      Timer timer = new Timer();\n      synchronized (TIMERS_LOCK) {\n        if (timers == null) {\n          timers = new HashMap<>();\n        }\n        timers.put(dfsCluster, timer);\n      }\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n    } else if (haTesting && rndMode == 2) {\n      int rnd = random().nextInt(30000);\n      Timer timer = new Timer();\n      synchronized (TIMERS_LOCK) {\n        if (timers == null) {\n          timers = new HashMap<>();\n        }\n        timers.put(dfsCluster, timer);\n      }\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          // TODO: randomly transition to standby\n//          try {\n//            dfsCluster.transitionToStandby(0);\n//            dfsCluster.transitionToActive(1);\n//          } catch (IOException e) {\n//            throw new RuntimeException();\n//          }\n\n        }\n      }, rnd);\n    }  else {\n      // TODO: we could do much better at testing this\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      badTlogOutStreamFs = FileSystem.get(hdfsDirPath.toUri(), getClientConfiguration(dfsCluster));\n      badTlogOutStream = badTlogOutStreamFs.create(hdfsDirPath);\n    }\n\n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n\n    return dfsCluster;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7e614eceb81b081076e753ed62268c93c8a2a28e","date":1576119727,"type":3,"author":"Kevin Risden","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean,boolean).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean,boolean).mjava","sourceNew":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting, boolean haTesting) throws Exception {\n    checkAssumptions();\n\n    if (!HA_TESTING_ENABLED) haTesting = false;\n\n    Configuration conf = getBasicConfiguration(new Configuration());\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    // Disable metrics logging for HDFS\n    conf.setInt(\"dfs.namenode.metrics.logger.period.seconds\", 0);\n    conf.setInt(\"dfs.datanode.metrics.logger.period.seconds\", 0);\n\n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", DirectoryFactory.LOCK_TYPE_HDFS);\n\n    // test-files/solr/solr.xml sets this to be 15000. This isn't long enough for HDFS in some cases.\n    System.setProperty(\"socketTimeout\", \"90000\");\n\n    String blockcacheGlobal = System.getProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(random().nextBoolean()));\n    System.setProperty(\"solr.hdfs.blockcache.global\", blockcacheGlobal);\n    // Limit memory usage for HDFS tests\n    if(Boolean.parseBoolean(blockcacheGlobal)) {\n      System.setProperty(\"solr.hdfs.blockcache.blocksperbank\", \"4096\");\n    } else {\n      System.setProperty(\"solr.hdfs.blockcache.blocksperbank\", \"512\");\n      System.setProperty(\"tests.hdfs.numdatanodes\", \"1\");\n    }\n\n    int dataNodes = Integer.getInteger(\"tests.hdfs.numdatanodes\", 2);\n    final MiniDFSCluster.Builder dfsClusterBuilder = new MiniDFSCluster.Builder(conf)\n        .numDataNodes(dataNodes).format(true);\n    if (haTesting) {\n      dfsClusterBuilder.nnTopology(MiniDFSNNTopology.simpleHATopology());\n    }\n\n    MiniDFSCluster dfsCluster = dfsClusterBuilder.build();\n    HdfsUtil.TEST_CONF = getClientConfiguration(dfsCluster);\n    System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n\n    dfsCluster.waitActive();\n\n    if (haTesting) dfsCluster.transitionToActive(0);\n\n    int rndMode = random().nextInt(3);\n    if (safeModeTesting && rndMode == 1) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n\n      int rnd = random().nextInt(10000);\n      Timer timer = new Timer();\n      synchronized (TIMERS_LOCK) {\n        if (timers == null) {\n          timers = new HashMap<>();\n        }\n        timers.put(dfsCluster, timer);\n      }\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n    } else if (haTesting && rndMode == 2) {\n      int rnd = random().nextInt(30000);\n      Timer timer = new Timer();\n      synchronized (TIMERS_LOCK) {\n        if (timers == null) {\n          timers = new HashMap<>();\n        }\n        timers.put(dfsCluster, timer);\n      }\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          // TODO: randomly transition to standby\n//          try {\n//            dfsCluster.transitionToStandby(0);\n//            dfsCluster.transitionToActive(1);\n//          } catch (IOException e) {\n//            throw new RuntimeException();\n//          }\n\n        }\n      }, rnd);\n    }  else {\n      // TODO: we could do much better at testing this\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      badTlogOutStreamFs = FileSystem.get(hdfsDirPath.toUri(), getClientConfiguration(dfsCluster));\n      badTlogOutStream = badTlogOutStreamFs.create(hdfsDirPath);\n    }\n\n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n\n    return dfsCluster;\n  }\n\n","sourceOld":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting, boolean haTesting) throws Exception {\n    checkFastDateFormat();\n    checkGeneratedIdMatches();\n\n    if (!HA_TESTING_ENABLED) haTesting = false;\n\n    Configuration conf = getBasicConfiguration(new Configuration());\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    // Disable metrics logging for HDFS\n    conf.setInt(\"dfs.namenode.metrics.logger.period.seconds\", 0);\n    conf.setInt(\"dfs.datanode.metrics.logger.period.seconds\", 0);\n\n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", DirectoryFactory.LOCK_TYPE_HDFS);\n\n    // test-files/solr/solr.xml sets this to be 15000. This isn't long enough for HDFS in some cases.\n    System.setProperty(\"socketTimeout\", \"90000\");\n\n    String blockcacheGlobal = System.getProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(random().nextBoolean()));\n    System.setProperty(\"solr.hdfs.blockcache.global\", blockcacheGlobal);\n    // Limit memory usage for HDFS tests\n    if(Boolean.parseBoolean(blockcacheGlobal)) {\n      System.setProperty(\"solr.hdfs.blockcache.blocksperbank\", \"4096\");\n    } else {\n      System.setProperty(\"solr.hdfs.blockcache.blocksperbank\", \"512\");\n      System.setProperty(\"tests.hdfs.numdatanodes\", \"1\");\n    }\n\n    int dataNodes = Integer.getInteger(\"tests.hdfs.numdatanodes\", 2);\n    final MiniDFSCluster.Builder dfsClusterBuilder = new MiniDFSCluster.Builder(conf)\n        .numDataNodes(dataNodes).format(true);\n    if (haTesting) {\n      dfsClusterBuilder.nnTopology(MiniDFSNNTopology.simpleHATopology());\n    }\n\n    if (!NativeIO.isAvailable()) {\n      throw new AssumptionViolatedException(\"NativeIO not available for HDFS.\");\n    }\n\n    MiniDFSCluster dfsCluster = dfsClusterBuilder.build();\n    HdfsUtil.TEST_CONF = getClientConfiguration(dfsCluster);\n    System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n\n    dfsCluster.waitActive();\n\n    if (haTesting) dfsCluster.transitionToActive(0);\n\n    int rndMode = random().nextInt(3);\n    if (safeModeTesting && rndMode == 1) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n\n      int rnd = random().nextInt(10000);\n      Timer timer = new Timer();\n      synchronized (TIMERS_LOCK) {\n        if (timers == null) {\n          timers = new HashMap<>();\n        }\n        timers.put(dfsCluster, timer);\n      }\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n    } else if (haTesting && rndMode == 2) {\n      int rnd = random().nextInt(30000);\n      Timer timer = new Timer();\n      synchronized (TIMERS_LOCK) {\n        if (timers == null) {\n          timers = new HashMap<>();\n        }\n        timers.put(dfsCluster, timer);\n      }\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          // TODO: randomly transition to standby\n//          try {\n//            dfsCluster.transitionToStandby(0);\n//            dfsCluster.transitionToActive(1);\n//          } catch (IOException e) {\n//            throw new RuntimeException();\n//          }\n\n        }\n      }, rnd);\n    }  else {\n      // TODO: we could do much better at testing this\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      badTlogOutStreamFs = FileSystem.get(hdfsDirPath.toUri(), getClientConfiguration(dfsCluster));\n      badTlogOutStream = badTlogOutStreamFs.create(hdfsDirPath);\n    }\n\n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n\n    return dfsCluster;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"df724d84dab24a0cc54bec95a8680867adc7f171","date":1576156608,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean,boolean).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean,boolean).mjava","sourceNew":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting, boolean haTesting) throws Exception {\n    checkAssumptions();\n\n    if (!HA_TESTING_ENABLED) haTesting = false;\n\n    Configuration conf = getBasicConfiguration(new Configuration());\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    // Disable metrics logging for HDFS\n    conf.setInt(\"dfs.namenode.metrics.logger.period.seconds\", 0);\n    conf.setInt(\"dfs.datanode.metrics.logger.period.seconds\", 0);\n\n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", DirectoryFactory.LOCK_TYPE_HDFS);\n\n    // test-files/solr/solr.xml sets this to be 15000. This isn't long enough for HDFS in some cases.\n    System.setProperty(\"socketTimeout\", \"90000\");\n\n    String blockcacheGlobal = System.getProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(random().nextBoolean()));\n    System.setProperty(\"solr.hdfs.blockcache.global\", blockcacheGlobal);\n    // Limit memory usage for HDFS tests\n    if(Boolean.parseBoolean(blockcacheGlobal)) {\n      System.setProperty(\"solr.hdfs.blockcache.blocksperbank\", \"4096\");\n    } else {\n      System.setProperty(\"solr.hdfs.blockcache.blocksperbank\", \"512\");\n      System.setProperty(\"tests.hdfs.numdatanodes\", \"1\");\n    }\n\n    int dataNodes = Integer.getInteger(\"tests.hdfs.numdatanodes\", 2);\n    final MiniDFSCluster.Builder dfsClusterBuilder = new MiniDFSCluster.Builder(conf)\n        .numDataNodes(dataNodes).format(true);\n    if (haTesting) {\n      dfsClusterBuilder.nnTopology(MiniDFSNNTopology.simpleHATopology());\n    }\n\n    MiniDFSCluster dfsCluster = dfsClusterBuilder.build();\n    HdfsUtil.TEST_CONF = getClientConfiguration(dfsCluster);\n    System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n\n    dfsCluster.waitActive();\n\n    if (haTesting) dfsCluster.transitionToActive(0);\n\n    int rndMode = random().nextInt(3);\n    if (safeModeTesting && rndMode == 1) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n\n      int rnd = random().nextInt(10000);\n      Timer timer = new Timer();\n      synchronized (TIMERS_LOCK) {\n        if (timers == null) {\n          timers = new HashMap<>();\n        }\n        timers.put(dfsCluster, timer);\n      }\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n    } else if (haTesting && rndMode == 2) {\n      int rnd = random().nextInt(30000);\n      Timer timer = new Timer();\n      synchronized (TIMERS_LOCK) {\n        if (timers == null) {\n          timers = new HashMap<>();\n        }\n        timers.put(dfsCluster, timer);\n      }\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          // TODO: randomly transition to standby\n//          try {\n//            dfsCluster.transitionToStandby(0);\n//            dfsCluster.transitionToActive(1);\n//          } catch (IOException e) {\n//            throw new RuntimeException();\n//          }\n\n        }\n      }, rnd);\n    }  else {\n      // TODO: we could do much better at testing this\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      badTlogOutStreamFs = FileSystem.get(hdfsDirPath.toUri(), getClientConfiguration(dfsCluster));\n      badTlogOutStream = badTlogOutStreamFs.create(hdfsDirPath);\n    }\n\n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n\n    return dfsCluster;\n  }\n\n","sourceOld":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting, boolean haTesting) throws Exception {\n    checkFastDateFormat();\n    checkGeneratedIdMatches();\n\n    if (!HA_TESTING_ENABLED) haTesting = false;\n\n    Configuration conf = getBasicConfiguration(new Configuration());\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    // Disable metrics logging for HDFS\n    conf.setInt(\"dfs.namenode.metrics.logger.period.seconds\", 0);\n    conf.setInt(\"dfs.datanode.metrics.logger.period.seconds\", 0);\n\n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", DirectoryFactory.LOCK_TYPE_HDFS);\n\n    // test-files/solr/solr.xml sets this to be 15000. This isn't long enough for HDFS in some cases.\n    System.setProperty(\"socketTimeout\", \"90000\");\n\n    String blockcacheGlobal = System.getProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(random().nextBoolean()));\n    System.setProperty(\"solr.hdfs.blockcache.global\", blockcacheGlobal);\n    // Limit memory usage for HDFS tests\n    if(Boolean.parseBoolean(blockcacheGlobal)) {\n      System.setProperty(\"solr.hdfs.blockcache.blocksperbank\", \"4096\");\n    } else {\n      System.setProperty(\"solr.hdfs.blockcache.blocksperbank\", \"512\");\n      System.setProperty(\"tests.hdfs.numdatanodes\", \"1\");\n    }\n\n    int dataNodes = Integer.getInteger(\"tests.hdfs.numdatanodes\", 2);\n    final MiniDFSCluster.Builder dfsClusterBuilder = new MiniDFSCluster.Builder(conf)\n        .numDataNodes(dataNodes).format(true);\n    if (haTesting) {\n      dfsClusterBuilder.nnTopology(MiniDFSNNTopology.simpleHATopology());\n    }\n\n    if (!NativeIO.isAvailable()) {\n      throw new AssumptionViolatedException(\"NativeIO not available for HDFS.\");\n    }\n\n    MiniDFSCluster dfsCluster = dfsClusterBuilder.build();\n    HdfsUtil.TEST_CONF = getClientConfiguration(dfsCluster);\n    System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n\n    dfsCluster.waitActive();\n\n    if (haTesting) dfsCluster.transitionToActive(0);\n\n    int rndMode = random().nextInt(3);\n    if (safeModeTesting && rndMode == 1) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n\n      int rnd = random().nextInt(10000);\n      Timer timer = new Timer();\n      synchronized (TIMERS_LOCK) {\n        if (timers == null) {\n          timers = new HashMap<>();\n        }\n        timers.put(dfsCluster, timer);\n      }\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n    } else if (haTesting && rndMode == 2) {\n      int rnd = random().nextInt(30000);\n      Timer timer = new Timer();\n      synchronized (TIMERS_LOCK) {\n        if (timers == null) {\n          timers = new HashMap<>();\n        }\n        timers.put(dfsCluster, timer);\n      }\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          // TODO: randomly transition to standby\n//          try {\n//            dfsCluster.transitionToStandby(0);\n//            dfsCluster.transitionToActive(1);\n//          } catch (IOException e) {\n//            throw new RuntimeException();\n//          }\n\n        }\n      }, rnd);\n    }  else {\n      // TODO: we could do much better at testing this\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      badTlogOutStreamFs = FileSystem.get(hdfsDirPath.toUri(), getClientConfiguration(dfsCluster));\n      badTlogOutStream = badTlogOutStreamFs.create(hdfsDirPath);\n    }\n\n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n\n    return dfsCluster;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7fda2c65b4739b1a2340a010e1cbf2c9a7b9ee72","date":1576292928,"type":3,"author":"Kevin Risden","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean,boolean).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean,boolean).mjava","sourceNew":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting, boolean haTesting) throws Exception {\n    checkAssumptions();\n\n    if (!HA_TESTING_ENABLED) haTesting = false;\n\n    DefaultMetricsSystem.setInstance(new FakeMetricsSystem());\n\n    Configuration conf = getBasicConfiguration(new Configuration());\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    // Disable metrics logging for HDFS\n    conf.setInt(\"dfs.namenode.metrics.logger.period.seconds\", 0);\n    conf.setInt(\"dfs.datanode.metrics.logger.period.seconds\", 0);\n\n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", DirectoryFactory.LOCK_TYPE_HDFS);\n\n    // test-files/solr/solr.xml sets this to be 15000. This isn't long enough for HDFS in some cases.\n    System.setProperty(\"socketTimeout\", \"90000\");\n\n    String blockcacheGlobal = System.getProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(random().nextBoolean()));\n    System.setProperty(\"solr.hdfs.blockcache.global\", blockcacheGlobal);\n    // Limit memory usage for HDFS tests\n    if(Boolean.parseBoolean(blockcacheGlobal)) {\n      System.setProperty(\"solr.hdfs.blockcache.blocksperbank\", \"4096\");\n    } else {\n      System.setProperty(\"solr.hdfs.blockcache.blocksperbank\", \"512\");\n      System.setProperty(\"tests.hdfs.numdatanodes\", \"1\");\n    }\n\n    int dataNodes = Integer.getInteger(\"tests.hdfs.numdatanodes\", 2);\n    final MiniDFSCluster.Builder dfsClusterBuilder = new MiniDFSCluster.Builder(conf)\n        .numDataNodes(dataNodes).format(true);\n    if (haTesting) {\n      dfsClusterBuilder.nnTopology(MiniDFSNNTopology.simpleHATopology());\n    }\n\n    MiniDFSCluster dfsCluster = dfsClusterBuilder.build();\n    HdfsUtil.TEST_CONF = getClientConfiguration(dfsCluster);\n    System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n\n    dfsCluster.waitActive();\n\n    if (haTesting) dfsCluster.transitionToActive(0);\n\n    int rndMode = random().nextInt(3);\n    if (safeModeTesting && rndMode == 1) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n\n      int rnd = random().nextInt(10000);\n      Timer timer = new Timer();\n      synchronized (TIMERS_LOCK) {\n        if (timers == null) {\n          timers = new HashMap<>();\n        }\n        timers.put(dfsCluster, timer);\n      }\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n    } else if (haTesting && rndMode == 2) {\n      int rnd = random().nextInt(30000);\n      Timer timer = new Timer();\n      synchronized (TIMERS_LOCK) {\n        if (timers == null) {\n          timers = new HashMap<>();\n        }\n        timers.put(dfsCluster, timer);\n      }\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          // TODO: randomly transition to standby\n//          try {\n//            dfsCluster.transitionToStandby(0);\n//            dfsCluster.transitionToActive(1);\n//          } catch (IOException e) {\n//            throw new RuntimeException();\n//          }\n\n        }\n      }, rnd);\n    }  else {\n      // TODO: we could do much better at testing this\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      badTlogOutStreamFs = FileSystem.get(hdfsDirPath.toUri(), getClientConfiguration(dfsCluster));\n      badTlogOutStream = badTlogOutStreamFs.create(hdfsDirPath);\n    }\n\n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n\n    return dfsCluster;\n  }\n\n","sourceOld":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting, boolean haTesting) throws Exception {\n    checkAssumptions();\n\n    if (!HA_TESTING_ENABLED) haTesting = false;\n\n    Configuration conf = getBasicConfiguration(new Configuration());\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    // Disable metrics logging for HDFS\n    conf.setInt(\"dfs.namenode.metrics.logger.period.seconds\", 0);\n    conf.setInt(\"dfs.datanode.metrics.logger.period.seconds\", 0);\n\n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", DirectoryFactory.LOCK_TYPE_HDFS);\n\n    // test-files/solr/solr.xml sets this to be 15000. This isn't long enough for HDFS in some cases.\n    System.setProperty(\"socketTimeout\", \"90000\");\n\n    String blockcacheGlobal = System.getProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(random().nextBoolean()));\n    System.setProperty(\"solr.hdfs.blockcache.global\", blockcacheGlobal);\n    // Limit memory usage for HDFS tests\n    if(Boolean.parseBoolean(blockcacheGlobal)) {\n      System.setProperty(\"solr.hdfs.blockcache.blocksperbank\", \"4096\");\n    } else {\n      System.setProperty(\"solr.hdfs.blockcache.blocksperbank\", \"512\");\n      System.setProperty(\"tests.hdfs.numdatanodes\", \"1\");\n    }\n\n    int dataNodes = Integer.getInteger(\"tests.hdfs.numdatanodes\", 2);\n    final MiniDFSCluster.Builder dfsClusterBuilder = new MiniDFSCluster.Builder(conf)\n        .numDataNodes(dataNodes).format(true);\n    if (haTesting) {\n      dfsClusterBuilder.nnTopology(MiniDFSNNTopology.simpleHATopology());\n    }\n\n    MiniDFSCluster dfsCluster = dfsClusterBuilder.build();\n    HdfsUtil.TEST_CONF = getClientConfiguration(dfsCluster);\n    System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n\n    dfsCluster.waitActive();\n\n    if (haTesting) dfsCluster.transitionToActive(0);\n\n    int rndMode = random().nextInt(3);\n    if (safeModeTesting && rndMode == 1) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n\n      int rnd = random().nextInt(10000);\n      Timer timer = new Timer();\n      synchronized (TIMERS_LOCK) {\n        if (timers == null) {\n          timers = new HashMap<>();\n        }\n        timers.put(dfsCluster, timer);\n      }\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n    } else if (haTesting && rndMode == 2) {\n      int rnd = random().nextInt(30000);\n      Timer timer = new Timer();\n      synchronized (TIMERS_LOCK) {\n        if (timers == null) {\n          timers = new HashMap<>();\n        }\n        timers.put(dfsCluster, timer);\n      }\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          // TODO: randomly transition to standby\n//          try {\n//            dfsCluster.transitionToStandby(0);\n//            dfsCluster.transitionToActive(1);\n//          } catch (IOException e) {\n//            throw new RuntimeException();\n//          }\n\n        }\n      }, rnd);\n    }  else {\n      // TODO: we could do much better at testing this\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      badTlogOutStreamFs = FileSystem.get(hdfsDirPath.toUri(), getClientConfiguration(dfsCluster));\n      badTlogOutStream = badTlogOutStreamFs.create(hdfsDirPath);\n    }\n\n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n\n    return dfsCluster;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"06595b0c22c7d3075c4104d3820cccf95d9d8a43","date":1576491645,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean,boolean).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean,boolean).mjava","sourceNew":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting, boolean haTesting) throws Exception {\n    checkAssumptions();\n\n    if (!HA_TESTING_ENABLED) haTesting = false;\n\n    DefaultMetricsSystem.setInstance(new FakeMetricsSystem());\n\n    Configuration conf = getBasicConfiguration(new Configuration());\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    // Disable metrics logging for HDFS\n    conf.setInt(\"dfs.namenode.metrics.logger.period.seconds\", 0);\n    conf.setInt(\"dfs.datanode.metrics.logger.period.seconds\", 0);\n\n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", DirectoryFactory.LOCK_TYPE_HDFS);\n\n    // test-files/solr/solr.xml sets this to be 15000. This isn't long enough for HDFS in some cases.\n    System.setProperty(\"socketTimeout\", \"90000\");\n\n    String blockcacheGlobal = System.getProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(random().nextBoolean()));\n    System.setProperty(\"solr.hdfs.blockcache.global\", blockcacheGlobal);\n    // Limit memory usage for HDFS tests\n    if(Boolean.parseBoolean(blockcacheGlobal)) {\n      System.setProperty(\"solr.hdfs.blockcache.blocksperbank\", \"4096\");\n    } else {\n      System.setProperty(\"solr.hdfs.blockcache.blocksperbank\", \"512\");\n      System.setProperty(\"tests.hdfs.numdatanodes\", \"1\");\n    }\n\n    int dataNodes = Integer.getInteger(\"tests.hdfs.numdatanodes\", 2);\n    final MiniDFSCluster.Builder dfsClusterBuilder = new MiniDFSCluster.Builder(conf)\n        .numDataNodes(dataNodes).format(true);\n    if (haTesting) {\n      dfsClusterBuilder.nnTopology(MiniDFSNNTopology.simpleHATopology());\n    }\n\n    MiniDFSCluster dfsCluster = dfsClusterBuilder.build();\n    HdfsUtil.TEST_CONF = getClientConfiguration(dfsCluster);\n    System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n\n    dfsCluster.waitActive();\n\n    if (haTesting) dfsCluster.transitionToActive(0);\n\n    int rndMode = random().nextInt(3);\n    if (safeModeTesting && rndMode == 1) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n\n      int rnd = random().nextInt(10000);\n      Timer timer = new Timer();\n      synchronized (TIMERS_LOCK) {\n        if (timers == null) {\n          timers = new HashMap<>();\n        }\n        timers.put(dfsCluster, timer);\n      }\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n    } else if (haTesting && rndMode == 2) {\n      int rnd = random().nextInt(30000);\n      Timer timer = new Timer();\n      synchronized (TIMERS_LOCK) {\n        if (timers == null) {\n          timers = new HashMap<>();\n        }\n        timers.put(dfsCluster, timer);\n      }\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          // TODO: randomly transition to standby\n//          try {\n//            dfsCluster.transitionToStandby(0);\n//            dfsCluster.transitionToActive(1);\n//          } catch (IOException e) {\n//            throw new RuntimeException();\n//          }\n\n        }\n      }, rnd);\n    }  else {\n      // TODO: we could do much better at testing this\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      badTlogOutStreamFs = FileSystem.get(hdfsDirPath.toUri(), getClientConfiguration(dfsCluster));\n      badTlogOutStream = badTlogOutStreamFs.create(hdfsDirPath);\n    }\n\n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n\n    return dfsCluster;\n  }\n\n","sourceOld":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting, boolean haTesting) throws Exception {\n    checkAssumptions();\n\n    if (!HA_TESTING_ENABLED) haTesting = false;\n\n    Configuration conf = getBasicConfiguration(new Configuration());\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    // Disable metrics logging for HDFS\n    conf.setInt(\"dfs.namenode.metrics.logger.period.seconds\", 0);\n    conf.setInt(\"dfs.datanode.metrics.logger.period.seconds\", 0);\n\n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", DirectoryFactory.LOCK_TYPE_HDFS);\n\n    // test-files/solr/solr.xml sets this to be 15000. This isn't long enough for HDFS in some cases.\n    System.setProperty(\"socketTimeout\", \"90000\");\n\n    String blockcacheGlobal = System.getProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(random().nextBoolean()));\n    System.setProperty(\"solr.hdfs.blockcache.global\", blockcacheGlobal);\n    // Limit memory usage for HDFS tests\n    if(Boolean.parseBoolean(blockcacheGlobal)) {\n      System.setProperty(\"solr.hdfs.blockcache.blocksperbank\", \"4096\");\n    } else {\n      System.setProperty(\"solr.hdfs.blockcache.blocksperbank\", \"512\");\n      System.setProperty(\"tests.hdfs.numdatanodes\", \"1\");\n    }\n\n    int dataNodes = Integer.getInteger(\"tests.hdfs.numdatanodes\", 2);\n    final MiniDFSCluster.Builder dfsClusterBuilder = new MiniDFSCluster.Builder(conf)\n        .numDataNodes(dataNodes).format(true);\n    if (haTesting) {\n      dfsClusterBuilder.nnTopology(MiniDFSNNTopology.simpleHATopology());\n    }\n\n    MiniDFSCluster dfsCluster = dfsClusterBuilder.build();\n    HdfsUtil.TEST_CONF = getClientConfiguration(dfsCluster);\n    System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n\n    dfsCluster.waitActive();\n\n    if (haTesting) dfsCluster.transitionToActive(0);\n\n    int rndMode = random().nextInt(3);\n    if (safeModeTesting && rndMode == 1) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n\n      int rnd = random().nextInt(10000);\n      Timer timer = new Timer();\n      synchronized (TIMERS_LOCK) {\n        if (timers == null) {\n          timers = new HashMap<>();\n        }\n        timers.put(dfsCluster, timer);\n      }\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n    } else if (haTesting && rndMode == 2) {\n      int rnd = random().nextInt(30000);\n      Timer timer = new Timer();\n      synchronized (TIMERS_LOCK) {\n        if (timers == null) {\n          timers = new HashMap<>();\n        }\n        timers.put(dfsCluster, timer);\n      }\n      timer.schedule(new TimerTask() {\n\n        @Override\n        public void run() {\n          // TODO: randomly transition to standby\n//          try {\n//            dfsCluster.transitionToStandby(0);\n//            dfsCluster.transitionToActive(1);\n//          } catch (IOException e) {\n//            throw new RuntimeException();\n//          }\n\n        }\n      }, rnd);\n    }  else {\n      // TODO: we could do much better at testing this\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      badTlogOutStreamFs = FileSystem.get(hdfsDirPath.toUri(), getClientConfiguration(dfsCluster));\n      badTlogOutStream = badTlogOutStreamFs.create(hdfsDirPath);\n    }\n\n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n\n    return dfsCluster;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"2c801a37c38aedbd2ddbd27f2aaeb30cd5c7af0f":["44ca189138a5b6e1989d12ab992fab60e235ddc7"],"7e614eceb81b081076e753ed62268c93c8a2a28e":["cc0ebd2dff6817d143057f270f7b9b3076c2fedf"],"99b97f030e743d8dd4d7685202f71718d7e5af22":["ac0d6b5d90b3c22d588d4e59ec2e9fa6562ebd7f"],"ac0d6b5d90b3c22d588d4e59ec2e9fa6562ebd7f":["54d917b2424232699b135ef134cbd0f8add290c9"],"7c3523a0ab04c3002eee3896c75ea5f10f388bcc":["e50357e583524185222c1c691f5c333b34f7cbb2","54d917b2424232699b135ef134cbd0f8add290c9"],"7fda2c65b4739b1a2340a010e1cbf2c9a7b9ee72":["7e614eceb81b081076e753ed62268c93c8a2a28e"],"add53de9835b2cd1a7a80b4e0036afee171c9fdf":["2c801a37c38aedbd2ddbd27f2aaeb30cd5c7af0f"],"df724d84dab24a0cc54bec95a8680867adc7f171":["6c6b7e01cb749c3b01e226e06085dfb1d9ed8eab","7e614eceb81b081076e753ed62268c93c8a2a28e"],"34b74c124d68d8e306d6ef09624f3cd053b8a2fd":["99b97f030e743d8dd4d7685202f71718d7e5af22"],"54d917b2424232699b135ef134cbd0f8add290c9":["e50357e583524185222c1c691f5c333b34f7cbb2"],"a229cb50768e988c50a2106bdae3a92154f428bf":["add53de9835b2cd1a7a80b4e0036afee171c9fdf","6bdf107cf16be0f22504ae184fed81596665a244"],"6c6b7e01cb749c3b01e226e06085dfb1d9ed8eab":["a229cb50768e988c50a2106bdae3a92154f428bf","cc0ebd2dff6817d143057f270f7b9b3076c2fedf"],"cc0ebd2dff6817d143057f270f7b9b3076c2fedf":["6bdf107cf16be0f22504ae184fed81596665a244"],"6bdf107cf16be0f22504ae184fed81596665a244":["add53de9835b2cd1a7a80b4e0036afee171c9fdf"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"e50357e583524185222c1c691f5c333b34f7cbb2":["3def6e0e7b7566dd7f04a3514e77ee97a40fc78a"],"44ca189138a5b6e1989d12ab992fab60e235ddc7":["34b74c124d68d8e306d6ef09624f3cd053b8a2fd"],"06595b0c22c7d3075c4104d3820cccf95d9d8a43":["df724d84dab24a0cc54bec95a8680867adc7f171","7fda2c65b4739b1a2340a010e1cbf2c9a7b9ee72"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["7fda2c65b4739b1a2340a010e1cbf2c9a7b9ee72"],"3def6e0e7b7566dd7f04a3514e77ee97a40fc78a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"2c801a37c38aedbd2ddbd27f2aaeb30cd5c7af0f":["add53de9835b2cd1a7a80b4e0036afee171c9fdf"],"7e614eceb81b081076e753ed62268c93c8a2a28e":["7fda2c65b4739b1a2340a010e1cbf2c9a7b9ee72","df724d84dab24a0cc54bec95a8680867adc7f171"],"99b97f030e743d8dd4d7685202f71718d7e5af22":["34b74c124d68d8e306d6ef09624f3cd053b8a2fd"],"ac0d6b5d90b3c22d588d4e59ec2e9fa6562ebd7f":["99b97f030e743d8dd4d7685202f71718d7e5af22"],"7c3523a0ab04c3002eee3896c75ea5f10f388bcc":[],"7fda2c65b4739b1a2340a010e1cbf2c9a7b9ee72":["06595b0c22c7d3075c4104d3820cccf95d9d8a43","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"add53de9835b2cd1a7a80b4e0036afee171c9fdf":["a229cb50768e988c50a2106bdae3a92154f428bf","6bdf107cf16be0f22504ae184fed81596665a244"],"df724d84dab24a0cc54bec95a8680867adc7f171":["06595b0c22c7d3075c4104d3820cccf95d9d8a43"],"34b74c124d68d8e306d6ef09624f3cd053b8a2fd":["44ca189138a5b6e1989d12ab992fab60e235ddc7"],"54d917b2424232699b135ef134cbd0f8add290c9":["ac0d6b5d90b3c22d588d4e59ec2e9fa6562ebd7f","7c3523a0ab04c3002eee3896c75ea5f10f388bcc"],"a229cb50768e988c50a2106bdae3a92154f428bf":["6c6b7e01cb749c3b01e226e06085dfb1d9ed8eab"],"6c6b7e01cb749c3b01e226e06085dfb1d9ed8eab":["df724d84dab24a0cc54bec95a8680867adc7f171"],"cc0ebd2dff6817d143057f270f7b9b3076c2fedf":["7e614eceb81b081076e753ed62268c93c8a2a28e","6c6b7e01cb749c3b01e226e06085dfb1d9ed8eab"],"6bdf107cf16be0f22504ae184fed81596665a244":["a229cb50768e988c50a2106bdae3a92154f428bf","cc0ebd2dff6817d143057f270f7b9b3076c2fedf"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3def6e0e7b7566dd7f04a3514e77ee97a40fc78a"],"e50357e583524185222c1c691f5c333b34f7cbb2":["7c3523a0ab04c3002eee3896c75ea5f10f388bcc","54d917b2424232699b135ef134cbd0f8add290c9"],"44ca189138a5b6e1989d12ab992fab60e235ddc7":["2c801a37c38aedbd2ddbd27f2aaeb30cd5c7af0f"],"06595b0c22c7d3075c4104d3820cccf95d9d8a43":[],"3def6e0e7b7566dd7f04a3514e77ee97a40fc78a":["e50357e583524185222c1c691f5c333b34f7cbb2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["7c3523a0ab04c3002eee3896c75ea5f10f388bcc","06595b0c22c7d3075c4104d3820cccf95d9d8a43","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}