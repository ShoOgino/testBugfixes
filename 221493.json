{"path":"lucene/sandbox/src/java/org/apache/lucene/rangetree/RangeTreeWriter#build(int,int,PathSlice,IndexOutput,long,long,long[],long[]).mjava","commits":[{"id":"8c12c43c449a172df0f2b122918f0f5fc0e9a470","date":1438415333,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/sandbox/src/java/org/apache/lucene/rangetree/RangeTreeWriter#build(int,int,PathSlice,IndexOutput,long,long,long[],long[]).mjava","pathOld":"/dev/null","sourceNew":"  /** The incoming PathSlice for the dim we will split is already partitioned/sorted. */\n  private void build(int nodeID, int leafNodeOffset,\n                     PathSlice source,\n                     IndexOutput out,\n                     long minValue, long maxValue,\n                     long[] blockMinValues,\n                     long[] leafBlockFPs) throws IOException {\n\n    long count = source.count;\n\n    if (source.writer instanceof OfflineSliceWriter && count <= maxValuesSortInHeap) {\n      // Cutover to heap:\n      SliceWriter writer = new HeapSliceWriter((int) count);\n      SliceReader reader = source.writer.getReader(source.start);\n      for(int i=0;i<count;i++) {\n        boolean hasNext = reader.next();\n        assert hasNext;\n        writer.append(reader.value(), reader.ord(), reader.docID());\n      }\n      source = new PathSlice(writer, 0, count);\n    }\n\n    // We should never hit dead-end nodes on recursion even in the adversarial cases:\n    assert count > 0;\n\n    if (nodeID >= leafNodeOffset) {\n      // Leaf node: write block\n      assert maxValue >= minValue;\n\n      //System.out.println(\"\\nleaf:\\n  lat range: \" + ((long) maxLatEnc-minLatEnc));\n      //System.out.println(\"  lon range: \" + ((long) maxLonEnc-minLonEnc));\n\n      // Sort by docID in the leaf so we can .or(DISI) at search time:\n      SliceReader reader = source.writer.getReader(source.start);\n\n      int[] docIDs = new int[(int) count];\n\n      boolean success = false;\n      try {\n        for (int i=0;i<source.count;i++) {\n\n          // NOTE: we discard ord at this point; we only needed it temporarily\n          // during building to uniquely identify each point to properly handle\n          // the multi-valued case (one docID having multiple values):\n\n          // We also discard lat/lon, since at search time, we reside on the\n          // wrapped doc values for this:\n\n          boolean result = reader.next();\n          assert result;\n          docIDs[i] = reader.docID();\n        }\n        success = true;\n      } finally {\n        if (success) {\n          IOUtils.close(reader);\n        } else {\n          IOUtils.closeWhileHandlingException(reader);\n        }\n      }\n\n      // TODO: not clear we need to do this anymore (we used to make a DISI over\n      // the block at search time), but maybe it buys some memory\n      // locality/sequentiality at search time?\n      Arrays.sort(docIDs);\n\n      // Dedup docIDs: for the multi-valued case where more than one value for the doc\n      // wound up in this leaf cell, we only need to store the docID once:\n      int lastDocID = -1;\n      int uniqueCount = 0;\n      for(int i=0;i<docIDs.length;i++) {\n        int docID = docIDs[i];\n        if (docID != lastDocID) {\n          uniqueCount++;\n          lastDocID = docID;\n        }\n      }\n      assert uniqueCount <= count;\n\n      // TODO: in theory we could compute exactly what this fp will be, since we fixed-width (writeInt) encode docID, and up-front we know\n      // how many docIDs are in every leaf since we don't do anything special about multiple splitValue boundary case?\n      long startFP = out.getFilePointer();\n      out.writeVInt(uniqueCount);\n\n      // Save the block file pointer:\n      int blockID = nodeID - leafNodeOffset;\n      leafBlockFPs[blockID] = startFP;\n      //System.out.println(\"    leafFP=\" + startFP);\n\n      blockMinValues[blockID] = minValue;\n\n      lastDocID = -1;\n      for (int i=0;i<docIDs.length;i++) {\n        // Absolute int encode; with \"vInt of deltas\" encoding, the .kdd size dropped from\n        // 697 MB -> 539 MB, but query time for 225 queries went from 1.65 sec -> 2.64 sec.\n        // I think if we also indexed prefix terms here we could do less costly compression\n        // on those lists:\n        int docID = docIDs[i];\n        if (docID != lastDocID) {\n          out.writeInt(docID);\n          lastDocID = docID;\n        }\n      }\n      //long endFP = out.getFilePointer();\n      //System.out.println(\"  bytes/doc: \" + ((endFP - startFP) / count));\n    } else {\n      // Inner node: sort, partition/recurse\n\n      assert nodeID < blockMinValues.length: \"nodeID=\" + nodeID + \" blockMinValues.length=\" + blockMinValues.length;\n\n      assert source.count == count;\n\n      long leftCount = source.count / 2;\n\n      // NOTE: we don't tweak leftCount for the boundary cases, which means at search time if we are looking for exactly splitValue then we\n      // must search both left and right trees:\n      long splitValue = getSplitValue(source, leftCount, minValue, maxValue);\n\n      build(2*nodeID, leafNodeOffset,\n            new PathSlice(source.writer, source.start, leftCount),\n            out,\n            minValue, splitValue,\n            blockMinValues, leafBlockFPs);\n\n      build(2*nodeID+1, leafNodeOffset,\n            new PathSlice(source.writer, source.start+leftCount, count-leftCount),\n            out,\n            splitValue, maxValue,\n            blockMinValues, leafBlockFPs);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4f558588ddbab152a12681d16572d483aa59616b","date":1439595052,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/sandbox/src/java/org/apache/lucene/rangetree/RangeTreeWriter#build(int,int,PathSlice,IndexOutput,long,long,long[],long[]).mjava","pathOld":"lucene/sandbox/src/java/org/apache/lucene/rangetree/RangeTreeWriter#build(int,int,PathSlice,IndexOutput,long,long,long[],long[]).mjava","sourceNew":"  /** The incoming PathSlice for the dim we will split is already partitioned/sorted. */\n  private void build(int nodeID, int leafNodeOffset,\n                     PathSlice source,\n                     IndexOutput out,\n                     long minValue, long maxValue,\n                     long[] blockMinValues,\n                     long[] leafBlockFPs) throws IOException {\n\n    long count = source.count;\n\n    if (source.writer instanceof OfflineSliceWriter && count <= maxValuesSortInHeap) {\n      // Cutover to heap:\n      SliceWriter writer = new HeapSliceWriter((int) count);\n      SliceReader reader = source.writer.getReader(source.start);\n      for(int i=0;i<count;i++) {\n        boolean hasNext = reader.next();\n        assert hasNext;\n        writer.append(reader.value(), reader.ord(), reader.docID());\n      }\n      writer.close();\n      source = new PathSlice(writer, 0, count);\n    }\n\n    // We should never hit dead-end nodes on recursion even in the adversarial cases:\n    assert count > 0;\n\n    if (nodeID >= leafNodeOffset) {\n      // Leaf node: write block\n      assert maxValue >= minValue;\n\n      //System.out.println(\"\\nleaf:\\n  lat range: \" + ((long) maxLatEnc-minLatEnc));\n      //System.out.println(\"  lon range: \" + ((long) maxLonEnc-minLonEnc));\n\n      // Sort by docID in the leaf so we can .or(DISI) at search time:\n      SliceReader reader = source.writer.getReader(source.start);\n\n      int[] docIDs = new int[(int) count];\n\n      boolean success = false;\n      try {\n        for (int i=0;i<source.count;i++) {\n\n          // NOTE: we discard ord at this point; we only needed it temporarily\n          // during building to uniquely identify each point to properly handle\n          // the multi-valued case (one docID having multiple values):\n\n          // We also discard lat/lon, since at search time, we reside on the\n          // wrapped doc values for this:\n\n          boolean result = reader.next();\n          assert result;\n          docIDs[i] = reader.docID();\n        }\n        success = true;\n      } finally {\n        if (success) {\n          IOUtils.close(reader);\n        } else {\n          IOUtils.closeWhileHandlingException(reader);\n        }\n      }\n\n      // TODO: not clear we need to do this anymore (we used to make a DISI over\n      // the block at search time), but maybe it buys some memory\n      // locality/sequentiality at search time?\n      Arrays.sort(docIDs);\n\n      // Dedup docIDs: for the multi-valued case where more than one value for the doc\n      // wound up in this leaf cell, we only need to store the docID once:\n      int lastDocID = -1;\n      int uniqueCount = 0;\n      for(int i=0;i<docIDs.length;i++) {\n        int docID = docIDs[i];\n        if (docID != lastDocID) {\n          uniqueCount++;\n          lastDocID = docID;\n        }\n      }\n      assert uniqueCount <= count;\n\n      // TODO: in theory we could compute exactly what this fp will be, since we fixed-width (writeInt) encode docID, and up-front we know\n      // how many docIDs are in every leaf since we don't do anything special about multiple splitValue boundary case?\n      long startFP = out.getFilePointer();\n      out.writeVInt(uniqueCount);\n\n      // Save the block file pointer:\n      int blockID = nodeID - leafNodeOffset;\n      leafBlockFPs[blockID] = startFP;\n      //System.out.println(\"    leafFP=\" + startFP);\n\n      blockMinValues[blockID] = minValue;\n\n      lastDocID = -1;\n      for (int i=0;i<docIDs.length;i++) {\n        // Absolute int encode; with \"vInt of deltas\" encoding, the .kdd size dropped from\n        // 697 MB -> 539 MB, but query time for 225 queries went from 1.65 sec -> 2.64 sec.\n        // I think if we also indexed prefix terms here we could do less costly compression\n        // on those lists:\n        int docID = docIDs[i];\n        if (docID != lastDocID) {\n          out.writeInt(docID);\n          lastDocID = docID;\n        }\n      }\n      //long endFP = out.getFilePointer();\n      //System.out.println(\"  bytes/doc: \" + ((endFP - startFP) / count));\n    } else {\n      // Inner node: sort, partition/recurse\n\n      assert nodeID < blockMinValues.length: \"nodeID=\" + nodeID + \" blockMinValues.length=\" + blockMinValues.length;\n\n      assert source.count == count;\n\n      long leftCount = source.count / 2;\n\n      // NOTE: we don't tweak leftCount for the boundary cases, which means at search time if we are looking for exactly splitValue then we\n      // must search both left and right trees:\n      long splitValue = getSplitValue(source, leftCount, minValue, maxValue);\n\n      build(2*nodeID, leafNodeOffset,\n            new PathSlice(source.writer, source.start, leftCount),\n            out,\n            minValue, splitValue,\n            blockMinValues, leafBlockFPs);\n\n      build(2*nodeID+1, leafNodeOffset,\n            new PathSlice(source.writer, source.start+leftCount, count-leftCount),\n            out,\n            splitValue, maxValue,\n            blockMinValues, leafBlockFPs);\n    }\n  }\n\n","sourceOld":"  /** The incoming PathSlice for the dim we will split is already partitioned/sorted. */\n  private void build(int nodeID, int leafNodeOffset,\n                     PathSlice source,\n                     IndexOutput out,\n                     long minValue, long maxValue,\n                     long[] blockMinValues,\n                     long[] leafBlockFPs) throws IOException {\n\n    long count = source.count;\n\n    if (source.writer instanceof OfflineSliceWriter && count <= maxValuesSortInHeap) {\n      // Cutover to heap:\n      SliceWriter writer = new HeapSliceWriter((int) count);\n      SliceReader reader = source.writer.getReader(source.start);\n      for(int i=0;i<count;i++) {\n        boolean hasNext = reader.next();\n        assert hasNext;\n        writer.append(reader.value(), reader.ord(), reader.docID());\n      }\n      source = new PathSlice(writer, 0, count);\n    }\n\n    // We should never hit dead-end nodes on recursion even in the adversarial cases:\n    assert count > 0;\n\n    if (nodeID >= leafNodeOffset) {\n      // Leaf node: write block\n      assert maxValue >= minValue;\n\n      //System.out.println(\"\\nleaf:\\n  lat range: \" + ((long) maxLatEnc-minLatEnc));\n      //System.out.println(\"  lon range: \" + ((long) maxLonEnc-minLonEnc));\n\n      // Sort by docID in the leaf so we can .or(DISI) at search time:\n      SliceReader reader = source.writer.getReader(source.start);\n\n      int[] docIDs = new int[(int) count];\n\n      boolean success = false;\n      try {\n        for (int i=0;i<source.count;i++) {\n\n          // NOTE: we discard ord at this point; we only needed it temporarily\n          // during building to uniquely identify each point to properly handle\n          // the multi-valued case (one docID having multiple values):\n\n          // We also discard lat/lon, since at search time, we reside on the\n          // wrapped doc values for this:\n\n          boolean result = reader.next();\n          assert result;\n          docIDs[i] = reader.docID();\n        }\n        success = true;\n      } finally {\n        if (success) {\n          IOUtils.close(reader);\n        } else {\n          IOUtils.closeWhileHandlingException(reader);\n        }\n      }\n\n      // TODO: not clear we need to do this anymore (we used to make a DISI over\n      // the block at search time), but maybe it buys some memory\n      // locality/sequentiality at search time?\n      Arrays.sort(docIDs);\n\n      // Dedup docIDs: for the multi-valued case where more than one value for the doc\n      // wound up in this leaf cell, we only need to store the docID once:\n      int lastDocID = -1;\n      int uniqueCount = 0;\n      for(int i=0;i<docIDs.length;i++) {\n        int docID = docIDs[i];\n        if (docID != lastDocID) {\n          uniqueCount++;\n          lastDocID = docID;\n        }\n      }\n      assert uniqueCount <= count;\n\n      // TODO: in theory we could compute exactly what this fp will be, since we fixed-width (writeInt) encode docID, and up-front we know\n      // how many docIDs are in every leaf since we don't do anything special about multiple splitValue boundary case?\n      long startFP = out.getFilePointer();\n      out.writeVInt(uniqueCount);\n\n      // Save the block file pointer:\n      int blockID = nodeID - leafNodeOffset;\n      leafBlockFPs[blockID] = startFP;\n      //System.out.println(\"    leafFP=\" + startFP);\n\n      blockMinValues[blockID] = minValue;\n\n      lastDocID = -1;\n      for (int i=0;i<docIDs.length;i++) {\n        // Absolute int encode; with \"vInt of deltas\" encoding, the .kdd size dropped from\n        // 697 MB -> 539 MB, but query time for 225 queries went from 1.65 sec -> 2.64 sec.\n        // I think if we also indexed prefix terms here we could do less costly compression\n        // on those lists:\n        int docID = docIDs[i];\n        if (docID != lastDocID) {\n          out.writeInt(docID);\n          lastDocID = docID;\n        }\n      }\n      //long endFP = out.getFilePointer();\n      //System.out.println(\"  bytes/doc: \" + ((endFP - startFP) / count));\n    } else {\n      // Inner node: sort, partition/recurse\n\n      assert nodeID < blockMinValues.length: \"nodeID=\" + nodeID + \" blockMinValues.length=\" + blockMinValues.length;\n\n      assert source.count == count;\n\n      long leftCount = source.count / 2;\n\n      // NOTE: we don't tweak leftCount for the boundary cases, which means at search time if we are looking for exactly splitValue then we\n      // must search both left and right trees:\n      long splitValue = getSplitValue(source, leftCount, minValue, maxValue);\n\n      build(2*nodeID, leafNodeOffset,\n            new PathSlice(source.writer, source.start, leftCount),\n            out,\n            minValue, splitValue,\n            blockMinValues, leafBlockFPs);\n\n      build(2*nodeID+1, leafNodeOffset,\n            new PathSlice(source.writer, source.start+leftCount, count-leftCount),\n            out,\n            splitValue, maxValue,\n            blockMinValues, leafBlockFPs);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8ccfbb043f2ebf23df8782dd32a68ff1b399c3d2","date":1443129829,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/sandbox/src/java/org/apache/lucene/rangetree/RangeTreeWriter#build(int,int,PathSlice,IndexOutput,long,long,long[],long[]).mjava","pathOld":"lucene/sandbox/src/java/org/apache/lucene/rangetree/RangeTreeWriter#build(int,int,PathSlice,IndexOutput,long,long,long[],long[]).mjava","sourceNew":"  /** The incoming PathSlice for the dim we will split is already partitioned/sorted. */\n  private void build(int nodeID, int leafNodeOffset,\n                     PathSlice source,\n                     IndexOutput out,\n                     long minValue, long maxValue,\n                     long[] blockMinValues,\n                     long[] leafBlockFPs) throws IOException {\n\n    long count = source.count;\n\n    if (source.writer instanceof OfflineSliceWriter && count <= maxValuesSortInHeap) {\n      // Cutover to heap:\n      SliceWriter writer = new HeapSliceWriter((int) count);\n      SliceReader reader = source.writer.getReader(source.start);\n      try {\n        for(int i=0;i<count;i++) {\n          boolean hasNext = reader.next();\n          assert hasNext;\n          writer.append(reader.value(), reader.ord(), reader.docID());\n        }\n      } finally {\n        IOUtils.close(reader, writer);\n      }\n      source = new PathSlice(writer, 0, count);\n    }\n\n    // We should never hit dead-end nodes on recursion even in the adversarial cases:\n    assert count > 0;\n\n    if (nodeID >= leafNodeOffset) {\n      // Leaf node: write block\n      assert maxValue >= minValue;\n\n      //System.out.println(\"\\nleaf:\\n  lat range: \" + ((long) maxLatEnc-minLatEnc));\n      //System.out.println(\"  lon range: \" + ((long) maxLonEnc-minLonEnc));\n\n      // Sort by docID in the leaf so we can .or(DISI) at search time:\n      SliceReader reader = source.writer.getReader(source.start);\n\n      int[] docIDs = new int[(int) count];\n\n      boolean success = false;\n      try {\n        for (int i=0;i<source.count;i++) {\n\n          // NOTE: we discard ord at this point; we only needed it temporarily\n          // during building to uniquely identify each point to properly handle\n          // the multi-valued case (one docID having multiple values):\n\n          // We also discard lat/lon, since at search time, we reside on the\n          // wrapped doc values for this:\n\n          boolean result = reader.next();\n          assert result;\n          docIDs[i] = reader.docID();\n        }\n        success = true;\n      } finally {\n        if (success) {\n          IOUtils.close(reader);\n        } else {\n          IOUtils.closeWhileHandlingException(reader);\n        }\n      }\n\n      // TODO: not clear we need to do this anymore (we used to make a DISI over\n      // the block at search time), but maybe it buys some memory\n      // locality/sequentiality at search time?\n      Arrays.sort(docIDs);\n\n      // Dedup docIDs: for the multi-valued case where more than one value for the doc\n      // wound up in this leaf cell, we only need to store the docID once:\n      int lastDocID = -1;\n      int uniqueCount = 0;\n      for(int i=0;i<docIDs.length;i++) {\n        int docID = docIDs[i];\n        if (docID != lastDocID) {\n          uniqueCount++;\n          lastDocID = docID;\n        }\n      }\n      assert uniqueCount <= count;\n\n      // TODO: in theory we could compute exactly what this fp will be, since we fixed-width (writeInt) encode docID, and up-front we know\n      // how many docIDs are in every leaf since we don't do anything special about multiple splitValue boundary case?\n      long startFP = out.getFilePointer();\n      out.writeVInt(uniqueCount);\n\n      // Save the block file pointer:\n      int blockID = nodeID - leafNodeOffset;\n      leafBlockFPs[blockID] = startFP;\n      //System.out.println(\"    leafFP=\" + startFP);\n\n      blockMinValues[blockID] = minValue;\n\n      lastDocID = -1;\n      for (int i=0;i<docIDs.length;i++) {\n        // Absolute int encode; with \"vInt of deltas\" encoding, the .kdd size dropped from\n        // 697 MB -> 539 MB, but query time for 225 queries went from 1.65 sec -> 2.64 sec.\n        // I think if we also indexed prefix terms here we could do less costly compression\n        // on those lists:\n        int docID = docIDs[i];\n        if (docID != lastDocID) {\n          out.writeInt(docID);\n          lastDocID = docID;\n        }\n      }\n      //long endFP = out.getFilePointer();\n      //System.out.println(\"  bytes/doc: \" + ((endFP - startFP) / count));\n    } else {\n      // Inner node: sort, partition/recurse\n\n      assert nodeID < blockMinValues.length: \"nodeID=\" + nodeID + \" blockMinValues.length=\" + blockMinValues.length;\n\n      assert source.count == count;\n\n      long leftCount = source.count / 2;\n\n      // NOTE: we don't tweak leftCount for the boundary cases, which means at search time if we are looking for exactly splitValue then we\n      // must search both left and right trees:\n      long splitValue = getSplitValue(source, leftCount, minValue, maxValue);\n\n      build(2*nodeID, leafNodeOffset,\n            new PathSlice(source.writer, source.start, leftCount),\n            out,\n            minValue, splitValue,\n            blockMinValues, leafBlockFPs);\n\n      build(2*nodeID+1, leafNodeOffset,\n            new PathSlice(source.writer, source.start+leftCount, count-leftCount),\n            out,\n            splitValue, maxValue,\n            blockMinValues, leafBlockFPs);\n    }\n  }\n\n","sourceOld":"  /** The incoming PathSlice for the dim we will split is already partitioned/sorted. */\n  private void build(int nodeID, int leafNodeOffset,\n                     PathSlice source,\n                     IndexOutput out,\n                     long minValue, long maxValue,\n                     long[] blockMinValues,\n                     long[] leafBlockFPs) throws IOException {\n\n    long count = source.count;\n\n    if (source.writer instanceof OfflineSliceWriter && count <= maxValuesSortInHeap) {\n      // Cutover to heap:\n      SliceWriter writer = new HeapSliceWriter((int) count);\n      SliceReader reader = source.writer.getReader(source.start);\n      for(int i=0;i<count;i++) {\n        boolean hasNext = reader.next();\n        assert hasNext;\n        writer.append(reader.value(), reader.ord(), reader.docID());\n      }\n      writer.close();\n      source = new PathSlice(writer, 0, count);\n    }\n\n    // We should never hit dead-end nodes on recursion even in the adversarial cases:\n    assert count > 0;\n\n    if (nodeID >= leafNodeOffset) {\n      // Leaf node: write block\n      assert maxValue >= minValue;\n\n      //System.out.println(\"\\nleaf:\\n  lat range: \" + ((long) maxLatEnc-minLatEnc));\n      //System.out.println(\"  lon range: \" + ((long) maxLonEnc-minLonEnc));\n\n      // Sort by docID in the leaf so we can .or(DISI) at search time:\n      SliceReader reader = source.writer.getReader(source.start);\n\n      int[] docIDs = new int[(int) count];\n\n      boolean success = false;\n      try {\n        for (int i=0;i<source.count;i++) {\n\n          // NOTE: we discard ord at this point; we only needed it temporarily\n          // during building to uniquely identify each point to properly handle\n          // the multi-valued case (one docID having multiple values):\n\n          // We also discard lat/lon, since at search time, we reside on the\n          // wrapped doc values for this:\n\n          boolean result = reader.next();\n          assert result;\n          docIDs[i] = reader.docID();\n        }\n        success = true;\n      } finally {\n        if (success) {\n          IOUtils.close(reader);\n        } else {\n          IOUtils.closeWhileHandlingException(reader);\n        }\n      }\n\n      // TODO: not clear we need to do this anymore (we used to make a DISI over\n      // the block at search time), but maybe it buys some memory\n      // locality/sequentiality at search time?\n      Arrays.sort(docIDs);\n\n      // Dedup docIDs: for the multi-valued case where more than one value for the doc\n      // wound up in this leaf cell, we only need to store the docID once:\n      int lastDocID = -1;\n      int uniqueCount = 0;\n      for(int i=0;i<docIDs.length;i++) {\n        int docID = docIDs[i];\n        if (docID != lastDocID) {\n          uniqueCount++;\n          lastDocID = docID;\n        }\n      }\n      assert uniqueCount <= count;\n\n      // TODO: in theory we could compute exactly what this fp will be, since we fixed-width (writeInt) encode docID, and up-front we know\n      // how many docIDs are in every leaf since we don't do anything special about multiple splitValue boundary case?\n      long startFP = out.getFilePointer();\n      out.writeVInt(uniqueCount);\n\n      // Save the block file pointer:\n      int blockID = nodeID - leafNodeOffset;\n      leafBlockFPs[blockID] = startFP;\n      //System.out.println(\"    leafFP=\" + startFP);\n\n      blockMinValues[blockID] = minValue;\n\n      lastDocID = -1;\n      for (int i=0;i<docIDs.length;i++) {\n        // Absolute int encode; with \"vInt of deltas\" encoding, the .kdd size dropped from\n        // 697 MB -> 539 MB, but query time for 225 queries went from 1.65 sec -> 2.64 sec.\n        // I think if we also indexed prefix terms here we could do less costly compression\n        // on those lists:\n        int docID = docIDs[i];\n        if (docID != lastDocID) {\n          out.writeInt(docID);\n          lastDocID = docID;\n        }\n      }\n      //long endFP = out.getFilePointer();\n      //System.out.println(\"  bytes/doc: \" + ((endFP - startFP) / count));\n    } else {\n      // Inner node: sort, partition/recurse\n\n      assert nodeID < blockMinValues.length: \"nodeID=\" + nodeID + \" blockMinValues.length=\" + blockMinValues.length;\n\n      assert source.count == count;\n\n      long leftCount = source.count / 2;\n\n      // NOTE: we don't tweak leftCount for the boundary cases, which means at search time if we are looking for exactly splitValue then we\n      // must search both left and right trees:\n      long splitValue = getSplitValue(source, leftCount, minValue, maxValue);\n\n      build(2*nodeID, leafNodeOffset,\n            new PathSlice(source.writer, source.start, leftCount),\n            out,\n            minValue, splitValue,\n            blockMinValues, leafBlockFPs);\n\n      build(2*nodeID+1, leafNodeOffset,\n            new PathSlice(source.writer, source.start+leftCount, count-leftCount),\n            out,\n            splitValue, maxValue,\n            blockMinValues, leafBlockFPs);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1904709ea0185dc04e3d77ea01c79e909caf2796","date":1447006699,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/sandbox/src/java/org/apache/lucene/rangetree/RangeTreeWriter#build(int,int,PathSlice,IndexOutput,long,long,long[],long[]).mjava","sourceNew":null,"sourceOld":"  /** The incoming PathSlice for the dim we will split is already partitioned/sorted. */\n  private void build(int nodeID, int leafNodeOffset,\n                     PathSlice source,\n                     IndexOutput out,\n                     long minValue, long maxValue,\n                     long[] blockMinValues,\n                     long[] leafBlockFPs) throws IOException {\n\n    long count = source.count;\n\n    if (source.writer instanceof OfflineSliceWriter && count <= maxValuesSortInHeap) {\n      // Cutover to heap:\n      SliceWriter writer = new HeapSliceWriter((int) count);\n      SliceReader reader = source.writer.getReader(source.start);\n      try {\n        for(int i=0;i<count;i++) {\n          boolean hasNext = reader.next();\n          assert hasNext;\n          writer.append(reader.value(), reader.ord(), reader.docID());\n        }\n      } finally {\n        IOUtils.close(reader, writer);\n      }\n      source = new PathSlice(writer, 0, count);\n    }\n\n    // We should never hit dead-end nodes on recursion even in the adversarial cases:\n    assert count > 0;\n\n    if (nodeID >= leafNodeOffset) {\n      // Leaf node: write block\n      assert maxValue >= minValue;\n\n      //System.out.println(\"\\nleaf:\\n  lat range: \" + ((long) maxLatEnc-minLatEnc));\n      //System.out.println(\"  lon range: \" + ((long) maxLonEnc-minLonEnc));\n\n      // Sort by docID in the leaf so we can .or(DISI) at search time:\n      SliceReader reader = source.writer.getReader(source.start);\n\n      int[] docIDs = new int[(int) count];\n\n      boolean success = false;\n      try {\n        for (int i=0;i<source.count;i++) {\n\n          // NOTE: we discard ord at this point; we only needed it temporarily\n          // during building to uniquely identify each point to properly handle\n          // the multi-valued case (one docID having multiple values):\n\n          // We also discard lat/lon, since at search time, we reside on the\n          // wrapped doc values for this:\n\n          boolean result = reader.next();\n          assert result;\n          docIDs[i] = reader.docID();\n        }\n        success = true;\n      } finally {\n        if (success) {\n          IOUtils.close(reader);\n        } else {\n          IOUtils.closeWhileHandlingException(reader);\n        }\n      }\n\n      // TODO: not clear we need to do this anymore (we used to make a DISI over\n      // the block at search time), but maybe it buys some memory\n      // locality/sequentiality at search time?\n      Arrays.sort(docIDs);\n\n      // Dedup docIDs: for the multi-valued case where more than one value for the doc\n      // wound up in this leaf cell, we only need to store the docID once:\n      int lastDocID = -1;\n      int uniqueCount = 0;\n      for(int i=0;i<docIDs.length;i++) {\n        int docID = docIDs[i];\n        if (docID != lastDocID) {\n          uniqueCount++;\n          lastDocID = docID;\n        }\n      }\n      assert uniqueCount <= count;\n\n      // TODO: in theory we could compute exactly what this fp will be, since we fixed-width (writeInt) encode docID, and up-front we know\n      // how many docIDs are in every leaf since we don't do anything special about multiple splitValue boundary case?\n      long startFP = out.getFilePointer();\n      out.writeVInt(uniqueCount);\n\n      // Save the block file pointer:\n      int blockID = nodeID - leafNodeOffset;\n      leafBlockFPs[blockID] = startFP;\n      //System.out.println(\"    leafFP=\" + startFP);\n\n      blockMinValues[blockID] = minValue;\n\n      lastDocID = -1;\n      for (int i=0;i<docIDs.length;i++) {\n        // Absolute int encode; with \"vInt of deltas\" encoding, the .kdd size dropped from\n        // 697 MB -> 539 MB, but query time for 225 queries went from 1.65 sec -> 2.64 sec.\n        // I think if we also indexed prefix terms here we could do less costly compression\n        // on those lists:\n        int docID = docIDs[i];\n        if (docID != lastDocID) {\n          out.writeInt(docID);\n          lastDocID = docID;\n        }\n      }\n      //long endFP = out.getFilePointer();\n      //System.out.println(\"  bytes/doc: \" + ((endFP - startFP) / count));\n    } else {\n      // Inner node: sort, partition/recurse\n\n      assert nodeID < blockMinValues.length: \"nodeID=\" + nodeID + \" blockMinValues.length=\" + blockMinValues.length;\n\n      assert source.count == count;\n\n      long leftCount = source.count / 2;\n\n      // NOTE: we don't tweak leftCount for the boundary cases, which means at search time if we are looking for exactly splitValue then we\n      // must search both left and right trees:\n      long splitValue = getSplitValue(source, leftCount, minValue, maxValue);\n\n      build(2*nodeID, leafNodeOffset,\n            new PathSlice(source.writer, source.start, leftCount),\n            out,\n            minValue, splitValue,\n            blockMinValues, leafBlockFPs);\n\n      build(2*nodeID+1, leafNodeOffset,\n            new PathSlice(source.writer, source.start+leftCount, count-leftCount),\n            out,\n            splitValue, maxValue,\n            blockMinValues, leafBlockFPs);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"8c12c43c449a172df0f2b122918f0f5fc0e9a470":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"8ccfbb043f2ebf23df8782dd32a68ff1b399c3d2":["4f558588ddbab152a12681d16572d483aa59616b"],"4f558588ddbab152a12681d16572d483aa59616b":["8c12c43c449a172df0f2b122918f0f5fc0e9a470"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"1904709ea0185dc04e3d77ea01c79e909caf2796":["8ccfbb043f2ebf23df8782dd32a68ff1b399c3d2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["1904709ea0185dc04e3d77ea01c79e909caf2796"]},"commit2Childs":{"8c12c43c449a172df0f2b122918f0f5fc0e9a470":["4f558588ddbab152a12681d16572d483aa59616b"],"8ccfbb043f2ebf23df8782dd32a68ff1b399c3d2":["1904709ea0185dc04e3d77ea01c79e909caf2796"],"4f558588ddbab152a12681d16572d483aa59616b":["8ccfbb043f2ebf23df8782dd32a68ff1b399c3d2"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["8c12c43c449a172df0f2b122918f0f5fc0e9a470"],"1904709ea0185dc04e3d77ea01c79e909caf2796":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}