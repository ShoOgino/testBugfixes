{"path":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean,boolean).mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean,boolean).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean,boolean).mjava","sourceNew":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private synchronized final boolean doFlush(boolean flushDocStores, boolean flushDeletes) throws CorruptIndexException, IOException {\n    try {\n      return doFlushInternal(flushDocStores, flushDeletes);\n    } finally {\n      docWriter.clearFlushPending();\n    }\n  }\n\n","sourceOld":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private synchronized final boolean doFlush(boolean flushDocStores, boolean flushDeletes) throws CorruptIndexException, IOException {\n    try {\n      return doFlushInternal(flushDocStores, flushDeletes);\n    } finally {\n      docWriter.clearFlushPending();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"579b8e50af5927e2366553f5e237b2c6e46b258e","date":1274137954,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean,boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean,boolean).mjava","sourceNew":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private synchronized final boolean doFlush(boolean flushDocStores, boolean flushDeletes) throws CorruptIndexException, IOException {\n    try {\n      try {\n        return doFlushInternal(flushDocStores, flushDeletes);\n      } finally {\n        docWriter.balanceRAM();\n      }\n    } finally {\n      docWriter.clearFlushPending();\n    }\n  }\n\n","sourceOld":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private synchronized final boolean doFlush(boolean flushDocStores, boolean flushDeletes) throws CorruptIndexException, IOException {\n    try {\n      return doFlushInternal(flushDocStores, flushDeletes);\n    } finally {\n      docWriter.clearFlushPending();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9b832cbed6eb3d54a8bb9339296bdda8eeb53014","date":1279708040,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean,boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean,boolean).mjava","sourceNew":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private synchronized final boolean doFlush(boolean flushDocStores, boolean flushDeletes) throws CorruptIndexException, IOException {\n    return docWriter.flushAllThreads(flushDocStores, flushDeletes);\n    // nocommit\n//    try {\n//      try {\n//        return doFlushInternal(flushDocStores, flushDeletes);\n//      } finally {\n//        docWriter.balanceRAM();\n//      }\n//    } finally {\n//      docWriter.clearFlushPending();\n//    }\n  }\n\n","sourceOld":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private synchronized final boolean doFlush(boolean flushDocStores, boolean flushDeletes) throws CorruptIndexException, IOException {\n    try {\n      try {\n        return doFlushInternal(flushDocStores, flushDeletes);\n      } finally {\n        docWriter.balanceRAM();\n      }\n    } finally {\n      docWriter.clearFlushPending();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"334c1175813aea771a71728cd2c4ee4754fd0603","date":1279710173,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean,boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean,boolean).mjava","sourceNew":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private synchronized final boolean doFlush(boolean flushDocStores, boolean flushDeletes) throws CorruptIndexException, IOException {\n    try {\n      try {\n        return doFlushInternal(flushDocStores, flushDeletes);\n      } finally {\n        docWriter.balanceRAM();\n      }\n    } finally {\n      docWriter.clearFlushPending();\n    }\n  }\n\n","sourceOld":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private synchronized final boolean doFlush(boolean flushDocStores, boolean flushDeletes) throws CorruptIndexException, IOException {\n    return docWriter.flushAllThreads(flushDocStores, flushDeletes);\n    // nocommit\n//    try {\n//      try {\n//        return doFlushInternal(flushDocStores, flushDeletes);\n//      } finally {\n//        docWriter.balanceRAM();\n//      }\n//    } finally {\n//      docWriter.clearFlushPending();\n//    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8fe956d65251358d755c56f14fe8380644790e47","date":1279711318,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean,boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean,boolean).mjava","sourceNew":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private synchronized final boolean doFlush(boolean flushDocStores, boolean flushDeletes) throws CorruptIndexException, IOException {\n    return docWriter.flushAllThreads(flushDocStores, flushDeletes);\n    // nocommit\n//    try {\n//      try {\n//        return doFlushInternal(flushDocStores, flushDeletes);\n//      } finally {\n//        docWriter.balanceRAM();\n//      }\n//    } finally {\n//      docWriter.clearFlushPending();\n//    }\n  }\n\n","sourceOld":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private synchronized final boolean doFlush(boolean flushDocStores, boolean flushDeletes) throws CorruptIndexException, IOException {\n    try {\n      try {\n        return doFlushInternal(flushDocStores, flushDeletes);\n      } finally {\n        docWriter.balanceRAM();\n      }\n    } finally {\n      docWriter.clearFlushPending();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"833a7987bc1c94455fde83e3311f72bddedcfb93","date":1279951470,"type":5,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean,boolean).mjava","sourceNew":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private synchronized final boolean doFlush(boolean flushDeletes) throws CorruptIndexException, IOException {\n    return docWriter.flushAllThreads(flushDeletes);\n    // nocommit\n//    try {\n//      try {\n//        return doFlushInternal(flushDocStores, flushDeletes);\n//      } finally {\n//        docWriter.balanceRAM();\n//      }\n//    } finally {\n//      docWriter.clearFlushPending();\n//    }\n  }\n\n","sourceOld":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private synchronized final boolean doFlush(boolean flushDocStores, boolean flushDeletes) throws CorruptIndexException, IOException {\n    return docWriter.flushAllThreads(flushDocStores, flushDeletes);\n    // nocommit\n//    try {\n//      try {\n//        return doFlushInternal(flushDocStores, flushDeletes);\n//      } finally {\n//        docWriter.balanceRAM();\n//      }\n//    } finally {\n//      docWriter.clearFlushPending();\n//    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"44fcbde6fb2ac44ee3b45e013e54a42911e689ff","date":1292065621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean,boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean,boolean).mjava","sourceNew":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private synchronized final boolean doFlush(boolean closeDocStores, boolean applyAllDeletes) throws CorruptIndexException, IOException {\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n\n    assert testPoint(\"startDoFlush\");\n\n    // We may be flushing because it was triggered by doc\n    // count, del count, ram usage (in which case flush\n    // pending is already set), or we may be flushing\n    // due to external event eg getReader or commit is\n    // called (in which case we now set it, and this will\n    // pause all threads):\n    flushControl.setFlushPendingNoWait(\"explicit flush\");\n\n    boolean success = false;\n\n    try {\n\n      if (infoStream != null) {\n        message(\"  start flush: applyAllDeletes=\" + applyAllDeletes + \" closeDocStores=\" + closeDocStores);\n        message(\"  index before flush \" + segString());\n      }\n    \n      final SegmentInfo newSegment = docWriter.flush(this, closeDocStores, deleter, mergePolicy, segmentInfos);\n      if (newSegment != null) {\n        setDiagnostics(newSegment, \"flush\");\n        segmentInfos.add(newSegment);\n        checkpoint();\n      }\n\n      if (!applyAllDeletes) {\n        // If deletes alone are consuming > 1/2 our RAM\n        // buffer, force them all to apply now. This is to\n        // prevent too-frequent flushing of a long tail of\n        // tiny segments:\n        if (flushControl.getFlushDeletes() ||\n            (config.getRAMBufferSizeMB() != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n             bufferedDeletes.bytesUsed() > (1024*1024*config.getRAMBufferSizeMB()/2))) {\n          applyAllDeletes = true;\n          if (infoStream != null) {\n            message(\"force apply deletes bytesUsed=\" + bufferedDeletes.bytesUsed() + \" vs ramBuffer=\" + (1024*1024*config.getRAMBufferSizeMB()));\n          }\n        }\n      }\n\n      if (applyAllDeletes) {\n        if (infoStream != null) {\n          message(\"apply all deletes during flush\");\n        }\n        flushDeletesCount.incrementAndGet();\n        if (bufferedDeletes.applyDeletes(readerPool, segmentInfos, segmentInfos)) {\n          checkpoint();\n        }\n        flushControl.clearDeletes();\n      } else if (infoStream != null) {\n        message(\"don't apply deletes now delTermCount=\" + bufferedDeletes.numTerms() + \" bytesUsed=\" + bufferedDeletes.bytesUsed());\n      }\n\n      doAfterFlush();\n      flushCount.incrementAndGet();\n\n      success = true;\n\n      return newSegment != null;\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      flushControl.clearFlushPending();\n      if (!success && infoStream != null) {\n        message(\"hit exception during flush\");\n      }\n    }\n  }\n\n","sourceOld":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private synchronized final boolean doFlush(boolean flushDocStores, boolean flushDeletes) throws CorruptIndexException, IOException {\n    try {\n      try {\n        return doFlushInternal(flushDocStores, flushDeletes);\n      } finally {\n        docWriter.balanceRAM();\n      }\n    } finally {\n      docWriter.clearFlushPending();\n    }\n  }\n\n","bugFix":null,"bugIntro":["7d45e9e2ad7f57776540627c78f5e22e469ccdc1"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7e1cbd7e289dc1243c7a59e1a83d078163a147fe","date":1292268032,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean,boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean,boolean).mjava","sourceNew":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private synchronized final boolean doFlush(boolean closeDocStores, boolean applyAllDeletes) throws CorruptIndexException, IOException {\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n\n    assert testPoint(\"startDoFlush\");\n\n    // We may be flushing because it was triggered by doc\n    // count, del count, ram usage (in which case flush\n    // pending is already set), or we may be flushing\n    // due to external event eg getReader or commit is\n    // called (in which case we now set it, and this will\n    // pause all threads):\n    flushControl.setFlushPendingNoWait(\"explicit flush\");\n\n    boolean success = false;\n\n    try {\n\n      if (infoStream != null) {\n        message(\"  start flush: applyAllDeletes=\" + applyAllDeletes + \" closeDocStores=\" + closeDocStores);\n        message(\"  index before flush \" + segString());\n      }\n    \n      final SegmentInfo newSegment = docWriter.flush(this, closeDocStores, deleter, mergePolicy, segmentInfos);\n      if (newSegment != null) {\n        setDiagnostics(newSegment, \"flush\");\n        segmentInfos.add(newSegment);\n        checkpoint();\n      }\n\n      if (!applyAllDeletes) {\n        // If deletes alone are consuming > 1/2 our RAM\n        // buffer, force them all to apply now. This is to\n        // prevent too-frequent flushing of a long tail of\n        // tiny segments:\n        if (flushControl.getFlushDeletes() ||\n            (config.getRAMBufferSizeMB() != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n             bufferedDeletes.bytesUsed() > (1024*1024*config.getRAMBufferSizeMB()/2))) {\n          applyAllDeletes = true;\n          if (infoStream != null) {\n            message(\"force apply deletes bytesUsed=\" + bufferedDeletes.bytesUsed() + \" vs ramBuffer=\" + (1024*1024*config.getRAMBufferSizeMB()));\n          }\n        }\n      }\n\n      if (applyAllDeletes) {\n        if (infoStream != null) {\n          message(\"apply all deletes during flush\");\n        }\n        flushDeletesCount.incrementAndGet();\n        if (bufferedDeletes.applyDeletes(readerPool, segmentInfos, segmentInfos)) {\n          checkpoint();\n        }\n        flushControl.clearDeletes();\n      } else if (infoStream != null) {\n        message(\"don't apply deletes now delTermCount=\" + bufferedDeletes.numTerms() + \" bytesUsed=\" + bufferedDeletes.bytesUsed());\n      }\n\n      doAfterFlush();\n      flushCount.incrementAndGet();\n\n      success = true;\n\n      return newSegment != null;\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      flushControl.clearFlushPending();\n      if (!success) {\n        if (infoStream != null) {\n          message(\"hit exception during flush\");\n        }\n        if (docWriter != null) {\n          final Collection<String> files = docWriter.abortedFiles();\n          if (files != null) {\n            deleter.deleteNewFiles(files);\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private synchronized final boolean doFlush(boolean closeDocStores, boolean applyAllDeletes) throws CorruptIndexException, IOException {\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n\n    assert testPoint(\"startDoFlush\");\n\n    // We may be flushing because it was triggered by doc\n    // count, del count, ram usage (in which case flush\n    // pending is already set), or we may be flushing\n    // due to external event eg getReader or commit is\n    // called (in which case we now set it, and this will\n    // pause all threads):\n    flushControl.setFlushPendingNoWait(\"explicit flush\");\n\n    boolean success = false;\n\n    try {\n\n      if (infoStream != null) {\n        message(\"  start flush: applyAllDeletes=\" + applyAllDeletes + \" closeDocStores=\" + closeDocStores);\n        message(\"  index before flush \" + segString());\n      }\n    \n      final SegmentInfo newSegment = docWriter.flush(this, closeDocStores, deleter, mergePolicy, segmentInfos);\n      if (newSegment != null) {\n        setDiagnostics(newSegment, \"flush\");\n        segmentInfos.add(newSegment);\n        checkpoint();\n      }\n\n      if (!applyAllDeletes) {\n        // If deletes alone are consuming > 1/2 our RAM\n        // buffer, force them all to apply now. This is to\n        // prevent too-frequent flushing of a long tail of\n        // tiny segments:\n        if (flushControl.getFlushDeletes() ||\n            (config.getRAMBufferSizeMB() != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n             bufferedDeletes.bytesUsed() > (1024*1024*config.getRAMBufferSizeMB()/2))) {\n          applyAllDeletes = true;\n          if (infoStream != null) {\n            message(\"force apply deletes bytesUsed=\" + bufferedDeletes.bytesUsed() + \" vs ramBuffer=\" + (1024*1024*config.getRAMBufferSizeMB()));\n          }\n        }\n      }\n\n      if (applyAllDeletes) {\n        if (infoStream != null) {\n          message(\"apply all deletes during flush\");\n        }\n        flushDeletesCount.incrementAndGet();\n        if (bufferedDeletes.applyDeletes(readerPool, segmentInfos, segmentInfos)) {\n          checkpoint();\n        }\n        flushControl.clearDeletes();\n      } else if (infoStream != null) {\n        message(\"don't apply deletes now delTermCount=\" + bufferedDeletes.numTerms() + \" bytesUsed=\" + bufferedDeletes.bytesUsed());\n      }\n\n      doAfterFlush();\n      flushCount.incrementAndGet();\n\n      success = true;\n\n      return newSegment != null;\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      flushControl.clearFlushPending();\n      if (!success && infoStream != null) {\n        message(\"hit exception during flush\");\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5","date":1292695408,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean,boolean).mjava","sourceNew":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private synchronized boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n\n    assert testPoint(\"startDoFlush\");\n\n    // We may be flushing because it was triggered by doc\n    // count, del count, ram usage (in which case flush\n    // pending is already set), or we may be flushing\n    // due to external event eg getReader or commit is\n    // called (in which case we now set it, and this will\n    // pause all threads):\n    flushControl.setFlushPendingNoWait(\"explicit flush\");\n\n    boolean success = false;\n\n    try {\n\n      if (infoStream != null) {\n        message(\"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        message(\"  index before flush \" + segString());\n      }\n    \n      final SegmentInfo newSegment = docWriter.flush(this, deleter, mergePolicy, segmentInfos);\n      if (newSegment != null) {\n        setDiagnostics(newSegment, \"flush\");\n        segmentInfos.add(newSegment);\n        checkpoint();\n      }\n\n      if (!applyAllDeletes) {\n        // If deletes alone are consuming > 1/2 our RAM\n        // buffer, force them all to apply now. This is to\n        // prevent too-frequent flushing of a long tail of\n        // tiny segments:\n        if (flushControl.getFlushDeletes() ||\n            (config.getRAMBufferSizeMB() != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n             bufferedDeletes.bytesUsed() > (1024*1024*config.getRAMBufferSizeMB()/2))) {\n          applyAllDeletes = true;\n          if (infoStream != null) {\n            message(\"force apply deletes bytesUsed=\" + bufferedDeletes.bytesUsed() + \" vs ramBuffer=\" + (1024*1024*config.getRAMBufferSizeMB()));\n          }\n        }\n      }\n\n      if (applyAllDeletes) {\n        if (infoStream != null) {\n          message(\"apply all deletes during flush\");\n        }\n        flushDeletesCount.incrementAndGet();\n        if (bufferedDeletes.applyDeletes(readerPool, segmentInfos, segmentInfos)) {\n          checkpoint();\n        }\n        flushControl.clearDeletes();\n      } else if (infoStream != null) {\n        message(\"don't apply deletes now delTermCount=\" + bufferedDeletes.numTerms() + \" bytesUsed=\" + bufferedDeletes.bytesUsed());\n      }\n\n      doAfterFlush();\n      flushCount.incrementAndGet();\n\n      success = true;\n\n      return newSegment != null;\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      flushControl.clearFlushPending();\n      if (!success && infoStream != null)\n        message(\"hit exception during flush\");\n    }\n  }\n\n","sourceOld":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private synchronized final boolean doFlush(boolean closeDocStores, boolean applyAllDeletes) throws CorruptIndexException, IOException {\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n\n    assert testPoint(\"startDoFlush\");\n\n    // We may be flushing because it was triggered by doc\n    // count, del count, ram usage (in which case flush\n    // pending is already set), or we may be flushing\n    // due to external event eg getReader or commit is\n    // called (in which case we now set it, and this will\n    // pause all threads):\n    flushControl.setFlushPendingNoWait(\"explicit flush\");\n\n    boolean success = false;\n\n    try {\n\n      if (infoStream != null) {\n        message(\"  start flush: applyAllDeletes=\" + applyAllDeletes + \" closeDocStores=\" + closeDocStores);\n        message(\"  index before flush \" + segString());\n      }\n    \n      final SegmentInfo newSegment = docWriter.flush(this, closeDocStores, deleter, mergePolicy, segmentInfos);\n      if (newSegment != null) {\n        setDiagnostics(newSegment, \"flush\");\n        segmentInfos.add(newSegment);\n        checkpoint();\n      }\n\n      if (!applyAllDeletes) {\n        // If deletes alone are consuming > 1/2 our RAM\n        // buffer, force them all to apply now. This is to\n        // prevent too-frequent flushing of a long tail of\n        // tiny segments:\n        if (flushControl.getFlushDeletes() ||\n            (config.getRAMBufferSizeMB() != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n             bufferedDeletes.bytesUsed() > (1024*1024*config.getRAMBufferSizeMB()/2))) {\n          applyAllDeletes = true;\n          if (infoStream != null) {\n            message(\"force apply deletes bytesUsed=\" + bufferedDeletes.bytesUsed() + \" vs ramBuffer=\" + (1024*1024*config.getRAMBufferSizeMB()));\n          }\n        }\n      }\n\n      if (applyAllDeletes) {\n        if (infoStream != null) {\n          message(\"apply all deletes during flush\");\n        }\n        flushDeletesCount.incrementAndGet();\n        if (bufferedDeletes.applyDeletes(readerPool, segmentInfos, segmentInfos)) {\n          checkpoint();\n        }\n        flushControl.clearDeletes();\n      } else if (infoStream != null) {\n        message(\"don't apply deletes now delTermCount=\" + bufferedDeletes.numTerms() + \" bytesUsed=\" + bufferedDeletes.bytesUsed());\n      }\n\n      doAfterFlush();\n      flushCount.incrementAndGet();\n\n      success = true;\n\n      return newSegment != null;\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      flushControl.clearFlushPending();\n      if (!success) {\n        if (infoStream != null) {\n          message(\"hit exception during flush\");\n        }\n        if (docWriter != null) {\n          final Collection<String> files = docWriter.abortedFiles();\n          if (files != null) {\n            deleter.deleteNewFiles(files);\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ab5cb6a74aefb78aa0569857970b9151dfe2e787","date":1292842407,"type":4,"author":"Simon Willnauer","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean,boolean).mjava","sourceNew":null,"sourceOld":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private synchronized final boolean doFlush(boolean flushDocStores, boolean flushDeletes) throws CorruptIndexException, IOException {\n    try {\n      try {\n        return doFlushInternal(flushDocStores, flushDeletes);\n      } finally {\n        docWriter.balanceRAM();\n      }\n    } finally {\n      docWriter.clearFlushPending();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"44fcbde6fb2ac44ee3b45e013e54a42911e689ff":["334c1175813aea771a71728cd2c4ee4754fd0603"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"8fe956d65251358d755c56f14fe8380644790e47":["579b8e50af5927e2366553f5e237b2c6e46b258e"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["334c1175813aea771a71728cd2c4ee4754fd0603","4948bc5d29211f0c9b5ccc31b2632cdd27066ea5"],"334c1175813aea771a71728cd2c4ee4754fd0603":["9b832cbed6eb3d54a8bb9339296bdda8eeb53014"],"579b8e50af5927e2366553f5e237b2c6e46b258e":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5":["7e1cbd7e289dc1243c7a59e1a83d078163a147fe"],"7e1cbd7e289dc1243c7a59e1a83d078163a147fe":["44fcbde6fb2ac44ee3b45e013e54a42911e689ff"],"9b832cbed6eb3d54a8bb9339296bdda8eeb53014":["579b8e50af5927e2366553f5e237b2c6e46b258e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["4948bc5d29211f0c9b5ccc31b2632cdd27066ea5"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"833a7987bc1c94455fde83e3311f72bddedcfb93":["8fe956d65251358d755c56f14fe8380644790e47"]},"commit2Childs":{"44fcbde6fb2ac44ee3b45e013e54a42911e689ff":["7e1cbd7e289dc1243c7a59e1a83d078163a147fe"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"8fe956d65251358d755c56f14fe8380644790e47":["833a7987bc1c94455fde83e3311f72bddedcfb93"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":[],"334c1175813aea771a71728cd2c4ee4754fd0603":["44fcbde6fb2ac44ee3b45e013e54a42911e689ff","ab5cb6a74aefb78aa0569857970b9151dfe2e787"],"579b8e50af5927e2366553f5e237b2c6e46b258e":["8fe956d65251358d755c56f14fe8380644790e47","9b832cbed6eb3d54a8bb9339296bdda8eeb53014"],"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5":["ab5cb6a74aefb78aa0569857970b9151dfe2e787","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"9b832cbed6eb3d54a8bb9339296bdda8eeb53014":["334c1175813aea771a71728cd2c4ee4754fd0603"],"7e1cbd7e289dc1243c7a59e1a83d078163a147fe":["4948bc5d29211f0c9b5ccc31b2632cdd27066ea5"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["579b8e50af5927e2366553f5e237b2c6e46b258e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"833a7987bc1c94455fde83e3311f72bddedcfb93":[]},"heads":["ab5cb6a74aefb78aa0569857970b9151dfe2e787","cd5edd1f2b162a5cfa08efd17851a07373a96817","833a7987bc1c94455fde83e3311f72bddedcfb93"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}