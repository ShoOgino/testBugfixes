{"path":"lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.Writer#finishInternal(int).mjava","commits":[{"id":"85eb75e0c0203e44dcf686f35876cf6080f3a671","date":1317221550,"type":0,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.Writer#finishInternal(int).mjava","pathOld":"/dev/null","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finishInternal(int docCount) throws IOException {\n      final int count = hash.size();\n      final IndexOutput datOut = getOrCreateDataOut();\n      long offset = 0;\n      long lastOffset = 0;\n      final int[] index = new int[count+1];\n      final long[] offsets = new long[count];\n      final int[] sortedEntries = hash.sort(comp);\n      // first dump bytes data, recording index & offset as\n      // we go\n      for (int i = 0; i < count; i++) {\n        final int e = sortedEntries[i];\n        offsets[i] = offset;\n        index[e+1] = 1 + i;\n\n        final BytesRef bytes = hash.get(e, new BytesRef());\n        // TODO: we could prefix code...\n        datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n        lastOffset = offset;\n        offset += bytes.length;\n      }\n\n      final IndexOutput idxOut = getOrCreateIndexOut();\n      // total bytes of data\n      idxOut.writeLong(offset);\n      // write index -- first doc -> 1+ord\n      writeIndex(idxOut, docCount, count, index, docToEntry);\n      // next ord (0-based) -> offset\n      PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count,\n          PackedInts.bitsRequired(lastOffset));\n      for (int i = 0; i < count; i++) {\n        offsetWriter.add(offsets[i]);\n      }\n      offsetWriter.finish();\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e128967bca58657bc0039d4bfe631e63e81f1977","date":1317978310,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.Writer#finishInternal(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.Writer#finishInternal(int).mjava","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finishInternal(int docCount) throws IOException {\n      fillDefault(docCount);\n      final int count = hash.size();\n      final IndexOutput datOut = getOrCreateDataOut();\n      long offset = 0;\n      final int[] index = new int[count];\n      final long[] offsets = new long[count];\n      final int[] sortedEntries = hash.sort(comp);\n      // first dump bytes data, recording index & offset as\n      // we go\n      for (int i = 0; i < count; i++) {\n        final int e = sortedEntries[i];\n        offsets[i] = offset;\n        index[e] = i;\n\n        final BytesRef bytes = hash.get(e, new BytesRef());\n        // TODO: we could prefix code...\n        datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n        offset += bytes.length;\n      }\n      final IndexOutput idxOut = getOrCreateIndexOut();\n      // total bytes of data\n      idxOut.writeLong(offset);\n      // write index\n      writeIndex(idxOut, docCount, count, index, docToEntry);\n      // next ord (0-based) -> offset\n      PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count+1,\n          PackedInts.bitsRequired(offset));\n      for (int i = 0; i < count; i++) {\n        offsetWriter.add(offsets[i]);\n      }\n      offsetWriter.add(offset);\n      offsetWriter.finish();\n    }\n\n","sourceOld":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finishInternal(int docCount) throws IOException {\n      final int count = hash.size();\n      final IndexOutput datOut = getOrCreateDataOut();\n      long offset = 0;\n      long lastOffset = 0;\n      final int[] index = new int[count+1];\n      final long[] offsets = new long[count];\n      final int[] sortedEntries = hash.sort(comp);\n      // first dump bytes data, recording index & offset as\n      // we go\n      for (int i = 0; i < count; i++) {\n        final int e = sortedEntries[i];\n        offsets[i] = offset;\n        index[e+1] = 1 + i;\n\n        final BytesRef bytes = hash.get(e, new BytesRef());\n        // TODO: we could prefix code...\n        datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n        lastOffset = offset;\n        offset += bytes.length;\n      }\n\n      final IndexOutput idxOut = getOrCreateIndexOut();\n      // total bytes of data\n      idxOut.writeLong(offset);\n      // write index -- first doc -> 1+ord\n      writeIndex(idxOut, docCount, count, index, docToEntry);\n      // next ord (0-based) -> offset\n      PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count,\n          PackedInts.bitsRequired(lastOffset));\n      for (int i = 0; i < count; i++) {\n        offsetWriter.add(offsets[i]);\n      }\n      offsetWriter.finish();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1346e81f172438a4f28d91266cdd03851f94b831","date":1318404834,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.Writer#finishInternal(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.Writer#finishInternal(int).mjava","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finishInternal(int docCount) throws IOException {\n      fillDefault(docCount);\n      final int count = hash.size();\n      final IndexOutput datOut = getOrCreateDataOut();\n      final IndexOutput idxOut = getOrCreateIndexOut();\n      long offset = 0;\n      final int[] index = new int[count];\n      final int[] sortedEntries = hash.sort(comp);\n      // total bytes of data\n      idxOut.writeLong(maxBytes);\n      PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count+1,\n          PackedInts.bitsRequired(maxBytes));\n      // first dump bytes data, recording index & write offset as\n      // we go\n      final BytesRef spare = new BytesRef();\n      for (int i = 0; i < count; i++) {\n        final int e = sortedEntries[i];\n        offsetWriter.add(offset);\n        index[e] = i;\n        final BytesRef bytes = hash.get(e, spare);\n        // TODO: we could prefix code...\n        datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n        offset += bytes.length;\n      }\n      offsetWriter.add(offset);\n      offsetWriter.finish();\n      // write index\n      writeIndex(idxOut, docCount, count, index, docToEntry);\n\n    }\n\n","sourceOld":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finishInternal(int docCount) throws IOException {\n      fillDefault(docCount);\n      final int count = hash.size();\n      final IndexOutput datOut = getOrCreateDataOut();\n      long offset = 0;\n      final int[] index = new int[count];\n      final long[] offsets = new long[count];\n      final int[] sortedEntries = hash.sort(comp);\n      // first dump bytes data, recording index & offset as\n      // we go\n      for (int i = 0; i < count; i++) {\n        final int e = sortedEntries[i];\n        offsets[i] = offset;\n        index[e] = i;\n\n        final BytesRef bytes = hash.get(e, new BytesRef());\n        // TODO: we could prefix code...\n        datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n        offset += bytes.length;\n      }\n      final IndexOutput idxOut = getOrCreateIndexOut();\n      // total bytes of data\n      idxOut.writeLong(offset);\n      // write index\n      writeIndex(idxOut, docCount, count, index, docToEntry);\n      // next ord (0-based) -> offset\n      PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count+1,\n          PackedInts.bitsRequired(offset));\n      for (int i = 0; i < count; i++) {\n        offsetWriter.add(offsets[i]);\n      }\n      offsetWriter.add(offset);\n      offsetWriter.finish();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"142daaa8f2b32cb229bd23e7aabd06d2a7d3c6fb","date":1320691430,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.Writer#finishInternal(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.Writer#finishInternal(int).mjava","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finishInternal(int docCount) throws IOException {\n      fillDefault(docCount);\n      final int count = hash.size();\n      final IndexOutput datOut = getOrCreateDataOut();\n      final IndexOutput idxOut = getOrCreateIndexOut();\n      long offset = 0;\n      final int[] index = new int[count];\n      final int[] sortedEntries = hash.sort(comp);\n      // total bytes of data\n      idxOut.writeLong(maxBytes);\n      PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count+1,\n          PackedInts.bitsRequired(maxBytes));\n      // first dump bytes data, recording index & write offset as\n      // we go\n      final BytesRef spare = new BytesRef();\n      for (int i = 0; i < count; i++) {\n        final int e = sortedEntries[i];\n        offsetWriter.add(offset);\n        index[e] = i;\n        final BytesRef bytes = hash.get(e, spare);\n        // TODO: we could prefix code...\n        datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n        offset += bytes.length;\n      }\n      // write sentinel\n      offsetWriter.add(offset);\n      offsetWriter.finish();\n      // write index\n      writeIndex(idxOut, docCount, count, index, docToEntry);\n\n    }\n\n","sourceOld":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finishInternal(int docCount) throws IOException {\n      fillDefault(docCount);\n      final int count = hash.size();\n      final IndexOutput datOut = getOrCreateDataOut();\n      final IndexOutput idxOut = getOrCreateIndexOut();\n      long offset = 0;\n      final int[] index = new int[count];\n      final int[] sortedEntries = hash.sort(comp);\n      // total bytes of data\n      idxOut.writeLong(maxBytes);\n      PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count+1,\n          PackedInts.bitsRequired(maxBytes));\n      // first dump bytes data, recording index & write offset as\n      // we go\n      final BytesRef spare = new BytesRef();\n      for (int i = 0; i < count; i++) {\n        final int e = sortedEntries[i];\n        offsetWriter.add(offset);\n        index[e] = i;\n        final BytesRef bytes = hash.get(e, spare);\n        // TODO: we could prefix code...\n        datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n        offset += bytes.length;\n      }\n      offsetWriter.add(offset);\n      offsetWriter.finish();\n      // write index\n      writeIndex(idxOut, docCount, count, index, docToEntry);\n\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f9efc72acdea22f5285be0a808f8bba51bb8e367","date":1323217280,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/VarSortedBytesImpl.Writer#finishInternal(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.Writer#finishInternal(int).mjava","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finishInternal(int docCount) throws IOException {\n      fillDefault(docCount);\n      final int count = hash.size();\n      final IndexOutput datOut = getOrCreateDataOut();\n      final IndexOutput idxOut = getOrCreateIndexOut();\n      long offset = 0;\n      final int[] index = new int[count];\n      final int[] sortedEntries = hash.sort(comp);\n      // total bytes of data\n      idxOut.writeLong(maxBytes);\n      PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count+1,\n          PackedInts.bitsRequired(maxBytes));\n      // first dump bytes data, recording index & write offset as\n      // we go\n      final BytesRef spare = new BytesRef();\n      for (int i = 0; i < count; i++) {\n        final int e = sortedEntries[i];\n        offsetWriter.add(offset);\n        index[e] = i;\n        final BytesRef bytes = hash.get(e, spare);\n        // TODO: we could prefix code...\n        datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n        offset += bytes.length;\n      }\n      // write sentinel\n      offsetWriter.add(offset);\n      offsetWriter.finish();\n      // write index\n      writeIndex(idxOut, docCount, count, index, docToEntry);\n\n    }\n\n","sourceOld":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finishInternal(int docCount) throws IOException {\n      fillDefault(docCount);\n      final int count = hash.size();\n      final IndexOutput datOut = getOrCreateDataOut();\n      final IndexOutput idxOut = getOrCreateIndexOut();\n      long offset = 0;\n      final int[] index = new int[count];\n      final int[] sortedEntries = hash.sort(comp);\n      // total bytes of data\n      idxOut.writeLong(maxBytes);\n      PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count+1,\n          PackedInts.bitsRequired(maxBytes));\n      // first dump bytes data, recording index & write offset as\n      // we go\n      final BytesRef spare = new BytesRef();\n      for (int i = 0; i < count; i++) {\n        final int e = sortedEntries[i];\n        offsetWriter.add(offset);\n        index[e] = i;\n        final BytesRef bytes = hash.get(e, spare);\n        // TODO: we could prefix code...\n        datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n        offset += bytes.length;\n      }\n      // write sentinel\n      offsetWriter.add(offset);\n      offsetWriter.finish();\n      // write index\n      writeIndex(idxOut, docCount, count, index, docToEntry);\n\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d638301ad1cfcae567b681b893bc8781f0ee48a5","date":1323801546,"type":5,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/VarSortedBytesImpl.Writer#finishInternal(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.Writer#finishInternal(int).mjava","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finishInternal(int docCount) throws IOException {\n      fillDefault(docCount);\n      final int count = hash.size();\n      final IndexOutput datOut = getOrCreateDataOut();\n      final IndexOutput idxOut = getOrCreateIndexOut();\n      long offset = 0;\n      final int[] index = new int[count];\n      final int[] sortedEntries = hash.sort(comp);\n      // total bytes of data\n      idxOut.writeLong(maxBytes);\n      PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count+1,\n          PackedInts.bitsRequired(maxBytes));\n      // first dump bytes data, recording index & write offset as\n      // we go\n      final BytesRef spare = new BytesRef();\n      for (int i = 0; i < count; i++) {\n        final int e = sortedEntries[i];\n        offsetWriter.add(offset);\n        index[e] = i;\n        final BytesRef bytes = hash.get(e, spare);\n        // TODO: we could prefix code...\n        datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n        offset += bytes.length;\n      }\n      // write sentinel\n      offsetWriter.add(offset);\n      offsetWriter.finish();\n      // write index\n      writeIndex(idxOut, docCount, count, index, docToEntry);\n\n    }\n\n","sourceOld":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finishInternal(int docCount) throws IOException {\n      fillDefault(docCount);\n      final int count = hash.size();\n      final IndexOutput datOut = getOrCreateDataOut();\n      final IndexOutput idxOut = getOrCreateIndexOut();\n      long offset = 0;\n      final int[] index = new int[count];\n      final int[] sortedEntries = hash.sort(comp);\n      // total bytes of data\n      idxOut.writeLong(maxBytes);\n      PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count+1,\n          PackedInts.bitsRequired(maxBytes));\n      // first dump bytes data, recording index & write offset as\n      // we go\n      final BytesRef spare = new BytesRef();\n      for (int i = 0; i < count; i++) {\n        final int e = sortedEntries[i];\n        offsetWriter.add(offset);\n        index[e] = i;\n        final BytesRef bytes = hash.get(e, spare);\n        // TODO: we could prefix code...\n        datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n        offset += bytes.length;\n      }\n      // write sentinel\n      offsetWriter.add(offset);\n      offsetWriter.finish();\n      // write index\n      writeIndex(idxOut, docCount, count, index, docToEntry);\n\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"1346e81f172438a4f28d91266cdd03851f94b831":["e128967bca58657bc0039d4bfe631e63e81f1977"],"142daaa8f2b32cb229bd23e7aabd06d2a7d3c6fb":["1346e81f172438a4f28d91266cdd03851f94b831"],"85eb75e0c0203e44dcf686f35876cf6080f3a671":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"f9efc72acdea22f5285be0a808f8bba51bb8e367":["142daaa8f2b32cb229bd23e7aabd06d2a7d3c6fb"],"e128967bca58657bc0039d4bfe631e63e81f1977":["85eb75e0c0203e44dcf686f35876cf6080f3a671"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d638301ad1cfcae567b681b893bc8781f0ee48a5"],"d638301ad1cfcae567b681b893bc8781f0ee48a5":["142daaa8f2b32cb229bd23e7aabd06d2a7d3c6fb","f9efc72acdea22f5285be0a808f8bba51bb8e367"]},"commit2Childs":{"1346e81f172438a4f28d91266cdd03851f94b831":["142daaa8f2b32cb229bd23e7aabd06d2a7d3c6fb"],"142daaa8f2b32cb229bd23e7aabd06d2a7d3c6fb":["f9efc72acdea22f5285be0a808f8bba51bb8e367","d638301ad1cfcae567b681b893bc8781f0ee48a5"],"85eb75e0c0203e44dcf686f35876cf6080f3a671":["e128967bca58657bc0039d4bfe631e63e81f1977"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["85eb75e0c0203e44dcf686f35876cf6080f3a671"],"f9efc72acdea22f5285be0a808f8bba51bb8e367":["d638301ad1cfcae567b681b893bc8781f0ee48a5"],"e128967bca58657bc0039d4bfe631e63e81f1977":["1346e81f172438a4f28d91266cdd03851f94b831"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"d638301ad1cfcae567b681b893bc8781f0ee48a5":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}