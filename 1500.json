{"path":"lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42TermVectorsWriter#flushOffsets(int[]).mjava","commits":[{"id":"3d5291145ae0cea7e6e6a2379f3a32643bf71bf6","date":1411857884,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42TermVectorsWriter#flushOffsets(int[]).mjava","pathOld":"/dev/null","sourceNew":"  private void flushOffsets(int[] fieldNums) throws IOException {\n    boolean hasOffsets = false;\n    long[] sumPos = new long[fieldNums.length];\n    long[] sumOffsets = new long[fieldNums.length];\n    for (DocData dd : pendingDocs) {\n      for (FieldData fd : dd.fields) {\n        hasOffsets |= fd.hasOffsets;\n        if (fd.hasOffsets && fd.hasPositions) {\n          final int fieldNumOff = Arrays.binarySearch(fieldNums, fd.fieldNum);\n          int pos = 0;\n          for (int i = 0; i < fd.numTerms; ++i) {\n            int previousPos = 0;\n            int previousOff = 0;\n            for (int j = 0; j < fd.freqs[i]; ++j) {\n              final int position = positionsBuf[fd.posStart + pos];\n              final int startOffset = startOffsetsBuf[fd.offStart + pos];\n              sumPos[fieldNumOff] += position - previousPos;\n              sumOffsets[fieldNumOff] += startOffset - previousOff;\n              previousPos = position;\n              previousOff = startOffset;\n              ++pos;\n            }\n          }\n          assert pos == fd.totalPositions;\n        }\n      }\n    }\n\n    if (!hasOffsets) {\n      // nothing to do\n      return;\n    }\n\n    final float[] charsPerTerm = new float[fieldNums.length];\n    for (int i = 0; i < fieldNums.length; ++i) {\n      charsPerTerm[i] = (sumPos[i] <= 0 || sumOffsets[i] <= 0) ? 0 : (float) ((double) sumOffsets[i] / sumPos[i]);\n    }\n\n    // start offsets\n    for (int i = 0; i < fieldNums.length; ++i) {\n      vectorsStream.writeInt(Float.floatToRawIntBits(charsPerTerm[i]));\n    }\n\n    writer.reset(vectorsStream);\n    for (DocData dd : pendingDocs) {\n      for (FieldData fd : dd.fields) {\n        if ((fd.flags & OFFSETS) != 0) {\n          final int fieldNumOff = Arrays.binarySearch(fieldNums, fd.fieldNum);\n          final float cpt = charsPerTerm[fieldNumOff];\n          int pos = 0;\n          for (int i = 0; i < fd.numTerms; ++i) {\n            int previousPos = 0;\n            int previousOff = 0;\n            for (int j = 0; j < fd.freqs[i]; ++j) {\n              final int position = fd.hasPositions ? positionsBuf[fd.posStart + pos] : 0;\n              final int startOffset = startOffsetsBuf[fd.offStart + pos];\n              writer.add(startOffset - previousOff - (int) (cpt * (position - previousPos)));\n              previousPos = position;\n              previousOff = startOffset;\n              ++pos;\n            }\n          }\n        }\n      }\n    }\n    writer.finish();\n\n    // lengths\n    writer.reset(vectorsStream);\n    for (DocData dd : pendingDocs) {\n      for (FieldData fd : dd.fields) {\n        if ((fd.flags & OFFSETS) != 0) {\n          int pos = 0;\n          for (int i = 0; i < fd.numTerms; ++i) {\n            for (int j = 0; j < fd.freqs[i]; ++j) {\n              writer.add(lengthsBuf[fd.offStart + pos++] - fd.prefixLengths[i] - fd.suffixLengths[i]);\n            }\n          }\n          assert pos == fd.totalPositions;\n        }\n      }\n    }\n    writer.finish();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9bb9a29a5e71a90295f175df8919802993142c9a","date":1412517673,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42TermVectorsWriter#flushOffsets(int[]).mjava","pathOld":"/dev/null","sourceNew":"  private void flushOffsets(int[] fieldNums) throws IOException {\n    boolean hasOffsets = false;\n    long[] sumPos = new long[fieldNums.length];\n    long[] sumOffsets = new long[fieldNums.length];\n    for (DocData dd : pendingDocs) {\n      for (FieldData fd : dd.fields) {\n        hasOffsets |= fd.hasOffsets;\n        if (fd.hasOffsets && fd.hasPositions) {\n          final int fieldNumOff = Arrays.binarySearch(fieldNums, fd.fieldNum);\n          int pos = 0;\n          for (int i = 0; i < fd.numTerms; ++i) {\n            int previousPos = 0;\n            int previousOff = 0;\n            for (int j = 0; j < fd.freqs[i]; ++j) {\n              final int position = positionsBuf[fd.posStart + pos];\n              final int startOffset = startOffsetsBuf[fd.offStart + pos];\n              sumPos[fieldNumOff] += position - previousPos;\n              sumOffsets[fieldNumOff] += startOffset - previousOff;\n              previousPos = position;\n              previousOff = startOffset;\n              ++pos;\n            }\n          }\n          assert pos == fd.totalPositions;\n        }\n      }\n    }\n\n    if (!hasOffsets) {\n      // nothing to do\n      return;\n    }\n\n    final float[] charsPerTerm = new float[fieldNums.length];\n    for (int i = 0; i < fieldNums.length; ++i) {\n      charsPerTerm[i] = (sumPos[i] <= 0 || sumOffsets[i] <= 0) ? 0 : (float) ((double) sumOffsets[i] / sumPos[i]);\n    }\n\n    // start offsets\n    for (int i = 0; i < fieldNums.length; ++i) {\n      vectorsStream.writeInt(Float.floatToRawIntBits(charsPerTerm[i]));\n    }\n\n    writer.reset(vectorsStream);\n    for (DocData dd : pendingDocs) {\n      for (FieldData fd : dd.fields) {\n        if ((fd.flags & OFFSETS) != 0) {\n          final int fieldNumOff = Arrays.binarySearch(fieldNums, fd.fieldNum);\n          final float cpt = charsPerTerm[fieldNumOff];\n          int pos = 0;\n          for (int i = 0; i < fd.numTerms; ++i) {\n            int previousPos = 0;\n            int previousOff = 0;\n            for (int j = 0; j < fd.freqs[i]; ++j) {\n              final int position = fd.hasPositions ? positionsBuf[fd.posStart + pos] : 0;\n              final int startOffset = startOffsetsBuf[fd.offStart + pos];\n              writer.add(startOffset - previousOff - (int) (cpt * (position - previousPos)));\n              previousPos = position;\n              previousOff = startOffset;\n              ++pos;\n            }\n          }\n        }\n      }\n    }\n    writer.finish();\n\n    // lengths\n    writer.reset(vectorsStream);\n    for (DocData dd : pendingDocs) {\n      for (FieldData fd : dd.fields) {\n        if ((fd.flags & OFFSETS) != 0) {\n          int pos = 0;\n          for (int i = 0; i < fd.numTerms; ++i) {\n            for (int j = 0; j < fd.freqs[i]; ++j) {\n              writer.add(lengthsBuf[fd.offStart + pos++] - fd.prefixLengths[i] - fd.suffixLengths[i]);\n            }\n          }\n          assert pos == fd.totalPositions;\n        }\n      }\n    }\n    writer.finish();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"71387d8cb6923eb831b17a8b734608ba2e21c653","date":1414126093,"type":4,"author":"Robert Muir","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42TermVectorsWriter#flushOffsets(int[]).mjava","sourceNew":null,"sourceOld":"  private void flushOffsets(int[] fieldNums) throws IOException {\n    boolean hasOffsets = false;\n    long[] sumPos = new long[fieldNums.length];\n    long[] sumOffsets = new long[fieldNums.length];\n    for (DocData dd : pendingDocs) {\n      for (FieldData fd : dd.fields) {\n        hasOffsets |= fd.hasOffsets;\n        if (fd.hasOffsets && fd.hasPositions) {\n          final int fieldNumOff = Arrays.binarySearch(fieldNums, fd.fieldNum);\n          int pos = 0;\n          for (int i = 0; i < fd.numTerms; ++i) {\n            int previousPos = 0;\n            int previousOff = 0;\n            for (int j = 0; j < fd.freqs[i]; ++j) {\n              final int position = positionsBuf[fd.posStart + pos];\n              final int startOffset = startOffsetsBuf[fd.offStart + pos];\n              sumPos[fieldNumOff] += position - previousPos;\n              sumOffsets[fieldNumOff] += startOffset - previousOff;\n              previousPos = position;\n              previousOff = startOffset;\n              ++pos;\n            }\n          }\n          assert pos == fd.totalPositions;\n        }\n      }\n    }\n\n    if (!hasOffsets) {\n      // nothing to do\n      return;\n    }\n\n    final float[] charsPerTerm = new float[fieldNums.length];\n    for (int i = 0; i < fieldNums.length; ++i) {\n      charsPerTerm[i] = (sumPos[i] <= 0 || sumOffsets[i] <= 0) ? 0 : (float) ((double) sumOffsets[i] / sumPos[i]);\n    }\n\n    // start offsets\n    for (int i = 0; i < fieldNums.length; ++i) {\n      vectorsStream.writeInt(Float.floatToRawIntBits(charsPerTerm[i]));\n    }\n\n    writer.reset(vectorsStream);\n    for (DocData dd : pendingDocs) {\n      for (FieldData fd : dd.fields) {\n        if ((fd.flags & OFFSETS) != 0) {\n          final int fieldNumOff = Arrays.binarySearch(fieldNums, fd.fieldNum);\n          final float cpt = charsPerTerm[fieldNumOff];\n          int pos = 0;\n          for (int i = 0; i < fd.numTerms; ++i) {\n            int previousPos = 0;\n            int previousOff = 0;\n            for (int j = 0; j < fd.freqs[i]; ++j) {\n              final int position = fd.hasPositions ? positionsBuf[fd.posStart + pos] : 0;\n              final int startOffset = startOffsetsBuf[fd.offStart + pos];\n              writer.add(startOffset - previousOff - (int) (cpt * (position - previousPos)));\n              previousPos = position;\n              previousOff = startOffset;\n              ++pos;\n            }\n          }\n        }\n      }\n    }\n    writer.finish();\n\n    // lengths\n    writer.reset(vectorsStream);\n    for (DocData dd : pendingDocs) {\n      for (FieldData fd : dd.fields) {\n        if ((fd.flags & OFFSETS) != 0) {\n          int pos = 0;\n          for (int i = 0; i < fd.numTerms; ++i) {\n            for (int j = 0; j < fd.freqs[i]; ++j) {\n              writer.add(lengthsBuf[fd.offStart + pos++] - fd.prefixLengths[i] - fd.suffixLengths[i]);\n            }\n          }\n          assert pos == fd.totalPositions;\n        }\n      }\n    }\n    writer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"9bb9a29a5e71a90295f175df8919802993142c9a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","3d5291145ae0cea7e6e6a2379f3a32643bf71bf6"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3d5291145ae0cea7e6e6a2379f3a32643bf71bf6":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"71387d8cb6923eb831b17a8b734608ba2e21c653":["9bb9a29a5e71a90295f175df8919802993142c9a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["71387d8cb6923eb831b17a8b734608ba2e21c653"]},"commit2Childs":{"9bb9a29a5e71a90295f175df8919802993142c9a":["71387d8cb6923eb831b17a8b734608ba2e21c653"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9bb9a29a5e71a90295f175df8919802993142c9a","3d5291145ae0cea7e6e6a2379f3a32643bf71bf6"],"3d5291145ae0cea7e6e6a2379f3a32643bf71bf6":["9bb9a29a5e71a90295f175df8919802993142c9a"],"71387d8cb6923eb831b17a8b734608ba2e21c653":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}