{"path":"solr/core/src/test/org/apache/solr/cloud/TestCloudRecovery#corruptedLogTest().mjava","commits":[{"id":"40b4f9867bcb3a1cf45f8a02b05af0bdf552746f","date":1477377788,"type":0,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestCloudRecovery#corruptedLogTest().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  public void corruptedLogTest() throws Exception {\n    AtomicInteger countReplayLog = new AtomicInteger(0);\n    DirectUpdateHandler2.commitOnClose = false;\n    UpdateLog.testing_logReplayFinishHook = countReplayLog::incrementAndGet;\n\n    CloudSolrClient cloudClient = cluster.getSolrClient();\n    cloudClient.add(COLLECTION, sdoc(\"id\", \"1000\"));\n    cloudClient.add(COLLECTION, sdoc(\"id\", \"1001\"));\n    for (int i = 0; i < 10; i++) {\n      cloudClient.add(COLLECTION, sdoc(\"id\", String.valueOf(i)));\n    }\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"q\", \"*:*\");\n    QueryResponse resp = cloudClient.query(COLLECTION, params);\n    assertEquals(0, resp.getResults().getNumFound());\n\n    int logHeaderSize = Integer.MAX_VALUE;\n    Map<File, byte[]> contentFiles = new HashMap<>();\n    for (JettySolrRunner solrRunner : cluster.getJettySolrRunners()) {\n      for (SolrCore solrCore : solrRunner.getCoreContainer().getCores()) {\n        File tlogFolder = new File(solrCore.getUlogDir(), UpdateLog.TLOG_NAME);\n        String[] tLogFiles = tlogFolder.list();\n        Arrays.sort(tLogFiles);\n        File lastTLogFile = new File(tlogFolder.getAbsolutePath() + \"/\" + tLogFiles[tLogFiles.length - 1]);\n        byte[] tlogBytes = IOUtils.toByteArray(new FileInputStream(lastTLogFile));\n        contentFiles.put(lastTLogFile, tlogBytes);\n        logHeaderSize = Math.min(tlogBytes.length, logHeaderSize);\n      }\n    }\n\n    ChaosMonkey.stop(cluster.getJettySolrRunners());\n    assertTrue(\"Timeout waiting for all not live\", ClusterStateUtil.waitForAllReplicasNotLive(cloudClient.getZkStateReader(), 45000));\n\n    for (Map.Entry<File, byte[]> entry : contentFiles.entrySet()) {\n      byte[] tlogBytes = entry.getValue();\n\n      if (tlogBytes.length <= logHeaderSize) continue;\n      FileOutputStream stream = new FileOutputStream(entry.getKey());\n      int skipLastBytes = Math.max(random().nextInt(tlogBytes.length - logHeaderSize), 2);\n      for (int i = 0; i < entry.getValue().length - skipLastBytes; i++) {\n        stream.write(tlogBytes[i]);\n      }\n      stream.close();\n    }\n\n    ChaosMonkey.start(cluster.getJettySolrRunners());\n    assertTrue(\"Timeout waiting for all live and active\", ClusterStateUtil.waitForAllActiveAndLiveReplicas(cloudClient.getZkStateReader(), COLLECTION, 120000));\n\n    resp = cloudClient.query(COLLECTION, params);\n    // Make sure cluster still healthy\n    assertTrue(resp.getResults().getNumFound() >= 2);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"60b61628d1912768f51eccaa8ead5a5a32ab34c6","date":1477427681,"type":0,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestCloudRecovery#corruptedLogTest().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  public void corruptedLogTest() throws Exception {\n    AtomicInteger countReplayLog = new AtomicInteger(0);\n    DirectUpdateHandler2.commitOnClose = false;\n    UpdateLog.testing_logReplayFinishHook = countReplayLog::incrementAndGet;\n\n    CloudSolrClient cloudClient = cluster.getSolrClient();\n    cloudClient.add(COLLECTION, sdoc(\"id\", \"1000\"));\n    cloudClient.add(COLLECTION, sdoc(\"id\", \"1001\"));\n    for (int i = 0; i < 10; i++) {\n      cloudClient.add(COLLECTION, sdoc(\"id\", String.valueOf(i)));\n    }\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"q\", \"*:*\");\n    QueryResponse resp = cloudClient.query(COLLECTION, params);\n    assertEquals(0, resp.getResults().getNumFound());\n\n    int logHeaderSize = Integer.MAX_VALUE;\n    Map<File, byte[]> contentFiles = new HashMap<>();\n    for (JettySolrRunner solrRunner : cluster.getJettySolrRunners()) {\n      for (SolrCore solrCore : solrRunner.getCoreContainer().getCores()) {\n        File tlogFolder = new File(solrCore.getUlogDir(), UpdateLog.TLOG_NAME);\n        String[] tLogFiles = tlogFolder.list();\n        Arrays.sort(tLogFiles);\n        File lastTLogFile = new File(tlogFolder.getAbsolutePath() + \"/\" + tLogFiles[tLogFiles.length - 1]);\n        byte[] tlogBytes = IOUtils.toByteArray(new FileInputStream(lastTLogFile));\n        contentFiles.put(lastTLogFile, tlogBytes);\n        logHeaderSize = Math.min(tlogBytes.length, logHeaderSize);\n      }\n    }\n\n    ChaosMonkey.stop(cluster.getJettySolrRunners());\n    assertTrue(\"Timeout waiting for all not live\", ClusterStateUtil.waitForAllReplicasNotLive(cloudClient.getZkStateReader(), 45000));\n\n    for (Map.Entry<File, byte[]> entry : contentFiles.entrySet()) {\n      byte[] tlogBytes = entry.getValue();\n\n      if (tlogBytes.length <= logHeaderSize) continue;\n      FileOutputStream stream = new FileOutputStream(entry.getKey());\n      int skipLastBytes = Math.max(random().nextInt(tlogBytes.length - logHeaderSize), 2);\n      for (int i = 0; i < entry.getValue().length - skipLastBytes; i++) {\n        stream.write(tlogBytes[i]);\n      }\n      stream.close();\n    }\n\n    ChaosMonkey.start(cluster.getJettySolrRunners());\n    assertTrue(\"Timeout waiting for all live and active\", ClusterStateUtil.waitForAllActiveAndLiveReplicas(cloudClient.getZkStateReader(), COLLECTION, 120000));\n\n    resp = cloudClient.query(COLLECTION, params);\n    // Make sure cluster still healthy\n    assertTrue(resp.getResults().getNumFound() >= 2);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"80d0e6d59ae23f4a6f30eaf40bfb40742300287f","date":1477598926,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestCloudRecovery#corruptedLogTest().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  public void corruptedLogTest() throws Exception {\n    AtomicInteger countReplayLog = new AtomicInteger(0);\n    DirectUpdateHandler2.commitOnClose = false;\n    UpdateLog.testing_logReplayFinishHook = countReplayLog::incrementAndGet;\n\n    CloudSolrClient cloudClient = cluster.getSolrClient();\n    cloudClient.add(COLLECTION, sdoc(\"id\", \"1000\"));\n    cloudClient.add(COLLECTION, sdoc(\"id\", \"1001\"));\n    for (int i = 0; i < 10; i++) {\n      cloudClient.add(COLLECTION, sdoc(\"id\", String.valueOf(i)));\n    }\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"q\", \"*:*\");\n    QueryResponse resp = cloudClient.query(COLLECTION, params);\n    assertEquals(0, resp.getResults().getNumFound());\n\n    int logHeaderSize = Integer.MAX_VALUE;\n    Map<File, byte[]> contentFiles = new HashMap<>();\n    for (JettySolrRunner solrRunner : cluster.getJettySolrRunners()) {\n      for (SolrCore solrCore : solrRunner.getCoreContainer().getCores()) {\n        File tlogFolder = new File(solrCore.getUlogDir(), UpdateLog.TLOG_NAME);\n        String[] tLogFiles = tlogFolder.list();\n        Arrays.sort(tLogFiles);\n        File lastTLogFile = new File(tlogFolder.getAbsolutePath() + \"/\" + tLogFiles[tLogFiles.length - 1]);\n        byte[] tlogBytes = IOUtils.toByteArray(new FileInputStream(lastTLogFile));\n        contentFiles.put(lastTLogFile, tlogBytes);\n        logHeaderSize = Math.min(tlogBytes.length, logHeaderSize);\n      }\n    }\n\n    ChaosMonkey.stop(cluster.getJettySolrRunners());\n    assertTrue(\"Timeout waiting for all not live\", ClusterStateUtil.waitForAllReplicasNotLive(cloudClient.getZkStateReader(), 45000));\n\n    for (Map.Entry<File, byte[]> entry : contentFiles.entrySet()) {\n      byte[] tlogBytes = entry.getValue();\n\n      if (tlogBytes.length <= logHeaderSize) continue;\n      FileOutputStream stream = new FileOutputStream(entry.getKey());\n      int skipLastBytes = Math.max(random().nextInt(tlogBytes.length - logHeaderSize), 2);\n      for (int i = 0; i < entry.getValue().length - skipLastBytes; i++) {\n        stream.write(tlogBytes[i]);\n      }\n      stream.close();\n    }\n\n    ChaosMonkey.start(cluster.getJettySolrRunners());\n    assertTrue(\"Timeout waiting for all live and active\", ClusterStateUtil.waitForAllActiveAndLiveReplicas(cloudClient.getZkStateReader(), COLLECTION, 120000));\n\n    resp = cloudClient.query(COLLECTION, params);\n    // Make sure cluster still healthy\n    assertTrue(resp.getResults().getNumFound() >= 2);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ddd2d71ed1b949bd0bdb81cb2e586348ba842095","date":1526614574,"type":3,"author":"Cao Manh Dat","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestCloudRecovery#corruptedLogTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/TestCloudRecovery#corruptedLogTest().mjava","sourceNew":"  @Test\n  public void corruptedLogTest() throws Exception {\n    AtomicInteger countReplayLog = new AtomicInteger(0);\n    DirectUpdateHandler2.commitOnClose = false;\n    UpdateLog.testing_logReplayFinishHook = countReplayLog::incrementAndGet;\n\n    CloudSolrClient cloudClient = cluster.getSolrClient();\n    cloudClient.add(COLLECTION, sdoc(\"id\", \"1000\"));\n    cloudClient.add(COLLECTION, sdoc(\"id\", \"1001\"));\n    for (int i = 0; i < 10; i++) {\n      cloudClient.add(COLLECTION, sdoc(\"id\", String.valueOf(i)));\n    }\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"q\", \"*:*\");\n    QueryResponse resp = cloudClient.query(COLLECTION, params);\n    assertEquals(0, resp.getResults().getNumFound());\n\n    int logHeaderSize = Integer.MAX_VALUE;\n    Map<String, byte[]> contentFiles = new HashMap<>();\n    for (JettySolrRunner solrRunner : cluster.getJettySolrRunners()) {\n      for (SolrCore solrCore : solrRunner.getCoreContainer().getCores()) {\n        File tlogFolder = new File(solrCore.getUlogDir(), UpdateLog.TLOG_NAME);\n        String[] tLogFiles = tlogFolder.list();\n        Arrays.sort(tLogFiles);\n        String lastTLogFile = tlogFolder.getAbsolutePath() + \"/\" + tLogFiles[tLogFiles.length - 1];\n        try (FileInputStream inputStream = new FileInputStream(lastTLogFile)){\n          byte[] tlogBytes = IOUtils.toByteArray(inputStream);\n          contentFiles.put(lastTLogFile, tlogBytes);\n          logHeaderSize = Math.min(tlogBytes.length, logHeaderSize);\n        }\n      }\n    }\n\n    ChaosMonkey.stop(cluster.getJettySolrRunners());\n    assertTrue(\"Timeout waiting for all not live\", ClusterStateUtil.waitForAllReplicasNotLive(cloudClient.getZkStateReader(), 45000));\n\n    for (Map.Entry<String, byte[]> entry : contentFiles.entrySet()) {\n      byte[] tlogBytes = entry.getValue();\n\n      if (tlogBytes.length <= logHeaderSize) continue;\n      try (FileOutputStream stream = new FileOutputStream(entry.getKey())) {\n        int skipLastBytes = Math.max(random().nextInt(tlogBytes.length - logHeaderSize), 2);\n        for (int i = 0; i < entry.getValue().length - skipLastBytes; i++) {\n          stream.write(tlogBytes[i]);\n        }\n      }\n    }\n\n    ChaosMonkey.start(cluster.getJettySolrRunners());\n    assertTrue(\"Timeout waiting for all live and active\", ClusterStateUtil.waitForAllActiveAndLiveReplicas(cloudClient.getZkStateReader(), COLLECTION, 120000));\n\n    resp = cloudClient.query(COLLECTION, params);\n    // Make sure cluster still healthy\n    assertTrue(resp.getResults().getNumFound() >= 2);\n  }\n\n","sourceOld":"  @Test\n  public void corruptedLogTest() throws Exception {\n    AtomicInteger countReplayLog = new AtomicInteger(0);\n    DirectUpdateHandler2.commitOnClose = false;\n    UpdateLog.testing_logReplayFinishHook = countReplayLog::incrementAndGet;\n\n    CloudSolrClient cloudClient = cluster.getSolrClient();\n    cloudClient.add(COLLECTION, sdoc(\"id\", \"1000\"));\n    cloudClient.add(COLLECTION, sdoc(\"id\", \"1001\"));\n    for (int i = 0; i < 10; i++) {\n      cloudClient.add(COLLECTION, sdoc(\"id\", String.valueOf(i)));\n    }\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"q\", \"*:*\");\n    QueryResponse resp = cloudClient.query(COLLECTION, params);\n    assertEquals(0, resp.getResults().getNumFound());\n\n    int logHeaderSize = Integer.MAX_VALUE;\n    Map<File, byte[]> contentFiles = new HashMap<>();\n    for (JettySolrRunner solrRunner : cluster.getJettySolrRunners()) {\n      for (SolrCore solrCore : solrRunner.getCoreContainer().getCores()) {\n        File tlogFolder = new File(solrCore.getUlogDir(), UpdateLog.TLOG_NAME);\n        String[] tLogFiles = tlogFolder.list();\n        Arrays.sort(tLogFiles);\n        File lastTLogFile = new File(tlogFolder.getAbsolutePath() + \"/\" + tLogFiles[tLogFiles.length - 1]);\n        byte[] tlogBytes = IOUtils.toByteArray(new FileInputStream(lastTLogFile));\n        contentFiles.put(lastTLogFile, tlogBytes);\n        logHeaderSize = Math.min(tlogBytes.length, logHeaderSize);\n      }\n    }\n\n    ChaosMonkey.stop(cluster.getJettySolrRunners());\n    assertTrue(\"Timeout waiting for all not live\", ClusterStateUtil.waitForAllReplicasNotLive(cloudClient.getZkStateReader(), 45000));\n\n    for (Map.Entry<File, byte[]> entry : contentFiles.entrySet()) {\n      byte[] tlogBytes = entry.getValue();\n\n      if (tlogBytes.length <= logHeaderSize) continue;\n      FileOutputStream stream = new FileOutputStream(entry.getKey());\n      int skipLastBytes = Math.max(random().nextInt(tlogBytes.length - logHeaderSize), 2);\n      for (int i = 0; i < entry.getValue().length - skipLastBytes; i++) {\n        stream.write(tlogBytes[i]);\n      }\n      stream.close();\n    }\n\n    ChaosMonkey.start(cluster.getJettySolrRunners());\n    assertTrue(\"Timeout waiting for all live and active\", ClusterStateUtil.waitForAllActiveAndLiveReplicas(cloudClient.getZkStateReader(), COLLECTION, 120000));\n\n    resp = cloudClient.query(COLLECTION, params);\n    // Make sure cluster still healthy\n    assertTrue(resp.getResults().getNumFound() >= 2);\n  }\n\n","bugFix":null,"bugIntro":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"44dd40f6c2c1465aebf4677bab10f696c7ea18d8","date":1539566013,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestCloudRecovery#corruptedLogTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/TestCloudRecovery#corruptedLogTest().mjava","sourceNew":"  @Test\n  @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 14-Oct-2018\n  public void corruptedLogTest() throws Exception {\n    AtomicInteger countReplayLog = new AtomicInteger(0);\n    DirectUpdateHandler2.commitOnClose = false;\n    UpdateLog.testing_logReplayFinishHook = countReplayLog::incrementAndGet;\n\n    CloudSolrClient cloudClient = cluster.getSolrClient();\n    cloudClient.add(COLLECTION, sdoc(\"id\", \"1000\"));\n    cloudClient.add(COLLECTION, sdoc(\"id\", \"1001\"));\n    for (int i = 0; i < 10; i++) {\n      cloudClient.add(COLLECTION, sdoc(\"id\", String.valueOf(i)));\n    }\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"q\", \"*:*\");\n    QueryResponse resp = cloudClient.query(COLLECTION, params);\n    assertEquals(0, resp.getResults().getNumFound());\n\n    int logHeaderSize = Integer.MAX_VALUE;\n    Map<String, byte[]> contentFiles = new HashMap<>();\n    for (JettySolrRunner solrRunner : cluster.getJettySolrRunners()) {\n      for (SolrCore solrCore : solrRunner.getCoreContainer().getCores()) {\n        File tlogFolder = new File(solrCore.getUlogDir(), UpdateLog.TLOG_NAME);\n        String[] tLogFiles = tlogFolder.list();\n        Arrays.sort(tLogFiles);\n        String lastTLogFile = tlogFolder.getAbsolutePath() + \"/\" + tLogFiles[tLogFiles.length - 1];\n        try (FileInputStream inputStream = new FileInputStream(lastTLogFile)){\n          byte[] tlogBytes = IOUtils.toByteArray(inputStream);\n          contentFiles.put(lastTLogFile, tlogBytes);\n          logHeaderSize = Math.min(tlogBytes.length, logHeaderSize);\n        }\n      }\n    }\n\n    ChaosMonkey.stop(cluster.getJettySolrRunners());\n    assertTrue(\"Timeout waiting for all not live\", ClusterStateUtil.waitForAllReplicasNotLive(cloudClient.getZkStateReader(), 45000));\n\n    for (Map.Entry<String, byte[]> entry : contentFiles.entrySet()) {\n      byte[] tlogBytes = entry.getValue();\n\n      if (tlogBytes.length <= logHeaderSize) continue;\n      try (FileOutputStream stream = new FileOutputStream(entry.getKey())) {\n        int skipLastBytes = Math.max(random().nextInt(tlogBytes.length - logHeaderSize), 2);\n        for (int i = 0; i < entry.getValue().length - skipLastBytes; i++) {\n          stream.write(tlogBytes[i]);\n        }\n      }\n    }\n\n    ChaosMonkey.start(cluster.getJettySolrRunners());\n    assertTrue(\"Timeout waiting for all live and active\", ClusterStateUtil.waitForAllActiveAndLiveReplicas(cloudClient.getZkStateReader(), COLLECTION, 120000));\n\n    resp = cloudClient.query(COLLECTION, params);\n    // Make sure cluster still healthy\n    assertTrue(resp.getResults().getNumFound() >= 2);\n  }\n\n","sourceOld":"  @Test\n  public void corruptedLogTest() throws Exception {\n    AtomicInteger countReplayLog = new AtomicInteger(0);\n    DirectUpdateHandler2.commitOnClose = false;\n    UpdateLog.testing_logReplayFinishHook = countReplayLog::incrementAndGet;\n\n    CloudSolrClient cloudClient = cluster.getSolrClient();\n    cloudClient.add(COLLECTION, sdoc(\"id\", \"1000\"));\n    cloudClient.add(COLLECTION, sdoc(\"id\", \"1001\"));\n    for (int i = 0; i < 10; i++) {\n      cloudClient.add(COLLECTION, sdoc(\"id\", String.valueOf(i)));\n    }\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"q\", \"*:*\");\n    QueryResponse resp = cloudClient.query(COLLECTION, params);\n    assertEquals(0, resp.getResults().getNumFound());\n\n    int logHeaderSize = Integer.MAX_VALUE;\n    Map<String, byte[]> contentFiles = new HashMap<>();\n    for (JettySolrRunner solrRunner : cluster.getJettySolrRunners()) {\n      for (SolrCore solrCore : solrRunner.getCoreContainer().getCores()) {\n        File tlogFolder = new File(solrCore.getUlogDir(), UpdateLog.TLOG_NAME);\n        String[] tLogFiles = tlogFolder.list();\n        Arrays.sort(tLogFiles);\n        String lastTLogFile = tlogFolder.getAbsolutePath() + \"/\" + tLogFiles[tLogFiles.length - 1];\n        try (FileInputStream inputStream = new FileInputStream(lastTLogFile)){\n          byte[] tlogBytes = IOUtils.toByteArray(inputStream);\n          contentFiles.put(lastTLogFile, tlogBytes);\n          logHeaderSize = Math.min(tlogBytes.length, logHeaderSize);\n        }\n      }\n    }\n\n    ChaosMonkey.stop(cluster.getJettySolrRunners());\n    assertTrue(\"Timeout waiting for all not live\", ClusterStateUtil.waitForAllReplicasNotLive(cloudClient.getZkStateReader(), 45000));\n\n    for (Map.Entry<String, byte[]> entry : contentFiles.entrySet()) {\n      byte[] tlogBytes = entry.getValue();\n\n      if (tlogBytes.length <= logHeaderSize) continue;\n      try (FileOutputStream stream = new FileOutputStream(entry.getKey())) {\n        int skipLastBytes = Math.max(random().nextInt(tlogBytes.length - logHeaderSize), 2);\n        for (int i = 0; i < entry.getValue().length - skipLastBytes; i++) {\n          stream.write(tlogBytes[i]);\n        }\n      }\n    }\n\n    ChaosMonkey.start(cluster.getJettySolrRunners());\n    assertTrue(\"Timeout waiting for all live and active\", ClusterStateUtil.waitForAllActiveAndLiveReplicas(cloudClient.getZkStateReader(), COLLECTION, 120000));\n\n    resp = cloudClient.query(COLLECTION, params);\n    // Make sure cluster still healthy\n    assertTrue(resp.getResults().getNumFound() >= 2);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bb222a3f9d9421d5c95afce73013fbd8de07ea1f","date":1543514331,"type":3,"author":"markrmiller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestCloudRecovery#corruptedLogTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/TestCloudRecovery#corruptedLogTest().mjava","sourceNew":"  @Test\n  @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 14-Oct-2018\n  public void corruptedLogTest() throws Exception {\n    AtomicInteger countReplayLog = new AtomicInteger(0);\n    DirectUpdateHandler2.commitOnClose = false;\n    UpdateLog.testing_logReplayFinishHook = countReplayLog::incrementAndGet;\n\n    CloudSolrClient cloudClient = cluster.getSolrClient();\n    cloudClient.add(COLLECTION, sdoc(\"id\", \"1000\"));\n    cloudClient.add(COLLECTION, sdoc(\"id\", \"1001\"));\n    for (int i = 0; i < 10; i++) {\n      cloudClient.add(COLLECTION, sdoc(\"id\", String.valueOf(i)));\n    }\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"q\", \"*:*\");\n    QueryResponse resp = cloudClient.query(COLLECTION, params);\n    assertEquals(0, resp.getResults().getNumFound());\n\n    int logHeaderSize = Integer.MAX_VALUE;\n    Map<String, byte[]> contentFiles = new HashMap<>();\n    for (JettySolrRunner solrRunner : cluster.getJettySolrRunners()) {\n      for (SolrCore solrCore : solrRunner.getCoreContainer().getCores()) {\n        File tlogFolder = new File(solrCore.getUlogDir(), UpdateLog.TLOG_NAME);\n        String[] tLogFiles = tlogFolder.list();\n        Arrays.sort(tLogFiles);\n        String lastTLogFile = tlogFolder.getAbsolutePath() + \"/\" + tLogFiles[tLogFiles.length - 1];\n        try (FileInputStream inputStream = new FileInputStream(lastTLogFile)){\n          byte[] tlogBytes = IOUtils.toByteArray(inputStream);\n          contentFiles.put(lastTLogFile, tlogBytes);\n          logHeaderSize = Math.min(tlogBytes.length, logHeaderSize);\n        }\n      }\n    }\n\n    ChaosMonkey.stop(cluster.getJettySolrRunners());\n    \n    for (JettySolrRunner j : cluster.getJettySolrRunners()) {\n      cluster.waitForJettyToStop(j);\n    }\n    \n    assertTrue(\"Timeout waiting for all not live\", ClusterStateUtil.waitForAllReplicasNotLive(cloudClient.getZkStateReader(), 45000));\n\n    for (Map.Entry<String, byte[]> entry : contentFiles.entrySet()) {\n      byte[] tlogBytes = entry.getValue();\n\n      if (tlogBytes.length <= logHeaderSize) continue;\n      try (FileOutputStream stream = new FileOutputStream(entry.getKey())) {\n        int skipLastBytes = Math.max(random().nextInt(tlogBytes.length - logHeaderSize)-2, 2);\n        for (int i = 0; i < entry.getValue().length - skipLastBytes; i++) {\n          stream.write(tlogBytes[i]);\n        }\n      }\n    }\n\n    ChaosMonkey.start(cluster.getJettySolrRunners());\n    cluster.waitForAllNodes(30);\n    \n    Thread.sleep(1000);\n    \n    assertTrue(\"Timeout waiting for all live and active\", ClusterStateUtil.waitForAllActiveAndLiveReplicas(cloudClient.getZkStateReader(), COLLECTION, 120000));\n    \n    cluster.waitForActiveCollection(COLLECTION, 2, 2 * (nrtReplicas + tlogReplicas));\n    \n    cloudClient.getZkStateReader().forceUpdateCollection(COLLECTION);\n    \n    resp = cloudClient.query(COLLECTION, params);\n    // Make sure cluster still healthy\n    // TODO: AwaitsFix - this will fail under test beasting\n    // assertTrue(resp.toString(), resp.getResults().getNumFound() >= 2);\n  }\n\n","sourceOld":"  @Test\n  @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 14-Oct-2018\n  public void corruptedLogTest() throws Exception {\n    AtomicInteger countReplayLog = new AtomicInteger(0);\n    DirectUpdateHandler2.commitOnClose = false;\n    UpdateLog.testing_logReplayFinishHook = countReplayLog::incrementAndGet;\n\n    CloudSolrClient cloudClient = cluster.getSolrClient();\n    cloudClient.add(COLLECTION, sdoc(\"id\", \"1000\"));\n    cloudClient.add(COLLECTION, sdoc(\"id\", \"1001\"));\n    for (int i = 0; i < 10; i++) {\n      cloudClient.add(COLLECTION, sdoc(\"id\", String.valueOf(i)));\n    }\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"q\", \"*:*\");\n    QueryResponse resp = cloudClient.query(COLLECTION, params);\n    assertEquals(0, resp.getResults().getNumFound());\n\n    int logHeaderSize = Integer.MAX_VALUE;\n    Map<String, byte[]> contentFiles = new HashMap<>();\n    for (JettySolrRunner solrRunner : cluster.getJettySolrRunners()) {\n      for (SolrCore solrCore : solrRunner.getCoreContainer().getCores()) {\n        File tlogFolder = new File(solrCore.getUlogDir(), UpdateLog.TLOG_NAME);\n        String[] tLogFiles = tlogFolder.list();\n        Arrays.sort(tLogFiles);\n        String lastTLogFile = tlogFolder.getAbsolutePath() + \"/\" + tLogFiles[tLogFiles.length - 1];\n        try (FileInputStream inputStream = new FileInputStream(lastTLogFile)){\n          byte[] tlogBytes = IOUtils.toByteArray(inputStream);\n          contentFiles.put(lastTLogFile, tlogBytes);\n          logHeaderSize = Math.min(tlogBytes.length, logHeaderSize);\n        }\n      }\n    }\n\n    ChaosMonkey.stop(cluster.getJettySolrRunners());\n    assertTrue(\"Timeout waiting for all not live\", ClusterStateUtil.waitForAllReplicasNotLive(cloudClient.getZkStateReader(), 45000));\n\n    for (Map.Entry<String, byte[]> entry : contentFiles.entrySet()) {\n      byte[] tlogBytes = entry.getValue();\n\n      if (tlogBytes.length <= logHeaderSize) continue;\n      try (FileOutputStream stream = new FileOutputStream(entry.getKey())) {\n        int skipLastBytes = Math.max(random().nextInt(tlogBytes.length - logHeaderSize), 2);\n        for (int i = 0; i < entry.getValue().length - skipLastBytes; i++) {\n          stream.write(tlogBytes[i]);\n        }\n      }\n    }\n\n    ChaosMonkey.start(cluster.getJettySolrRunners());\n    assertTrue(\"Timeout waiting for all live and active\", ClusterStateUtil.waitForAllActiveAndLiveReplicas(cloudClient.getZkStateReader(), COLLECTION, 120000));\n\n    resp = cloudClient.query(COLLECTION, params);\n    // Make sure cluster still healthy\n    assertTrue(resp.getResults().getNumFound() >= 2);\n  }\n\n","bugFix":["ddd2d71ed1b949bd0bdb81cb2e586348ba842095","40b4f9867bcb3a1cf45f8a02b05af0bdf552746f"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b5c929d2716fa79d443b93a82adb1da5b578ebd8","date":1550428858,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestCloudRecovery#corruptedLogTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/TestCloudRecovery#corruptedLogTest().mjava","sourceNew":"  @Test\n  // commented out on: 17-Feb-2019   @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 14-Oct-2018\n  public void corruptedLogTest() throws Exception {\n    AtomicInteger countReplayLog = new AtomicInteger(0);\n    DirectUpdateHandler2.commitOnClose = false;\n    UpdateLog.testing_logReplayFinishHook = countReplayLog::incrementAndGet;\n\n    CloudSolrClient cloudClient = cluster.getSolrClient();\n    cloudClient.add(COLLECTION, sdoc(\"id\", \"1000\"));\n    cloudClient.add(COLLECTION, sdoc(\"id\", \"1001\"));\n    for (int i = 0; i < 10; i++) {\n      cloudClient.add(COLLECTION, sdoc(\"id\", String.valueOf(i)));\n    }\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"q\", \"*:*\");\n    QueryResponse resp = cloudClient.query(COLLECTION, params);\n    assertEquals(0, resp.getResults().getNumFound());\n\n    int logHeaderSize = Integer.MAX_VALUE;\n    Map<String, byte[]> contentFiles = new HashMap<>();\n    for (JettySolrRunner solrRunner : cluster.getJettySolrRunners()) {\n      for (SolrCore solrCore : solrRunner.getCoreContainer().getCores()) {\n        File tlogFolder = new File(solrCore.getUlogDir(), UpdateLog.TLOG_NAME);\n        String[] tLogFiles = tlogFolder.list();\n        Arrays.sort(tLogFiles);\n        String lastTLogFile = tlogFolder.getAbsolutePath() + \"/\" + tLogFiles[tLogFiles.length - 1];\n        try (FileInputStream inputStream = new FileInputStream(lastTLogFile)){\n          byte[] tlogBytes = IOUtils.toByteArray(inputStream);\n          contentFiles.put(lastTLogFile, tlogBytes);\n          logHeaderSize = Math.min(tlogBytes.length, logHeaderSize);\n        }\n      }\n    }\n\n    ChaosMonkey.stop(cluster.getJettySolrRunners());\n    \n    for (JettySolrRunner j : cluster.getJettySolrRunners()) {\n      cluster.waitForJettyToStop(j);\n    }\n    \n    assertTrue(\"Timeout waiting for all not live\", ClusterStateUtil.waitForAllReplicasNotLive(cloudClient.getZkStateReader(), 45000));\n\n    for (Map.Entry<String, byte[]> entry : contentFiles.entrySet()) {\n      byte[] tlogBytes = entry.getValue();\n\n      if (tlogBytes.length <= logHeaderSize) continue;\n      try (FileOutputStream stream = new FileOutputStream(entry.getKey())) {\n        int skipLastBytes = Math.max(random().nextInt(tlogBytes.length - logHeaderSize)-2, 2);\n        for (int i = 0; i < entry.getValue().length - skipLastBytes; i++) {\n          stream.write(tlogBytes[i]);\n        }\n      }\n    }\n\n    ChaosMonkey.start(cluster.getJettySolrRunners());\n    cluster.waitForAllNodes(30);\n    \n    Thread.sleep(1000);\n    \n    assertTrue(\"Timeout waiting for all live and active\", ClusterStateUtil.waitForAllActiveAndLiveReplicas(cloudClient.getZkStateReader(), COLLECTION, 120000));\n    \n    cluster.waitForActiveCollection(COLLECTION, 2, 2 * (nrtReplicas + tlogReplicas));\n    \n    cloudClient.getZkStateReader().forceUpdateCollection(COLLECTION);\n    \n    resp = cloudClient.query(COLLECTION, params);\n    // Make sure cluster still healthy\n    // TODO: AwaitsFix - this will fail under test beasting\n    // assertTrue(resp.toString(), resp.getResults().getNumFound() >= 2);\n  }\n\n","sourceOld":"  @Test\n  @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 14-Oct-2018\n  public void corruptedLogTest() throws Exception {\n    AtomicInteger countReplayLog = new AtomicInteger(0);\n    DirectUpdateHandler2.commitOnClose = false;\n    UpdateLog.testing_logReplayFinishHook = countReplayLog::incrementAndGet;\n\n    CloudSolrClient cloudClient = cluster.getSolrClient();\n    cloudClient.add(COLLECTION, sdoc(\"id\", \"1000\"));\n    cloudClient.add(COLLECTION, sdoc(\"id\", \"1001\"));\n    for (int i = 0; i < 10; i++) {\n      cloudClient.add(COLLECTION, sdoc(\"id\", String.valueOf(i)));\n    }\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"q\", \"*:*\");\n    QueryResponse resp = cloudClient.query(COLLECTION, params);\n    assertEquals(0, resp.getResults().getNumFound());\n\n    int logHeaderSize = Integer.MAX_VALUE;\n    Map<String, byte[]> contentFiles = new HashMap<>();\n    for (JettySolrRunner solrRunner : cluster.getJettySolrRunners()) {\n      for (SolrCore solrCore : solrRunner.getCoreContainer().getCores()) {\n        File tlogFolder = new File(solrCore.getUlogDir(), UpdateLog.TLOG_NAME);\n        String[] tLogFiles = tlogFolder.list();\n        Arrays.sort(tLogFiles);\n        String lastTLogFile = tlogFolder.getAbsolutePath() + \"/\" + tLogFiles[tLogFiles.length - 1];\n        try (FileInputStream inputStream = new FileInputStream(lastTLogFile)){\n          byte[] tlogBytes = IOUtils.toByteArray(inputStream);\n          contentFiles.put(lastTLogFile, tlogBytes);\n          logHeaderSize = Math.min(tlogBytes.length, logHeaderSize);\n        }\n      }\n    }\n\n    ChaosMonkey.stop(cluster.getJettySolrRunners());\n    \n    for (JettySolrRunner j : cluster.getJettySolrRunners()) {\n      cluster.waitForJettyToStop(j);\n    }\n    \n    assertTrue(\"Timeout waiting for all not live\", ClusterStateUtil.waitForAllReplicasNotLive(cloudClient.getZkStateReader(), 45000));\n\n    for (Map.Entry<String, byte[]> entry : contentFiles.entrySet()) {\n      byte[] tlogBytes = entry.getValue();\n\n      if (tlogBytes.length <= logHeaderSize) continue;\n      try (FileOutputStream stream = new FileOutputStream(entry.getKey())) {\n        int skipLastBytes = Math.max(random().nextInt(tlogBytes.length - logHeaderSize)-2, 2);\n        for (int i = 0; i < entry.getValue().length - skipLastBytes; i++) {\n          stream.write(tlogBytes[i]);\n        }\n      }\n    }\n\n    ChaosMonkey.start(cluster.getJettySolrRunners());\n    cluster.waitForAllNodes(30);\n    \n    Thread.sleep(1000);\n    \n    assertTrue(\"Timeout waiting for all live and active\", ClusterStateUtil.waitForAllActiveAndLiveReplicas(cloudClient.getZkStateReader(), COLLECTION, 120000));\n    \n    cluster.waitForActiveCollection(COLLECTION, 2, 2 * (nrtReplicas + tlogReplicas));\n    \n    cloudClient.getZkStateReader().forceUpdateCollection(COLLECTION);\n    \n    resp = cloudClient.query(COLLECTION, params);\n    // Make sure cluster still healthy\n    // TODO: AwaitsFix - this will fail under test beasting\n    // assertTrue(resp.toString(), resp.getResults().getNumFound() >= 2);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b6a0ad05ae2af8aa028b1a6099a8222fad0bc8c1","date":1579200426,"type":3,"author":"Chris Hostetter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestCloudRecovery#corruptedLogTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/TestCloudRecovery#corruptedLogTest().mjava","sourceNew":"  @Test\n  // commented out on: 17-Feb-2019   @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 14-Oct-2018\n  public void corruptedLogTest() throws Exception {\n    AtomicInteger countReplayLog = new AtomicInteger(0);\n    TestInjection.skipIndexWriterCommitOnClose = true;\n    UpdateLog.testing_logReplayFinishHook = countReplayLog::incrementAndGet;\n\n    CloudSolrClient cloudClient = cluster.getSolrClient();\n    cloudClient.add(COLLECTION, sdoc(\"id\", \"1000\"));\n    cloudClient.add(COLLECTION, sdoc(\"id\", \"1001\"));\n    for (int i = 0; i < 10; i++) {\n      cloudClient.add(COLLECTION, sdoc(\"id\", String.valueOf(i)));\n    }\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"q\", \"*:*\");\n    QueryResponse resp = cloudClient.query(COLLECTION, params);\n    assertEquals(0, resp.getResults().getNumFound());\n\n    int logHeaderSize = Integer.MAX_VALUE;\n    Map<String, byte[]> contentFiles = new HashMap<>();\n    for (JettySolrRunner solrRunner : cluster.getJettySolrRunners()) {\n      for (SolrCore solrCore : solrRunner.getCoreContainer().getCores()) {\n        File tlogFolder = new File(solrCore.getUlogDir(), UpdateLog.TLOG_NAME);\n        String[] tLogFiles = tlogFolder.list();\n        Arrays.sort(tLogFiles);\n        String lastTLogFile = tlogFolder.getAbsolutePath() + \"/\" + tLogFiles[tLogFiles.length - 1];\n        try (FileInputStream inputStream = new FileInputStream(lastTLogFile)){\n          byte[] tlogBytes = IOUtils.toByteArray(inputStream);\n          contentFiles.put(lastTLogFile, tlogBytes);\n          logHeaderSize = Math.min(tlogBytes.length, logHeaderSize);\n        }\n      }\n    }\n\n    ChaosMonkey.stop(cluster.getJettySolrRunners());\n    \n    for (JettySolrRunner j : cluster.getJettySolrRunners()) {\n      cluster.waitForJettyToStop(j);\n    }\n    \n    assertTrue(\"Timeout waiting for all not live\", ClusterStateUtil.waitForAllReplicasNotLive(cloudClient.getZkStateReader(), 45000));\n\n    for (Map.Entry<String, byte[]> entry : contentFiles.entrySet()) {\n      byte[] tlogBytes = entry.getValue();\n\n      if (tlogBytes.length <= logHeaderSize) continue;\n      try (FileOutputStream stream = new FileOutputStream(entry.getKey())) {\n        int skipLastBytes = Math.max(random().nextInt(tlogBytes.length - logHeaderSize)-2, 2);\n        for (int i = 0; i < entry.getValue().length - skipLastBytes; i++) {\n          stream.write(tlogBytes[i]);\n        }\n      }\n    }\n\n    ChaosMonkey.start(cluster.getJettySolrRunners());\n    cluster.waitForAllNodes(30);\n    \n    Thread.sleep(1000);\n    \n    assertTrue(\"Timeout waiting for all live and active\", ClusterStateUtil.waitForAllActiveAndLiveReplicas(cloudClient.getZkStateReader(), COLLECTION, 120000));\n    \n    cluster.waitForActiveCollection(COLLECTION, 2, 2 * (nrtReplicas + tlogReplicas));\n    \n    cloudClient.getZkStateReader().forceUpdateCollection(COLLECTION);\n    \n    resp = cloudClient.query(COLLECTION, params);\n    // Make sure cluster still healthy\n    // TODO: AwaitsFix - this will fail under test beasting\n    // assertTrue(resp.toString(), resp.getResults().getNumFound() >= 2);\n  }\n\n","sourceOld":"  @Test\n  // commented out on: 17-Feb-2019   @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 14-Oct-2018\n  public void corruptedLogTest() throws Exception {\n    AtomicInteger countReplayLog = new AtomicInteger(0);\n    DirectUpdateHandler2.commitOnClose = false;\n    UpdateLog.testing_logReplayFinishHook = countReplayLog::incrementAndGet;\n\n    CloudSolrClient cloudClient = cluster.getSolrClient();\n    cloudClient.add(COLLECTION, sdoc(\"id\", \"1000\"));\n    cloudClient.add(COLLECTION, sdoc(\"id\", \"1001\"));\n    for (int i = 0; i < 10; i++) {\n      cloudClient.add(COLLECTION, sdoc(\"id\", String.valueOf(i)));\n    }\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"q\", \"*:*\");\n    QueryResponse resp = cloudClient.query(COLLECTION, params);\n    assertEquals(0, resp.getResults().getNumFound());\n\n    int logHeaderSize = Integer.MAX_VALUE;\n    Map<String, byte[]> contentFiles = new HashMap<>();\n    for (JettySolrRunner solrRunner : cluster.getJettySolrRunners()) {\n      for (SolrCore solrCore : solrRunner.getCoreContainer().getCores()) {\n        File tlogFolder = new File(solrCore.getUlogDir(), UpdateLog.TLOG_NAME);\n        String[] tLogFiles = tlogFolder.list();\n        Arrays.sort(tLogFiles);\n        String lastTLogFile = tlogFolder.getAbsolutePath() + \"/\" + tLogFiles[tLogFiles.length - 1];\n        try (FileInputStream inputStream = new FileInputStream(lastTLogFile)){\n          byte[] tlogBytes = IOUtils.toByteArray(inputStream);\n          contentFiles.put(lastTLogFile, tlogBytes);\n          logHeaderSize = Math.min(tlogBytes.length, logHeaderSize);\n        }\n      }\n    }\n\n    ChaosMonkey.stop(cluster.getJettySolrRunners());\n    \n    for (JettySolrRunner j : cluster.getJettySolrRunners()) {\n      cluster.waitForJettyToStop(j);\n    }\n    \n    assertTrue(\"Timeout waiting for all not live\", ClusterStateUtil.waitForAllReplicasNotLive(cloudClient.getZkStateReader(), 45000));\n\n    for (Map.Entry<String, byte[]> entry : contentFiles.entrySet()) {\n      byte[] tlogBytes = entry.getValue();\n\n      if (tlogBytes.length <= logHeaderSize) continue;\n      try (FileOutputStream stream = new FileOutputStream(entry.getKey())) {\n        int skipLastBytes = Math.max(random().nextInt(tlogBytes.length - logHeaderSize)-2, 2);\n        for (int i = 0; i < entry.getValue().length - skipLastBytes; i++) {\n          stream.write(tlogBytes[i]);\n        }\n      }\n    }\n\n    ChaosMonkey.start(cluster.getJettySolrRunners());\n    cluster.waitForAllNodes(30);\n    \n    Thread.sleep(1000);\n    \n    assertTrue(\"Timeout waiting for all live and active\", ClusterStateUtil.waitForAllActiveAndLiveReplicas(cloudClient.getZkStateReader(), COLLECTION, 120000));\n    \n    cluster.waitForActiveCollection(COLLECTION, 2, 2 * (nrtReplicas + tlogReplicas));\n    \n    cloudClient.getZkStateReader().forceUpdateCollection(COLLECTION);\n    \n    resp = cloudClient.query(COLLECTION, params);\n    // Make sure cluster still healthy\n    // TODO: AwaitsFix - this will fail under test beasting\n    // assertTrue(resp.toString(), resp.getResults().getNumFound() >= 2);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"44dd40f6c2c1465aebf4677bab10f696c7ea18d8":["ddd2d71ed1b949bd0bdb81cb2e586348ba842095"],"60b61628d1912768f51eccaa8ead5a5a32ab34c6":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","40b4f9867bcb3a1cf45f8a02b05af0bdf552746f"],"40b4f9867bcb3a1cf45f8a02b05af0bdf552746f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"bb222a3f9d9421d5c95afce73013fbd8de07ea1f":["44dd40f6c2c1465aebf4677bab10f696c7ea18d8"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"80d0e6d59ae23f4a6f30eaf40bfb40742300287f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","60b61628d1912768f51eccaa8ead5a5a32ab34c6"],"b5c929d2716fa79d443b93a82adb1da5b578ebd8":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"b6a0ad05ae2af8aa028b1a6099a8222fad0bc8c1":["b5c929d2716fa79d443b93a82adb1da5b578ebd8"],"ddd2d71ed1b949bd0bdb81cb2e586348ba842095":["60b61628d1912768f51eccaa8ead5a5a32ab34c6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b6a0ad05ae2af8aa028b1a6099a8222fad0bc8c1"]},"commit2Childs":{"44dd40f6c2c1465aebf4677bab10f696c7ea18d8":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"60b61628d1912768f51eccaa8ead5a5a32ab34c6":["80d0e6d59ae23f4a6f30eaf40bfb40742300287f","ddd2d71ed1b949bd0bdb81cb2e586348ba842095"],"40b4f9867bcb3a1cf45f8a02b05af0bdf552746f":["60b61628d1912768f51eccaa8ead5a5a32ab34c6"],"bb222a3f9d9421d5c95afce73013fbd8de07ea1f":["b5c929d2716fa79d443b93a82adb1da5b578ebd8"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["60b61628d1912768f51eccaa8ead5a5a32ab34c6","40b4f9867bcb3a1cf45f8a02b05af0bdf552746f","80d0e6d59ae23f4a6f30eaf40bfb40742300287f"],"80d0e6d59ae23f4a6f30eaf40bfb40742300287f":[],"b5c929d2716fa79d443b93a82adb1da5b578ebd8":["b6a0ad05ae2af8aa028b1a6099a8222fad0bc8c1"],"b6a0ad05ae2af8aa028b1a6099a8222fad0bc8c1":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"ddd2d71ed1b949bd0bdb81cb2e586348ba842095":["44dd40f6c2c1465aebf4677bab10f696c7ea18d8"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["80d0e6d59ae23f4a6f30eaf40bfb40742300287f","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}