{"path":"src/java/org/apache/lucene/index/TermsHashPerField#add(Token).mjava","commits":[{"id":"5350389bf83287111f7760b9e3db3af8e3648474","date":1216372812,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/TermsHashPerField#add(Token).mjava","pathOld":"/dev/null","sourceNew":"  // Primary entry point (for first TermsHash)\n  void add(Token token) throws IOException {\n\n    assert !postingsCompacted;\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n\n    // Get the text of this term.\n    final char[] tokenText = token.termBuffer();\n    final int tokenTextLen = token.termLength();\n\n    // Compute hashcode & replace any invalid UTF16 sequences\n    int downto = tokenTextLen;\n    int code = 0;\n    while (downto > 0) {\n      char ch = tokenText[--downto];\n\n      if (ch >= UnicodeUtil.UNI_SUR_LOW_START && ch <= UnicodeUtil.UNI_SUR_LOW_END) {\n        if (0 == downto) {\n          // Unpaired\n          ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;\n        } else {\n          final char ch2 = tokenText[downto-1];\n          if (ch2 >= UnicodeUtil.UNI_SUR_HIGH_START && ch2 <= UnicodeUtil.UNI_SUR_HIGH_END) {\n            // OK: high followed by low.  This is a valid\n            // surrogate pair.\n            code = ((code*31) + ch)*31+ch2;\n            downto--;\n            continue;\n          } else {\n            // Unpaired\n            ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;\n          }            \n        }\n      } else if (ch >= UnicodeUtil.UNI_SUR_HIGH_START && ch <= UnicodeUtil.UNI_SUR_HIGH_END)\n        // Unpaired\n        ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;\n\n      code = (code*31) + ch;\n    }\n\n    int hashPos = code & postingsHashMask;\n\n    // Locate RawPostingList in hash\n    p = postingsHash[hashPos];\n\n    if (p != null && !postingEquals(tokenText, tokenTextLen)) {\n      // Conflict: keep searching different locations in\n      // the hash table.\n      final int inc = ((code>>8)+code)|1;\n      do {\n        code += inc;\n        hashPos = code & postingsHashMask;\n        p = postingsHash[hashPos];\n      } while (p != null && !postingEquals(tokenText, tokenTextLen));\n    }\n\n    if (p == null) {\n\n      // First time we are seeing this token since we last\n      // flushed the hash.\n      final int textLen1 = 1+tokenTextLen;\n      if (textLen1 + charPool.charUpto > DocumentsWriter.CHAR_BLOCK_SIZE) {\n        if (textLen1 > DocumentsWriter.CHAR_BLOCK_SIZE) {\n          // Just skip this term, to remain as robust as\n          // possible during indexing.  A TokenFilter\n          // can be inserted into the analyzer chain if\n          // other behavior is wanted (pruning the term\n          // to a prefix, throwing an exception, etc).\n\n          if (docState.maxTermPrefix == null)\n            docState.maxTermPrefix = new String(tokenText, 0, 30);\n\n          consumer.skippingLongTerm(token);\n          return;\n        }\n        charPool.nextBuffer();\n      }\n\n      // Refill?\n      if (0 == perThread.freePostingsCount)\n        perThread.morePostings();\n\n      // Pull next free RawPostingList from free list\n      p = perThread.freePostings[--perThread.freePostingsCount];\n      assert p != null;\n\n      final char[] text = charPool.buffer;\n      final int textUpto = charPool.charUpto;\n      p.textStart = textUpto + charPool.charOffset;\n      charPool.charUpto += textLen1;\n      System.arraycopy(tokenText, 0, text, textUpto, tokenTextLen);\n      text[textUpto+tokenTextLen] = 0xffff;\n          \n      assert postingsHash[hashPos] == null;\n      postingsHash[hashPos] = p;\n      numPostings++;\n\n      if (numPostings == postingsHashHalfSize)\n        rehashPostings(2*postingsHashSize);\n\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriter.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (DocumentsWriter.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE)\n        bytePool.nextBuffer();\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      p.intStart = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      p.byteStart = intUptos[intUptoStart];\n\n      consumer.newTerm(token, p);\n\n    } else {\n      intUptos = intPool.buffers[p.intStart >> DocumentsWriter.INT_BLOCK_SHIFT];\n      intUptoStart = p.intStart & DocumentsWriter.INT_BLOCK_MASK;\n      consumer.addTerm(token, p);\n    }\n\n    if (doNextCall)\n      nextPerField.add(token, p.textStart);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"74a5e7f20b4a444da9df3b2c0f331fa7a1f64223","date":1227051709,"type":5,"author":"Michael Busch","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","pathOld":"src/java/org/apache/lucene/index/TermsHashPerField#add(Token).mjava","sourceNew":"  // Primary entry point (for first TermsHash)\n  void add() throws IOException {\n\n    assert !postingsCompacted;\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n\n    // Get the text of this term.\n    final char[] tokenText = termAtt.termBuffer();;\n    final int tokenTextLen = termAtt.termLength();\n\n    // Compute hashcode & replace any invalid UTF16 sequences\n    int downto = tokenTextLen;\n    int code = 0;\n    while (downto > 0) {\n      char ch = tokenText[--downto];\n\n      if (ch >= UnicodeUtil.UNI_SUR_LOW_START && ch <= UnicodeUtil.UNI_SUR_LOW_END) {\n        if (0 == downto) {\n          // Unpaired\n          ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;\n        } else {\n          final char ch2 = tokenText[downto-1];\n          if (ch2 >= UnicodeUtil.UNI_SUR_HIGH_START && ch2 <= UnicodeUtil.UNI_SUR_HIGH_END) {\n            // OK: high followed by low.  This is a valid\n            // surrogate pair.\n            code = ((code*31) + ch)*31+ch2;\n            downto--;\n            continue;\n          } else {\n            // Unpaired\n            ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;\n          }            \n        }\n      } else if (ch >= UnicodeUtil.UNI_SUR_HIGH_START && ch <= UnicodeUtil.UNI_SUR_HIGH_END)\n        // Unpaired\n        ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;\n\n      code = (code*31) + ch;\n    }\n\n    int hashPos = code & postingsHashMask;\n\n    // Locate RawPostingList in hash\n    p = postingsHash[hashPos];\n\n    if (p != null && !postingEquals(tokenText, tokenTextLen)) {\n      // Conflict: keep searching different locations in\n      // the hash table.\n      final int inc = ((code>>8)+code)|1;\n      do {\n        code += inc;\n        hashPos = code & postingsHashMask;\n        p = postingsHash[hashPos];\n      } while (p != null && !postingEquals(tokenText, tokenTextLen));\n    }\n\n    if (p == null) {\n\n      // First time we are seeing this token since we last\n      // flushed the hash.\n      final int textLen1 = 1+tokenTextLen;\n      if (textLen1 + charPool.charUpto > DocumentsWriter.CHAR_BLOCK_SIZE) {\n        if (textLen1 > DocumentsWriter.CHAR_BLOCK_SIZE) {\n          // Just skip this term, to remain as robust as\n          // possible during indexing.  A TokenFilter\n          // can be inserted into the analyzer chain if\n          // other behavior is wanted (pruning the term\n          // to a prefix, throwing an exception, etc).\n\n          if (docState.maxTermPrefix == null)\n            docState.maxTermPrefix = new String(tokenText, 0, 30);\n\n          consumer.skippingLongTerm();\n          return;\n        }\n        charPool.nextBuffer();\n      }\n\n      // Refill?\n      if (0 == perThread.freePostingsCount)\n        perThread.morePostings();\n\n      // Pull next free RawPostingList from free list\n      p = perThread.freePostings[--perThread.freePostingsCount];\n      assert p != null;\n\n      final char[] text = charPool.buffer;\n      final int textUpto = charPool.charUpto;\n      p.textStart = textUpto + charPool.charOffset;\n      charPool.charUpto += textLen1;\n      System.arraycopy(tokenText, 0, text, textUpto, tokenTextLen);\n      text[textUpto+tokenTextLen] = 0xffff;\n          \n      assert postingsHash[hashPos] == null;\n      postingsHash[hashPos] = p;\n      numPostings++;\n\n      if (numPostings == postingsHashHalfSize)\n        rehashPostings(2*postingsHashSize);\n\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriter.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (DocumentsWriter.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE)\n        bytePool.nextBuffer();\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      p.intStart = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      p.byteStart = intUptos[intUptoStart];\n\n      consumer.newTerm(p);\n\n    } else {\n      intUptos = intPool.buffers[p.intStart >> DocumentsWriter.INT_BLOCK_SHIFT];\n      intUptoStart = p.intStart & DocumentsWriter.INT_BLOCK_MASK;\n      consumer.addTerm(p);\n    }\n\n    if (doNextCall)\n      nextPerField.add(p.textStart);\n  }\n\n","sourceOld":"  // Primary entry point (for first TermsHash)\n  void add(Token token) throws IOException {\n\n    assert !postingsCompacted;\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n\n    // Get the text of this term.\n    final char[] tokenText = token.termBuffer();\n    final int tokenTextLen = token.termLength();\n\n    // Compute hashcode & replace any invalid UTF16 sequences\n    int downto = tokenTextLen;\n    int code = 0;\n    while (downto > 0) {\n      char ch = tokenText[--downto];\n\n      if (ch >= UnicodeUtil.UNI_SUR_LOW_START && ch <= UnicodeUtil.UNI_SUR_LOW_END) {\n        if (0 == downto) {\n          // Unpaired\n          ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;\n        } else {\n          final char ch2 = tokenText[downto-1];\n          if (ch2 >= UnicodeUtil.UNI_SUR_HIGH_START && ch2 <= UnicodeUtil.UNI_SUR_HIGH_END) {\n            // OK: high followed by low.  This is a valid\n            // surrogate pair.\n            code = ((code*31) + ch)*31+ch2;\n            downto--;\n            continue;\n          } else {\n            // Unpaired\n            ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;\n          }            \n        }\n      } else if (ch >= UnicodeUtil.UNI_SUR_HIGH_START && ch <= UnicodeUtil.UNI_SUR_HIGH_END)\n        // Unpaired\n        ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;\n\n      code = (code*31) + ch;\n    }\n\n    int hashPos = code & postingsHashMask;\n\n    // Locate RawPostingList in hash\n    p = postingsHash[hashPos];\n\n    if (p != null && !postingEquals(tokenText, tokenTextLen)) {\n      // Conflict: keep searching different locations in\n      // the hash table.\n      final int inc = ((code>>8)+code)|1;\n      do {\n        code += inc;\n        hashPos = code & postingsHashMask;\n        p = postingsHash[hashPos];\n      } while (p != null && !postingEquals(tokenText, tokenTextLen));\n    }\n\n    if (p == null) {\n\n      // First time we are seeing this token since we last\n      // flushed the hash.\n      final int textLen1 = 1+tokenTextLen;\n      if (textLen1 + charPool.charUpto > DocumentsWriter.CHAR_BLOCK_SIZE) {\n        if (textLen1 > DocumentsWriter.CHAR_BLOCK_SIZE) {\n          // Just skip this term, to remain as robust as\n          // possible during indexing.  A TokenFilter\n          // can be inserted into the analyzer chain if\n          // other behavior is wanted (pruning the term\n          // to a prefix, throwing an exception, etc).\n\n          if (docState.maxTermPrefix == null)\n            docState.maxTermPrefix = new String(tokenText, 0, 30);\n\n          consumer.skippingLongTerm(token);\n          return;\n        }\n        charPool.nextBuffer();\n      }\n\n      // Refill?\n      if (0 == perThread.freePostingsCount)\n        perThread.morePostings();\n\n      // Pull next free RawPostingList from free list\n      p = perThread.freePostings[--perThread.freePostingsCount];\n      assert p != null;\n\n      final char[] text = charPool.buffer;\n      final int textUpto = charPool.charUpto;\n      p.textStart = textUpto + charPool.charOffset;\n      charPool.charUpto += textLen1;\n      System.arraycopy(tokenText, 0, text, textUpto, tokenTextLen);\n      text[textUpto+tokenTextLen] = 0xffff;\n          \n      assert postingsHash[hashPos] == null;\n      postingsHash[hashPos] = p;\n      numPostings++;\n\n      if (numPostings == postingsHashHalfSize)\n        rehashPostings(2*postingsHashSize);\n\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriter.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (DocumentsWriter.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE)\n        bytePool.nextBuffer();\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      p.intStart = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      p.byteStart = intUptos[intUptoStart];\n\n      consumer.newTerm(token, p);\n\n    } else {\n      intUptos = intPool.buffers[p.intStart >> DocumentsWriter.INT_BLOCK_SHIFT];\n      intUptoStart = p.intStart & DocumentsWriter.INT_BLOCK_MASK;\n      consumer.addTerm(token, p);\n    }\n\n    if (doNextCall)\n      nextPerField.add(token, p.textStart);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"74a5e7f20b4a444da9df3b2c0f331fa7a1f64223":["5350389bf83287111f7760b9e3db3af8e3648474"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"5350389bf83287111f7760b9e3db3af8e3648474":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["74a5e7f20b4a444da9df3b2c0f331fa7a1f64223"]},"commit2Childs":{"74a5e7f20b4a444da9df3b2c0f331fa7a1f64223":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["5350389bf83287111f7760b9e3db3af8e3648474"],"5350389bf83287111f7760b9e3db3af8e3648474":["74a5e7f20b4a444da9df3b2c0f331fa7a1f64223"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}