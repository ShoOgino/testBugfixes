{"path":"solr/contrib/ltr/src/test/org/apache/solr/ltr/model/TestNeuralNetworkModel#testLinearAlgebra().mjava","commits":[{"id":"ef667cf0740be872811ddbabc4952c3a21c1fa2e","date":1520015275,"type":0,"author":"Christine Poerschke","isMerge":false,"pathNew":"solr/contrib/ltr/src/test/org/apache/solr/ltr/model/TestNeuralNetworkModel#testLinearAlgebra().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  public void testLinearAlgebra() {\n\n    final double layer1Node1Weight1 = 1.0;\n    final double layer1Node1Weight2 = 2.0;\n    final double layer1Node1Weight3 = 3.0;\n    final double layer1Node1Weight4 = 4.0;\n    final double layer1Node2Weight1 = 5.0;\n    final double layer1Node2Weight2 = 6.0;\n    final double layer1Node2Weight3 = 7.0;\n    final double layer1Node2Weight4 = 8.0;\n    final double layer1Node3Weight1 = 9.0;\n    final double layer1Node3Weight2 = 10.0;\n    final double layer1Node3Weight3 = 11.0;\n    final double layer1Node3Weight4 = 12.0;\n\n    double[][] matrixOne = { { layer1Node1Weight1, layer1Node1Weight2, layer1Node1Weight3, layer1Node1Weight4 },\n                             { layer1Node2Weight1, layer1Node2Weight2, layer1Node2Weight3, layer1Node2Weight4 },\n                             { layer1Node3Weight1, layer1Node3Weight2, layer1Node3Weight3, layer1Node3Weight4 } };\n\n    final double layer1Node1Bias = 13.0;\n    final double layer1Node2Bias = 14.0;\n    final double layer1Node3Bias = 15.0;\n\n    double[] biasOne = { layer1Node1Bias, layer1Node2Bias, layer1Node3Bias };\n\n    final double outputNodeWeight1 = 16.0;\n    final double outputNodeWeight2 = 17.0;\n    final double outputNodeWeight3 = 18.0;\n\n    double[][] matrixTwo = { { outputNodeWeight1, outputNodeWeight2, outputNodeWeight3 } };\n\n    final double outputNodeBias = 19.0;\n\n    double[] biasTwo = { outputNodeBias };\n\n    Map<String,Object> params = new HashMap<String,Object>();\n    ArrayList<Map<String,Object>> layers = new ArrayList<Map<String,Object>>();\n\n    layers.add(createLayerParams(matrixOne, biasOne, \"relu\"));\n    layers.add(createLayerParams(matrixTwo, biasTwo, \"relu\"));\n\n    params.put(\"layers\", layers);\n\n    final List<Feature> allFeaturesInStore\n       = getFeatures(new String[] {\"constantOne\", \"constantTwo\", \"constantThree\", \"constantFour\", \"constantFive\"});\n\n    final List<Feature> featuresInModel = new ArrayList<>(allFeaturesInStore);\n    Collections.shuffle(featuresInModel, random()); // store and model order of features can vary\n    featuresInModel.remove(0); // models need not use all the store's features\n    assertEquals(4, featuresInModel.size()); // the test model uses four features\n\n    final List<Normalizer> norms =\n        new ArrayList<Normalizer>(\n            Collections.nCopies(featuresInModel.size(),IdentityNormalizer.INSTANCE));\n    final LTRScoringModel ltrScoringModel = createNeuralNetworkModel(\"test_score\",\n        featuresInModel, norms, \"test_score\", allFeaturesInStore, params);\n\n    {\n      // pretend all features scored zero\n      float[] testVec = { 0.0f, 0.0f, 0.0f, 0.0f };\n      // with all zero inputs the layer1 node outputs are layer1 node biases only\n      final double layer1Node1Output = layer1Node1Bias;\n      final double layer1Node2Output = layer1Node2Bias;\n      final double layer1Node3Output = layer1Node3Bias;\n      // with just one layer the output node calculation is easy\n      final double outputNodeOutput =\n          (layer1Node1Output*outputNodeWeight1) +\n          (layer1Node2Output*outputNodeWeight2) +\n          (layer1Node3Output*outputNodeWeight3) +\n          outputNodeBias;\n      assertEquals(735.0, outputNodeOutput, 0.001);\n      // and the expected score is that of the output node\n      final double expectedScore = outputNodeOutput;\n      float score = ltrScoringModel.score(testVec);\n      assertEquals(expectedScore, score, 0.001);\n    }\n\n    {\n      // pretend all features scored one\n      float[] testVec = { 1.0f, 1.0f, 1.0f, 1.0f };\n      // with all one inputs the layer1 node outputs are simply sum of weights and biases\n      final double layer1Node1Output = layer1Node1Weight1 + layer1Node1Weight2 + layer1Node1Weight3 + layer1Node1Weight4 + layer1Node1Bias;\n      final double layer1Node2Output = layer1Node2Weight1 + layer1Node2Weight2 + layer1Node2Weight3 + layer1Node2Weight4 + layer1Node2Bias;\n      final double layer1Node3Output = layer1Node3Weight1 + layer1Node3Weight2 + layer1Node3Weight3 + layer1Node3Weight4 + layer1Node3Bias;\n      // with just one layer the output node calculation is easy\n      final double outputNodeOutput =\n          (layer1Node1Output*outputNodeWeight1) +\n          (layer1Node2Output*outputNodeWeight2) +\n          (layer1Node3Output*outputNodeWeight3) +\n          outputNodeBias;\n      assertEquals(2093.0, outputNodeOutput, 0.001);\n      // and the expected score is that of the output node\n      final double expectedScore = outputNodeOutput;\n      float score = ltrScoringModel.score(testVec);\n      assertEquals(expectedScore, score, 0.001);\n    }\n\n    {\n      // pretend all features scored random numbers in 0.0 to 1.0 range\n      final float input1 = random().nextFloat();\n      final float input2 = random().nextFloat();\n      final float input3 = random().nextFloat();\n      final float input4 = random().nextFloat();\n      float[] testVec = {input1, input2, input3, input4};\n      // the layer1 node outputs are sum of input-times-weight plus bias\n      final double layer1Node1Output = input1*layer1Node1Weight1 + input2*layer1Node1Weight2 + input3*layer1Node1Weight3 + input4*layer1Node1Weight4 + layer1Node1Bias;\n      final double layer1Node2Output = input1*layer1Node2Weight1 + input2*layer1Node2Weight2 + input3*layer1Node2Weight3 + input4*layer1Node2Weight4 + layer1Node2Bias;\n      final double layer1Node3Output = input1*layer1Node3Weight1 + input2*layer1Node3Weight2 + input3*layer1Node3Weight3 + input4*layer1Node3Weight4 + layer1Node3Bias;\n      // with just one layer the output node calculation is easy\n      final double outputNodeOutput =\n          (layer1Node1Output*outputNodeWeight1) +\n          (layer1Node2Output*outputNodeWeight2) +\n          (layer1Node3Output*outputNodeWeight3) +\n          outputNodeBias;\n      assertTrue(\"outputNodeOutput=\"+outputNodeOutput, 735.0 <= outputNodeOutput); // inputs between zero and one produced output greater than 74\n      assertTrue(\"outputNodeOutput=\"+outputNodeOutput, outputNodeOutput <= 2093.0); // inputs between zero and one produced output less than 294\n      // and the expected score is that of the output node\n      final double expectedScore = outputNodeOutput;\n      float score = ltrScoringModel.score(testVec);\n      assertEquals(expectedScore, score, 0.001);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"ef667cf0740be872811ddbabc4952c3a21c1fa2e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["ef667cf0740be872811ddbabc4952c3a21c1fa2e"]},"commit2Childs":{"ef667cf0740be872811ddbabc4952c3a21c1fa2e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["ef667cf0740be872811ddbabc4952c3a21c1fa2e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}