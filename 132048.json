{"path":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorDV#collectDocs().mjava","commits":[{"id":"51b068b0154fe2220646b2dcc2133e168a31c743","date":1446661675,"type":1,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorDV#collectDocs().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorDV[FacetField]#collectDocs().mjava","sourceNew":"  @Override\n  protected void collectDocs() throws IOException {\n    int domainSize = fcontext.base.size();\n\n    if (nTerms <= 0 || domainSize < effectiveMincount) { // TODO: what about allBuckets? missing bucket?\n      return;\n    }\n\n    // TODO: refactor some of this logic into a base class\n    boolean countOnly = collectAcc==null && allBucketsAcc==null;\n    boolean fullRange = startTermIndex == 0 && endTermIndex == si.getValueCount();\n\n    // Are we expecting many hits per bucket?\n    // FUTURE: pro-rate for nTerms?\n    // FUTURE: better take into account number of values in multi-valued fields.  This info is available for indexed fields.\n    // FUTURE: take into account that bigger ord maps are more expensive than smaller ones\n    // One test: 5M doc index, faceting on a single-valued field with almost 1M unique values, crossover point where global counting was slower\n    // than per-segment counting was a domain of 658k docs.  At that point, top 10 buckets had 6-7 matches each.\n    // this was for heap docvalues produced by UninvertingReader\n    // Since these values were randomly distributed, lets round our domain multiplier up to account for less random real world data.\n    long domainMultiplier = multiValuedField ? 4L : 2L;\n    boolean manyHitsPerBucket = domainSize * domainMultiplier > (si.getValueCount() + 3);  // +3 to increase test coverage with small tests\n\n    // If we're only calculating counts, we're not prefixing, and we expect to collect many documents per unique value,\n    // then collect per-segment before mapping to global ords at the end.  This will save redundant seg->global ord mappings.\n    // FUTURE: there are probably some other non \"countOnly\" cases where we can use this as well (i.e. those where\n    // the docid is not used)\n    boolean canDoPerSeg = countOnly && fullRange;\n    boolean accumSeg = manyHitsPerBucket && canDoPerSeg;\n\n    if (freq.perSeg != null) accumSeg = canDoPerSeg && freq.perSeg;  // internal - override perSeg heuristic\n\n    final List<LeafReaderContext> leaves = fcontext.searcher.getIndexReader().leaves();\n    Filter filter = fcontext.base.getTopFilter();\n\n    for (int subIdx = 0; subIdx < leaves.size(); subIdx++) {\n      LeafReaderContext subCtx = leaves.get(subIdx);\n\n      setNextReaderFirstPhase(subCtx);\n\n      DocIdSet dis = filter.getDocIdSet(subCtx, null); // solr docsets already exclude any deleted docs\n      DocIdSetIterator disi = dis.iterator();\n\n      SortedDocValues singleDv = null;\n      SortedSetDocValues multiDv = null;\n      if (multiValuedField) {\n        // TODO: get sub from multi?\n        multiDv = subCtx.reader().getSortedSetDocValues(sf.getName());\n        if (multiDv == null) {\n          multiDv = DocValues.emptySortedSet();\n        }\n        // some codecs may optimize SortedSet storage for single-valued fields\n        // this will be null if this is not a wrapped single valued docvalues.\n        if (unwrap_singleValued_multiDv) {\n          singleDv = DocValues.unwrapSingleton(multiDv);\n        }\n      } else {\n        singleDv = subCtx.reader().getSortedDocValues(sf.getName());\n        if (singleDv == null) {\n          singleDv = DocValues.emptySorted();\n        }\n      }\n\n      LongValues toGlobal = ordinalMap == null ? null : ordinalMap.getGlobalOrds(subIdx);\n\n      if (singleDv != null) {\n        if (accumSeg) {\n          collectPerSeg(singleDv, disi, toGlobal);\n        } else {\n          if (canDoPerSeg && toGlobal != null) {\n            collectCounts(singleDv, disi, toGlobal);\n          } else {\n            collectDocs(singleDv, disi, toGlobal);\n          }\n        }\n      } else {\n        if (accumSeg) {\n          collectPerSeg(multiDv, disi, toGlobal);\n        } else {\n          if (canDoPerSeg && toGlobal != null) {\n            collectCounts(multiDv, disi, toGlobal);\n          } else {\n            collectDocs(multiDv, disi, toGlobal);\n          }\n        }\n      }\n    }\n\n    reuse = null;  // better GC\n  }\n\n","sourceOld":"  @Override\n  protected void collectDocs() throws IOException {\n    if (nTerms <= 0 || fcontext.base.size() < effectiveMincount) { // TODO: what about allBuckets? missing bucket?\n      return;\n    }\n\n    final List<LeafReaderContext> leaves = fcontext.searcher.getIndexReader().leaves();\n    Filter filter = fcontext.base.getTopFilter();\n\n    for (int subIdx = 0; subIdx < leaves.size(); subIdx++) {\n      LeafReaderContext subCtx = leaves.get(subIdx);\n\n      setNextReaderFirstPhase(subCtx);\n\n      DocIdSet dis = filter.getDocIdSet(subCtx, null); // solr docsets already exclude any deleted docs\n      DocIdSetIterator disi = dis.iterator();\n\n      SortedDocValues singleDv = null;\n      SortedSetDocValues multiDv = null;\n      if (multiValuedField) {\n        // TODO: get sub from multi?\n        multiDv = subCtx.reader().getSortedSetDocValues(sf.getName());\n        if (multiDv == null) {\n          multiDv = DocValues.emptySortedSet();\n        }\n        // some codecs may optimize SortedSet storage for single-valued fields\n        // this will be null if this is not a wrapped single valued docvalues.\n        if (unwrap_singleValued_multiDv) {\n          singleDv = DocValues.unwrapSingleton(multiDv);\n        }\n      } else {\n        singleDv = subCtx.reader().getSortedDocValues(sf.getName());\n        if (singleDv == null) {\n          singleDv = DocValues.emptySorted();\n        }\n      }\n\n      LongValues toGlobal = ordinalMap == null ? null : ordinalMap.getGlobalOrds(subIdx);\n\n      if (singleDv != null) {\n        collectDocs(singleDv, disi, toGlobal);\n      } else {\n        collectDocs(multiDv, disi, toGlobal);\n      }\n    }\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"79759974460bc59933cd169acc94f5c6b16368d5","date":1471318443,"type":5,"author":"David Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByArrayDV#collectDocs().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorDV#collectDocs().mjava","sourceNew":"  @Override\n  protected void collectDocs() throws IOException {\n    int domainSize = fcontext.base.size();\n\n    if (nTerms <= 0 || domainSize < effectiveMincount) { // TODO: what about allBuckets? missing bucket?\n      return;\n    }\n\n    // TODO: refactor some of this logic into a base class\n    boolean countOnly = collectAcc==null && allBucketsAcc==null;\n    boolean fullRange = startTermIndex == 0 && endTermIndex == si.getValueCount();\n\n    // Are we expecting many hits per bucket?\n    // FUTURE: pro-rate for nTerms?\n    // FUTURE: better take into account number of values in multi-valued fields.  This info is available for indexed fields.\n    // FUTURE: take into account that bigger ord maps are more expensive than smaller ones\n    // One test: 5M doc index, faceting on a single-valued field with almost 1M unique values, crossover point where global counting was slower\n    // than per-segment counting was a domain of 658k docs.  At that point, top 10 buckets had 6-7 matches each.\n    // this was for heap docvalues produced by UninvertingReader\n    // Since these values were randomly distributed, lets round our domain multiplier up to account for less random real world data.\n    long domainMultiplier = multiValuedField ? 4L : 2L;\n    boolean manyHitsPerBucket = domainSize * domainMultiplier > (si.getValueCount() + 3);  // +3 to increase test coverage with small tests\n\n    // If we're only calculating counts, we're not prefixing, and we expect to collect many documents per unique value,\n    // then collect per-segment before mapping to global ords at the end.  This will save redundant seg->global ord mappings.\n    // FUTURE: there are probably some other non \"countOnly\" cases where we can use this as well (i.e. those where\n    // the docid is not used)\n    boolean canDoPerSeg = countOnly && fullRange;\n    boolean accumSeg = manyHitsPerBucket && canDoPerSeg;\n\n    if (freq.perSeg != null) accumSeg = canDoPerSeg && freq.perSeg;  // internal - override perSeg heuristic\n\n    final List<LeafReaderContext> leaves = fcontext.searcher.getIndexReader().leaves();\n    Filter filter = fcontext.base.getTopFilter();\n\n    for (int subIdx = 0; subIdx < leaves.size(); subIdx++) {\n      LeafReaderContext subCtx = leaves.get(subIdx);\n\n      setNextReaderFirstPhase(subCtx);\n\n      DocIdSet dis = filter.getDocIdSet(subCtx, null); // solr docsets already exclude any deleted docs\n      DocIdSetIterator disi = dis.iterator();\n\n      SortedDocValues singleDv = null;\n      SortedSetDocValues multiDv = null;\n      if (multiValuedField) {\n        // TODO: get sub from multi?\n        multiDv = subCtx.reader().getSortedSetDocValues(sf.getName());\n        if (multiDv == null) {\n          multiDv = DocValues.emptySortedSet();\n        }\n        // some codecs may optimize SortedSet storage for single-valued fields\n        // this will be null if this is not a wrapped single valued docvalues.\n        if (unwrap_singleValued_multiDv) {\n          singleDv = DocValues.unwrapSingleton(multiDv);\n        }\n      } else {\n        singleDv = subCtx.reader().getSortedDocValues(sf.getName());\n        if (singleDv == null) {\n          singleDv = DocValues.emptySorted();\n        }\n      }\n\n      LongValues toGlobal = ordinalMap == null ? null : ordinalMap.getGlobalOrds(subIdx);\n\n      if (singleDv != null) {\n        if (accumSeg) {\n          collectPerSeg(singleDv, disi, toGlobal);\n        } else {\n          if (canDoPerSeg && toGlobal != null) {\n            collectCounts(singleDv, disi, toGlobal);\n          } else {\n            collectDocs(singleDv, disi, toGlobal);\n          }\n        }\n      } else {\n        if (accumSeg) {\n          collectPerSeg(multiDv, disi, toGlobal);\n        } else {\n          if (canDoPerSeg && toGlobal != null) {\n            collectCounts(multiDv, disi, toGlobal);\n          } else {\n            collectDocs(multiDv, disi, toGlobal);\n          }\n        }\n      }\n    }\n\n    reuse = null;  // better GC\n  }\n\n","sourceOld":"  @Override\n  protected void collectDocs() throws IOException {\n    int domainSize = fcontext.base.size();\n\n    if (nTerms <= 0 || domainSize < effectiveMincount) { // TODO: what about allBuckets? missing bucket?\n      return;\n    }\n\n    // TODO: refactor some of this logic into a base class\n    boolean countOnly = collectAcc==null && allBucketsAcc==null;\n    boolean fullRange = startTermIndex == 0 && endTermIndex == si.getValueCount();\n\n    // Are we expecting many hits per bucket?\n    // FUTURE: pro-rate for nTerms?\n    // FUTURE: better take into account number of values in multi-valued fields.  This info is available for indexed fields.\n    // FUTURE: take into account that bigger ord maps are more expensive than smaller ones\n    // One test: 5M doc index, faceting on a single-valued field with almost 1M unique values, crossover point where global counting was slower\n    // than per-segment counting was a domain of 658k docs.  At that point, top 10 buckets had 6-7 matches each.\n    // this was for heap docvalues produced by UninvertingReader\n    // Since these values were randomly distributed, lets round our domain multiplier up to account for less random real world data.\n    long domainMultiplier = multiValuedField ? 4L : 2L;\n    boolean manyHitsPerBucket = domainSize * domainMultiplier > (si.getValueCount() + 3);  // +3 to increase test coverage with small tests\n\n    // If we're only calculating counts, we're not prefixing, and we expect to collect many documents per unique value,\n    // then collect per-segment before mapping to global ords at the end.  This will save redundant seg->global ord mappings.\n    // FUTURE: there are probably some other non \"countOnly\" cases where we can use this as well (i.e. those where\n    // the docid is not used)\n    boolean canDoPerSeg = countOnly && fullRange;\n    boolean accumSeg = manyHitsPerBucket && canDoPerSeg;\n\n    if (freq.perSeg != null) accumSeg = canDoPerSeg && freq.perSeg;  // internal - override perSeg heuristic\n\n    final List<LeafReaderContext> leaves = fcontext.searcher.getIndexReader().leaves();\n    Filter filter = fcontext.base.getTopFilter();\n\n    for (int subIdx = 0; subIdx < leaves.size(); subIdx++) {\n      LeafReaderContext subCtx = leaves.get(subIdx);\n\n      setNextReaderFirstPhase(subCtx);\n\n      DocIdSet dis = filter.getDocIdSet(subCtx, null); // solr docsets already exclude any deleted docs\n      DocIdSetIterator disi = dis.iterator();\n\n      SortedDocValues singleDv = null;\n      SortedSetDocValues multiDv = null;\n      if (multiValuedField) {\n        // TODO: get sub from multi?\n        multiDv = subCtx.reader().getSortedSetDocValues(sf.getName());\n        if (multiDv == null) {\n          multiDv = DocValues.emptySortedSet();\n        }\n        // some codecs may optimize SortedSet storage for single-valued fields\n        // this will be null if this is not a wrapped single valued docvalues.\n        if (unwrap_singleValued_multiDv) {\n          singleDv = DocValues.unwrapSingleton(multiDv);\n        }\n      } else {\n        singleDv = subCtx.reader().getSortedDocValues(sf.getName());\n        if (singleDv == null) {\n          singleDv = DocValues.emptySorted();\n        }\n      }\n\n      LongValues toGlobal = ordinalMap == null ? null : ordinalMap.getGlobalOrds(subIdx);\n\n      if (singleDv != null) {\n        if (accumSeg) {\n          collectPerSeg(singleDv, disi, toGlobal);\n        } else {\n          if (canDoPerSeg && toGlobal != null) {\n            collectCounts(singleDv, disi, toGlobal);\n          } else {\n            collectDocs(singleDv, disi, toGlobal);\n          }\n        }\n      } else {\n        if (accumSeg) {\n          collectPerSeg(multiDv, disi, toGlobal);\n        } else {\n          if (canDoPerSeg && toGlobal != null) {\n            collectCounts(multiDv, disi, toGlobal);\n          } else {\n            collectDocs(multiDv, disi, toGlobal);\n          }\n        }\n      }\n    }\n\n    reuse = null;  // better GC\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6","date":1471496851,"type":5,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByArrayDV#collectDocs().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorDV#collectDocs().mjava","sourceNew":"  @Override\n  protected void collectDocs() throws IOException {\n    int domainSize = fcontext.base.size();\n\n    if (nTerms <= 0 || domainSize < effectiveMincount) { // TODO: what about allBuckets? missing bucket?\n      return;\n    }\n\n    // TODO: refactor some of this logic into a base class\n    boolean countOnly = collectAcc==null && allBucketsAcc==null;\n    boolean fullRange = startTermIndex == 0 && endTermIndex == si.getValueCount();\n\n    // Are we expecting many hits per bucket?\n    // FUTURE: pro-rate for nTerms?\n    // FUTURE: better take into account number of values in multi-valued fields.  This info is available for indexed fields.\n    // FUTURE: take into account that bigger ord maps are more expensive than smaller ones\n    // One test: 5M doc index, faceting on a single-valued field with almost 1M unique values, crossover point where global counting was slower\n    // than per-segment counting was a domain of 658k docs.  At that point, top 10 buckets had 6-7 matches each.\n    // this was for heap docvalues produced by UninvertingReader\n    // Since these values were randomly distributed, lets round our domain multiplier up to account for less random real world data.\n    long domainMultiplier = multiValuedField ? 4L : 2L;\n    boolean manyHitsPerBucket = domainSize * domainMultiplier > (si.getValueCount() + 3);  // +3 to increase test coverage with small tests\n\n    // If we're only calculating counts, we're not prefixing, and we expect to collect many documents per unique value,\n    // then collect per-segment before mapping to global ords at the end.  This will save redundant seg->global ord mappings.\n    // FUTURE: there are probably some other non \"countOnly\" cases where we can use this as well (i.e. those where\n    // the docid is not used)\n    boolean canDoPerSeg = countOnly && fullRange;\n    boolean accumSeg = manyHitsPerBucket && canDoPerSeg;\n\n    if (freq.perSeg != null) accumSeg = canDoPerSeg && freq.perSeg;  // internal - override perSeg heuristic\n\n    final List<LeafReaderContext> leaves = fcontext.searcher.getIndexReader().leaves();\n    Filter filter = fcontext.base.getTopFilter();\n\n    for (int subIdx = 0; subIdx < leaves.size(); subIdx++) {\n      LeafReaderContext subCtx = leaves.get(subIdx);\n\n      setNextReaderFirstPhase(subCtx);\n\n      DocIdSet dis = filter.getDocIdSet(subCtx, null); // solr docsets already exclude any deleted docs\n      DocIdSetIterator disi = dis.iterator();\n\n      SortedDocValues singleDv = null;\n      SortedSetDocValues multiDv = null;\n      if (multiValuedField) {\n        // TODO: get sub from multi?\n        multiDv = subCtx.reader().getSortedSetDocValues(sf.getName());\n        if (multiDv == null) {\n          multiDv = DocValues.emptySortedSet();\n        }\n        // some codecs may optimize SortedSet storage for single-valued fields\n        // this will be null if this is not a wrapped single valued docvalues.\n        if (unwrap_singleValued_multiDv) {\n          singleDv = DocValues.unwrapSingleton(multiDv);\n        }\n      } else {\n        singleDv = subCtx.reader().getSortedDocValues(sf.getName());\n        if (singleDv == null) {\n          singleDv = DocValues.emptySorted();\n        }\n      }\n\n      LongValues toGlobal = ordinalMap == null ? null : ordinalMap.getGlobalOrds(subIdx);\n\n      if (singleDv != null) {\n        if (accumSeg) {\n          collectPerSeg(singleDv, disi, toGlobal);\n        } else {\n          if (canDoPerSeg && toGlobal != null) {\n            collectCounts(singleDv, disi, toGlobal);\n          } else {\n            collectDocs(singleDv, disi, toGlobal);\n          }\n        }\n      } else {\n        if (accumSeg) {\n          collectPerSeg(multiDv, disi, toGlobal);\n        } else {\n          if (canDoPerSeg && toGlobal != null) {\n            collectCounts(multiDv, disi, toGlobal);\n          } else {\n            collectDocs(multiDv, disi, toGlobal);\n          }\n        }\n      }\n    }\n\n    reuse = null;  // better GC\n  }\n\n","sourceOld":"  @Override\n  protected void collectDocs() throws IOException {\n    int domainSize = fcontext.base.size();\n\n    if (nTerms <= 0 || domainSize < effectiveMincount) { // TODO: what about allBuckets? missing bucket?\n      return;\n    }\n\n    // TODO: refactor some of this logic into a base class\n    boolean countOnly = collectAcc==null && allBucketsAcc==null;\n    boolean fullRange = startTermIndex == 0 && endTermIndex == si.getValueCount();\n\n    // Are we expecting many hits per bucket?\n    // FUTURE: pro-rate for nTerms?\n    // FUTURE: better take into account number of values in multi-valued fields.  This info is available for indexed fields.\n    // FUTURE: take into account that bigger ord maps are more expensive than smaller ones\n    // One test: 5M doc index, faceting on a single-valued field with almost 1M unique values, crossover point where global counting was slower\n    // than per-segment counting was a domain of 658k docs.  At that point, top 10 buckets had 6-7 matches each.\n    // this was for heap docvalues produced by UninvertingReader\n    // Since these values were randomly distributed, lets round our domain multiplier up to account for less random real world data.\n    long domainMultiplier = multiValuedField ? 4L : 2L;\n    boolean manyHitsPerBucket = domainSize * domainMultiplier > (si.getValueCount() + 3);  // +3 to increase test coverage with small tests\n\n    // If we're only calculating counts, we're not prefixing, and we expect to collect many documents per unique value,\n    // then collect per-segment before mapping to global ords at the end.  This will save redundant seg->global ord mappings.\n    // FUTURE: there are probably some other non \"countOnly\" cases where we can use this as well (i.e. those where\n    // the docid is not used)\n    boolean canDoPerSeg = countOnly && fullRange;\n    boolean accumSeg = manyHitsPerBucket && canDoPerSeg;\n\n    if (freq.perSeg != null) accumSeg = canDoPerSeg && freq.perSeg;  // internal - override perSeg heuristic\n\n    final List<LeafReaderContext> leaves = fcontext.searcher.getIndexReader().leaves();\n    Filter filter = fcontext.base.getTopFilter();\n\n    for (int subIdx = 0; subIdx < leaves.size(); subIdx++) {\n      LeafReaderContext subCtx = leaves.get(subIdx);\n\n      setNextReaderFirstPhase(subCtx);\n\n      DocIdSet dis = filter.getDocIdSet(subCtx, null); // solr docsets already exclude any deleted docs\n      DocIdSetIterator disi = dis.iterator();\n\n      SortedDocValues singleDv = null;\n      SortedSetDocValues multiDv = null;\n      if (multiValuedField) {\n        // TODO: get sub from multi?\n        multiDv = subCtx.reader().getSortedSetDocValues(sf.getName());\n        if (multiDv == null) {\n          multiDv = DocValues.emptySortedSet();\n        }\n        // some codecs may optimize SortedSet storage for single-valued fields\n        // this will be null if this is not a wrapped single valued docvalues.\n        if (unwrap_singleValued_multiDv) {\n          singleDv = DocValues.unwrapSingleton(multiDv);\n        }\n      } else {\n        singleDv = subCtx.reader().getSortedDocValues(sf.getName());\n        if (singleDv == null) {\n          singleDv = DocValues.emptySorted();\n        }\n      }\n\n      LongValues toGlobal = ordinalMap == null ? null : ordinalMap.getGlobalOrds(subIdx);\n\n      if (singleDv != null) {\n        if (accumSeg) {\n          collectPerSeg(singleDv, disi, toGlobal);\n        } else {\n          if (canDoPerSeg && toGlobal != null) {\n            collectCounts(singleDv, disi, toGlobal);\n          } else {\n            collectDocs(singleDv, disi, toGlobal);\n          }\n        }\n      } else {\n        if (accumSeg) {\n          collectPerSeg(multiDv, disi, toGlobal);\n        } else {\n          if (canDoPerSeg && toGlobal != null) {\n            collectCounts(multiDv, disi, toGlobal);\n          } else {\n            collectDocs(multiDv, disi, toGlobal);\n          }\n        }\n      }\n    }\n\n    reuse = null;  // better GC\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"403d05f7f8d69b65659157eff1bc1d2717f04c66","date":1471692961,"type":5,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByArrayDV#collectDocs().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorDV#collectDocs().mjava","sourceNew":"  @Override\n  protected void collectDocs() throws IOException {\n    int domainSize = fcontext.base.size();\n\n    if (nTerms <= 0 || domainSize < effectiveMincount) { // TODO: what about allBuckets? missing bucket?\n      return;\n    }\n\n    // TODO: refactor some of this logic into a base class\n    boolean countOnly = collectAcc==null && allBucketsAcc==null;\n    boolean fullRange = startTermIndex == 0 && endTermIndex == si.getValueCount();\n\n    // Are we expecting many hits per bucket?\n    // FUTURE: pro-rate for nTerms?\n    // FUTURE: better take into account number of values in multi-valued fields.  This info is available for indexed fields.\n    // FUTURE: take into account that bigger ord maps are more expensive than smaller ones\n    // One test: 5M doc index, faceting on a single-valued field with almost 1M unique values, crossover point where global counting was slower\n    // than per-segment counting was a domain of 658k docs.  At that point, top 10 buckets had 6-7 matches each.\n    // this was for heap docvalues produced by UninvertingReader\n    // Since these values were randomly distributed, lets round our domain multiplier up to account for less random real world data.\n    long domainMultiplier = multiValuedField ? 4L : 2L;\n    boolean manyHitsPerBucket = domainSize * domainMultiplier > (si.getValueCount() + 3);  // +3 to increase test coverage with small tests\n\n    // If we're only calculating counts, we're not prefixing, and we expect to collect many documents per unique value,\n    // then collect per-segment before mapping to global ords at the end.  This will save redundant seg->global ord mappings.\n    // FUTURE: there are probably some other non \"countOnly\" cases where we can use this as well (i.e. those where\n    // the docid is not used)\n    boolean canDoPerSeg = countOnly && fullRange;\n    boolean accumSeg = manyHitsPerBucket && canDoPerSeg;\n\n    if (freq.perSeg != null) accumSeg = canDoPerSeg && freq.perSeg;  // internal - override perSeg heuristic\n\n    final List<LeafReaderContext> leaves = fcontext.searcher.getIndexReader().leaves();\n    Filter filter = fcontext.base.getTopFilter();\n\n    for (int subIdx = 0; subIdx < leaves.size(); subIdx++) {\n      LeafReaderContext subCtx = leaves.get(subIdx);\n\n      setNextReaderFirstPhase(subCtx);\n\n      DocIdSet dis = filter.getDocIdSet(subCtx, null); // solr docsets already exclude any deleted docs\n      DocIdSetIterator disi = dis.iterator();\n\n      SortedDocValues singleDv = null;\n      SortedSetDocValues multiDv = null;\n      if (multiValuedField) {\n        // TODO: get sub from multi?\n        multiDv = subCtx.reader().getSortedSetDocValues(sf.getName());\n        if (multiDv == null) {\n          multiDv = DocValues.emptySortedSet();\n        }\n        // some codecs may optimize SortedSet storage for single-valued fields\n        // this will be null if this is not a wrapped single valued docvalues.\n        if (unwrap_singleValued_multiDv) {\n          singleDv = DocValues.unwrapSingleton(multiDv);\n        }\n      } else {\n        singleDv = subCtx.reader().getSortedDocValues(sf.getName());\n        if (singleDv == null) {\n          singleDv = DocValues.emptySorted();\n        }\n      }\n\n      LongValues toGlobal = ordinalMap == null ? null : ordinalMap.getGlobalOrds(subIdx);\n\n      if (singleDv != null) {\n        if (accumSeg) {\n          collectPerSeg(singleDv, disi, toGlobal);\n        } else {\n          if (canDoPerSeg && toGlobal != null) {\n            collectCounts(singleDv, disi, toGlobal);\n          } else {\n            collectDocs(singleDv, disi, toGlobal);\n          }\n        }\n      } else {\n        if (accumSeg) {\n          collectPerSeg(multiDv, disi, toGlobal);\n        } else {\n          if (canDoPerSeg && toGlobal != null) {\n            collectCounts(multiDv, disi, toGlobal);\n          } else {\n            collectDocs(multiDv, disi, toGlobal);\n          }\n        }\n      }\n    }\n\n    reuse = null;  // better GC\n  }\n\n","sourceOld":"  @Override\n  protected void collectDocs() throws IOException {\n    int domainSize = fcontext.base.size();\n\n    if (nTerms <= 0 || domainSize < effectiveMincount) { // TODO: what about allBuckets? missing bucket?\n      return;\n    }\n\n    // TODO: refactor some of this logic into a base class\n    boolean countOnly = collectAcc==null && allBucketsAcc==null;\n    boolean fullRange = startTermIndex == 0 && endTermIndex == si.getValueCount();\n\n    // Are we expecting many hits per bucket?\n    // FUTURE: pro-rate for nTerms?\n    // FUTURE: better take into account number of values in multi-valued fields.  This info is available for indexed fields.\n    // FUTURE: take into account that bigger ord maps are more expensive than smaller ones\n    // One test: 5M doc index, faceting on a single-valued field with almost 1M unique values, crossover point where global counting was slower\n    // than per-segment counting was a domain of 658k docs.  At that point, top 10 buckets had 6-7 matches each.\n    // this was for heap docvalues produced by UninvertingReader\n    // Since these values were randomly distributed, lets round our domain multiplier up to account for less random real world data.\n    long domainMultiplier = multiValuedField ? 4L : 2L;\n    boolean manyHitsPerBucket = domainSize * domainMultiplier > (si.getValueCount() + 3);  // +3 to increase test coverage with small tests\n\n    // If we're only calculating counts, we're not prefixing, and we expect to collect many documents per unique value,\n    // then collect per-segment before mapping to global ords at the end.  This will save redundant seg->global ord mappings.\n    // FUTURE: there are probably some other non \"countOnly\" cases where we can use this as well (i.e. those where\n    // the docid is not used)\n    boolean canDoPerSeg = countOnly && fullRange;\n    boolean accumSeg = manyHitsPerBucket && canDoPerSeg;\n\n    if (freq.perSeg != null) accumSeg = canDoPerSeg && freq.perSeg;  // internal - override perSeg heuristic\n\n    final List<LeafReaderContext> leaves = fcontext.searcher.getIndexReader().leaves();\n    Filter filter = fcontext.base.getTopFilter();\n\n    for (int subIdx = 0; subIdx < leaves.size(); subIdx++) {\n      LeafReaderContext subCtx = leaves.get(subIdx);\n\n      setNextReaderFirstPhase(subCtx);\n\n      DocIdSet dis = filter.getDocIdSet(subCtx, null); // solr docsets already exclude any deleted docs\n      DocIdSetIterator disi = dis.iterator();\n\n      SortedDocValues singleDv = null;\n      SortedSetDocValues multiDv = null;\n      if (multiValuedField) {\n        // TODO: get sub from multi?\n        multiDv = subCtx.reader().getSortedSetDocValues(sf.getName());\n        if (multiDv == null) {\n          multiDv = DocValues.emptySortedSet();\n        }\n        // some codecs may optimize SortedSet storage for single-valued fields\n        // this will be null if this is not a wrapped single valued docvalues.\n        if (unwrap_singleValued_multiDv) {\n          singleDv = DocValues.unwrapSingleton(multiDv);\n        }\n      } else {\n        singleDv = subCtx.reader().getSortedDocValues(sf.getName());\n        if (singleDv == null) {\n          singleDv = DocValues.emptySorted();\n        }\n      }\n\n      LongValues toGlobal = ordinalMap == null ? null : ordinalMap.getGlobalOrds(subIdx);\n\n      if (singleDv != null) {\n        if (accumSeg) {\n          collectPerSeg(singleDv, disi, toGlobal);\n        } else {\n          if (canDoPerSeg && toGlobal != null) {\n            collectCounts(singleDv, disi, toGlobal);\n          } else {\n            collectDocs(singleDv, disi, toGlobal);\n          }\n        }\n      } else {\n        if (accumSeg) {\n          collectPerSeg(multiDv, disi, toGlobal);\n        } else {\n          if (canDoPerSeg && toGlobal != null) {\n            collectCounts(multiDv, disi, toGlobal);\n          } else {\n            collectDocs(multiDv, disi, toGlobal);\n          }\n        }\n      }\n    }\n\n    reuse = null;  // better GC\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":5,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByArrayDV#collectDocs().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorDV#collectDocs().mjava","sourceNew":"  @Override\n  protected void collectDocs() throws IOException {\n    int domainSize = fcontext.base.size();\n\n    if (nTerms <= 0 || domainSize < effectiveMincount) { // TODO: what about allBuckets? missing bucket?\n      return;\n    }\n\n    // TODO: refactor some of this logic into a base class\n    boolean countOnly = collectAcc==null && allBucketsAcc==null;\n    boolean fullRange = startTermIndex == 0 && endTermIndex == si.getValueCount();\n\n    // Are we expecting many hits per bucket?\n    // FUTURE: pro-rate for nTerms?\n    // FUTURE: better take into account number of values in multi-valued fields.  This info is available for indexed fields.\n    // FUTURE: take into account that bigger ord maps are more expensive than smaller ones\n    // One test: 5M doc index, faceting on a single-valued field with almost 1M unique values, crossover point where global counting was slower\n    // than per-segment counting was a domain of 658k docs.  At that point, top 10 buckets had 6-7 matches each.\n    // this was for heap docvalues produced by UninvertingReader\n    // Since these values were randomly distributed, lets round our domain multiplier up to account for less random real world data.\n    long domainMultiplier = multiValuedField ? 4L : 2L;\n    boolean manyHitsPerBucket = domainSize * domainMultiplier > (si.getValueCount() + 3);  // +3 to increase test coverage with small tests\n\n    // If we're only calculating counts, we're not prefixing, and we expect to collect many documents per unique value,\n    // then collect per-segment before mapping to global ords at the end.  This will save redundant seg->global ord mappings.\n    // FUTURE: there are probably some other non \"countOnly\" cases where we can use this as well (i.e. those where\n    // the docid is not used)\n    boolean canDoPerSeg = countOnly && fullRange;\n    boolean accumSeg = manyHitsPerBucket && canDoPerSeg;\n\n    if (freq.perSeg != null) accumSeg = canDoPerSeg && freq.perSeg;  // internal - override perSeg heuristic\n\n    final List<LeafReaderContext> leaves = fcontext.searcher.getIndexReader().leaves();\n    Filter filter = fcontext.base.getTopFilter();\n\n    for (int subIdx = 0; subIdx < leaves.size(); subIdx++) {\n      LeafReaderContext subCtx = leaves.get(subIdx);\n\n      setNextReaderFirstPhase(subCtx);\n\n      DocIdSet dis = filter.getDocIdSet(subCtx, null); // solr docsets already exclude any deleted docs\n      DocIdSetIterator disi = dis.iterator();\n\n      SortedDocValues singleDv = null;\n      SortedSetDocValues multiDv = null;\n      if (multiValuedField) {\n        // TODO: get sub from multi?\n        multiDv = subCtx.reader().getSortedSetDocValues(sf.getName());\n        if (multiDv == null) {\n          multiDv = DocValues.emptySortedSet();\n        }\n        // some codecs may optimize SortedSet storage for single-valued fields\n        // this will be null if this is not a wrapped single valued docvalues.\n        if (unwrap_singleValued_multiDv) {\n          singleDv = DocValues.unwrapSingleton(multiDv);\n        }\n      } else {\n        singleDv = subCtx.reader().getSortedDocValues(sf.getName());\n        if (singleDv == null) {\n          singleDv = DocValues.emptySorted();\n        }\n      }\n\n      LongValues toGlobal = ordinalMap == null ? null : ordinalMap.getGlobalOrds(subIdx);\n\n      if (singleDv != null) {\n        if (accumSeg) {\n          collectPerSeg(singleDv, disi, toGlobal);\n        } else {\n          if (canDoPerSeg && toGlobal != null) {\n            collectCounts(singleDv, disi, toGlobal);\n          } else {\n            collectDocs(singleDv, disi, toGlobal);\n          }\n        }\n      } else {\n        if (accumSeg) {\n          collectPerSeg(multiDv, disi, toGlobal);\n        } else {\n          if (canDoPerSeg && toGlobal != null) {\n            collectCounts(multiDv, disi, toGlobal);\n          } else {\n            collectDocs(multiDv, disi, toGlobal);\n          }\n        }\n      }\n    }\n\n    reuse = null;  // better GC\n  }\n\n","sourceOld":"  @Override\n  protected void collectDocs() throws IOException {\n    int domainSize = fcontext.base.size();\n\n    if (nTerms <= 0 || domainSize < effectiveMincount) { // TODO: what about allBuckets? missing bucket?\n      return;\n    }\n\n    // TODO: refactor some of this logic into a base class\n    boolean countOnly = collectAcc==null && allBucketsAcc==null;\n    boolean fullRange = startTermIndex == 0 && endTermIndex == si.getValueCount();\n\n    // Are we expecting many hits per bucket?\n    // FUTURE: pro-rate for nTerms?\n    // FUTURE: better take into account number of values in multi-valued fields.  This info is available for indexed fields.\n    // FUTURE: take into account that bigger ord maps are more expensive than smaller ones\n    // One test: 5M doc index, faceting on a single-valued field with almost 1M unique values, crossover point where global counting was slower\n    // than per-segment counting was a domain of 658k docs.  At that point, top 10 buckets had 6-7 matches each.\n    // this was for heap docvalues produced by UninvertingReader\n    // Since these values were randomly distributed, lets round our domain multiplier up to account for less random real world data.\n    long domainMultiplier = multiValuedField ? 4L : 2L;\n    boolean manyHitsPerBucket = domainSize * domainMultiplier > (si.getValueCount() + 3);  // +3 to increase test coverage with small tests\n\n    // If we're only calculating counts, we're not prefixing, and we expect to collect many documents per unique value,\n    // then collect per-segment before mapping to global ords at the end.  This will save redundant seg->global ord mappings.\n    // FUTURE: there are probably some other non \"countOnly\" cases where we can use this as well (i.e. those where\n    // the docid is not used)\n    boolean canDoPerSeg = countOnly && fullRange;\n    boolean accumSeg = manyHitsPerBucket && canDoPerSeg;\n\n    if (freq.perSeg != null) accumSeg = canDoPerSeg && freq.perSeg;  // internal - override perSeg heuristic\n\n    final List<LeafReaderContext> leaves = fcontext.searcher.getIndexReader().leaves();\n    Filter filter = fcontext.base.getTopFilter();\n\n    for (int subIdx = 0; subIdx < leaves.size(); subIdx++) {\n      LeafReaderContext subCtx = leaves.get(subIdx);\n\n      setNextReaderFirstPhase(subCtx);\n\n      DocIdSet dis = filter.getDocIdSet(subCtx, null); // solr docsets already exclude any deleted docs\n      DocIdSetIterator disi = dis.iterator();\n\n      SortedDocValues singleDv = null;\n      SortedSetDocValues multiDv = null;\n      if (multiValuedField) {\n        // TODO: get sub from multi?\n        multiDv = subCtx.reader().getSortedSetDocValues(sf.getName());\n        if (multiDv == null) {\n          multiDv = DocValues.emptySortedSet();\n        }\n        // some codecs may optimize SortedSet storage for single-valued fields\n        // this will be null if this is not a wrapped single valued docvalues.\n        if (unwrap_singleValued_multiDv) {\n          singleDv = DocValues.unwrapSingleton(multiDv);\n        }\n      } else {\n        singleDv = subCtx.reader().getSortedDocValues(sf.getName());\n        if (singleDv == null) {\n          singleDv = DocValues.emptySorted();\n        }\n      }\n\n      LongValues toGlobal = ordinalMap == null ? null : ordinalMap.getGlobalOrds(subIdx);\n\n      if (singleDv != null) {\n        if (accumSeg) {\n          collectPerSeg(singleDv, disi, toGlobal);\n        } else {\n          if (canDoPerSeg && toGlobal != null) {\n            collectCounts(singleDv, disi, toGlobal);\n          } else {\n            collectDocs(singleDv, disi, toGlobal);\n          }\n        }\n      } else {\n        if (accumSeg) {\n          collectPerSeg(multiDv, disi, toGlobal);\n        } else {\n          if (canDoPerSeg && toGlobal != null) {\n            collectCounts(multiDv, disi, toGlobal);\n          } else {\n            collectDocs(multiDv, disi, toGlobal);\n          }\n        }\n      }\n    }\n\n    reuse = null;  // better GC\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"51b068b0154fe2220646b2dcc2133e168a31c743":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"403d05f7f8d69b65659157eff1bc1d2717f04c66":["51b068b0154fe2220646b2dcc2133e168a31c743","2c8bedceb91e64a3f0e831450058fc4a76d2c0a6"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6":["51b068b0154fe2220646b2dcc2133e168a31c743","79759974460bc59933cd169acc94f5c6b16368d5"],"79759974460bc59933cd169acc94f5c6b16368d5":["51b068b0154fe2220646b2dcc2133e168a31c743"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["51b068b0154fe2220646b2dcc2133e168a31c743","403d05f7f8d69b65659157eff1bc1d2717f04c66"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["403d05f7f8d69b65659157eff1bc1d2717f04c66"]},"commit2Childs":{"51b068b0154fe2220646b2dcc2133e168a31c743":["403d05f7f8d69b65659157eff1bc1d2717f04c66","2c8bedceb91e64a3f0e831450058fc4a76d2c0a6","79759974460bc59933cd169acc94f5c6b16368d5","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"403d05f7f8d69b65659157eff1bc1d2717f04c66":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["51b068b0154fe2220646b2dcc2133e168a31c743"],"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6":["403d05f7f8d69b65659157eff1bc1d2717f04c66"],"79759974460bc59933cd169acc94f5c6b16368d5":["2c8bedceb91e64a3f0e831450058fc4a76d2c0a6"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}