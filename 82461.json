{"path":"solr/core/src/java/org/apache/solr/search/SolrIndexSearcher#getDocListC(QueryResult,QueryCommand).mjava","commits":[{"id":"c903c3d15906a3da96b8c0c2fb704491005fdbdb","date":1453508227,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/SolrIndexSearcher#getDocListC(QueryResult,QueryCommand).mjava","pathOld":"solr/src/java/org/apache/solr/search/SolrIndexSearcher#getDocListC(QueryResult,QueryCommand).mjava","sourceNew":"  /**\n   * getDocList version that uses+populates query and filter caches.\n   * In the event of a timeout, the cache is not populated.\n   */\n  private void getDocListC(QueryResult qr, QueryCommand cmd) throws IOException {\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n    QueryResultKey key=null;\n    int maxDocRequested = cmd.getOffset() + cmd.getLen();\n    // check for overflow, and check for # docs in index\n    if (maxDocRequested < 0 || maxDocRequested > maxDoc()) maxDocRequested = maxDoc();\n    int supersetMaxDoc= maxDocRequested;\n    DocList superset = null;\n\n    int flags = cmd.getFlags();\n    Query q = cmd.getQuery();\n    if (q instanceof ExtendedQuery) {\n      ExtendedQuery eq = (ExtendedQuery)q;\n      if (!eq.getCache()) {\n        flags |= (NO_CHECK_QCACHE | NO_SET_QCACHE | NO_CHECK_FILTERCACHE);\n      }\n    }\n\n\n    // we can try and look up the complete query in the cache.\n    // we can't do that if filter!=null though (we don't want to\n    // do hashCode() and equals() for a big DocSet).\n    if (queryResultCache != null && cmd.getFilter()==null\n        && (flags & (NO_CHECK_QCACHE|NO_SET_QCACHE)) != ((NO_CHECK_QCACHE|NO_SET_QCACHE)))\n    {\n        // all of the current flags can be reused during warming,\n        // so set all of them on the cache key.\n        key = new QueryResultKey(q, cmd.getFilterList(), cmd.getSort(), flags);\n        if ((flags & NO_CHECK_QCACHE)==0) {\n          superset = queryResultCache.get(key);\n\n          if (superset != null) {\n            // check that the cache entry has scores recorded if we need them\n            if ((flags & GET_SCORES)==0 || superset.hasScores()) {\n              // NOTE: subset() returns null if the DocList has fewer docs than\n              // requested\n              out.docList = superset.subset(cmd.getOffset(),cmd.getLen());\n            }\n          }\n          if (out.docList != null) {\n            // found the docList in the cache... now check if we need the docset too.\n            // OPT: possible future optimization - if the doclist contains all the matches,\n            // use it to make the docset instead of rerunning the query.\n            if (out.docSet==null && ((flags & GET_DOCSET)!=0) ) {\n              if (cmd.getFilterList()==null) {\n                out.docSet = getDocSet(cmd.getQuery());\n              } else {\n                List<Query> newList = new ArrayList<Query>(cmd.getFilterList().size()+1);\n                newList.add(cmd.getQuery());\n                newList.addAll(cmd.getFilterList());\n                out.docSet = getDocSet(newList);\n              }\n            }\n            return;\n          }\n        }\n\n      // If we are going to generate the result, bump up to the\n      // next resultWindowSize for better caching.\n\n      if ((flags & NO_SET_QCACHE) == 0) {\n        // handle 0 special case as well as avoid idiv in the common case.\n        if (maxDocRequested < queryResultWindowSize) {\n          supersetMaxDoc=queryResultWindowSize;\n        } else {\n          supersetMaxDoc = ((maxDocRequested -1)/queryResultWindowSize + 1)*queryResultWindowSize;\n          if (supersetMaxDoc < 0) supersetMaxDoc=maxDocRequested;\n        }\n      } else {\n        key = null;  // we won't be caching the result\n      }\n    }\n\n\n    // OK, so now we need to generate an answer.\n    // One way to do that would be to check if we have an unordered list\n    // of results for the base query.  If so, we can apply the filters and then\n    // sort by the resulting set.  This can only be used if:\n    // - the sort doesn't contain score\n    // - we don't want score returned.\n\n    // check if we should try and use the filter cache\n    boolean useFilterCache=false;\n    if ((flags & (GET_SCORES|NO_CHECK_FILTERCACHE))==0 && useFilterForSortedQuery && cmd.getSort() != null && filterCache != null) {\n      useFilterCache=true;\n      SortField[] sfields = cmd.getSort().getSort();\n      for (SortField sf : sfields) {\n        if (sf.getType() == SortField.Type.SCORE) {\n          useFilterCache=false;\n          break;\n        }\n      }\n    }\n\n    // disable useFilterCache optimization temporarily\n    if (useFilterCache) {\n      // now actually use the filter cache.\n      // for large filters that match few documents, this may be\n      // slower than simply re-executing the query.\n      if (out.docSet == null) {\n        out.docSet = getDocSet(cmd.getQuery(),cmd.getFilter());\n        DocSet bigFilt = getDocSet(cmd.getFilterList());\n        if (bigFilt != null) out.docSet = out.docSet.intersection(bigFilt);\n      }\n      // todo: there could be a sortDocSet that could take a list of\n      // the filters instead of anding them first...\n      // perhaps there should be a multi-docset-iterator\n      superset = sortDocSet(out.docSet,cmd.getSort(),supersetMaxDoc);\n      out.docList = superset.subset(cmd.getOffset(),cmd.getLen());\n    } else {\n      // do it the normal way...\n      cmd.setSupersetMaxDoc(supersetMaxDoc);\n      if ((flags & GET_DOCSET)!=0) {\n        // this currently conflates returning the docset for the base query vs\n        // the base query and all filters.\n        DocSet qDocSet = getDocListAndSetNC(qr,cmd);\n        // cache the docSet matching the query w/o filtering\n        if (qDocSet!=null && filterCache!=null && !qr.isPartialResults()) filterCache.put(cmd.getQuery(),qDocSet);\n      } else {\n        getDocListNC(qr,cmd);\n        //Parameters: cmd.getQuery(),theFilt,cmd.getSort(),0,supersetMaxDoc,cmd.getFlags(),cmd.getTimeAllowed(),responseHeader);\n      }\n      if (key != null) {\n        superset = out.docList;\n        out.docList = superset.subset(cmd.getOffset(),cmd.getLen());\n      }\n    }\n\n    // lastly, put the superset in the cache if the size is less than or equal\n    // to queryResultMaxDocsCached\n    if (key != null && superset.size() <= queryResultMaxDocsCached && !qr.isPartialResults()) {\n      queryResultCache.put(key, superset);\n    }\n  }\n\n","sourceOld":"  /**\n   * getDocList version that uses+populates query and filter caches.\n   * In the event of a timeout, the cache is not populated.\n   */\n  private void getDocListC(QueryResult qr, QueryCommand cmd) throws IOException {\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n    QueryResultKey key=null;\n    int maxDocRequested = cmd.getOffset() + cmd.getLen();\n    // check for overflow, and check for # docs in index\n    if (maxDocRequested < 0 || maxDocRequested > maxDoc()) maxDocRequested = maxDoc();\n    int supersetMaxDoc= maxDocRequested;\n    DocList superset = null;\n\n    int flags = cmd.getFlags();\n    Query q = cmd.getQuery();\n    if (q instanceof ExtendedQuery) {\n      ExtendedQuery eq = (ExtendedQuery)q;\n      if (!eq.getCache()) {\n        flags |= (NO_CHECK_QCACHE | NO_SET_QCACHE | NO_CHECK_FILTERCACHE);\n      }\n    }\n\n\n    // we can try and look up the complete query in the cache.\n    // we can't do that if filter!=null though (we don't want to\n    // do hashCode() and equals() for a big DocSet).\n    if (queryResultCache != null && cmd.getFilter()==null\n        && (flags & (NO_CHECK_QCACHE|NO_SET_QCACHE)) != ((NO_CHECK_QCACHE|NO_SET_QCACHE)))\n    {\n        // all of the current flags can be reused during warming,\n        // so set all of them on the cache key.\n        key = new QueryResultKey(q, cmd.getFilterList(), cmd.getSort(), flags);\n        if ((flags & NO_CHECK_QCACHE)==0) {\n          superset = queryResultCache.get(key);\n\n          if (superset != null) {\n            // check that the cache entry has scores recorded if we need them\n            if ((flags & GET_SCORES)==0 || superset.hasScores()) {\n              // NOTE: subset() returns null if the DocList has fewer docs than\n              // requested\n              out.docList = superset.subset(cmd.getOffset(),cmd.getLen());\n            }\n          }\n          if (out.docList != null) {\n            // found the docList in the cache... now check if we need the docset too.\n            // OPT: possible future optimization - if the doclist contains all the matches,\n            // use it to make the docset instead of rerunning the query.\n            if (out.docSet==null && ((flags & GET_DOCSET)!=0) ) {\n              if (cmd.getFilterList()==null) {\n                out.docSet = getDocSet(cmd.getQuery());\n              } else {\n                List<Query> newList = new ArrayList<Query>(cmd.getFilterList().size()+1);\n                newList.add(cmd.getQuery());\n                newList.addAll(cmd.getFilterList());\n                out.docSet = getDocSet(newList);\n              }\n            }\n            return;\n          }\n        }\n\n      // If we are going to generate the result, bump up to the\n      // next resultWindowSize for better caching.\n\n      if ((flags & NO_SET_QCACHE) == 0) {\n        // handle 0 special case as well as avoid idiv in the common case.\n        if (maxDocRequested < queryResultWindowSize) {\n          supersetMaxDoc=queryResultWindowSize;\n        } else {\n          supersetMaxDoc = ((maxDocRequested -1)/queryResultWindowSize + 1)*queryResultWindowSize;\n          if (supersetMaxDoc < 0) supersetMaxDoc=maxDocRequested;\n        }\n      } else {\n        key = null;  // we won't be caching the result\n      }\n    }\n\n\n    // OK, so now we need to generate an answer.\n    // One way to do that would be to check if we have an unordered list\n    // of results for the base query.  If so, we can apply the filters and then\n    // sort by the resulting set.  This can only be used if:\n    // - the sort doesn't contain score\n    // - we don't want score returned.\n\n    // check if we should try and use the filter cache\n    boolean useFilterCache=false;\n    if ((flags & (GET_SCORES|NO_CHECK_FILTERCACHE))==0 && useFilterForSortedQuery && cmd.getSort() != null && filterCache != null) {\n      useFilterCache=true;\n      SortField[] sfields = cmd.getSort().getSort();\n      for (SortField sf : sfields) {\n        if (sf.getType() == SortField.Type.SCORE) {\n          useFilterCache=false;\n          break;\n        }\n      }\n    }\n\n    // disable useFilterCache optimization temporarily\n    if (useFilterCache) {\n      // now actually use the filter cache.\n      // for large filters that match few documents, this may be\n      // slower than simply re-executing the query.\n      if (out.docSet == null) {\n        out.docSet = getDocSet(cmd.getQuery(),cmd.getFilter());\n        DocSet bigFilt = getDocSet(cmd.getFilterList());\n        if (bigFilt != null) out.docSet = out.docSet.intersection(bigFilt);\n      }\n      // todo: there could be a sortDocSet that could take a list of\n      // the filters instead of anding them first...\n      // perhaps there should be a multi-docset-iterator\n      superset = sortDocSet(out.docSet,cmd.getSort(),supersetMaxDoc);\n      out.docList = superset.subset(cmd.getOffset(),cmd.getLen());\n    } else {\n      // do it the normal way...\n      cmd.setSupersetMaxDoc(supersetMaxDoc);\n      if ((flags & GET_DOCSET)!=0) {\n        // this currently conflates returning the docset for the base query vs\n        // the base query and all filters.\n        DocSet qDocSet = getDocListAndSetNC(qr,cmd);\n        // cache the docSet matching the query w/o filtering\n        if (qDocSet!=null && filterCache!=null && !qr.isPartialResults()) filterCache.put(cmd.getQuery(),qDocSet);\n      } else {\n        getDocListNC(qr,cmd);\n        //Parameters: cmd.getQuery(),theFilt,cmd.getSort(),0,supersetMaxDoc,cmd.getFlags(),cmd.getTimeAllowed(),responseHeader);\n      }\n      if (key != null) {\n        superset = out.docList;\n        out.docList = superset.subset(cmd.getOffset(),cmd.getLen());\n      }\n    }\n\n    // lastly, put the superset in the cache if the size is less than or equal\n    // to queryResultMaxDocsCached\n    if (key != null && superset.size() <= queryResultMaxDocsCached && !qr.isPartialResults()) {\n      queryResultCache.put(key, superset);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a258fbb26824fd104ed795e5d9033d2d040049ee","date":1453508252,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/SolrIndexSearcher#getDocListC(QueryResult,QueryCommand).mjava","pathOld":"solr/src/java/org/apache/solr/search/SolrIndexSearcher#getDocListC(QueryResult,QueryCommand).mjava","sourceNew":"  /**\n   * getDocList version that uses+populates query and filter caches.\n   * In the event of a timeout, the cache is not populated.\n   */\n  private void getDocListC(QueryResult qr, QueryCommand cmd) throws IOException {\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n    QueryResultKey key=null;\n    int maxDocRequested = cmd.getOffset() + cmd.getLen();\n    // check for overflow, and check for # docs in index\n    if (maxDocRequested < 0 || maxDocRequested > maxDoc()) maxDocRequested = maxDoc();\n    int supersetMaxDoc= maxDocRequested;\n    DocList superset = null;\n\n    int flags = cmd.getFlags();\n    Query q = cmd.getQuery();\n    if (q instanceof ExtendedQuery) {\n      ExtendedQuery eq = (ExtendedQuery)q;\n      if (!eq.getCache()) {\n        flags |= (NO_CHECK_QCACHE | NO_SET_QCACHE | NO_CHECK_FILTERCACHE);\n      }\n    }\n\n\n    // we can try and look up the complete query in the cache.\n    // we can't do that if filter!=null though (we don't want to\n    // do hashCode() and equals() for a big DocSet).\n    if (queryResultCache != null && cmd.getFilter()==null\n        && (flags & (NO_CHECK_QCACHE|NO_SET_QCACHE)) != ((NO_CHECK_QCACHE|NO_SET_QCACHE)))\n    {\n        // all of the current flags can be reused during warming,\n        // so set all of them on the cache key.\n        key = new QueryResultKey(q, cmd.getFilterList(), cmd.getSort(), flags);\n        if ((flags & NO_CHECK_QCACHE)==0) {\n          superset = queryResultCache.get(key);\n\n          if (superset != null) {\n            // check that the cache entry has scores recorded if we need them\n            if ((flags & GET_SCORES)==0 || superset.hasScores()) {\n              // NOTE: subset() returns null if the DocList has fewer docs than\n              // requested\n              out.docList = superset.subset(cmd.getOffset(),cmd.getLen());\n            }\n          }\n          if (out.docList != null) {\n            // found the docList in the cache... now check if we need the docset too.\n            // OPT: possible future optimization - if the doclist contains all the matches,\n            // use it to make the docset instead of rerunning the query.\n            if (out.docSet==null && ((flags & GET_DOCSET)!=0) ) {\n              if (cmd.getFilterList()==null) {\n                out.docSet = getDocSet(cmd.getQuery());\n              } else {\n                List<Query> newList = new ArrayList<Query>(cmd.getFilterList().size()+1);\n                newList.add(cmd.getQuery());\n                newList.addAll(cmd.getFilterList());\n                out.docSet = getDocSet(newList);\n              }\n            }\n            return;\n          }\n        }\n\n      // If we are going to generate the result, bump up to the\n      // next resultWindowSize for better caching.\n\n      if ((flags & NO_SET_QCACHE) == 0) {\n        // handle 0 special case as well as avoid idiv in the common case.\n        if (maxDocRequested < queryResultWindowSize) {\n          supersetMaxDoc=queryResultWindowSize;\n        } else {\n          supersetMaxDoc = ((maxDocRequested -1)/queryResultWindowSize + 1)*queryResultWindowSize;\n          if (supersetMaxDoc < 0) supersetMaxDoc=maxDocRequested;\n        }\n      } else {\n        key = null;  // we won't be caching the result\n      }\n    }\n\n\n    // OK, so now we need to generate an answer.\n    // One way to do that would be to check if we have an unordered list\n    // of results for the base query.  If so, we can apply the filters and then\n    // sort by the resulting set.  This can only be used if:\n    // - the sort doesn't contain score\n    // - we don't want score returned.\n\n    // check if we should try and use the filter cache\n    boolean useFilterCache=false;\n    if ((flags & (GET_SCORES|NO_CHECK_FILTERCACHE))==0 && useFilterForSortedQuery && cmd.getSort() != null && filterCache != null) {\n      useFilterCache=true;\n      SortField[] sfields = cmd.getSort().getSort();\n      for (SortField sf : sfields) {\n        if (sf.getType() == SortField.Type.SCORE) {\n          useFilterCache=false;\n          break;\n        }\n      }\n    }\n\n    // disable useFilterCache optimization temporarily\n    if (useFilterCache) {\n      // now actually use the filter cache.\n      // for large filters that match few documents, this may be\n      // slower than simply re-executing the query.\n      if (out.docSet == null) {\n        out.docSet = getDocSet(cmd.getQuery(),cmd.getFilter());\n        DocSet bigFilt = getDocSet(cmd.getFilterList());\n        if (bigFilt != null) out.docSet = out.docSet.intersection(bigFilt);\n      }\n      // todo: there could be a sortDocSet that could take a list of\n      // the filters instead of anding them first...\n      // perhaps there should be a multi-docset-iterator\n      superset = sortDocSet(out.docSet,cmd.getSort(),supersetMaxDoc);\n      out.docList = superset.subset(cmd.getOffset(),cmd.getLen());\n    } else {\n      // do it the normal way...\n      cmd.setSupersetMaxDoc(supersetMaxDoc);\n      if ((flags & GET_DOCSET)!=0) {\n        // this currently conflates returning the docset for the base query vs\n        // the base query and all filters.\n        DocSet qDocSet = getDocListAndSetNC(qr,cmd);\n        // cache the docSet matching the query w/o filtering\n        if (qDocSet!=null && filterCache!=null && !qr.isPartialResults()) filterCache.put(cmd.getQuery(),qDocSet);\n      } else {\n        getDocListNC(qr,cmd);\n        //Parameters: cmd.getQuery(),theFilt,cmd.getSort(),0,supersetMaxDoc,cmd.getFlags(),cmd.getTimeAllowed(),responseHeader);\n      }\n      if (key != null) {\n        superset = out.docList;\n        out.docList = superset.subset(cmd.getOffset(),cmd.getLen());\n      }\n    }\n\n    // lastly, put the superset in the cache if the size is less than or equal\n    // to queryResultMaxDocsCached\n    if (key != null && superset.size() <= queryResultMaxDocsCached && !qr.isPartialResults()) {\n      queryResultCache.put(key, superset);\n    }\n  }\n\n","sourceOld":"  /**\n   * getDocList version that uses+populates query and filter caches.\n   * In the event of a timeout, the cache is not populated.\n   */\n  private void getDocListC(QueryResult qr, QueryCommand cmd) throws IOException {\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n    QueryResultKey key=null;\n    int maxDocRequested = cmd.getOffset() + cmd.getLen();\n    // check for overflow, and check for # docs in index\n    if (maxDocRequested < 0 || maxDocRequested > maxDoc()) maxDocRequested = maxDoc();\n    int supersetMaxDoc= maxDocRequested;\n    DocList superset = null;\n\n    int flags = cmd.getFlags();\n    Query q = cmd.getQuery();\n    if (q instanceof ExtendedQuery) {\n      ExtendedQuery eq = (ExtendedQuery)q;\n      if (!eq.getCache()) {\n        flags |= (NO_CHECK_QCACHE | NO_SET_QCACHE | NO_CHECK_FILTERCACHE);\n      }\n    }\n\n\n    // we can try and look up the complete query in the cache.\n    // we can't do that if filter!=null though (we don't want to\n    // do hashCode() and equals() for a big DocSet).\n    if (queryResultCache != null && cmd.getFilter()==null\n        && (flags & (NO_CHECK_QCACHE|NO_SET_QCACHE)) != ((NO_CHECK_QCACHE|NO_SET_QCACHE)))\n    {\n        // all of the current flags can be reused during warming,\n        // so set all of them on the cache key.\n        key = new QueryResultKey(q, cmd.getFilterList(), cmd.getSort(), flags);\n        if ((flags & NO_CHECK_QCACHE)==0) {\n          superset = queryResultCache.get(key);\n\n          if (superset != null) {\n            // check that the cache entry has scores recorded if we need them\n            if ((flags & GET_SCORES)==0 || superset.hasScores()) {\n              // NOTE: subset() returns null if the DocList has fewer docs than\n              // requested\n              out.docList = superset.subset(cmd.getOffset(),cmd.getLen());\n            }\n          }\n          if (out.docList != null) {\n            // found the docList in the cache... now check if we need the docset too.\n            // OPT: possible future optimization - if the doclist contains all the matches,\n            // use it to make the docset instead of rerunning the query.\n            if (out.docSet==null && ((flags & GET_DOCSET)!=0) ) {\n              if (cmd.getFilterList()==null) {\n                out.docSet = getDocSet(cmd.getQuery());\n              } else {\n                List<Query> newList = new ArrayList<Query>(cmd.getFilterList().size()+1);\n                newList.add(cmd.getQuery());\n                newList.addAll(cmd.getFilterList());\n                out.docSet = getDocSet(newList);\n              }\n            }\n            return;\n          }\n        }\n\n      // If we are going to generate the result, bump up to the\n      // next resultWindowSize for better caching.\n\n      if ((flags & NO_SET_QCACHE) == 0) {\n        // handle 0 special case as well as avoid idiv in the common case.\n        if (maxDocRequested < queryResultWindowSize) {\n          supersetMaxDoc=queryResultWindowSize;\n        } else {\n          supersetMaxDoc = ((maxDocRequested -1)/queryResultWindowSize + 1)*queryResultWindowSize;\n          if (supersetMaxDoc < 0) supersetMaxDoc=maxDocRequested;\n        }\n      } else {\n        key = null;  // we won't be caching the result\n      }\n    }\n\n\n    // OK, so now we need to generate an answer.\n    // One way to do that would be to check if we have an unordered list\n    // of results for the base query.  If so, we can apply the filters and then\n    // sort by the resulting set.  This can only be used if:\n    // - the sort doesn't contain score\n    // - we don't want score returned.\n\n    // check if we should try and use the filter cache\n    boolean useFilterCache=false;\n    if ((flags & (GET_SCORES|NO_CHECK_FILTERCACHE))==0 && useFilterForSortedQuery && cmd.getSort() != null && filterCache != null) {\n      useFilterCache=true;\n      SortField[] sfields = cmd.getSort().getSort();\n      for (SortField sf : sfields) {\n        if (sf.getType() == SortField.Type.SCORE) {\n          useFilterCache=false;\n          break;\n        }\n      }\n    }\n\n    // disable useFilterCache optimization temporarily\n    if (useFilterCache) {\n      // now actually use the filter cache.\n      // for large filters that match few documents, this may be\n      // slower than simply re-executing the query.\n      if (out.docSet == null) {\n        out.docSet = getDocSet(cmd.getQuery(),cmd.getFilter());\n        DocSet bigFilt = getDocSet(cmd.getFilterList());\n        if (bigFilt != null) out.docSet = out.docSet.intersection(bigFilt);\n      }\n      // todo: there could be a sortDocSet that could take a list of\n      // the filters instead of anding them first...\n      // perhaps there should be a multi-docset-iterator\n      superset = sortDocSet(out.docSet,cmd.getSort(),supersetMaxDoc);\n      out.docList = superset.subset(cmd.getOffset(),cmd.getLen());\n    } else {\n      // do it the normal way...\n      cmd.setSupersetMaxDoc(supersetMaxDoc);\n      if ((flags & GET_DOCSET)!=0) {\n        // this currently conflates returning the docset for the base query vs\n        // the base query and all filters.\n        DocSet qDocSet = getDocListAndSetNC(qr,cmd);\n        // cache the docSet matching the query w/o filtering\n        if (qDocSet!=null && filterCache!=null && !qr.isPartialResults()) filterCache.put(cmd.getQuery(),qDocSet);\n      } else {\n        getDocListNC(qr,cmd);\n        //Parameters: cmd.getQuery(),theFilt,cmd.getSort(),0,supersetMaxDoc,cmd.getFlags(),cmd.getTimeAllowed(),responseHeader);\n      }\n      if (key != null) {\n        superset = out.docList;\n        out.docList = superset.subset(cmd.getOffset(),cmd.getLen());\n      }\n    }\n\n    // lastly, put the superset in the cache if the size is less than or equal\n    // to queryResultMaxDocsCached\n    if (key != null && superset.size() <= queryResultMaxDocsCached && !qr.isPartialResults()) {\n      queryResultCache.put(key, superset);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c26f00b574427b55127e869b935845554afde1fa","date":1310252513,"type":1,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/SolrIndexSearcher#getDocListC(QueryResult,QueryCommand).mjava","pathOld":"solr/src/java/org/apache/solr/search/SolrIndexSearcher#getDocListC(QueryResult,QueryCommand).mjava","sourceNew":"  /**\n   * getDocList version that uses+populates query and filter caches.\n   * In the event of a timeout, the cache is not populated.\n   */\n  private void getDocListC(QueryResult qr, QueryCommand cmd) throws IOException {\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n    QueryResultKey key=null;\n    int maxDocRequested = cmd.getOffset() + cmd.getLen();\n    // check for overflow, and check for # docs in index\n    if (maxDocRequested < 0 || maxDocRequested > maxDoc()) maxDocRequested = maxDoc();\n    int supersetMaxDoc= maxDocRequested;\n    DocList superset = null;\n\n    int flags = cmd.getFlags();\n    Query q = cmd.getQuery();\n    if (q instanceof ExtendedQuery) {\n      ExtendedQuery eq = (ExtendedQuery)q;\n      if (!eq.getCache()) {\n        flags |= (NO_CHECK_QCACHE | NO_SET_QCACHE | NO_CHECK_FILTERCACHE);\n      }\n    }\n\n\n    // we can try and look up the complete query in the cache.\n    // we can't do that if filter!=null though (we don't want to\n    // do hashCode() and equals() for a big DocSet).\n    if (queryResultCache != null && cmd.getFilter()==null\n        && (flags & (NO_CHECK_QCACHE|NO_SET_QCACHE)) != ((NO_CHECK_QCACHE|NO_SET_QCACHE)))\n    {\n        // all of the current flags can be reused during warming,\n        // so set all of them on the cache key.\n        key = new QueryResultKey(q, cmd.getFilterList(), cmd.getSort(), flags);\n        if ((flags & NO_CHECK_QCACHE)==0) {\n          superset = queryResultCache.get(key);\n\n          if (superset != null) {\n            // check that the cache entry has scores recorded if we need them\n            if ((flags & GET_SCORES)==0 || superset.hasScores()) {\n              // NOTE: subset() returns null if the DocList has fewer docs than\n              // requested\n              out.docList = superset.subset(cmd.getOffset(),cmd.getLen());\n            }\n          }\n          if (out.docList != null) {\n            // found the docList in the cache... now check if we need the docset too.\n            // OPT: possible future optimization - if the doclist contains all the matches,\n            // use it to make the docset instead of rerunning the query.\n            if (out.docSet==null && ((flags & GET_DOCSET)!=0) ) {\n              if (cmd.getFilterList()==null) {\n                out.docSet = getDocSet(cmd.getQuery());\n              } else {\n                List<Query> newList = new ArrayList<Query>(cmd.getFilterList().size()+1);\n                newList.add(cmd.getQuery());\n                newList.addAll(cmd.getFilterList());\n                out.docSet = getDocSet(newList);\n              }\n            }\n            return;\n          }\n        }\n\n      // If we are going to generate the result, bump up to the\n      // next resultWindowSize for better caching.\n\n      if ((flags & NO_SET_QCACHE) == 0) {\n        // handle 0 special case as well as avoid idiv in the common case.\n        if (maxDocRequested < queryResultWindowSize) {\n          supersetMaxDoc=queryResultWindowSize;\n        } else {\n          supersetMaxDoc = ((maxDocRequested -1)/queryResultWindowSize + 1)*queryResultWindowSize;\n          if (supersetMaxDoc < 0) supersetMaxDoc=maxDocRequested;\n        }\n      } else {\n        key = null;  // we won't be caching the result\n      }\n    }\n\n\n    // OK, so now we need to generate an answer.\n    // One way to do that would be to check if we have an unordered list\n    // of results for the base query.  If so, we can apply the filters and then\n    // sort by the resulting set.  This can only be used if:\n    // - the sort doesn't contain score\n    // - we don't want score returned.\n\n    // check if we should try and use the filter cache\n    boolean useFilterCache=false;\n    if ((flags & (GET_SCORES|NO_CHECK_FILTERCACHE))==0 && useFilterForSortedQuery && cmd.getSort() != null && filterCache != null) {\n      useFilterCache=true;\n      SortField[] sfields = cmd.getSort().getSort();\n      for (SortField sf : sfields) {\n        if (sf.getType() == SortField.Type.SCORE) {\n          useFilterCache=false;\n          break;\n        }\n      }\n    }\n\n    // disable useFilterCache optimization temporarily\n    if (useFilterCache) {\n      // now actually use the filter cache.\n      // for large filters that match few documents, this may be\n      // slower than simply re-executing the query.\n      if (out.docSet == null) {\n        out.docSet = getDocSet(cmd.getQuery(),cmd.getFilter());\n        DocSet bigFilt = getDocSet(cmd.getFilterList());\n        if (bigFilt != null) out.docSet = out.docSet.intersection(bigFilt);\n      }\n      // todo: there could be a sortDocSet that could take a list of\n      // the filters instead of anding them first...\n      // perhaps there should be a multi-docset-iterator\n      superset = sortDocSet(out.docSet,cmd.getSort(),supersetMaxDoc);\n      out.docList = superset.subset(cmd.getOffset(),cmd.getLen());\n    } else {\n      // do it the normal way...\n      cmd.setSupersetMaxDoc(supersetMaxDoc);\n      if ((flags & GET_DOCSET)!=0) {\n        // this currently conflates returning the docset for the base query vs\n        // the base query and all filters.\n        DocSet qDocSet = getDocListAndSetNC(qr,cmd);\n        // cache the docSet matching the query w/o filtering\n        if (qDocSet!=null && filterCache!=null && !qr.isPartialResults()) filterCache.put(cmd.getQuery(),qDocSet);\n      } else {\n        getDocListNC(qr,cmd);\n        //Parameters: cmd.getQuery(),theFilt,cmd.getSort(),0,supersetMaxDoc,cmd.getFlags(),cmd.getTimeAllowed(),responseHeader);\n      }\n      if (key != null) {\n        superset = out.docList;\n        out.docList = superset.subset(cmd.getOffset(),cmd.getLen());\n      }\n    }\n\n    // lastly, put the superset in the cache if the size is less than or equal\n    // to queryResultMaxDocsCached\n    if (key != null && superset.size() <= queryResultMaxDocsCached && !qr.isPartialResults()) {\n      queryResultCache.put(key, superset);\n    }\n  }\n\n","sourceOld":"  /**\n   * getDocList version that uses+populates query and filter caches.\n   * In the event of a timeout, the cache is not populated.\n   */\n  private void getDocListC(QueryResult qr, QueryCommand cmd) throws IOException {\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n    QueryResultKey key=null;\n    int maxDocRequested = cmd.getOffset() + cmd.getLen();\n    // check for overflow, and check for # docs in index\n    if (maxDocRequested < 0 || maxDocRequested > maxDoc()) maxDocRequested = maxDoc();\n    int supersetMaxDoc= maxDocRequested;\n    DocList superset = null;\n\n    int flags = cmd.getFlags();\n    Query q = cmd.getQuery();\n    if (q instanceof ExtendedQuery) {\n      ExtendedQuery eq = (ExtendedQuery)q;\n      if (!eq.getCache()) {\n        flags |= (NO_CHECK_QCACHE | NO_SET_QCACHE | NO_CHECK_FILTERCACHE);\n      }\n    }\n\n\n    // we can try and look up the complete query in the cache.\n    // we can't do that if filter!=null though (we don't want to\n    // do hashCode() and equals() for a big DocSet).\n    if (queryResultCache != null && cmd.getFilter()==null\n        && (flags & (NO_CHECK_QCACHE|NO_SET_QCACHE)) != ((NO_CHECK_QCACHE|NO_SET_QCACHE)))\n    {\n        // all of the current flags can be reused during warming,\n        // so set all of them on the cache key.\n        key = new QueryResultKey(q, cmd.getFilterList(), cmd.getSort(), flags);\n        if ((flags & NO_CHECK_QCACHE)==0) {\n          superset = queryResultCache.get(key);\n\n          if (superset != null) {\n            // check that the cache entry has scores recorded if we need them\n            if ((flags & GET_SCORES)==0 || superset.hasScores()) {\n              // NOTE: subset() returns null if the DocList has fewer docs than\n              // requested\n              out.docList = superset.subset(cmd.getOffset(),cmd.getLen());\n            }\n          }\n          if (out.docList != null) {\n            // found the docList in the cache... now check if we need the docset too.\n            // OPT: possible future optimization - if the doclist contains all the matches,\n            // use it to make the docset instead of rerunning the query.\n            if (out.docSet==null && ((flags & GET_DOCSET)!=0) ) {\n              if (cmd.getFilterList()==null) {\n                out.docSet = getDocSet(cmd.getQuery());\n              } else {\n                List<Query> newList = new ArrayList<Query>(cmd.getFilterList().size()+1);\n                newList.add(cmd.getQuery());\n                newList.addAll(cmd.getFilterList());\n                out.docSet = getDocSet(newList);\n              }\n            }\n            return;\n          }\n        }\n\n      // If we are going to generate the result, bump up to the\n      // next resultWindowSize for better caching.\n\n      if ((flags & NO_SET_QCACHE) == 0) {\n        // handle 0 special case as well as avoid idiv in the common case.\n        if (maxDocRequested < queryResultWindowSize) {\n          supersetMaxDoc=queryResultWindowSize;\n        } else {\n          supersetMaxDoc = ((maxDocRequested -1)/queryResultWindowSize + 1)*queryResultWindowSize;\n          if (supersetMaxDoc < 0) supersetMaxDoc=maxDocRequested;\n        }\n      } else {\n        key = null;  // we won't be caching the result\n      }\n    }\n\n\n    // OK, so now we need to generate an answer.\n    // One way to do that would be to check if we have an unordered list\n    // of results for the base query.  If so, we can apply the filters and then\n    // sort by the resulting set.  This can only be used if:\n    // - the sort doesn't contain score\n    // - we don't want score returned.\n\n    // check if we should try and use the filter cache\n    boolean useFilterCache=false;\n    if ((flags & (GET_SCORES|NO_CHECK_FILTERCACHE))==0 && useFilterForSortedQuery && cmd.getSort() != null && filterCache != null) {\n      useFilterCache=true;\n      SortField[] sfields = cmd.getSort().getSort();\n      for (SortField sf : sfields) {\n        if (sf.getType() == SortField.Type.SCORE) {\n          useFilterCache=false;\n          break;\n        }\n      }\n    }\n\n    // disable useFilterCache optimization temporarily\n    if (useFilterCache) {\n      // now actually use the filter cache.\n      // for large filters that match few documents, this may be\n      // slower than simply re-executing the query.\n      if (out.docSet == null) {\n        out.docSet = getDocSet(cmd.getQuery(),cmd.getFilter());\n        DocSet bigFilt = getDocSet(cmd.getFilterList());\n        if (bigFilt != null) out.docSet = out.docSet.intersection(bigFilt);\n      }\n      // todo: there could be a sortDocSet that could take a list of\n      // the filters instead of anding them first...\n      // perhaps there should be a multi-docset-iterator\n      superset = sortDocSet(out.docSet,cmd.getSort(),supersetMaxDoc);\n      out.docList = superset.subset(cmd.getOffset(),cmd.getLen());\n    } else {\n      // do it the normal way...\n      cmd.setSupersetMaxDoc(supersetMaxDoc);\n      if ((flags & GET_DOCSET)!=0) {\n        // this currently conflates returning the docset for the base query vs\n        // the base query and all filters.\n        DocSet qDocSet = getDocListAndSetNC(qr,cmd);\n        // cache the docSet matching the query w/o filtering\n        if (qDocSet!=null && filterCache!=null && !qr.isPartialResults()) filterCache.put(cmd.getQuery(),qDocSet);\n      } else {\n        getDocListNC(qr,cmd);\n        //Parameters: cmd.getQuery(),theFilt,cmd.getSort(),0,supersetMaxDoc,cmd.getFlags(),cmd.getTimeAllowed(),responseHeader);\n      }\n      if (key != null) {\n        superset = out.docList;\n        out.docList = superset.subset(cmd.getOffset(),cmd.getLen());\n      }\n    }\n\n    // lastly, put the superset in the cache if the size is less than or equal\n    // to queryResultMaxDocsCached\n    if (key != null && superset.size() <= queryResultMaxDocsCached && !qr.isPartialResults()) {\n      queryResultCache.put(key, superset);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"42f37a8cb4c5565e5233860fe796624f5ec2459f","date":1311372234,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/SolrIndexSearcher#getDocListC(QueryResult,QueryCommand).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/SolrIndexSearcher#getDocListC(QueryResult,QueryCommand).mjava","sourceNew":"  /**\n   * getDocList version that uses+populates query and filter caches.\n   * In the event of a timeout, the cache is not populated.\n   */\n  private void getDocListC(QueryResult qr, QueryCommand cmd) throws IOException {\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n    QueryResultKey key=null;\n    int maxDocRequested = cmd.getOffset() + cmd.getLen();\n    // check for overflow, and check for # docs in index\n    if (maxDocRequested < 0 || maxDocRequested > maxDoc()) maxDocRequested = maxDoc();\n    int supersetMaxDoc= maxDocRequested;\n    DocList superset = null;\n\n    int flags = cmd.getFlags();\n    Query q = cmd.getQuery();\n    if (q instanceof ExtendedQuery) {\n      ExtendedQuery eq = (ExtendedQuery)q;\n      if (!eq.getCache()) {\n        flags |= (NO_CHECK_QCACHE | NO_SET_QCACHE | NO_CHECK_FILTERCACHE);\n      }\n    }\n\n\n    // we can try and look up the complete query in the cache.\n    // we can't do that if filter!=null though (we don't want to\n    // do hashCode() and equals() for a big DocSet).\n    if (queryResultCache != null && cmd.getFilter()==null\n        && (flags & (NO_CHECK_QCACHE|NO_SET_QCACHE)) != ((NO_CHECK_QCACHE|NO_SET_QCACHE)))\n    {\n        // all of the current flags can be reused during warming,\n        // so set all of them on the cache key.\n        key = new QueryResultKey(q, cmd.getFilterList(), cmd.getSort(), flags);\n        if ((flags & NO_CHECK_QCACHE)==0) {\n          superset = queryResultCache.get(key);\n\n          if (superset != null) {\n            // check that the cache entry has scores recorded if we need them\n            if ((flags & GET_SCORES)==0 || superset.hasScores()) {\n              // NOTE: subset() returns null if the DocList has fewer docs than\n              // requested\n              out.docList = superset.subset(cmd.getOffset(),cmd.getLen());\n            }\n          }\n          if (out.docList != null) {\n            // found the docList in the cache... now check if we need the docset too.\n            // OPT: possible future optimization - if the doclist contains all the matches,\n            // use it to make the docset instead of rerunning the query.\n            if (out.docSet==null && ((flags & GET_DOCSET)!=0) ) {\n              if (cmd.getFilterList()==null) {\n                out.docSet = getDocSet(cmd.getQuery());\n              } else {\n                List<Query> newList = new ArrayList<Query>(cmd.getFilterList().size()+1);\n                newList.add(cmd.getQuery());\n                newList.addAll(cmd.getFilterList());\n                out.docSet = getDocSet(newList);\n              }\n            }\n            return;\n          }\n        }\n\n      // If we are going to generate the result, bump up to the\n      // next resultWindowSize for better caching.\n\n      if ((flags & NO_SET_QCACHE) == 0) {\n        // handle 0 special case as well as avoid idiv in the common case.\n        if (maxDocRequested < queryResultWindowSize) {\n          supersetMaxDoc=queryResultWindowSize;\n        } else {\n          supersetMaxDoc = ((maxDocRequested -1)/queryResultWindowSize + 1)*queryResultWindowSize;\n          if (supersetMaxDoc < 0) supersetMaxDoc=maxDocRequested;\n        }\n      } else {\n        key = null;  // we won't be caching the result\n      }\n    }\n\n\n    // OK, so now we need to generate an answer.\n    // One way to do that would be to check if we have an unordered list\n    // of results for the base query.  If so, we can apply the filters and then\n    // sort by the resulting set.  This can only be used if:\n    // - the sort doesn't contain score\n    // - we don't want score returned.\n\n    // check if we should try and use the filter cache\n    boolean useFilterCache=false;\n    if ((flags & (GET_SCORES|NO_CHECK_FILTERCACHE))==0 && useFilterForSortedQuery && cmd.getSort() != null && filterCache != null) {\n      useFilterCache=true;\n      SortField[] sfields = cmd.getSort().getSort();\n      for (SortField sf : sfields) {\n        if (sf.getType() == SortField.Type.SCORE) {\n          useFilterCache=false;\n          break;\n        }\n      }\n    }\n\n    // disable useFilterCache optimization temporarily\n    if (useFilterCache) {\n      // now actually use the filter cache.\n      // for large filters that match few documents, this may be\n      // slower than simply re-executing the query.\n      if (out.docSet == null) {\n        out.docSet = getDocSet(cmd.getQuery(),cmd.getFilter());\n        DocSet bigFilt = getDocSet(cmd.getFilterList());\n        if (bigFilt != null) out.docSet = out.docSet.intersection(bigFilt);\n      }\n      // todo: there could be a sortDocSet that could take a list of\n      // the filters instead of anding them first...\n      // perhaps there should be a multi-docset-iterator\n      superset = sortDocSet(out.docSet,cmd.getSort(),supersetMaxDoc);\n      out.docList = superset.subset(cmd.getOffset(),cmd.getLen());\n    } else {\n      // do it the normal way...\n      cmd.setSupersetMaxDoc(supersetMaxDoc);\n      if ((flags & GET_DOCSET)!=0) {\n        // this currently conflates returning the docset for the base query vs\n        // the base query and all filters.\n        DocSet qDocSet = getDocListAndSetNC(qr,cmd);\n        // cache the docSet matching the query w/o filtering\n        if (qDocSet!=null && filterCache!=null && !qr.isPartialResults()) filterCache.put(cmd.getQuery(),qDocSet);\n      } else {\n        getDocListNC(qr,cmd);\n        //Parameters: cmd.getQuery(),theFilt,cmd.getSort(),0,supersetMaxDoc,cmd.getFlags(),cmd.getTimeAllowed(),responseHeader);\n      }\n\n      superset = out.docList;\n      out.docList = superset.subset(cmd.getOffset(),cmd.getLen());\n    }\n\n    // lastly, put the superset in the cache if the size is less than or equal\n    // to queryResultMaxDocsCached\n    if (key != null && superset.size() <= queryResultMaxDocsCached && !qr.isPartialResults()) {\n      queryResultCache.put(key, superset);\n    }\n  }\n\n","sourceOld":"  /**\n   * getDocList version that uses+populates query and filter caches.\n   * In the event of a timeout, the cache is not populated.\n   */\n  private void getDocListC(QueryResult qr, QueryCommand cmd) throws IOException {\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n    QueryResultKey key=null;\n    int maxDocRequested = cmd.getOffset() + cmd.getLen();\n    // check for overflow, and check for # docs in index\n    if (maxDocRequested < 0 || maxDocRequested > maxDoc()) maxDocRequested = maxDoc();\n    int supersetMaxDoc= maxDocRequested;\n    DocList superset = null;\n\n    int flags = cmd.getFlags();\n    Query q = cmd.getQuery();\n    if (q instanceof ExtendedQuery) {\n      ExtendedQuery eq = (ExtendedQuery)q;\n      if (!eq.getCache()) {\n        flags |= (NO_CHECK_QCACHE | NO_SET_QCACHE | NO_CHECK_FILTERCACHE);\n      }\n    }\n\n\n    // we can try and look up the complete query in the cache.\n    // we can't do that if filter!=null though (we don't want to\n    // do hashCode() and equals() for a big DocSet).\n    if (queryResultCache != null && cmd.getFilter()==null\n        && (flags & (NO_CHECK_QCACHE|NO_SET_QCACHE)) != ((NO_CHECK_QCACHE|NO_SET_QCACHE)))\n    {\n        // all of the current flags can be reused during warming,\n        // so set all of them on the cache key.\n        key = new QueryResultKey(q, cmd.getFilterList(), cmd.getSort(), flags);\n        if ((flags & NO_CHECK_QCACHE)==0) {\n          superset = queryResultCache.get(key);\n\n          if (superset != null) {\n            // check that the cache entry has scores recorded if we need them\n            if ((flags & GET_SCORES)==0 || superset.hasScores()) {\n              // NOTE: subset() returns null if the DocList has fewer docs than\n              // requested\n              out.docList = superset.subset(cmd.getOffset(),cmd.getLen());\n            }\n          }\n          if (out.docList != null) {\n            // found the docList in the cache... now check if we need the docset too.\n            // OPT: possible future optimization - if the doclist contains all the matches,\n            // use it to make the docset instead of rerunning the query.\n            if (out.docSet==null && ((flags & GET_DOCSET)!=0) ) {\n              if (cmd.getFilterList()==null) {\n                out.docSet = getDocSet(cmd.getQuery());\n              } else {\n                List<Query> newList = new ArrayList<Query>(cmd.getFilterList().size()+1);\n                newList.add(cmd.getQuery());\n                newList.addAll(cmd.getFilterList());\n                out.docSet = getDocSet(newList);\n              }\n            }\n            return;\n          }\n        }\n\n      // If we are going to generate the result, bump up to the\n      // next resultWindowSize for better caching.\n\n      if ((flags & NO_SET_QCACHE) == 0) {\n        // handle 0 special case as well as avoid idiv in the common case.\n        if (maxDocRequested < queryResultWindowSize) {\n          supersetMaxDoc=queryResultWindowSize;\n        } else {\n          supersetMaxDoc = ((maxDocRequested -1)/queryResultWindowSize + 1)*queryResultWindowSize;\n          if (supersetMaxDoc < 0) supersetMaxDoc=maxDocRequested;\n        }\n      } else {\n        key = null;  // we won't be caching the result\n      }\n    }\n\n\n    // OK, so now we need to generate an answer.\n    // One way to do that would be to check if we have an unordered list\n    // of results for the base query.  If so, we can apply the filters and then\n    // sort by the resulting set.  This can only be used if:\n    // - the sort doesn't contain score\n    // - we don't want score returned.\n\n    // check if we should try and use the filter cache\n    boolean useFilterCache=false;\n    if ((flags & (GET_SCORES|NO_CHECK_FILTERCACHE))==0 && useFilterForSortedQuery && cmd.getSort() != null && filterCache != null) {\n      useFilterCache=true;\n      SortField[] sfields = cmd.getSort().getSort();\n      for (SortField sf : sfields) {\n        if (sf.getType() == SortField.Type.SCORE) {\n          useFilterCache=false;\n          break;\n        }\n      }\n    }\n\n    // disable useFilterCache optimization temporarily\n    if (useFilterCache) {\n      // now actually use the filter cache.\n      // for large filters that match few documents, this may be\n      // slower than simply re-executing the query.\n      if (out.docSet == null) {\n        out.docSet = getDocSet(cmd.getQuery(),cmd.getFilter());\n        DocSet bigFilt = getDocSet(cmd.getFilterList());\n        if (bigFilt != null) out.docSet = out.docSet.intersection(bigFilt);\n      }\n      // todo: there could be a sortDocSet that could take a list of\n      // the filters instead of anding them first...\n      // perhaps there should be a multi-docset-iterator\n      superset = sortDocSet(out.docSet,cmd.getSort(),supersetMaxDoc);\n      out.docList = superset.subset(cmd.getOffset(),cmd.getLen());\n    } else {\n      // do it the normal way...\n      cmd.setSupersetMaxDoc(supersetMaxDoc);\n      if ((flags & GET_DOCSET)!=0) {\n        // this currently conflates returning the docset for the base query vs\n        // the base query and all filters.\n        DocSet qDocSet = getDocListAndSetNC(qr,cmd);\n        // cache the docSet matching the query w/o filtering\n        if (qDocSet!=null && filterCache!=null && !qr.isPartialResults()) filterCache.put(cmd.getQuery(),qDocSet);\n      } else {\n        getDocListNC(qr,cmd);\n        //Parameters: cmd.getQuery(),theFilt,cmd.getSort(),0,supersetMaxDoc,cmd.getFlags(),cmd.getTimeAllowed(),responseHeader);\n      }\n      if (key != null) {\n        superset = out.docList;\n        out.docList = superset.subset(cmd.getOffset(),cmd.getLen());\n      }\n    }\n\n    // lastly, put the superset in the cache if the size is less than or equal\n    // to queryResultMaxDocsCached\n    if (key != null && superset.size() <= queryResultMaxDocsCached && !qr.isPartialResults()) {\n      queryResultCache.put(key, superset);\n    }\n  }\n\n","bugFix":["f8bf47b67b38083a0c4d9d2e3f53b59a48e8db34"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d5bc8e25f59990525f5beb14afe9c96240dcf4a2","date":1389042945,"type":3,"author":"Chris M. Hostetter","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/SolrIndexSearcher#getDocListC(QueryResult,QueryCommand).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/SolrIndexSearcher#getDocListC(QueryResult,QueryCommand).mjava","sourceNew":"  /**\n   * getDocList version that uses+populates query and filter caches.\n   * In the event of a timeout, the cache is not populated.\n   */\n  private void getDocListC(QueryResult qr, QueryCommand cmd) throws IOException {\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n    QueryResultKey key=null;\n    int maxDocRequested = cmd.getOffset() + cmd.getLen();\n    // check for overflow, and check for # docs in index\n    if (maxDocRequested < 0 || maxDocRequested > maxDoc()) maxDocRequested = maxDoc();\n    int supersetMaxDoc= maxDocRequested;\n    DocList superset = null;\n\n    int flags = cmd.getFlags();\n    Query q = cmd.getQuery();\n    if (q instanceof ExtendedQuery) {\n      ExtendedQuery eq = (ExtendedQuery)q;\n      if (!eq.getCache()) {\n        flags |= (NO_CHECK_QCACHE | NO_SET_QCACHE | NO_CHECK_FILTERCACHE);\n      }\n    }\n\n\n    // we can try and look up the complete query in the cache.\n    // we can't do that if filter!=null though (we don't want to\n    // do hashCode() and equals() for a big DocSet).\n    if (queryResultCache != null && cmd.getFilter()==null\n        && (flags & (NO_CHECK_QCACHE|NO_SET_QCACHE)) != ((NO_CHECK_QCACHE|NO_SET_QCACHE)))\n    {\n        // all of the current flags can be reused during warming,\n        // so set all of them on the cache key.\n        key = new QueryResultKey(q, cmd.getFilterList(), cmd.getSort(), flags);\n        if ((flags & NO_CHECK_QCACHE)==0) {\n          superset = queryResultCache.get(key);\n\n          if (superset != null) {\n            // check that the cache entry has scores recorded if we need them\n            if ((flags & GET_SCORES)==0 || superset.hasScores()) {\n              // NOTE: subset() returns null if the DocList has fewer docs than\n              // requested\n              out.docList = superset.subset(cmd.getOffset(),cmd.getLen());\n            }\n          }\n          if (out.docList != null) {\n            // found the docList in the cache... now check if we need the docset too.\n            // OPT: possible future optimization - if the doclist contains all the matches,\n            // use it to make the docset instead of rerunning the query.\n            if (out.docSet==null && ((flags & GET_DOCSET)!=0) ) {\n              if (cmd.getFilterList()==null) {\n                out.docSet = getDocSet(cmd.getQuery());\n              } else {\n                List<Query> newList = new ArrayList<Query>(cmd.getFilterList().size()+1);\n                newList.add(cmd.getQuery());\n                newList.addAll(cmd.getFilterList());\n                out.docSet = getDocSet(newList);\n              }\n            }\n            return;\n          }\n        }\n\n      // If we are going to generate the result, bump up to the\n      // next resultWindowSize for better caching.\n\n      if ((flags & NO_SET_QCACHE) == 0) {\n        // handle 0 special case as well as avoid idiv in the common case.\n        if (maxDocRequested < queryResultWindowSize) {\n          supersetMaxDoc=queryResultWindowSize;\n        } else {\n          supersetMaxDoc = ((maxDocRequested -1)/queryResultWindowSize + 1)*queryResultWindowSize;\n          if (supersetMaxDoc < 0) supersetMaxDoc=maxDocRequested;\n        }\n      } else {\n        key = null;  // we won't be caching the result\n      }\n    }\n    cmd.setSupersetMaxDoc(supersetMaxDoc);\n\n\n    // OK, so now we need to generate an answer.\n    // One way to do that would be to check if we have an unordered list\n    // of results for the base query.  If so, we can apply the filters and then\n    // sort by the resulting set.  This can only be used if:\n    // - the sort doesn't contain score\n    // - we don't want score returned.\n\n    // check if we should try and use the filter cache\n    boolean useFilterCache=false;\n    if ((flags & (GET_SCORES|NO_CHECK_FILTERCACHE))==0 && useFilterForSortedQuery && cmd.getSort() != null && filterCache != null) {\n      useFilterCache=true;\n      SortField[] sfields = cmd.getSort().getSort();\n      for (SortField sf : sfields) {\n        if (sf.getType() == SortField.Type.SCORE) {\n          useFilterCache=false;\n          break;\n        }\n      }\n    }\n\n    if (useFilterCache) {\n      // now actually use the filter cache.\n      // for large filters that match few documents, this may be\n      // slower than simply re-executing the query.\n      if (out.docSet == null) {\n        out.docSet = getDocSet(cmd.getQuery(),cmd.getFilter());\n        DocSet bigFilt = getDocSet(cmd.getFilterList());\n        if (bigFilt != null) out.docSet = out.docSet.intersection(bigFilt);\n      }\n      // todo: there could be a sortDocSet that could take a list of\n      // the filters instead of anding them first...\n      // perhaps there should be a multi-docset-iterator\n      sortDocSet(qr, cmd);\n    } else {\n      // do it the normal way...\n      if ((flags & GET_DOCSET)!=0) {\n        // this currently conflates returning the docset for the base query vs\n        // the base query and all filters.\n        DocSet qDocSet = getDocListAndSetNC(qr,cmd);\n        // cache the docSet matching the query w/o filtering\n        if (qDocSet!=null && filterCache!=null && !qr.isPartialResults()) filterCache.put(cmd.getQuery(),qDocSet);\n      } else {\n        getDocListNC(qr,cmd);\n      }\n      assert null != out.docList : \"docList is null\";\n    }\n\n    if (null == cmd.getCursorMark()) {\n      // Kludge...\n      // we can't use DocSlice.subset, even though it should be an identity op\n      // because it gets confused by situations where there are lots of matches, but\n      // less docs in the slice then were requested, (due to the cursor)\n      // so we have to short circuit the call.\n      // None of which is really a problem since we can't use caching with\n      // cursors anyway, but it still looks weird to have to special case this\n      // behavior based on this condition - hence the long explanation.\n      superset = out.docList;\n      out.docList = superset.subset(cmd.getOffset(),cmd.getLen());\n    } else {\n      // sanity check our cursor assumptions\n      assert null == superset : \"cursor: superset isn't null\";\n      assert 0 == cmd.getOffset() : \"cursor: command offset mismatch\";\n      assert 0 == out.docList.offset() : \"cursor: docList offset mismatch\";\n      assert cmd.getLen() >= supersetMaxDoc : \"cursor: superset len mismatch: \" +\n        cmd.getLen() + \" vs \" + supersetMaxDoc;\n    }\n\n    // lastly, put the superset in the cache if the size is less than or equal\n    // to queryResultMaxDocsCached\n    if (key != null && superset.size() <= queryResultMaxDocsCached && !qr.isPartialResults()) {\n      queryResultCache.put(key, superset);\n    }\n  }\n\n","sourceOld":"  /**\n   * getDocList version that uses+populates query and filter caches.\n   * In the event of a timeout, the cache is not populated.\n   */\n  private void getDocListC(QueryResult qr, QueryCommand cmd) throws IOException {\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n    QueryResultKey key=null;\n    int maxDocRequested = cmd.getOffset() + cmd.getLen();\n    // check for overflow, and check for # docs in index\n    if (maxDocRequested < 0 || maxDocRequested > maxDoc()) maxDocRequested = maxDoc();\n    int supersetMaxDoc= maxDocRequested;\n    DocList superset = null;\n\n    int flags = cmd.getFlags();\n    Query q = cmd.getQuery();\n    if (q instanceof ExtendedQuery) {\n      ExtendedQuery eq = (ExtendedQuery)q;\n      if (!eq.getCache()) {\n        flags |= (NO_CHECK_QCACHE | NO_SET_QCACHE | NO_CHECK_FILTERCACHE);\n      }\n    }\n\n\n    // we can try and look up the complete query in the cache.\n    // we can't do that if filter!=null though (we don't want to\n    // do hashCode() and equals() for a big DocSet).\n    if (queryResultCache != null && cmd.getFilter()==null\n        && (flags & (NO_CHECK_QCACHE|NO_SET_QCACHE)) != ((NO_CHECK_QCACHE|NO_SET_QCACHE)))\n    {\n        // all of the current flags can be reused during warming,\n        // so set all of them on the cache key.\n        key = new QueryResultKey(q, cmd.getFilterList(), cmd.getSort(), flags);\n        if ((flags & NO_CHECK_QCACHE)==0) {\n          superset = queryResultCache.get(key);\n\n          if (superset != null) {\n            // check that the cache entry has scores recorded if we need them\n            if ((flags & GET_SCORES)==0 || superset.hasScores()) {\n              // NOTE: subset() returns null if the DocList has fewer docs than\n              // requested\n              out.docList = superset.subset(cmd.getOffset(),cmd.getLen());\n            }\n          }\n          if (out.docList != null) {\n            // found the docList in the cache... now check if we need the docset too.\n            // OPT: possible future optimization - if the doclist contains all the matches,\n            // use it to make the docset instead of rerunning the query.\n            if (out.docSet==null && ((flags & GET_DOCSET)!=0) ) {\n              if (cmd.getFilterList()==null) {\n                out.docSet = getDocSet(cmd.getQuery());\n              } else {\n                List<Query> newList = new ArrayList<Query>(cmd.getFilterList().size()+1);\n                newList.add(cmd.getQuery());\n                newList.addAll(cmd.getFilterList());\n                out.docSet = getDocSet(newList);\n              }\n            }\n            return;\n          }\n        }\n\n      // If we are going to generate the result, bump up to the\n      // next resultWindowSize for better caching.\n\n      if ((flags & NO_SET_QCACHE) == 0) {\n        // handle 0 special case as well as avoid idiv in the common case.\n        if (maxDocRequested < queryResultWindowSize) {\n          supersetMaxDoc=queryResultWindowSize;\n        } else {\n          supersetMaxDoc = ((maxDocRequested -1)/queryResultWindowSize + 1)*queryResultWindowSize;\n          if (supersetMaxDoc < 0) supersetMaxDoc=maxDocRequested;\n        }\n      } else {\n        key = null;  // we won't be caching the result\n      }\n    }\n\n\n    // OK, so now we need to generate an answer.\n    // One way to do that would be to check if we have an unordered list\n    // of results for the base query.  If so, we can apply the filters and then\n    // sort by the resulting set.  This can only be used if:\n    // - the sort doesn't contain score\n    // - we don't want score returned.\n\n    // check if we should try and use the filter cache\n    boolean useFilterCache=false;\n    if ((flags & (GET_SCORES|NO_CHECK_FILTERCACHE))==0 && useFilterForSortedQuery && cmd.getSort() != null && filterCache != null) {\n      useFilterCache=true;\n      SortField[] sfields = cmd.getSort().getSort();\n      for (SortField sf : sfields) {\n        if (sf.getType() == SortField.Type.SCORE) {\n          useFilterCache=false;\n          break;\n        }\n      }\n    }\n\n    // disable useFilterCache optimization temporarily\n    if (useFilterCache) {\n      // now actually use the filter cache.\n      // for large filters that match few documents, this may be\n      // slower than simply re-executing the query.\n      if (out.docSet == null) {\n        out.docSet = getDocSet(cmd.getQuery(),cmd.getFilter());\n        DocSet bigFilt = getDocSet(cmd.getFilterList());\n        if (bigFilt != null) out.docSet = out.docSet.intersection(bigFilt);\n      }\n      // todo: there could be a sortDocSet that could take a list of\n      // the filters instead of anding them first...\n      // perhaps there should be a multi-docset-iterator\n      superset = sortDocSet(out.docSet,cmd.getSort(),supersetMaxDoc);\n      out.docList = superset.subset(cmd.getOffset(),cmd.getLen());\n    } else {\n      // do it the normal way...\n      cmd.setSupersetMaxDoc(supersetMaxDoc);\n      if ((flags & GET_DOCSET)!=0) {\n        // this currently conflates returning the docset for the base query vs\n        // the base query and all filters.\n        DocSet qDocSet = getDocListAndSetNC(qr,cmd);\n        // cache the docSet matching the query w/o filtering\n        if (qDocSet!=null && filterCache!=null && !qr.isPartialResults()) filterCache.put(cmd.getQuery(),qDocSet);\n      } else {\n        getDocListNC(qr,cmd);\n        //Parameters: cmd.getQuery(),theFilt,cmd.getSort(),0,supersetMaxDoc,cmd.getFlags(),cmd.getTimeAllowed(),responseHeader);\n      }\n\n      superset = out.docList;\n      out.docList = superset.subset(cmd.getOffset(),cmd.getLen());\n    }\n\n    // lastly, put the superset in the cache if the size is less than or equal\n    // to queryResultMaxDocsCached\n    if (key != null && superset.size() <= queryResultMaxDocsCached && !qr.isPartialResults()) {\n      queryResultCache.put(key, superset);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/SolrIndexSearcher#getDocListC(QueryResult,QueryCommand).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/SolrIndexSearcher#getDocListC(QueryResult,QueryCommand).mjava","sourceNew":"  /**\n   * getDocList version that uses+populates query and filter caches.\n   * In the event of a timeout, the cache is not populated.\n   */\n  private void getDocListC(QueryResult qr, QueryCommand cmd) throws IOException {\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n    QueryResultKey key=null;\n    int maxDocRequested = cmd.getOffset() + cmd.getLen();\n    // check for overflow, and check for # docs in index\n    if (maxDocRequested < 0 || maxDocRequested > maxDoc()) maxDocRequested = maxDoc();\n    int supersetMaxDoc= maxDocRequested;\n    DocList superset = null;\n\n    int flags = cmd.getFlags();\n    Query q = cmd.getQuery();\n    if (q instanceof ExtendedQuery) {\n      ExtendedQuery eq = (ExtendedQuery)q;\n      if (!eq.getCache()) {\n        flags |= (NO_CHECK_QCACHE | NO_SET_QCACHE | NO_CHECK_FILTERCACHE);\n      }\n    }\n\n\n    // we can try and look up the complete query in the cache.\n    // we can't do that if filter!=null though (we don't want to\n    // do hashCode() and equals() for a big DocSet).\n    if (queryResultCache != null && cmd.getFilter()==null\n        && (flags & (NO_CHECK_QCACHE|NO_SET_QCACHE)) != ((NO_CHECK_QCACHE|NO_SET_QCACHE)))\n    {\n        // all of the current flags can be reused during warming,\n        // so set all of them on the cache key.\n        key = new QueryResultKey(q, cmd.getFilterList(), cmd.getSort(), flags);\n        if ((flags & NO_CHECK_QCACHE)==0) {\n          superset = queryResultCache.get(key);\n\n          if (superset != null) {\n            // check that the cache entry has scores recorded if we need them\n            if ((flags & GET_SCORES)==0 || superset.hasScores()) {\n              // NOTE: subset() returns null if the DocList has fewer docs than\n              // requested\n              out.docList = superset.subset(cmd.getOffset(),cmd.getLen());\n            }\n          }\n          if (out.docList != null) {\n            // found the docList in the cache... now check if we need the docset too.\n            // OPT: possible future optimization - if the doclist contains all the matches,\n            // use it to make the docset instead of rerunning the query.\n            if (out.docSet==null && ((flags & GET_DOCSET)!=0) ) {\n              if (cmd.getFilterList()==null) {\n                out.docSet = getDocSet(cmd.getQuery());\n              } else {\n                List<Query> newList = new ArrayList<>(cmd.getFilterList().size()+1);\n                newList.add(cmd.getQuery());\n                newList.addAll(cmd.getFilterList());\n                out.docSet = getDocSet(newList);\n              }\n            }\n            return;\n          }\n        }\n\n      // If we are going to generate the result, bump up to the\n      // next resultWindowSize for better caching.\n\n      if ((flags & NO_SET_QCACHE) == 0) {\n        // handle 0 special case as well as avoid idiv in the common case.\n        if (maxDocRequested < queryResultWindowSize) {\n          supersetMaxDoc=queryResultWindowSize;\n        } else {\n          supersetMaxDoc = ((maxDocRequested -1)/queryResultWindowSize + 1)*queryResultWindowSize;\n          if (supersetMaxDoc < 0) supersetMaxDoc=maxDocRequested;\n        }\n      } else {\n        key = null;  // we won't be caching the result\n      }\n    }\n    cmd.setSupersetMaxDoc(supersetMaxDoc);\n\n\n    // OK, so now we need to generate an answer.\n    // One way to do that would be to check if we have an unordered list\n    // of results for the base query.  If so, we can apply the filters and then\n    // sort by the resulting set.  This can only be used if:\n    // - the sort doesn't contain score\n    // - we don't want score returned.\n\n    // check if we should try and use the filter cache\n    boolean useFilterCache=false;\n    if ((flags & (GET_SCORES|NO_CHECK_FILTERCACHE))==0 && useFilterForSortedQuery && cmd.getSort() != null && filterCache != null) {\n      useFilterCache=true;\n      SortField[] sfields = cmd.getSort().getSort();\n      for (SortField sf : sfields) {\n        if (sf.getType() == SortField.Type.SCORE) {\n          useFilterCache=false;\n          break;\n        }\n      }\n    }\n\n    if (useFilterCache) {\n      // now actually use the filter cache.\n      // for large filters that match few documents, this may be\n      // slower than simply re-executing the query.\n      if (out.docSet == null) {\n        out.docSet = getDocSet(cmd.getQuery(),cmd.getFilter());\n        DocSet bigFilt = getDocSet(cmd.getFilterList());\n        if (bigFilt != null) out.docSet = out.docSet.intersection(bigFilt);\n      }\n      // todo: there could be a sortDocSet that could take a list of\n      // the filters instead of anding them first...\n      // perhaps there should be a multi-docset-iterator\n      sortDocSet(qr, cmd);\n    } else {\n      // do it the normal way...\n      if ((flags & GET_DOCSET)!=0) {\n        // this currently conflates returning the docset for the base query vs\n        // the base query and all filters.\n        DocSet qDocSet = getDocListAndSetNC(qr,cmd);\n        // cache the docSet matching the query w/o filtering\n        if (qDocSet!=null && filterCache!=null && !qr.isPartialResults()) filterCache.put(cmd.getQuery(),qDocSet);\n      } else {\n        getDocListNC(qr,cmd);\n      }\n      assert null != out.docList : \"docList is null\";\n    }\n\n    if (null == cmd.getCursorMark()) {\n      // Kludge...\n      // we can't use DocSlice.subset, even though it should be an identity op\n      // because it gets confused by situations where there are lots of matches, but\n      // less docs in the slice then were requested, (due to the cursor)\n      // so we have to short circuit the call.\n      // None of which is really a problem since we can't use caching with\n      // cursors anyway, but it still looks weird to have to special case this\n      // behavior based on this condition - hence the long explanation.\n      superset = out.docList;\n      out.docList = superset.subset(cmd.getOffset(),cmd.getLen());\n    } else {\n      // sanity check our cursor assumptions\n      assert null == superset : \"cursor: superset isn't null\";\n      assert 0 == cmd.getOffset() : \"cursor: command offset mismatch\";\n      assert 0 == out.docList.offset() : \"cursor: docList offset mismatch\";\n      assert cmd.getLen() >= supersetMaxDoc : \"cursor: superset len mismatch: \" +\n        cmd.getLen() + \" vs \" + supersetMaxDoc;\n    }\n\n    // lastly, put the superset in the cache if the size is less than or equal\n    // to queryResultMaxDocsCached\n    if (key != null && superset.size() <= queryResultMaxDocsCached && !qr.isPartialResults()) {\n      queryResultCache.put(key, superset);\n    }\n  }\n\n","sourceOld":"  /**\n   * getDocList version that uses+populates query and filter caches.\n   * In the event of a timeout, the cache is not populated.\n   */\n  private void getDocListC(QueryResult qr, QueryCommand cmd) throws IOException {\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n    QueryResultKey key=null;\n    int maxDocRequested = cmd.getOffset() + cmd.getLen();\n    // check for overflow, and check for # docs in index\n    if (maxDocRequested < 0 || maxDocRequested > maxDoc()) maxDocRequested = maxDoc();\n    int supersetMaxDoc= maxDocRequested;\n    DocList superset = null;\n\n    int flags = cmd.getFlags();\n    Query q = cmd.getQuery();\n    if (q instanceof ExtendedQuery) {\n      ExtendedQuery eq = (ExtendedQuery)q;\n      if (!eq.getCache()) {\n        flags |= (NO_CHECK_QCACHE | NO_SET_QCACHE | NO_CHECK_FILTERCACHE);\n      }\n    }\n\n\n    // we can try and look up the complete query in the cache.\n    // we can't do that if filter!=null though (we don't want to\n    // do hashCode() and equals() for a big DocSet).\n    if (queryResultCache != null && cmd.getFilter()==null\n        && (flags & (NO_CHECK_QCACHE|NO_SET_QCACHE)) != ((NO_CHECK_QCACHE|NO_SET_QCACHE)))\n    {\n        // all of the current flags can be reused during warming,\n        // so set all of them on the cache key.\n        key = new QueryResultKey(q, cmd.getFilterList(), cmd.getSort(), flags);\n        if ((flags & NO_CHECK_QCACHE)==0) {\n          superset = queryResultCache.get(key);\n\n          if (superset != null) {\n            // check that the cache entry has scores recorded if we need them\n            if ((flags & GET_SCORES)==0 || superset.hasScores()) {\n              // NOTE: subset() returns null if the DocList has fewer docs than\n              // requested\n              out.docList = superset.subset(cmd.getOffset(),cmd.getLen());\n            }\n          }\n          if (out.docList != null) {\n            // found the docList in the cache... now check if we need the docset too.\n            // OPT: possible future optimization - if the doclist contains all the matches,\n            // use it to make the docset instead of rerunning the query.\n            if (out.docSet==null && ((flags & GET_DOCSET)!=0) ) {\n              if (cmd.getFilterList()==null) {\n                out.docSet = getDocSet(cmd.getQuery());\n              } else {\n                List<Query> newList = new ArrayList<Query>(cmd.getFilterList().size()+1);\n                newList.add(cmd.getQuery());\n                newList.addAll(cmd.getFilterList());\n                out.docSet = getDocSet(newList);\n              }\n            }\n            return;\n          }\n        }\n\n      // If we are going to generate the result, bump up to the\n      // next resultWindowSize for better caching.\n\n      if ((flags & NO_SET_QCACHE) == 0) {\n        // handle 0 special case as well as avoid idiv in the common case.\n        if (maxDocRequested < queryResultWindowSize) {\n          supersetMaxDoc=queryResultWindowSize;\n        } else {\n          supersetMaxDoc = ((maxDocRequested -1)/queryResultWindowSize + 1)*queryResultWindowSize;\n          if (supersetMaxDoc < 0) supersetMaxDoc=maxDocRequested;\n        }\n      } else {\n        key = null;  // we won't be caching the result\n      }\n    }\n    cmd.setSupersetMaxDoc(supersetMaxDoc);\n\n\n    // OK, so now we need to generate an answer.\n    // One way to do that would be to check if we have an unordered list\n    // of results for the base query.  If so, we can apply the filters and then\n    // sort by the resulting set.  This can only be used if:\n    // - the sort doesn't contain score\n    // - we don't want score returned.\n\n    // check if we should try and use the filter cache\n    boolean useFilterCache=false;\n    if ((flags & (GET_SCORES|NO_CHECK_FILTERCACHE))==0 && useFilterForSortedQuery && cmd.getSort() != null && filterCache != null) {\n      useFilterCache=true;\n      SortField[] sfields = cmd.getSort().getSort();\n      for (SortField sf : sfields) {\n        if (sf.getType() == SortField.Type.SCORE) {\n          useFilterCache=false;\n          break;\n        }\n      }\n    }\n\n    if (useFilterCache) {\n      // now actually use the filter cache.\n      // for large filters that match few documents, this may be\n      // slower than simply re-executing the query.\n      if (out.docSet == null) {\n        out.docSet = getDocSet(cmd.getQuery(),cmd.getFilter());\n        DocSet bigFilt = getDocSet(cmd.getFilterList());\n        if (bigFilt != null) out.docSet = out.docSet.intersection(bigFilt);\n      }\n      // todo: there could be a sortDocSet that could take a list of\n      // the filters instead of anding them first...\n      // perhaps there should be a multi-docset-iterator\n      sortDocSet(qr, cmd);\n    } else {\n      // do it the normal way...\n      if ((flags & GET_DOCSET)!=0) {\n        // this currently conflates returning the docset for the base query vs\n        // the base query and all filters.\n        DocSet qDocSet = getDocListAndSetNC(qr,cmd);\n        // cache the docSet matching the query w/o filtering\n        if (qDocSet!=null && filterCache!=null && !qr.isPartialResults()) filterCache.put(cmd.getQuery(),qDocSet);\n      } else {\n        getDocListNC(qr,cmd);\n      }\n      assert null != out.docList : \"docList is null\";\n    }\n\n    if (null == cmd.getCursorMark()) {\n      // Kludge...\n      // we can't use DocSlice.subset, even though it should be an identity op\n      // because it gets confused by situations where there are lots of matches, but\n      // less docs in the slice then were requested, (due to the cursor)\n      // so we have to short circuit the call.\n      // None of which is really a problem since we can't use caching with\n      // cursors anyway, but it still looks weird to have to special case this\n      // behavior based on this condition - hence the long explanation.\n      superset = out.docList;\n      out.docList = superset.subset(cmd.getOffset(),cmd.getLen());\n    } else {\n      // sanity check our cursor assumptions\n      assert null == superset : \"cursor: superset isn't null\";\n      assert 0 == cmd.getOffset() : \"cursor: command offset mismatch\";\n      assert 0 == out.docList.offset() : \"cursor: docList offset mismatch\";\n      assert cmd.getLen() >= supersetMaxDoc : \"cursor: superset len mismatch: \" +\n        cmd.getLen() + \" vs \" + supersetMaxDoc;\n    }\n\n    // lastly, put the superset in the cache if the size is less than or equal\n    // to queryResultMaxDocsCached\n    if (key != null && superset.size() <= queryResultMaxDocsCached && !qr.isPartialResults()) {\n      queryResultCache.put(key, superset);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ac9de183adbc9483681f275ac1e2d92ed19f52e1","date":1452414626,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/SolrIndexSearcher#getDocListC(QueryResult,QueryCommand).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/SolrIndexSearcher#getDocListC(QueryResult,QueryCommand).mjava","sourceNew":"  /**\n   * getDocList version that uses+populates query and filter caches. In the event of a timeout, the cache is not\n   * populated.\n   */\n  private void getDocListC(QueryResult qr, QueryCommand cmd) throws IOException {\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n    QueryResultKey key = null;\n    int maxDocRequested = cmd.getOffset() + cmd.getLen();\n    // check for overflow, and check for # docs in index\n    if (maxDocRequested < 0 || maxDocRequested > maxDoc()) maxDocRequested = maxDoc();\n    int supersetMaxDoc = maxDocRequested;\n    DocList superset = null;\n\n    int flags = cmd.getFlags();\n    Query q = cmd.getQuery();\n    if (q instanceof ExtendedQuery) {\n      ExtendedQuery eq = (ExtendedQuery) q;\n      if (!eq.getCache()) {\n        flags |= (NO_CHECK_QCACHE | NO_SET_QCACHE | NO_CHECK_FILTERCACHE);\n      }\n    }\n\n    // we can try and look up the complete query in the cache.\n    // we can't do that if filter!=null though (we don't want to\n    // do hashCode() and equals() for a big DocSet).\n    if (queryResultCache != null && cmd.getFilter() == null\n        && (flags & (NO_CHECK_QCACHE | NO_SET_QCACHE)) != ((NO_CHECK_QCACHE | NO_SET_QCACHE))) {\n      // all of the current flags can be reused during warming,\n      // so set all of them on the cache key.\n      key = new QueryResultKey(q, cmd.getFilterList(), cmd.getSort(), flags);\n      if ((flags & NO_CHECK_QCACHE) == 0) {\n        superset = queryResultCache.get(key);\n\n        if (superset != null) {\n          // check that the cache entry has scores recorded if we need them\n          if ((flags & GET_SCORES) == 0 || superset.hasScores()) {\n            // NOTE: subset() returns null if the DocList has fewer docs than\n            // requested\n            out.docList = superset.subset(cmd.getOffset(), cmd.getLen());\n          }\n        }\n        if (out.docList != null) {\n          // found the docList in the cache... now check if we need the docset too.\n          // OPT: possible future optimization - if the doclist contains all the matches,\n          // use it to make the docset instead of rerunning the query.\n          if (out.docSet == null && ((flags & GET_DOCSET) != 0)) {\n            if (cmd.getFilterList() == null) {\n              out.docSet = getDocSet(cmd.getQuery());\n            } else {\n              List<Query> newList = new ArrayList<>(cmd.getFilterList().size() + 1);\n              newList.add(cmd.getQuery());\n              newList.addAll(cmd.getFilterList());\n              out.docSet = getDocSet(newList);\n            }\n          }\n          return;\n        }\n      }\n\n      // If we are going to generate the result, bump up to the\n      // next resultWindowSize for better caching.\n\n      if ((flags & NO_SET_QCACHE) == 0) {\n        // handle 0 special case as well as avoid idiv in the common case.\n        if (maxDocRequested < queryResultWindowSize) {\n          supersetMaxDoc = queryResultWindowSize;\n        } else {\n          supersetMaxDoc = ((maxDocRequested - 1) / queryResultWindowSize + 1) * queryResultWindowSize;\n          if (supersetMaxDoc < 0) supersetMaxDoc = maxDocRequested;\n        }\n      } else {\n        key = null; // we won't be caching the result\n      }\n    }\n    cmd.setSupersetMaxDoc(supersetMaxDoc);\n\n    // OK, so now we need to generate an answer.\n    // One way to do that would be to check if we have an unordered list\n    // of results for the base query. If so, we can apply the filters and then\n    // sort by the resulting set. This can only be used if:\n    // - the sort doesn't contain score\n    // - we don't want score returned.\n\n    // check if we should try and use the filter cache\n    boolean useFilterCache = false;\n    if ((flags & (GET_SCORES | NO_CHECK_FILTERCACHE)) == 0 && useFilterForSortedQuery && cmd.getSort() != null\n        && filterCache != null) {\n      useFilterCache = true;\n      SortField[] sfields = cmd.getSort().getSort();\n      for (SortField sf : sfields) {\n        if (sf.getType() == SortField.Type.SCORE) {\n          useFilterCache = false;\n          break;\n        }\n      }\n    }\n\n    if (useFilterCache) {\n      // now actually use the filter cache.\n      // for large filters that match few documents, this may be\n      // slower than simply re-executing the query.\n      if (out.docSet == null) {\n        out.docSet = getDocSet(cmd.getQuery(), cmd.getFilter());\n        DocSet bigFilt = getDocSet(cmd.getFilterList());\n        if (bigFilt != null) out.docSet = out.docSet.intersection(bigFilt);\n      }\n      // todo: there could be a sortDocSet that could take a list of\n      // the filters instead of anding them first...\n      // perhaps there should be a multi-docset-iterator\n      sortDocSet(qr, cmd);\n    } else {\n      // do it the normal way...\n      if ((flags & GET_DOCSET) != 0) {\n        // this currently conflates returning the docset for the base query vs\n        // the base query and all filters.\n        DocSet qDocSet = getDocListAndSetNC(qr, cmd);\n        // cache the docSet matching the query w/o filtering\n        if (qDocSet != null && filterCache != null && !qr.isPartialResults()) filterCache.put(cmd.getQuery(), qDocSet);\n      } else {\n        getDocListNC(qr, cmd);\n      }\n      assert null != out.docList : \"docList is null\";\n    }\n\n    if (null == cmd.getCursorMark()) {\n      // Kludge...\n      // we can't use DocSlice.subset, even though it should be an identity op\n      // because it gets confused by situations where there are lots of matches, but\n      // less docs in the slice then were requested, (due to the cursor)\n      // so we have to short circuit the call.\n      // None of which is really a problem since we can't use caching with\n      // cursors anyway, but it still looks weird to have to special case this\n      // behavior based on this condition - hence the long explanation.\n      superset = out.docList;\n      out.docList = superset.subset(cmd.getOffset(), cmd.getLen());\n    } else {\n      // sanity check our cursor assumptions\n      assert null == superset : \"cursor: superset isn't null\";\n      assert 0 == cmd.getOffset() : \"cursor: command offset mismatch\";\n      assert 0 == out.docList.offset() : \"cursor: docList offset mismatch\";\n      assert cmd.getLen() >= supersetMaxDoc : \"cursor: superset len mismatch: \" + cmd.getLen() + \" vs \"\n          + supersetMaxDoc;\n    }\n\n    // lastly, put the superset in the cache if the size is less than or equal\n    // to queryResultMaxDocsCached\n    if (key != null && superset.size() <= queryResultMaxDocsCached && !qr.isPartialResults()) {\n      queryResultCache.put(key, superset);\n    }\n  }\n\n","sourceOld":"  /**\n   * getDocList version that uses+populates query and filter caches.\n   * In the event of a timeout, the cache is not populated.\n   */\n  private void getDocListC(QueryResult qr, QueryCommand cmd) throws IOException {\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n    QueryResultKey key=null;\n    int maxDocRequested = cmd.getOffset() + cmd.getLen();\n    // check for overflow, and check for # docs in index\n    if (maxDocRequested < 0 || maxDocRequested > maxDoc()) maxDocRequested = maxDoc();\n    int supersetMaxDoc= maxDocRequested;\n    DocList superset = null;\n\n    int flags = cmd.getFlags();\n    Query q = cmd.getQuery();\n    if (q instanceof ExtendedQuery) {\n      ExtendedQuery eq = (ExtendedQuery)q;\n      if (!eq.getCache()) {\n        flags |= (NO_CHECK_QCACHE | NO_SET_QCACHE | NO_CHECK_FILTERCACHE);\n      }\n    }\n\n\n    // we can try and look up the complete query in the cache.\n    // we can't do that if filter!=null though (we don't want to\n    // do hashCode() and equals() for a big DocSet).\n    if (queryResultCache != null && cmd.getFilter()==null\n        && (flags & (NO_CHECK_QCACHE|NO_SET_QCACHE)) != ((NO_CHECK_QCACHE|NO_SET_QCACHE)))\n    {\n        // all of the current flags can be reused during warming,\n        // so set all of them on the cache key.\n        key = new QueryResultKey(q, cmd.getFilterList(), cmd.getSort(), flags);\n        if ((flags & NO_CHECK_QCACHE)==0) {\n          superset = queryResultCache.get(key);\n\n          if (superset != null) {\n            // check that the cache entry has scores recorded if we need them\n            if ((flags & GET_SCORES)==0 || superset.hasScores()) {\n              // NOTE: subset() returns null if the DocList has fewer docs than\n              // requested\n              out.docList = superset.subset(cmd.getOffset(),cmd.getLen());\n            }\n          }\n          if (out.docList != null) {\n            // found the docList in the cache... now check if we need the docset too.\n            // OPT: possible future optimization - if the doclist contains all the matches,\n            // use it to make the docset instead of rerunning the query.\n            if (out.docSet==null && ((flags & GET_DOCSET)!=0) ) {\n              if (cmd.getFilterList()==null) {\n                out.docSet = getDocSet(cmd.getQuery());\n              } else {\n                List<Query> newList = new ArrayList<>(cmd.getFilterList().size()+1);\n                newList.add(cmd.getQuery());\n                newList.addAll(cmd.getFilterList());\n                out.docSet = getDocSet(newList);\n              }\n            }\n            return;\n          }\n        }\n\n      // If we are going to generate the result, bump up to the\n      // next resultWindowSize for better caching.\n\n      if ((flags & NO_SET_QCACHE) == 0) {\n        // handle 0 special case as well as avoid idiv in the common case.\n        if (maxDocRequested < queryResultWindowSize) {\n          supersetMaxDoc=queryResultWindowSize;\n        } else {\n          supersetMaxDoc = ((maxDocRequested -1)/queryResultWindowSize + 1)*queryResultWindowSize;\n          if (supersetMaxDoc < 0) supersetMaxDoc=maxDocRequested;\n        }\n      } else {\n        key = null;  // we won't be caching the result\n      }\n    }\n    cmd.setSupersetMaxDoc(supersetMaxDoc);\n\n\n    // OK, so now we need to generate an answer.\n    // One way to do that would be to check if we have an unordered list\n    // of results for the base query.  If so, we can apply the filters and then\n    // sort by the resulting set.  This can only be used if:\n    // - the sort doesn't contain score\n    // - we don't want score returned.\n\n    // check if we should try and use the filter cache\n    boolean useFilterCache=false;\n    if ((flags & (GET_SCORES|NO_CHECK_FILTERCACHE))==0 && useFilterForSortedQuery && cmd.getSort() != null && filterCache != null) {\n      useFilterCache=true;\n      SortField[] sfields = cmd.getSort().getSort();\n      for (SortField sf : sfields) {\n        if (sf.getType() == SortField.Type.SCORE) {\n          useFilterCache=false;\n          break;\n        }\n      }\n    }\n\n    if (useFilterCache) {\n      // now actually use the filter cache.\n      // for large filters that match few documents, this may be\n      // slower than simply re-executing the query.\n      if (out.docSet == null) {\n        out.docSet = getDocSet(cmd.getQuery(),cmd.getFilter());\n        DocSet bigFilt = getDocSet(cmd.getFilterList());\n        if (bigFilt != null) out.docSet = out.docSet.intersection(bigFilt);\n      }\n      // todo: there could be a sortDocSet that could take a list of\n      // the filters instead of anding them first...\n      // perhaps there should be a multi-docset-iterator\n      sortDocSet(qr, cmd);\n    } else {\n      // do it the normal way...\n      if ((flags & GET_DOCSET)!=0) {\n        // this currently conflates returning the docset for the base query vs\n        // the base query and all filters.\n        DocSet qDocSet = getDocListAndSetNC(qr,cmd);\n        // cache the docSet matching the query w/o filtering\n        if (qDocSet!=null && filterCache!=null && !qr.isPartialResults()) filterCache.put(cmd.getQuery(),qDocSet);\n      } else {\n        getDocListNC(qr,cmd);\n      }\n      assert null != out.docList : \"docList is null\";\n    }\n\n    if (null == cmd.getCursorMark()) {\n      // Kludge...\n      // we can't use DocSlice.subset, even though it should be an identity op\n      // because it gets confused by situations where there are lots of matches, but\n      // less docs in the slice then were requested, (due to the cursor)\n      // so we have to short circuit the call.\n      // None of which is really a problem since we can't use caching with\n      // cursors anyway, but it still looks weird to have to special case this\n      // behavior based on this condition - hence the long explanation.\n      superset = out.docList;\n      out.docList = superset.subset(cmd.getOffset(),cmd.getLen());\n    } else {\n      // sanity check our cursor assumptions\n      assert null == superset : \"cursor: superset isn't null\";\n      assert 0 == cmd.getOffset() : \"cursor: command offset mismatch\";\n      assert 0 == out.docList.offset() : \"cursor: docList offset mismatch\";\n      assert cmd.getLen() >= supersetMaxDoc : \"cursor: superset len mismatch: \" +\n        cmd.getLen() + \" vs \" + supersetMaxDoc;\n    }\n\n    // lastly, put the superset in the cache if the size is less than or equal\n    // to queryResultMaxDocsCached\n    if (key != null && superset.size() <= queryResultMaxDocsCached && !qr.isPartialResults()) {\n      queryResultCache.put(key, superset);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0d6197aa6dfead2636cc42b887d9ceef82f4491b","date":1519839500,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/SolrIndexSearcher#getDocListC(QueryResult,QueryCommand).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/SolrIndexSearcher#getDocListC(QueryResult,QueryCommand).mjava","sourceNew":"  /**\n   * getDocList version that uses+populates query and filter caches. In the event of a timeout, the cache is not\n   * populated.\n   */\n  private void getDocListC(QueryResult qr, QueryCommand cmd) throws IOException {\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n    QueryResultKey key = null;\n    int maxDocRequested = cmd.getOffset() + cmd.getLen();\n    // check for overflow, and check for # docs in index\n    if (maxDocRequested < 0 || maxDocRequested > maxDoc()) maxDocRequested = maxDoc();\n    int supersetMaxDoc = maxDocRequested;\n    DocList superset = null;\n\n    int flags = cmd.getFlags();\n    Query q = cmd.getQuery();\n    if (q instanceof ExtendedQuery) {\n      ExtendedQuery eq = (ExtendedQuery) q;\n      if (!eq.getCache()) {\n        flags |= (NO_CHECK_QCACHE | NO_SET_QCACHE | NO_CHECK_FILTERCACHE);\n      }\n    }\n\n    // we can try and look up the complete query in the cache.\n    // we can't do that if filter!=null though (we don't want to\n    // do hashCode() and equals() for a big DocSet).\n    if (queryResultCache != null && cmd.getFilter() == null\n        && (flags & (NO_CHECK_QCACHE | NO_SET_QCACHE)) != ((NO_CHECK_QCACHE | NO_SET_QCACHE))) {\n      // all of the current flags can be reused during warming,\n      // so set all of them on the cache key.\n      key = new QueryResultKey(q, cmd.getFilterList(), cmd.getSort(), flags);\n      if ((flags & NO_CHECK_QCACHE) == 0) {\n        superset = queryResultCache.get(key);\n\n        if (superset != null) {\n          // check that the cache entry has scores recorded if we need them\n          if ((flags & GET_SCORES) == 0 || superset.hasScores()) {\n            // NOTE: subset() returns null if the DocList has fewer docs than\n            // requested\n            out.docList = superset.subset(cmd.getOffset(), cmd.getLen());\n          }\n        }\n        if (out.docList != null) {\n          // found the docList in the cache... now check if we need the docset too.\n          // OPT: possible future optimization - if the doclist contains all the matches,\n          // use it to make the docset instead of rerunning the query.\n          if (out.docSet == null && ((flags & GET_DOCSET) != 0)) {\n            if (cmd.getFilterList() == null) {\n              out.docSet = getDocSet(cmd.getQuery());\n            } else {\n              List<Query> newList = new ArrayList<>(cmd.getFilterList().size() + 1);\n              newList.add(cmd.getQuery());\n              newList.addAll(cmd.getFilterList());\n              out.docSet = getDocSet(newList);\n            }\n          }\n          return;\n        }\n      }\n\n      // If we are going to generate the result, bump up to the\n      // next resultWindowSize for better caching.\n\n      if ((flags & NO_SET_QCACHE) == 0) {\n        // handle 0 special case as well as avoid idiv in the common case.\n        if (maxDocRequested < queryResultWindowSize) {\n          supersetMaxDoc = queryResultWindowSize;\n        } else {\n          supersetMaxDoc = ((maxDocRequested - 1) / queryResultWindowSize + 1) * queryResultWindowSize;\n          if (supersetMaxDoc < 0) supersetMaxDoc = maxDocRequested;\n        }\n      } else {\n        key = null; // we won't be caching the result\n      }\n    }\n    cmd.setSupersetMaxDoc(supersetMaxDoc);\n\n    // OK, so now we need to generate an answer.\n    // One way to do that would be to check if we have an unordered list\n    // of results for the base query. If so, we can apply the filters and then\n    // sort by the resulting set. This can only be used if:\n    // - the sort doesn't contain score\n    // - we don't want score returned.\n\n    // check if we should try and use the filter cache\n    boolean useFilterCache = false;\n    if ((flags & (GET_SCORES | NO_CHECK_FILTERCACHE)) == 0 && useFilterForSortedQuery && cmd.getSort() != null\n        && filterCache != null) {\n      useFilterCache = true;\n      SortField[] sfields = cmd.getSort().getSort();\n      for (SortField sf : sfields) {\n        if (sf.getType() == SortField.Type.SCORE) {\n          useFilterCache = false;\n          break;\n        }\n      }\n    }\n\n    if (useFilterCache) {\n      // now actually use the filter cache.\n      // for large filters that match few documents, this may be\n      // slower than simply re-executing the query.\n      if (out.docSet == null) {\n        out.docSet = getDocSet(cmd.getQuery(), cmd.getFilter());\n        List<Query> filterList = cmd.getFilterList();\n        if (filterList != null && !filterList.isEmpty()) {\n          out.docSet = out.docSet.intersection(getDocSet(cmd.getFilterList()));\n        }\n      }\n      // todo: there could be a sortDocSet that could take a list of\n      // the filters instead of anding them first...\n      // perhaps there should be a multi-docset-iterator\n      sortDocSet(qr, cmd);\n    } else {\n      // do it the normal way...\n      if ((flags & GET_DOCSET) != 0) {\n        // this currently conflates returning the docset for the base query vs\n        // the base query and all filters.\n        DocSet qDocSet = getDocListAndSetNC(qr, cmd);\n        // cache the docSet matching the query w/o filtering\n        if (qDocSet != null && filterCache != null && !qr.isPartialResults()) filterCache.put(cmd.getQuery(), qDocSet);\n      } else {\n        getDocListNC(qr, cmd);\n      }\n      assert null != out.docList : \"docList is null\";\n    }\n\n    if (null == cmd.getCursorMark()) {\n      // Kludge...\n      // we can't use DocSlice.subset, even though it should be an identity op\n      // because it gets confused by situations where there are lots of matches, but\n      // less docs in the slice then were requested, (due to the cursor)\n      // so we have to short circuit the call.\n      // None of which is really a problem since we can't use caching with\n      // cursors anyway, but it still looks weird to have to special case this\n      // behavior based on this condition - hence the long explanation.\n      superset = out.docList;\n      out.docList = superset.subset(cmd.getOffset(), cmd.getLen());\n    } else {\n      // sanity check our cursor assumptions\n      assert null == superset : \"cursor: superset isn't null\";\n      assert 0 == cmd.getOffset() : \"cursor: command offset mismatch\";\n      assert 0 == out.docList.offset() : \"cursor: docList offset mismatch\";\n      assert cmd.getLen() >= supersetMaxDoc : \"cursor: superset len mismatch: \" + cmd.getLen() + \" vs \"\n          + supersetMaxDoc;\n    }\n\n    // lastly, put the superset in the cache if the size is less than or equal\n    // to queryResultMaxDocsCached\n    if (key != null && superset.size() <= queryResultMaxDocsCached && !qr.isPartialResults()) {\n      queryResultCache.put(key, superset);\n    }\n  }\n\n","sourceOld":"  /**\n   * getDocList version that uses+populates query and filter caches. In the event of a timeout, the cache is not\n   * populated.\n   */\n  private void getDocListC(QueryResult qr, QueryCommand cmd) throws IOException {\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n    QueryResultKey key = null;\n    int maxDocRequested = cmd.getOffset() + cmd.getLen();\n    // check for overflow, and check for # docs in index\n    if (maxDocRequested < 0 || maxDocRequested > maxDoc()) maxDocRequested = maxDoc();\n    int supersetMaxDoc = maxDocRequested;\n    DocList superset = null;\n\n    int flags = cmd.getFlags();\n    Query q = cmd.getQuery();\n    if (q instanceof ExtendedQuery) {\n      ExtendedQuery eq = (ExtendedQuery) q;\n      if (!eq.getCache()) {\n        flags |= (NO_CHECK_QCACHE | NO_SET_QCACHE | NO_CHECK_FILTERCACHE);\n      }\n    }\n\n    // we can try and look up the complete query in the cache.\n    // we can't do that if filter!=null though (we don't want to\n    // do hashCode() and equals() for a big DocSet).\n    if (queryResultCache != null && cmd.getFilter() == null\n        && (flags & (NO_CHECK_QCACHE | NO_SET_QCACHE)) != ((NO_CHECK_QCACHE | NO_SET_QCACHE))) {\n      // all of the current flags can be reused during warming,\n      // so set all of them on the cache key.\n      key = new QueryResultKey(q, cmd.getFilterList(), cmd.getSort(), flags);\n      if ((flags & NO_CHECK_QCACHE) == 0) {\n        superset = queryResultCache.get(key);\n\n        if (superset != null) {\n          // check that the cache entry has scores recorded if we need them\n          if ((flags & GET_SCORES) == 0 || superset.hasScores()) {\n            // NOTE: subset() returns null if the DocList has fewer docs than\n            // requested\n            out.docList = superset.subset(cmd.getOffset(), cmd.getLen());\n          }\n        }\n        if (out.docList != null) {\n          // found the docList in the cache... now check if we need the docset too.\n          // OPT: possible future optimization - if the doclist contains all the matches,\n          // use it to make the docset instead of rerunning the query.\n          if (out.docSet == null && ((flags & GET_DOCSET) != 0)) {\n            if (cmd.getFilterList() == null) {\n              out.docSet = getDocSet(cmd.getQuery());\n            } else {\n              List<Query> newList = new ArrayList<>(cmd.getFilterList().size() + 1);\n              newList.add(cmd.getQuery());\n              newList.addAll(cmd.getFilterList());\n              out.docSet = getDocSet(newList);\n            }\n          }\n          return;\n        }\n      }\n\n      // If we are going to generate the result, bump up to the\n      // next resultWindowSize for better caching.\n\n      if ((flags & NO_SET_QCACHE) == 0) {\n        // handle 0 special case as well as avoid idiv in the common case.\n        if (maxDocRequested < queryResultWindowSize) {\n          supersetMaxDoc = queryResultWindowSize;\n        } else {\n          supersetMaxDoc = ((maxDocRequested - 1) / queryResultWindowSize + 1) * queryResultWindowSize;\n          if (supersetMaxDoc < 0) supersetMaxDoc = maxDocRequested;\n        }\n      } else {\n        key = null; // we won't be caching the result\n      }\n    }\n    cmd.setSupersetMaxDoc(supersetMaxDoc);\n\n    // OK, so now we need to generate an answer.\n    // One way to do that would be to check if we have an unordered list\n    // of results for the base query. If so, we can apply the filters and then\n    // sort by the resulting set. This can only be used if:\n    // - the sort doesn't contain score\n    // - we don't want score returned.\n\n    // check if we should try and use the filter cache\n    boolean useFilterCache = false;\n    if ((flags & (GET_SCORES | NO_CHECK_FILTERCACHE)) == 0 && useFilterForSortedQuery && cmd.getSort() != null\n        && filterCache != null) {\n      useFilterCache = true;\n      SortField[] sfields = cmd.getSort().getSort();\n      for (SortField sf : sfields) {\n        if (sf.getType() == SortField.Type.SCORE) {\n          useFilterCache = false;\n          break;\n        }\n      }\n    }\n\n    if (useFilterCache) {\n      // now actually use the filter cache.\n      // for large filters that match few documents, this may be\n      // slower than simply re-executing the query.\n      if (out.docSet == null) {\n        out.docSet = getDocSet(cmd.getQuery(), cmd.getFilter());\n        DocSet bigFilt = getDocSet(cmd.getFilterList());\n        if (bigFilt != null) out.docSet = out.docSet.intersection(bigFilt);\n      }\n      // todo: there could be a sortDocSet that could take a list of\n      // the filters instead of anding them first...\n      // perhaps there should be a multi-docset-iterator\n      sortDocSet(qr, cmd);\n    } else {\n      // do it the normal way...\n      if ((flags & GET_DOCSET) != 0) {\n        // this currently conflates returning the docset for the base query vs\n        // the base query and all filters.\n        DocSet qDocSet = getDocListAndSetNC(qr, cmd);\n        // cache the docSet matching the query w/o filtering\n        if (qDocSet != null && filterCache != null && !qr.isPartialResults()) filterCache.put(cmd.getQuery(), qDocSet);\n      } else {\n        getDocListNC(qr, cmd);\n      }\n      assert null != out.docList : \"docList is null\";\n    }\n\n    if (null == cmd.getCursorMark()) {\n      // Kludge...\n      // we can't use DocSlice.subset, even though it should be an identity op\n      // because it gets confused by situations where there are lots of matches, but\n      // less docs in the slice then were requested, (due to the cursor)\n      // so we have to short circuit the call.\n      // None of which is really a problem since we can't use caching with\n      // cursors anyway, but it still looks weird to have to special case this\n      // behavior based on this condition - hence the long explanation.\n      superset = out.docList;\n      out.docList = superset.subset(cmd.getOffset(), cmd.getLen());\n    } else {\n      // sanity check our cursor assumptions\n      assert null == superset : \"cursor: superset isn't null\";\n      assert 0 == cmd.getOffset() : \"cursor: command offset mismatch\";\n      assert 0 == out.docList.offset() : \"cursor: docList offset mismatch\";\n      assert cmd.getLen() >= supersetMaxDoc : \"cursor: superset len mismatch: \" + cmd.getLen() + \" vs \"\n          + supersetMaxDoc;\n    }\n\n    // lastly, put the superset in the cache if the size is less than or equal\n    // to queryResultMaxDocsCached\n    if (key != null && superset.size() <= queryResultMaxDocsCached && !qr.isPartialResults()) {\n      queryResultCache.put(key, superset);\n    }\n  }\n\n","bugFix":["db25c1f61b5ae826f10777da6551a832703967d5"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"640ded7811e1b7d29236a5e2934ec3cd266a8199","date":1588973147,"type":3,"author":"Tomas Fernandez Lobbe","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/SolrIndexSearcher#getDocListC(QueryResult,QueryCommand).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/SolrIndexSearcher#getDocListC(QueryResult,QueryCommand).mjava","sourceNew":"  /**\n   * getDocList version that uses+populates query and filter caches. In the event of a timeout, the cache is not\n   * populated.\n   */\n  private void getDocListC(QueryResult qr, QueryCommand cmd) throws IOException {\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n    QueryResultKey key = null;\n    int maxDocRequested = cmd.getOffset() + cmd.getLen();\n    // check for overflow, and check for # docs in index\n    if (maxDocRequested < 0 || maxDocRequested > maxDoc()) maxDocRequested = maxDoc();\n    int supersetMaxDoc = maxDocRequested;\n    DocList superset = null;\n\n    int flags = cmd.getFlags();\n    Query q = cmd.getQuery();\n    if (q instanceof ExtendedQuery) {\n      ExtendedQuery eq = (ExtendedQuery) q;\n      if (!eq.getCache()) {\n        flags |= (NO_CHECK_QCACHE | NO_SET_QCACHE | NO_CHECK_FILTERCACHE);\n      }\n    }\n\n    // we can try and look up the complete query in the cache.\n    // we can't do that if filter!=null though (we don't want to\n    // do hashCode() and equals() for a big DocSet).\n    if (queryResultCache != null && cmd.getFilter() == null\n        && (flags & (NO_CHECK_QCACHE | NO_SET_QCACHE)) != ((NO_CHECK_QCACHE | NO_SET_QCACHE))) {\n      // all of the current flags can be reused during warming,\n      // so set all of them on the cache key.\n      key = new QueryResultKey(q, cmd.getFilterList(), cmd.getSort(), flags, cmd.getMinExactHits());\n      if ((flags & NO_CHECK_QCACHE) == 0) {\n        superset = queryResultCache.get(key);\n\n        if (superset != null) {\n          // check that the cache entry has scores recorded if we need them\n          if ((flags & GET_SCORES) == 0 || superset.hasScores()) {\n            // NOTE: subset() returns null if the DocList has fewer docs than\n            // requested\n            out.docList = superset.subset(cmd.getOffset(), cmd.getLen());\n          }\n        }\n        if (out.docList != null) {\n          // found the docList in the cache... now check if we need the docset too.\n          // OPT: possible future optimization - if the doclist contains all the matches,\n          // use it to make the docset instead of rerunning the query.\n          if (out.docSet == null && ((flags & GET_DOCSET) != 0)) {\n            if (cmd.getFilterList() == null) {\n              out.docSet = getDocSet(cmd.getQuery());\n            } else {\n              List<Query> newList = new ArrayList<>(cmd.getFilterList().size() + 1);\n              newList.add(cmd.getQuery());\n              newList.addAll(cmd.getFilterList());\n              out.docSet = getDocSet(newList);\n            }\n          }\n          return;\n        }\n      }\n\n      // If we are going to generate the result, bump up to the\n      // next resultWindowSize for better caching.\n\n      if ((flags & NO_SET_QCACHE) == 0) {\n        // handle 0 special case as well as avoid idiv in the common case.\n        if (maxDocRequested < queryResultWindowSize) {\n          supersetMaxDoc = queryResultWindowSize;\n        } else {\n          supersetMaxDoc = ((maxDocRequested - 1) / queryResultWindowSize + 1) * queryResultWindowSize;\n          if (supersetMaxDoc < 0) supersetMaxDoc = maxDocRequested;\n        }\n      } else {\n        key = null; // we won't be caching the result\n      }\n    }\n    cmd.setSupersetMaxDoc(supersetMaxDoc);\n\n    // OK, so now we need to generate an answer.\n    // One way to do that would be to check if we have an unordered list\n    // of results for the base query. If so, we can apply the filters and then\n    // sort by the resulting set. This can only be used if:\n    // - the sort doesn't contain score\n    // - we don't want score returned.\n\n    // check if we should try and use the filter cache\n    boolean useFilterCache = false;\n    if ((flags & (GET_SCORES | NO_CHECK_FILTERCACHE)) == 0 && useFilterForSortedQuery && cmd.getSort() != null\n        && filterCache != null) {\n      useFilterCache = true;\n      SortField[] sfields = cmd.getSort().getSort();\n      for (SortField sf : sfields) {\n        if (sf.getType() == SortField.Type.SCORE) {\n          useFilterCache = false;\n          break;\n        }\n      }\n    }\n\n    if (useFilterCache) {\n      // now actually use the filter cache.\n      // for large filters that match few documents, this may be\n      // slower than simply re-executing the query.\n      if (out.docSet == null) {\n        out.docSet = getDocSet(cmd.getQuery(), cmd.getFilter());\n        List<Query> filterList = cmd.getFilterList();\n        if (filterList != null && !filterList.isEmpty()) {\n          out.docSet = out.docSet.intersection(getDocSet(cmd.getFilterList()));\n        }\n      }\n      // todo: there could be a sortDocSet that could take a list of\n      // the filters instead of anding them first...\n      // perhaps there should be a multi-docset-iterator\n      sortDocSet(qr, cmd);\n    } else {\n      // do it the normal way...\n      if ((flags & GET_DOCSET) != 0) {\n        // this currently conflates returning the docset for the base query vs\n        // the base query and all filters.\n        DocSet qDocSet = getDocListAndSetNC(qr, cmd);\n        // cache the docSet matching the query w/o filtering\n        if (qDocSet != null && filterCache != null && !qr.isPartialResults()) filterCache.put(cmd.getQuery(), qDocSet);\n      } else {\n        getDocListNC(qr, cmd);\n      }\n      assert null != out.docList : \"docList is null\";\n    }\n\n    if (null == cmd.getCursorMark()) {\n      // Kludge...\n      // we can't use DocSlice.subset, even though it should be an identity op\n      // because it gets confused by situations where there are lots of matches, but\n      // less docs in the slice then were requested, (due to the cursor)\n      // so we have to short circuit the call.\n      // None of which is really a problem since we can't use caching with\n      // cursors anyway, but it still looks weird to have to special case this\n      // behavior based on this condition - hence the long explanation.\n      superset = out.docList;\n      out.docList = superset.subset(cmd.getOffset(), cmd.getLen());\n    } else {\n      // sanity check our cursor assumptions\n      assert null == superset : \"cursor: superset isn't null\";\n      assert 0 == cmd.getOffset() : \"cursor: command offset mismatch\";\n      assert 0 == out.docList.offset() : \"cursor: docList offset mismatch\";\n      assert cmd.getLen() >= supersetMaxDoc : \"cursor: superset len mismatch: \" + cmd.getLen() + \" vs \"\n          + supersetMaxDoc;\n    }\n\n    // lastly, put the superset in the cache if the size is less than or equal\n    // to queryResultMaxDocsCached\n    if (key != null && superset.size() <= queryResultMaxDocsCached && !qr.isPartialResults()) {\n      queryResultCache.put(key, superset);\n    }\n  }\n\n","sourceOld":"  /**\n   * getDocList version that uses+populates query and filter caches. In the event of a timeout, the cache is not\n   * populated.\n   */\n  private void getDocListC(QueryResult qr, QueryCommand cmd) throws IOException {\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n    QueryResultKey key = null;\n    int maxDocRequested = cmd.getOffset() + cmd.getLen();\n    // check for overflow, and check for # docs in index\n    if (maxDocRequested < 0 || maxDocRequested > maxDoc()) maxDocRequested = maxDoc();\n    int supersetMaxDoc = maxDocRequested;\n    DocList superset = null;\n\n    int flags = cmd.getFlags();\n    Query q = cmd.getQuery();\n    if (q instanceof ExtendedQuery) {\n      ExtendedQuery eq = (ExtendedQuery) q;\n      if (!eq.getCache()) {\n        flags |= (NO_CHECK_QCACHE | NO_SET_QCACHE | NO_CHECK_FILTERCACHE);\n      }\n    }\n\n    // we can try and look up the complete query in the cache.\n    // we can't do that if filter!=null though (we don't want to\n    // do hashCode() and equals() for a big DocSet).\n    if (queryResultCache != null && cmd.getFilter() == null\n        && (flags & (NO_CHECK_QCACHE | NO_SET_QCACHE)) != ((NO_CHECK_QCACHE | NO_SET_QCACHE))) {\n      // all of the current flags can be reused during warming,\n      // so set all of them on the cache key.\n      key = new QueryResultKey(q, cmd.getFilterList(), cmd.getSort(), flags);\n      if ((flags & NO_CHECK_QCACHE) == 0) {\n        superset = queryResultCache.get(key);\n\n        if (superset != null) {\n          // check that the cache entry has scores recorded if we need them\n          if ((flags & GET_SCORES) == 0 || superset.hasScores()) {\n            // NOTE: subset() returns null if the DocList has fewer docs than\n            // requested\n            out.docList = superset.subset(cmd.getOffset(), cmd.getLen());\n          }\n        }\n        if (out.docList != null) {\n          // found the docList in the cache... now check if we need the docset too.\n          // OPT: possible future optimization - if the doclist contains all the matches,\n          // use it to make the docset instead of rerunning the query.\n          if (out.docSet == null && ((flags & GET_DOCSET) != 0)) {\n            if (cmd.getFilterList() == null) {\n              out.docSet = getDocSet(cmd.getQuery());\n            } else {\n              List<Query> newList = new ArrayList<>(cmd.getFilterList().size() + 1);\n              newList.add(cmd.getQuery());\n              newList.addAll(cmd.getFilterList());\n              out.docSet = getDocSet(newList);\n            }\n          }\n          return;\n        }\n      }\n\n      // If we are going to generate the result, bump up to the\n      // next resultWindowSize for better caching.\n\n      if ((flags & NO_SET_QCACHE) == 0) {\n        // handle 0 special case as well as avoid idiv in the common case.\n        if (maxDocRequested < queryResultWindowSize) {\n          supersetMaxDoc = queryResultWindowSize;\n        } else {\n          supersetMaxDoc = ((maxDocRequested - 1) / queryResultWindowSize + 1) * queryResultWindowSize;\n          if (supersetMaxDoc < 0) supersetMaxDoc = maxDocRequested;\n        }\n      } else {\n        key = null; // we won't be caching the result\n      }\n    }\n    cmd.setSupersetMaxDoc(supersetMaxDoc);\n\n    // OK, so now we need to generate an answer.\n    // One way to do that would be to check if we have an unordered list\n    // of results for the base query. If so, we can apply the filters and then\n    // sort by the resulting set. This can only be used if:\n    // - the sort doesn't contain score\n    // - we don't want score returned.\n\n    // check if we should try and use the filter cache\n    boolean useFilterCache = false;\n    if ((flags & (GET_SCORES | NO_CHECK_FILTERCACHE)) == 0 && useFilterForSortedQuery && cmd.getSort() != null\n        && filterCache != null) {\n      useFilterCache = true;\n      SortField[] sfields = cmd.getSort().getSort();\n      for (SortField sf : sfields) {\n        if (sf.getType() == SortField.Type.SCORE) {\n          useFilterCache = false;\n          break;\n        }\n      }\n    }\n\n    if (useFilterCache) {\n      // now actually use the filter cache.\n      // for large filters that match few documents, this may be\n      // slower than simply re-executing the query.\n      if (out.docSet == null) {\n        out.docSet = getDocSet(cmd.getQuery(), cmd.getFilter());\n        List<Query> filterList = cmd.getFilterList();\n        if (filterList != null && !filterList.isEmpty()) {\n          out.docSet = out.docSet.intersection(getDocSet(cmd.getFilterList()));\n        }\n      }\n      // todo: there could be a sortDocSet that could take a list of\n      // the filters instead of anding them first...\n      // perhaps there should be a multi-docset-iterator\n      sortDocSet(qr, cmd);\n    } else {\n      // do it the normal way...\n      if ((flags & GET_DOCSET) != 0) {\n        // this currently conflates returning the docset for the base query vs\n        // the base query and all filters.\n        DocSet qDocSet = getDocListAndSetNC(qr, cmd);\n        // cache the docSet matching the query w/o filtering\n        if (qDocSet != null && filterCache != null && !qr.isPartialResults()) filterCache.put(cmd.getQuery(), qDocSet);\n      } else {\n        getDocListNC(qr, cmd);\n      }\n      assert null != out.docList : \"docList is null\";\n    }\n\n    if (null == cmd.getCursorMark()) {\n      // Kludge...\n      // we can't use DocSlice.subset, even though it should be an identity op\n      // because it gets confused by situations where there are lots of matches, but\n      // less docs in the slice then were requested, (due to the cursor)\n      // so we have to short circuit the call.\n      // None of which is really a problem since we can't use caching with\n      // cursors anyway, but it still looks weird to have to special case this\n      // behavior based on this condition - hence the long explanation.\n      superset = out.docList;\n      out.docList = superset.subset(cmd.getOffset(), cmd.getLen());\n    } else {\n      // sanity check our cursor assumptions\n      assert null == superset : \"cursor: superset isn't null\";\n      assert 0 == cmd.getOffset() : \"cursor: command offset mismatch\";\n      assert 0 == out.docList.offset() : \"cursor: docList offset mismatch\";\n      assert cmd.getLen() >= supersetMaxDoc : \"cursor: superset len mismatch: \" + cmd.getLen() + \" vs \"\n          + supersetMaxDoc;\n    }\n\n    // lastly, put the superset in the cache if the size is less than or equal\n    // to queryResultMaxDocsCached\n    if (key != null && superset.size() <= queryResultMaxDocsCached && !qr.isPartialResults()) {\n      queryResultCache.put(key, superset);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6b8ad6d99eb2424679c78255c369b8fac243e7dd","date":1590104557,"type":3,"author":"Tomas Fernandez Lobbe","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/SolrIndexSearcher#getDocListC(QueryResult,QueryCommand).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/SolrIndexSearcher#getDocListC(QueryResult,QueryCommand).mjava","sourceNew":"  /**\n   * getDocList version that uses+populates query and filter caches. In the event of a timeout, the cache is not\n   * populated.\n   */\n  private void getDocListC(QueryResult qr, QueryCommand cmd) throws IOException {\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n    QueryResultKey key = null;\n    int maxDocRequested = cmd.getOffset() + cmd.getLen();\n    // check for overflow, and check for # docs in index\n    if (maxDocRequested < 0 || maxDocRequested > maxDoc()) maxDocRequested = maxDoc();\n    int supersetMaxDoc = maxDocRequested;\n    DocList superset = null;\n\n    int flags = cmd.getFlags();\n    Query q = cmd.getQuery();\n    if (q instanceof ExtendedQuery) {\n      ExtendedQuery eq = (ExtendedQuery) q;\n      if (!eq.getCache()) {\n        flags |= (NO_CHECK_QCACHE | NO_SET_QCACHE | NO_CHECK_FILTERCACHE);\n      }\n    }\n\n    // we can try and look up the complete query in the cache.\n    // we can't do that if filter!=null though (we don't want to\n    // do hashCode() and equals() for a big DocSet).\n    if (queryResultCache != null && cmd.getFilter() == null\n        && (flags & (NO_CHECK_QCACHE | NO_SET_QCACHE)) != ((NO_CHECK_QCACHE | NO_SET_QCACHE))) {\n      // all of the current flags can be reused during warming,\n      // so set all of them on the cache key.\n      key = new QueryResultKey(q, cmd.getFilterList(), cmd.getSort(), flags, cmd.getMinExactCount());\n      if ((flags & NO_CHECK_QCACHE) == 0) {\n        superset = queryResultCache.get(key);\n\n        if (superset != null) {\n          // check that the cache entry has scores recorded if we need them\n          if ((flags & GET_SCORES) == 0 || superset.hasScores()) {\n            // NOTE: subset() returns null if the DocList has fewer docs than\n            // requested\n            out.docList = superset.subset(cmd.getOffset(), cmd.getLen());\n          }\n        }\n        if (out.docList != null) {\n          // found the docList in the cache... now check if we need the docset too.\n          // OPT: possible future optimization - if the doclist contains all the matches,\n          // use it to make the docset instead of rerunning the query.\n          if (out.docSet == null && ((flags & GET_DOCSET) != 0)) {\n            if (cmd.getFilterList() == null) {\n              out.docSet = getDocSet(cmd.getQuery());\n            } else {\n              List<Query> newList = new ArrayList<>(cmd.getFilterList().size() + 1);\n              newList.add(cmd.getQuery());\n              newList.addAll(cmd.getFilterList());\n              out.docSet = getDocSet(newList);\n            }\n          }\n          return;\n        }\n      }\n\n      // If we are going to generate the result, bump up to the\n      // next resultWindowSize for better caching.\n\n      if ((flags & NO_SET_QCACHE) == 0) {\n        // handle 0 special case as well as avoid idiv in the common case.\n        if (maxDocRequested < queryResultWindowSize) {\n          supersetMaxDoc = queryResultWindowSize;\n        } else {\n          supersetMaxDoc = ((maxDocRequested - 1) / queryResultWindowSize + 1) * queryResultWindowSize;\n          if (supersetMaxDoc < 0) supersetMaxDoc = maxDocRequested;\n        }\n      } else {\n        key = null; // we won't be caching the result\n      }\n    }\n    cmd.setSupersetMaxDoc(supersetMaxDoc);\n\n    // OK, so now we need to generate an answer.\n    // One way to do that would be to check if we have an unordered list\n    // of results for the base query. If so, we can apply the filters and then\n    // sort by the resulting set. This can only be used if:\n    // - the sort doesn't contain score\n    // - we don't want score returned.\n\n    // check if we should try and use the filter cache\n    boolean useFilterCache = false;\n    if ((flags & (GET_SCORES | NO_CHECK_FILTERCACHE)) == 0 && useFilterForSortedQuery && cmd.getSort() != null\n        && filterCache != null) {\n      useFilterCache = true;\n      SortField[] sfields = cmd.getSort().getSort();\n      for (SortField sf : sfields) {\n        if (sf.getType() == SortField.Type.SCORE) {\n          useFilterCache = false;\n          break;\n        }\n      }\n    }\n\n    if (useFilterCache) {\n      // now actually use the filter cache.\n      // for large filters that match few documents, this may be\n      // slower than simply re-executing the query.\n      if (out.docSet == null) {\n        out.docSet = getDocSet(cmd.getQuery(), cmd.getFilter());\n        List<Query> filterList = cmd.getFilterList();\n        if (filterList != null && !filterList.isEmpty()) {\n          out.docSet = out.docSet.intersection(getDocSet(cmd.getFilterList()));\n        }\n      }\n      // todo: there could be a sortDocSet that could take a list of\n      // the filters instead of anding them first...\n      // perhaps there should be a multi-docset-iterator\n      sortDocSet(qr, cmd);\n    } else {\n      // do it the normal way...\n      if ((flags & GET_DOCSET) != 0) {\n        // this currently conflates returning the docset for the base query vs\n        // the base query and all filters.\n        DocSet qDocSet = getDocListAndSetNC(qr, cmd);\n        // cache the docSet matching the query w/o filtering\n        if (qDocSet != null && filterCache != null && !qr.isPartialResults()) filterCache.put(cmd.getQuery(), qDocSet);\n      } else {\n        getDocListNC(qr, cmd);\n      }\n      assert null != out.docList : \"docList is null\";\n    }\n\n    if (null == cmd.getCursorMark()) {\n      // Kludge...\n      // we can't use DocSlice.subset, even though it should be an identity op\n      // because it gets confused by situations where there are lots of matches, but\n      // less docs in the slice then were requested, (due to the cursor)\n      // so we have to short circuit the call.\n      // None of which is really a problem since we can't use caching with\n      // cursors anyway, but it still looks weird to have to special case this\n      // behavior based on this condition - hence the long explanation.\n      superset = out.docList;\n      out.docList = superset.subset(cmd.getOffset(), cmd.getLen());\n    } else {\n      // sanity check our cursor assumptions\n      assert null == superset : \"cursor: superset isn't null\";\n      assert 0 == cmd.getOffset() : \"cursor: command offset mismatch\";\n      assert 0 == out.docList.offset() : \"cursor: docList offset mismatch\";\n      assert cmd.getLen() >= supersetMaxDoc : \"cursor: superset len mismatch: \" + cmd.getLen() + \" vs \"\n          + supersetMaxDoc;\n    }\n\n    // lastly, put the superset in the cache if the size is less than or equal\n    // to queryResultMaxDocsCached\n    if (key != null && superset.size() <= queryResultMaxDocsCached && !qr.isPartialResults()) {\n      queryResultCache.put(key, superset);\n    }\n  }\n\n","sourceOld":"  /**\n   * getDocList version that uses+populates query and filter caches. In the event of a timeout, the cache is not\n   * populated.\n   */\n  private void getDocListC(QueryResult qr, QueryCommand cmd) throws IOException {\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n    QueryResultKey key = null;\n    int maxDocRequested = cmd.getOffset() + cmd.getLen();\n    // check for overflow, and check for # docs in index\n    if (maxDocRequested < 0 || maxDocRequested > maxDoc()) maxDocRequested = maxDoc();\n    int supersetMaxDoc = maxDocRequested;\n    DocList superset = null;\n\n    int flags = cmd.getFlags();\n    Query q = cmd.getQuery();\n    if (q instanceof ExtendedQuery) {\n      ExtendedQuery eq = (ExtendedQuery) q;\n      if (!eq.getCache()) {\n        flags |= (NO_CHECK_QCACHE | NO_SET_QCACHE | NO_CHECK_FILTERCACHE);\n      }\n    }\n\n    // we can try and look up the complete query in the cache.\n    // we can't do that if filter!=null though (we don't want to\n    // do hashCode() and equals() for a big DocSet).\n    if (queryResultCache != null && cmd.getFilter() == null\n        && (flags & (NO_CHECK_QCACHE | NO_SET_QCACHE)) != ((NO_CHECK_QCACHE | NO_SET_QCACHE))) {\n      // all of the current flags can be reused during warming,\n      // so set all of them on the cache key.\n      key = new QueryResultKey(q, cmd.getFilterList(), cmd.getSort(), flags, cmd.getMinExactHits());\n      if ((flags & NO_CHECK_QCACHE) == 0) {\n        superset = queryResultCache.get(key);\n\n        if (superset != null) {\n          // check that the cache entry has scores recorded if we need them\n          if ((flags & GET_SCORES) == 0 || superset.hasScores()) {\n            // NOTE: subset() returns null if the DocList has fewer docs than\n            // requested\n            out.docList = superset.subset(cmd.getOffset(), cmd.getLen());\n          }\n        }\n        if (out.docList != null) {\n          // found the docList in the cache... now check if we need the docset too.\n          // OPT: possible future optimization - if the doclist contains all the matches,\n          // use it to make the docset instead of rerunning the query.\n          if (out.docSet == null && ((flags & GET_DOCSET) != 0)) {\n            if (cmd.getFilterList() == null) {\n              out.docSet = getDocSet(cmd.getQuery());\n            } else {\n              List<Query> newList = new ArrayList<>(cmd.getFilterList().size() + 1);\n              newList.add(cmd.getQuery());\n              newList.addAll(cmd.getFilterList());\n              out.docSet = getDocSet(newList);\n            }\n          }\n          return;\n        }\n      }\n\n      // If we are going to generate the result, bump up to the\n      // next resultWindowSize for better caching.\n\n      if ((flags & NO_SET_QCACHE) == 0) {\n        // handle 0 special case as well as avoid idiv in the common case.\n        if (maxDocRequested < queryResultWindowSize) {\n          supersetMaxDoc = queryResultWindowSize;\n        } else {\n          supersetMaxDoc = ((maxDocRequested - 1) / queryResultWindowSize + 1) * queryResultWindowSize;\n          if (supersetMaxDoc < 0) supersetMaxDoc = maxDocRequested;\n        }\n      } else {\n        key = null; // we won't be caching the result\n      }\n    }\n    cmd.setSupersetMaxDoc(supersetMaxDoc);\n\n    // OK, so now we need to generate an answer.\n    // One way to do that would be to check if we have an unordered list\n    // of results for the base query. If so, we can apply the filters and then\n    // sort by the resulting set. This can only be used if:\n    // - the sort doesn't contain score\n    // - we don't want score returned.\n\n    // check if we should try and use the filter cache\n    boolean useFilterCache = false;\n    if ((flags & (GET_SCORES | NO_CHECK_FILTERCACHE)) == 0 && useFilterForSortedQuery && cmd.getSort() != null\n        && filterCache != null) {\n      useFilterCache = true;\n      SortField[] sfields = cmd.getSort().getSort();\n      for (SortField sf : sfields) {\n        if (sf.getType() == SortField.Type.SCORE) {\n          useFilterCache = false;\n          break;\n        }\n      }\n    }\n\n    if (useFilterCache) {\n      // now actually use the filter cache.\n      // for large filters that match few documents, this may be\n      // slower than simply re-executing the query.\n      if (out.docSet == null) {\n        out.docSet = getDocSet(cmd.getQuery(), cmd.getFilter());\n        List<Query> filterList = cmd.getFilterList();\n        if (filterList != null && !filterList.isEmpty()) {\n          out.docSet = out.docSet.intersection(getDocSet(cmd.getFilterList()));\n        }\n      }\n      // todo: there could be a sortDocSet that could take a list of\n      // the filters instead of anding them first...\n      // perhaps there should be a multi-docset-iterator\n      sortDocSet(qr, cmd);\n    } else {\n      // do it the normal way...\n      if ((flags & GET_DOCSET) != 0) {\n        // this currently conflates returning the docset for the base query vs\n        // the base query and all filters.\n        DocSet qDocSet = getDocListAndSetNC(qr, cmd);\n        // cache the docSet matching the query w/o filtering\n        if (qDocSet != null && filterCache != null && !qr.isPartialResults()) filterCache.put(cmd.getQuery(), qDocSet);\n      } else {\n        getDocListNC(qr, cmd);\n      }\n      assert null != out.docList : \"docList is null\";\n    }\n\n    if (null == cmd.getCursorMark()) {\n      // Kludge...\n      // we can't use DocSlice.subset, even though it should be an identity op\n      // because it gets confused by situations where there are lots of matches, but\n      // less docs in the slice then were requested, (due to the cursor)\n      // so we have to short circuit the call.\n      // None of which is really a problem since we can't use caching with\n      // cursors anyway, but it still looks weird to have to special case this\n      // behavior based on this condition - hence the long explanation.\n      superset = out.docList;\n      out.docList = superset.subset(cmd.getOffset(), cmd.getLen());\n    } else {\n      // sanity check our cursor assumptions\n      assert null == superset : \"cursor: superset isn't null\";\n      assert 0 == cmd.getOffset() : \"cursor: command offset mismatch\";\n      assert 0 == out.docList.offset() : \"cursor: docList offset mismatch\";\n      assert cmd.getLen() >= supersetMaxDoc : \"cursor: superset len mismatch: \" + cmd.getLen() + \" vs \"\n          + supersetMaxDoc;\n    }\n\n    // lastly, put the superset in the cache if the size is less than or equal\n    // to queryResultMaxDocsCached\n    if (key != null && superset.size() <= queryResultMaxDocsCached && !qr.isPartialResults()) {\n      queryResultCache.put(key, superset);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"72afa881b0b5c361ebd0b6d37927fe072151fbe0","date":1590107364,"type":3,"author":"Erick Erickson","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/SolrIndexSearcher#getDocListC(QueryResult,QueryCommand).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/SolrIndexSearcher#getDocListC(QueryResult,QueryCommand).mjava","sourceNew":"  /**\n   * getDocList version that uses+populates query and filter caches. In the event of a timeout, the cache is not\n   * populated.\n   */\n  private void getDocListC(QueryResult qr, QueryCommand cmd) throws IOException {\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n    QueryResultKey key = null;\n    int maxDocRequested = cmd.getOffset() + cmd.getLen();\n    // check for overflow, and check for # docs in index\n    if (maxDocRequested < 0 || maxDocRequested > maxDoc()) maxDocRequested = maxDoc();\n    int supersetMaxDoc = maxDocRequested;\n    DocList superset = null;\n\n    int flags = cmd.getFlags();\n    Query q = cmd.getQuery();\n    if (q instanceof ExtendedQuery) {\n      ExtendedQuery eq = (ExtendedQuery) q;\n      if (!eq.getCache()) {\n        flags |= (NO_CHECK_QCACHE | NO_SET_QCACHE | NO_CHECK_FILTERCACHE);\n      }\n    }\n\n    // we can try and look up the complete query in the cache.\n    // we can't do that if filter!=null though (we don't want to\n    // do hashCode() and equals() for a big DocSet).\n    if (queryResultCache != null && cmd.getFilter() == null\n        && (flags & (NO_CHECK_QCACHE | NO_SET_QCACHE)) != ((NO_CHECK_QCACHE | NO_SET_QCACHE))) {\n      // all of the current flags can be reused during warming,\n      // so set all of them on the cache key.\n      key = new QueryResultKey(q, cmd.getFilterList(), cmd.getSort(), flags, cmd.getMinExactCount());\n      if ((flags & NO_CHECK_QCACHE) == 0) {\n        superset = queryResultCache.get(key);\n\n        if (superset != null) {\n          // check that the cache entry has scores recorded if we need them\n          if ((flags & GET_SCORES) == 0 || superset.hasScores()) {\n            // NOTE: subset() returns null if the DocList has fewer docs than\n            // requested\n            out.docList = superset.subset(cmd.getOffset(), cmd.getLen());\n          }\n        }\n        if (out.docList != null) {\n          // found the docList in the cache... now check if we need the docset too.\n          // OPT: possible future optimization - if the doclist contains all the matches,\n          // use it to make the docset instead of rerunning the query.\n          if (out.docSet == null && ((flags & GET_DOCSET) != 0)) {\n            if (cmd.getFilterList() == null) {\n              out.docSet = getDocSet(cmd.getQuery());\n            } else {\n              List<Query> newList = new ArrayList<>(cmd.getFilterList().size() + 1);\n              newList.add(cmd.getQuery());\n              newList.addAll(cmd.getFilterList());\n              out.docSet = getDocSet(newList);\n            }\n          }\n          return;\n        }\n      }\n\n      // If we are going to generate the result, bump up to the\n      // next resultWindowSize for better caching.\n\n      if ((flags & NO_SET_QCACHE) == 0) {\n        // handle 0 special case as well as avoid idiv in the common case.\n        if (maxDocRequested < queryResultWindowSize) {\n          supersetMaxDoc = queryResultWindowSize;\n        } else {\n          supersetMaxDoc = ((maxDocRequested - 1) / queryResultWindowSize + 1) * queryResultWindowSize;\n          if (supersetMaxDoc < 0) supersetMaxDoc = maxDocRequested;\n        }\n      } else {\n        key = null; // we won't be caching the result\n      }\n    }\n    cmd.setSupersetMaxDoc(supersetMaxDoc);\n\n    // OK, so now we need to generate an answer.\n    // One way to do that would be to check if we have an unordered list\n    // of results for the base query. If so, we can apply the filters and then\n    // sort by the resulting set. This can only be used if:\n    // - the sort doesn't contain score\n    // - we don't want score returned.\n\n    // check if we should try and use the filter cache\n    boolean useFilterCache = false;\n    if ((flags & (GET_SCORES | NO_CHECK_FILTERCACHE)) == 0 && useFilterForSortedQuery && cmd.getSort() != null\n        && filterCache != null) {\n      useFilterCache = true;\n      SortField[] sfields = cmd.getSort().getSort();\n      for (SortField sf : sfields) {\n        if (sf.getType() == SortField.Type.SCORE) {\n          useFilterCache = false;\n          break;\n        }\n      }\n    }\n\n    if (useFilterCache) {\n      // now actually use the filter cache.\n      // for large filters that match few documents, this may be\n      // slower than simply re-executing the query.\n      if (out.docSet == null) {\n        out.docSet = getDocSet(cmd.getQuery(), cmd.getFilter());\n        List<Query> filterList = cmd.getFilterList();\n        if (filterList != null && !filterList.isEmpty()) {\n          out.docSet = out.docSet.intersection(getDocSet(cmd.getFilterList()));\n        }\n      }\n      // todo: there could be a sortDocSet that could take a list of\n      // the filters instead of anding them first...\n      // perhaps there should be a multi-docset-iterator\n      sortDocSet(qr, cmd);\n    } else {\n      // do it the normal way...\n      if ((flags & GET_DOCSET) != 0) {\n        // this currently conflates returning the docset for the base query vs\n        // the base query and all filters.\n        DocSet qDocSet = getDocListAndSetNC(qr, cmd);\n        // cache the docSet matching the query w/o filtering\n        if (qDocSet != null && filterCache != null && !qr.isPartialResults()) filterCache.put(cmd.getQuery(), qDocSet);\n      } else {\n        getDocListNC(qr, cmd);\n      }\n      assert null != out.docList : \"docList is null\";\n    }\n\n    if (null == cmd.getCursorMark()) {\n      // Kludge...\n      // we can't use DocSlice.subset, even though it should be an identity op\n      // because it gets confused by situations where there are lots of matches, but\n      // less docs in the slice then were requested, (due to the cursor)\n      // so we have to short circuit the call.\n      // None of which is really a problem since we can't use caching with\n      // cursors anyway, but it still looks weird to have to special case this\n      // behavior based on this condition - hence the long explanation.\n      superset = out.docList;\n      out.docList = superset.subset(cmd.getOffset(), cmd.getLen());\n    } else {\n      // sanity check our cursor assumptions\n      assert null == superset : \"cursor: superset isn't null\";\n      assert 0 == cmd.getOffset() : \"cursor: command offset mismatch\";\n      assert 0 == out.docList.offset() : \"cursor: docList offset mismatch\";\n      assert cmd.getLen() >= supersetMaxDoc : \"cursor: superset len mismatch: \" + cmd.getLen() + \" vs \"\n          + supersetMaxDoc;\n    }\n\n    // lastly, put the superset in the cache if the size is less than or equal\n    // to queryResultMaxDocsCached\n    if (key != null && superset.size() <= queryResultMaxDocsCached && !qr.isPartialResults()) {\n      queryResultCache.put(key, superset);\n    }\n  }\n\n","sourceOld":"  /**\n   * getDocList version that uses+populates query and filter caches. In the event of a timeout, the cache is not\n   * populated.\n   */\n  private void getDocListC(QueryResult qr, QueryCommand cmd) throws IOException {\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n    QueryResultKey key = null;\n    int maxDocRequested = cmd.getOffset() + cmd.getLen();\n    // check for overflow, and check for # docs in index\n    if (maxDocRequested < 0 || maxDocRequested > maxDoc()) maxDocRequested = maxDoc();\n    int supersetMaxDoc = maxDocRequested;\n    DocList superset = null;\n\n    int flags = cmd.getFlags();\n    Query q = cmd.getQuery();\n    if (q instanceof ExtendedQuery) {\n      ExtendedQuery eq = (ExtendedQuery) q;\n      if (!eq.getCache()) {\n        flags |= (NO_CHECK_QCACHE | NO_SET_QCACHE | NO_CHECK_FILTERCACHE);\n      }\n    }\n\n    // we can try and look up the complete query in the cache.\n    // we can't do that if filter!=null though (we don't want to\n    // do hashCode() and equals() for a big DocSet).\n    if (queryResultCache != null && cmd.getFilter() == null\n        && (flags & (NO_CHECK_QCACHE | NO_SET_QCACHE)) != ((NO_CHECK_QCACHE | NO_SET_QCACHE))) {\n      // all of the current flags can be reused during warming,\n      // so set all of them on the cache key.\n      key = new QueryResultKey(q, cmd.getFilterList(), cmd.getSort(), flags, cmd.getMinExactHits());\n      if ((flags & NO_CHECK_QCACHE) == 0) {\n        superset = queryResultCache.get(key);\n\n        if (superset != null) {\n          // check that the cache entry has scores recorded if we need them\n          if ((flags & GET_SCORES) == 0 || superset.hasScores()) {\n            // NOTE: subset() returns null if the DocList has fewer docs than\n            // requested\n            out.docList = superset.subset(cmd.getOffset(), cmd.getLen());\n          }\n        }\n        if (out.docList != null) {\n          // found the docList in the cache... now check if we need the docset too.\n          // OPT: possible future optimization - if the doclist contains all the matches,\n          // use it to make the docset instead of rerunning the query.\n          if (out.docSet == null && ((flags & GET_DOCSET) != 0)) {\n            if (cmd.getFilterList() == null) {\n              out.docSet = getDocSet(cmd.getQuery());\n            } else {\n              List<Query> newList = new ArrayList<>(cmd.getFilterList().size() + 1);\n              newList.add(cmd.getQuery());\n              newList.addAll(cmd.getFilterList());\n              out.docSet = getDocSet(newList);\n            }\n          }\n          return;\n        }\n      }\n\n      // If we are going to generate the result, bump up to the\n      // next resultWindowSize for better caching.\n\n      if ((flags & NO_SET_QCACHE) == 0) {\n        // handle 0 special case as well as avoid idiv in the common case.\n        if (maxDocRequested < queryResultWindowSize) {\n          supersetMaxDoc = queryResultWindowSize;\n        } else {\n          supersetMaxDoc = ((maxDocRequested - 1) / queryResultWindowSize + 1) * queryResultWindowSize;\n          if (supersetMaxDoc < 0) supersetMaxDoc = maxDocRequested;\n        }\n      } else {\n        key = null; // we won't be caching the result\n      }\n    }\n    cmd.setSupersetMaxDoc(supersetMaxDoc);\n\n    // OK, so now we need to generate an answer.\n    // One way to do that would be to check if we have an unordered list\n    // of results for the base query. If so, we can apply the filters and then\n    // sort by the resulting set. This can only be used if:\n    // - the sort doesn't contain score\n    // - we don't want score returned.\n\n    // check if we should try and use the filter cache\n    boolean useFilterCache = false;\n    if ((flags & (GET_SCORES | NO_CHECK_FILTERCACHE)) == 0 && useFilterForSortedQuery && cmd.getSort() != null\n        && filterCache != null) {\n      useFilterCache = true;\n      SortField[] sfields = cmd.getSort().getSort();\n      for (SortField sf : sfields) {\n        if (sf.getType() == SortField.Type.SCORE) {\n          useFilterCache = false;\n          break;\n        }\n      }\n    }\n\n    if (useFilterCache) {\n      // now actually use the filter cache.\n      // for large filters that match few documents, this may be\n      // slower than simply re-executing the query.\n      if (out.docSet == null) {\n        out.docSet = getDocSet(cmd.getQuery(), cmd.getFilter());\n        List<Query> filterList = cmd.getFilterList();\n        if (filterList != null && !filterList.isEmpty()) {\n          out.docSet = out.docSet.intersection(getDocSet(cmd.getFilterList()));\n        }\n      }\n      // todo: there could be a sortDocSet that could take a list of\n      // the filters instead of anding them first...\n      // perhaps there should be a multi-docset-iterator\n      sortDocSet(qr, cmd);\n    } else {\n      // do it the normal way...\n      if ((flags & GET_DOCSET) != 0) {\n        // this currently conflates returning the docset for the base query vs\n        // the base query and all filters.\n        DocSet qDocSet = getDocListAndSetNC(qr, cmd);\n        // cache the docSet matching the query w/o filtering\n        if (qDocSet != null && filterCache != null && !qr.isPartialResults()) filterCache.put(cmd.getQuery(), qDocSet);\n      } else {\n        getDocListNC(qr, cmd);\n      }\n      assert null != out.docList : \"docList is null\";\n    }\n\n    if (null == cmd.getCursorMark()) {\n      // Kludge...\n      // we can't use DocSlice.subset, even though it should be an identity op\n      // because it gets confused by situations where there are lots of matches, but\n      // less docs in the slice then were requested, (due to the cursor)\n      // so we have to short circuit the call.\n      // None of which is really a problem since we can't use caching with\n      // cursors anyway, but it still looks weird to have to special case this\n      // behavior based on this condition - hence the long explanation.\n      superset = out.docList;\n      out.docList = superset.subset(cmd.getOffset(), cmd.getLen());\n    } else {\n      // sanity check our cursor assumptions\n      assert null == superset : \"cursor: superset isn't null\";\n      assert 0 == cmd.getOffset() : \"cursor: command offset mismatch\";\n      assert 0 == out.docList.offset() : \"cursor: docList offset mismatch\";\n      assert cmd.getLen() >= supersetMaxDoc : \"cursor: superset len mismatch: \" + cmd.getLen() + \" vs \"\n          + supersetMaxDoc;\n    }\n\n    // lastly, put the superset in the cache if the size is less than or equal\n    // to queryResultMaxDocsCached\n    if (key != null && superset.size() <= queryResultMaxDocsCached && !qr.isPartialResults()) {\n      queryResultCache.put(key, superset);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["d5bc8e25f59990525f5beb14afe9c96240dcf4a2"],"640ded7811e1b7d29236a5e2934ec3cd266a8199":["0d6197aa6dfead2636cc42b887d9ceef82f4491b"],"c26f00b574427b55127e869b935845554afde1fa":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"d5bc8e25f59990525f5beb14afe9c96240dcf4a2":["42f37a8cb4c5565e5233860fe796624f5ec2459f"],"42f37a8cb4c5565e5233860fe796624f5ec2459f":["c26f00b574427b55127e869b935845554afde1fa"],"72afa881b0b5c361ebd0b6d37927fe072151fbe0":["640ded7811e1b7d29236a5e2934ec3cd266a8199","6b8ad6d99eb2424679c78255c369b8fac243e7dd"],"ac9de183adbc9483681f275ac1e2d92ed19f52e1":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"0d6197aa6dfead2636cc42b887d9ceef82f4491b":["ac9de183adbc9483681f275ac1e2d92ed19f52e1"],"a258fbb26824fd104ed795e5d9033d2d040049ee":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"6b8ad6d99eb2424679c78255c369b8fac243e7dd":["640ded7811e1b7d29236a5e2934ec3cd266a8199"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["72afa881b0b5c361ebd0b6d37927fe072151fbe0"]},"commit2Childs":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["ac9de183adbc9483681f275ac1e2d92ed19f52e1"],"640ded7811e1b7d29236a5e2934ec3cd266a8199":["72afa881b0b5c361ebd0b6d37927fe072151fbe0","6b8ad6d99eb2424679c78255c369b8fac243e7dd"],"c26f00b574427b55127e869b935845554afde1fa":["42f37a8cb4c5565e5233860fe796624f5ec2459f"],"d5bc8e25f59990525f5beb14afe9c96240dcf4a2":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"42f37a8cb4c5565e5233860fe796624f5ec2459f":["d5bc8e25f59990525f5beb14afe9c96240dcf4a2"],"72afa881b0b5c361ebd0b6d37927fe072151fbe0":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"ac9de183adbc9483681f275ac1e2d92ed19f52e1":["0d6197aa6dfead2636cc42b887d9ceef82f4491b"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["c26f00b574427b55127e869b935845554afde1fa"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c26f00b574427b55127e869b935845554afde1fa","c903c3d15906a3da96b8c0c2fb704491005fdbdb","a258fbb26824fd104ed795e5d9033d2d040049ee"],"0d6197aa6dfead2636cc42b887d9ceef82f4491b":["640ded7811e1b7d29236a5e2934ec3cd266a8199"],"a258fbb26824fd104ed795e5d9033d2d040049ee":[],"6b8ad6d99eb2424679c78255c369b8fac243e7dd":["72afa881b0b5c361ebd0b6d37927fe072151fbe0"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a258fbb26824fd104ed795e5d9033d2d040049ee","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}