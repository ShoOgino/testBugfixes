{"path":"contrib/analyzers/common/src/test/org/apache/lucene/analysis/cjk/TestCJKTokenizer#checkCJKTokenReusable(Analyzer,String,TestToken[]).mjava","commits":[{"id":"18359c8e12d55f66c27cfe7babe86283f06a6aa5","date":1250426225,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"contrib/analyzers/common/src/test/org/apache/lucene/analysis/cjk/TestCJKTokenizer#checkCJKTokenReusable(Analyzer,String,TestToken[]).mjava","pathOld":"/dev/null","sourceNew":"  public void checkCJKTokenReusable(final Analyzer a, final String str, final TestToken[] out_tokens) throws IOException {\n    TokenStream ts = a.reusableTokenStream(\"dummy\", new StringReader(str));\n    TermAttribute termAtt = (TermAttribute) ts.getAttribute(TermAttribute.class);\n    OffsetAttribute offsetAtt = (OffsetAttribute) ts.getAttribute(OffsetAttribute.class);\n    TypeAttribute typeAtt = (TypeAttribute) ts.getAttribute(TypeAttribute.class);\n    for (int i = 0; i < out_tokens.length; i++) {\n      assertTrue(ts.incrementToken());\n      assertEquals(termAtt.term(), out_tokens[i].termText);\n      assertEquals(offsetAtt.startOffset(), out_tokens[i].start);\n      assertEquals(offsetAtt.endOffset(), out_tokens[i].end);\n      assertEquals(typeAtt.type(), out_tokens[i].type);\n    }\n    assertFalse(ts.incrementToken());\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["e64a71406348a5942a2166256238aff8313d0914"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8d78f014fded44fbde905f4f84cdc21907b371e8","date":1254383623,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"contrib/analyzers/common/src/test/org/apache/lucene/analysis/cjk/TestCJKTokenizer#checkCJKTokenReusable(Analyzer,String,TestToken[]).mjava","pathOld":"contrib/analyzers/common/src/test/org/apache/lucene/analysis/cjk/TestCJKTokenizer#checkCJKTokenReusable(Analyzer,String,TestToken[]).mjava","sourceNew":"  public void checkCJKTokenReusable(final Analyzer a, final String str, final TestToken[] out_tokens) throws IOException {\n    TokenStream ts = a.reusableTokenStream(\"dummy\", new StringReader(str));\n    TermAttribute termAtt = ts.getAttribute(TermAttribute.class);\n    OffsetAttribute offsetAtt = ts.getAttribute(OffsetAttribute.class);\n    TypeAttribute typeAtt = ts.getAttribute(TypeAttribute.class);\n    for (int i = 0; i < out_tokens.length; i++) {\n      assertTrue(ts.incrementToken());\n      assertEquals(termAtt.term(), out_tokens[i].termText);\n      assertEquals(offsetAtt.startOffset(), out_tokens[i].start);\n      assertEquals(offsetAtt.endOffset(), out_tokens[i].end);\n      assertEquals(typeAtt.type(), out_tokens[i].type);\n    }\n    assertFalse(ts.incrementToken());\n  }\n\n","sourceOld":"  public void checkCJKTokenReusable(final Analyzer a, final String str, final TestToken[] out_tokens) throws IOException {\n    TokenStream ts = a.reusableTokenStream(\"dummy\", new StringReader(str));\n    TermAttribute termAtt = (TermAttribute) ts.getAttribute(TermAttribute.class);\n    OffsetAttribute offsetAtt = (OffsetAttribute) ts.getAttribute(OffsetAttribute.class);\n    TypeAttribute typeAtt = (TypeAttribute) ts.getAttribute(TypeAttribute.class);\n    for (int i = 0; i < out_tokens.length; i++) {\n      assertTrue(ts.incrementToken());\n      assertEquals(termAtt.term(), out_tokens[i].termText);\n      assertEquals(offsetAtt.startOffset(), out_tokens[i].start);\n      assertEquals(offsetAtt.endOffset(), out_tokens[i].end);\n      assertEquals(typeAtt.type(), out_tokens[i].type);\n    }\n    assertFalse(ts.incrementToken());\n  }\n\n","bugFix":null,"bugIntro":["e64a71406348a5942a2166256238aff8313d0914"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e64a71406348a5942a2166256238aff8313d0914","date":1263756357,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"contrib/analyzers/common/src/test/org/apache/lucene/analysis/cjk/TestCJKTokenizer#checkCJKTokenReusable(Analyzer,String,TestToken[]).mjava","pathOld":"contrib/analyzers/common/src/test/org/apache/lucene/analysis/cjk/TestCJKTokenizer#checkCJKTokenReusable(Analyzer,String,TestToken[]).mjava","sourceNew":"  public void checkCJKTokenReusable(final Analyzer a, final String str, final TestToken[] out_tokens) throws IOException {\n    Analyzer analyzer = new CJKAnalyzer(Version.LUCENE_CURRENT);\n    String terms[] = new String[out_tokens.length];\n    int startOffsets[] = new int[out_tokens.length];\n    int endOffsets[] = new int[out_tokens.length];\n    String types[] = new String[out_tokens.length];\n    for (int i = 0; i < out_tokens.length; i++) {\n      terms[i] = out_tokens[i].termText;\n      startOffsets[i] = out_tokens[i].start;\n      endOffsets[i] = out_tokens[i].end;\n      types[i] = out_tokens[i].type;\n    }\n    assertAnalyzesToReuse(analyzer, str, terms, startOffsets, endOffsets, types, null);\n  }\n\n","sourceOld":"  public void checkCJKTokenReusable(final Analyzer a, final String str, final TestToken[] out_tokens) throws IOException {\n    TokenStream ts = a.reusableTokenStream(\"dummy\", new StringReader(str));\n    TermAttribute termAtt = ts.getAttribute(TermAttribute.class);\n    OffsetAttribute offsetAtt = ts.getAttribute(OffsetAttribute.class);\n    TypeAttribute typeAtt = ts.getAttribute(TypeAttribute.class);\n    for (int i = 0; i < out_tokens.length; i++) {\n      assertTrue(ts.incrementToken());\n      assertEquals(termAtt.term(), out_tokens[i].termText);\n      assertEquals(offsetAtt.startOffset(), out_tokens[i].start);\n      assertEquals(offsetAtt.endOffset(), out_tokens[i].end);\n      assertEquals(typeAtt.type(), out_tokens[i].type);\n    }\n    assertFalse(ts.incrementToken());\n  }\n\n","bugFix":["18359c8e12d55f66c27cfe7babe86283f06a6aa5","8d78f014fded44fbde905f4f84cdc21907b371e8"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a9ac13b5f0ce5ef1b2ce168367d993a79594b23a","date":1267298041,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"contrib/analyzers/common/src/test/org/apache/lucene/analysis/cjk/TestCJKTokenizer#checkCJKTokenReusable(Analyzer,String,TestToken[]).mjava","pathOld":"contrib/analyzers/common/src/test/org/apache/lucene/analysis/cjk/TestCJKTokenizer#checkCJKTokenReusable(Analyzer,String,TestToken[]).mjava","sourceNew":"  public void checkCJKTokenReusable(final Analyzer a, final String str, final TestToken[] out_tokens) throws IOException {\n    Analyzer analyzer = new CJKAnalyzer(TEST_VERSION_CURRENT);\n    String terms[] = new String[out_tokens.length];\n    int startOffsets[] = new int[out_tokens.length];\n    int endOffsets[] = new int[out_tokens.length];\n    String types[] = new String[out_tokens.length];\n    for (int i = 0; i < out_tokens.length; i++) {\n      terms[i] = out_tokens[i].termText;\n      startOffsets[i] = out_tokens[i].start;\n      endOffsets[i] = out_tokens[i].end;\n      types[i] = out_tokens[i].type;\n    }\n    assertAnalyzesToReuse(analyzer, str, terms, startOffsets, endOffsets, types, null);\n  }\n\n","sourceOld":"  public void checkCJKTokenReusable(final Analyzer a, final String str, final TestToken[] out_tokens) throws IOException {\n    Analyzer analyzer = new CJKAnalyzer(Version.LUCENE_CURRENT);\n    String terms[] = new String[out_tokens.length];\n    int startOffsets[] = new int[out_tokens.length];\n    int endOffsets[] = new int[out_tokens.length];\n    String types[] = new String[out_tokens.length];\n    for (int i = 0; i < out_tokens.length; i++) {\n      terms[i] = out_tokens[i].termText;\n      startOffsets[i] = out_tokens[i].start;\n      endOffsets[i] = out_tokens[i].end;\n      types[i] = out_tokens[i].type;\n    }\n    assertAnalyzesToReuse(analyzer, str, terms, startOffsets, endOffsets, types, null);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/cjk/TestCJKTokenizer#checkCJKTokenReusable(Analyzer,String,TestToken[]).mjava","pathOld":"contrib/analyzers/common/src/test/org/apache/lucene/analysis/cjk/TestCJKTokenizer#checkCJKTokenReusable(Analyzer,String,TestToken[]).mjava","sourceNew":"  public void checkCJKTokenReusable(final Analyzer a, final String str, final TestToken[] out_tokens) throws IOException {\n    Analyzer analyzer = new CJKAnalyzer(TEST_VERSION_CURRENT);\n    String terms[] = new String[out_tokens.length];\n    int startOffsets[] = new int[out_tokens.length];\n    int endOffsets[] = new int[out_tokens.length];\n    String types[] = new String[out_tokens.length];\n    for (int i = 0; i < out_tokens.length; i++) {\n      terms[i] = out_tokens[i].termText;\n      startOffsets[i] = out_tokens[i].start;\n      endOffsets[i] = out_tokens[i].end;\n      types[i] = out_tokens[i].type;\n    }\n    assertAnalyzesToReuse(analyzer, str, terms, startOffsets, endOffsets, types, null);\n  }\n\n","sourceOld":"  public void checkCJKTokenReusable(final Analyzer a, final String str, final TestToken[] out_tokens) throws IOException {\n    Analyzer analyzer = new CJKAnalyzer(TEST_VERSION_CURRENT);\n    String terms[] = new String[out_tokens.length];\n    int startOffsets[] = new int[out_tokens.length];\n    int endOffsets[] = new int[out_tokens.length];\n    String types[] = new String[out_tokens.length];\n    for (int i = 0; i < out_tokens.length; i++) {\n      terms[i] = out_tokens[i].termText;\n      startOffsets[i] = out_tokens[i].start;\n      endOffsets[i] = out_tokens[i].end;\n      types[i] = out_tokens[i].type;\n    }\n    assertAnalyzesToReuse(analyzer, str, terms, startOffsets, endOffsets, types, null);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"18359c8e12d55f66c27cfe7babe86283f06a6aa5":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"e64a71406348a5942a2166256238aff8313d0914":["8d78f014fded44fbde905f4f84cdc21907b371e8"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"8d78f014fded44fbde905f4f84cdc21907b371e8":["18359c8e12d55f66c27cfe7babe86283f06a6aa5"],"a9ac13b5f0ce5ef1b2ce168367d993a79594b23a":["e64a71406348a5942a2166256238aff8313d0914"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a9ac13b5f0ce5ef1b2ce168367d993a79594b23a"]},"commit2Childs":{"18359c8e12d55f66c27cfe7babe86283f06a6aa5":["8d78f014fded44fbde905f4f84cdc21907b371e8"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["18359c8e12d55f66c27cfe7babe86283f06a6aa5"],"e64a71406348a5942a2166256238aff8313d0914":["a9ac13b5f0ce5ef1b2ce168367d993a79594b23a"],"8d78f014fded44fbde905f4f84cdc21907b371e8":["e64a71406348a5942a2166256238aff8313d0914"],"a9ac13b5f0ce5ef1b2ce168367d993a79594b23a":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}