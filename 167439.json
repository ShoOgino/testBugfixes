{"path":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionJustBeforeFlush().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionJustBeforeFlush().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionJustBeforeFlush().mjava","sourceNew":"  // LUCENE-1208\n  public void testExceptionJustBeforeFlush() throws IOException {\n    Directory dir = newDirectory();\n    MockIndexWriter w = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(2));\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"a field\", TextField.TYPE_STORED));\n    w.addDocument(doc);\n\n    Analyzer analyzer = new Analyzer(new Analyzer.PerFieldReuseStrategy()) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    Document crashDoc = new Document();\n    crashDoc.add(newField(\"crash\", \"do it on token 4\", TextField.TYPE_STORED));\n    try {\n      w.addDocument(crashDoc, analyzer);\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n    }\n    w.addDocument(doc);\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1208\n  public void testExceptionJustBeforeFlush() throws IOException {\n    Directory dir = newDirectory();\n    MockIndexWriter w = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(2));\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"a field\", TextField.TYPE_STORED));\n    w.addDocument(doc);\n\n    Analyzer analyzer = new Analyzer(new Analyzer.PerFieldReuseStrategy()) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    Document crashDoc = new Document();\n    crashDoc.add(newField(\"crash\", \"do it on token 4\", TextField.TYPE_STORED));\n    try {\n      w.addDocument(crashDoc, analyzer);\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n    }\n    w.addDocument(doc);\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionJustBeforeFlush().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionJustBeforeFlush().mjava","sourceNew":"  // LUCENE-1208\n  public void testExceptionJustBeforeFlush() throws IOException {\n    Directory dir = newDirectory();\n    MockIndexWriter w = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMaxBufferedDocs(2));\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"a field\", TextField.TYPE_STORED));\n    w.addDocument(doc);\n\n    Analyzer analyzer = new Analyzer(new Analyzer.PerFieldReuseStrategy()) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    Document crashDoc = new Document();\n    crashDoc.add(newField(\"crash\", \"do it on token 4\", TextField.TYPE_STORED));\n    try {\n      w.addDocument(crashDoc, analyzer);\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n    }\n    w.addDocument(doc);\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1208\n  public void testExceptionJustBeforeFlush() throws IOException {\n    Directory dir = newDirectory();\n    MockIndexWriter w = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(2));\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"a field\", TextField.TYPE_STORED));\n    w.addDocument(doc);\n\n    Analyzer analyzer = new Analyzer(new Analyzer.PerFieldReuseStrategy()) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    Document crashDoc = new Document();\n    crashDoc.add(newField(\"crash\", \"do it on token 4\", TextField.TYPE_STORED));\n    try {\n      w.addDocument(crashDoc, analyzer);\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n    }\n    w.addDocument(doc);\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"04f07771a2a7dd3a395700665ed839c3dae2def2","date":1339350139,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionJustBeforeFlush().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionJustBeforeFlush().mjava","sourceNew":"  // LUCENE-1208\n  public void testExceptionJustBeforeFlush() throws IOException {\n    Directory dir = newDirectory();\n    MockIndexWriter w = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMaxBufferedDocs(2));\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"a field\", Field.Store.YES));\n    w.addDocument(doc);\n\n    Analyzer analyzer = new Analyzer(new Analyzer.PerFieldReuseStrategy()) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    Document crashDoc = new Document();\n    crashDoc.add(newTextField(\"crash\", \"do it on token 4\", Field.Store.YES));\n    try {\n      w.addDocument(crashDoc, analyzer);\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n    }\n    w.addDocument(doc);\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1208\n  public void testExceptionJustBeforeFlush() throws IOException {\n    Directory dir = newDirectory();\n    MockIndexWriter w = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMaxBufferedDocs(2));\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"a field\", TextField.TYPE_STORED));\n    w.addDocument(doc);\n\n    Analyzer analyzer = new Analyzer(new Analyzer.PerFieldReuseStrategy()) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    Document crashDoc = new Document();\n    crashDoc.add(newField(\"crash\", \"do it on token 4\", TextField.TYPE_STORED));\n    try {\n      w.addDocument(crashDoc, analyzer);\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n    }\n    w.addDocument(doc);\n    w.close();\n    dir.close();\n  }\n\n","bugFix":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"978de4e2d23054c6624dd5928ddeb734dca68eec","date":1370592803,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionJustBeforeFlush().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionJustBeforeFlush().mjava","sourceNew":"  // LUCENE-1208\n  public void testExceptionJustBeforeFlush() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriter w = RandomIndexWriter.mockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMaxBufferedDocs(2), new TestPoint1());\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"a field\", Field.Store.YES));\n    w.addDocument(doc);\n\n    Analyzer analyzer = new Analyzer(new Analyzer.PerFieldReuseStrategy()) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    Document crashDoc = new Document();\n    crashDoc.add(newTextField(\"crash\", \"do it on token 4\", Field.Store.YES));\n    try {\n      w.addDocument(crashDoc, analyzer);\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n    }\n    w.addDocument(doc);\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1208\n  public void testExceptionJustBeforeFlush() throws IOException {\n    Directory dir = newDirectory();\n    MockIndexWriter w = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMaxBufferedDocs(2));\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"a field\", Field.Store.YES));\n    w.addDocument(doc);\n\n    Analyzer analyzer = new Analyzer(new Analyzer.PerFieldReuseStrategy()) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    Document crashDoc = new Document();\n    crashDoc.add(newTextField(\"crash\", \"do it on token 4\", Field.Store.YES));\n    try {\n      w.addDocument(crashDoc, analyzer);\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n    }\n    w.addDocument(doc);\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"38061899d760e06a12fe186bc1f09ca9ff0e64a6","date":1376491296,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionJustBeforeFlush().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionJustBeforeFlush().mjava","sourceNew":"  // LUCENE-1208\n  public void testExceptionJustBeforeFlush() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriter w = RandomIndexWriter.mockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMaxBufferedDocs(2), new TestPoint1());\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"a field\", Field.Store.YES));\n    w.addDocument(doc);\n\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    Document crashDoc = new Document();\n    crashDoc.add(newTextField(\"crash\", \"do it on token 4\", Field.Store.YES));\n    try {\n      w.addDocument(crashDoc, analyzer);\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n    }\n    w.addDocument(doc);\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1208\n  public void testExceptionJustBeforeFlush() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriter w = RandomIndexWriter.mockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMaxBufferedDocs(2), new TestPoint1());\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"a field\", Field.Store.YES));\n    w.addDocument(doc);\n\n    Analyzer analyzer = new Analyzer(new Analyzer.PerFieldReuseStrategy()) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    Document crashDoc = new Document();\n    crashDoc.add(newTextField(\"crash\", \"do it on token 4\", Field.Store.YES));\n    try {\n      w.addDocument(crashDoc, analyzer);\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n    }\n    w.addDocument(doc);\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3dffec77fb8f7d0e9ca4869dddd6af94528b4576","date":1377875202,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionJustBeforeFlush().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionJustBeforeFlush().mjava","sourceNew":"  // LUCENE-1208\n  public void testExceptionJustBeforeFlush() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriter w = RandomIndexWriter.mockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMaxBufferedDocs(2), new TestPoint1());\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"a field\", Field.Store.YES));\n    w.addDocument(doc);\n\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    Document crashDoc = new Document();\n    crashDoc.add(newTextField(\"crash\", \"do it on token 4\", Field.Store.YES));\n    try {\n      w.addDocument(crashDoc, analyzer);\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n    }\n    w.addDocument(doc);\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1208\n  public void testExceptionJustBeforeFlush() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriter w = RandomIndexWriter.mockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMaxBufferedDocs(2), new TestPoint1());\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"a field\", Field.Store.YES));\n    w.addDocument(doc);\n\n    Analyzer analyzer = new Analyzer(new Analyzer.PerFieldReuseStrategy()) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    Document crashDoc = new Document();\n    crashDoc.add(newTextField(\"crash\", \"do it on token 4\", Field.Store.YES));\n    try {\n      w.addDocument(crashDoc, analyzer);\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n    }\n    w.addDocument(doc);\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","date":1389274049,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionJustBeforeFlush().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionJustBeforeFlush().mjava","sourceNew":"  // LUCENE-1208\n  public void testExceptionJustBeforeFlush() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriter w = RandomIndexWriter.mockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMaxBufferedDocs(2), new TestPoint1());\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"a field\", Field.Store.YES));\n    w.addDocument(doc);\n\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    Document crashDoc = new Document();\n    crashDoc.add(newTextField(\"crash\", \"do it on token 4\", Field.Store.YES));\n    try {\n      w.addDocument(crashDoc, analyzer);\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n    }\n    w.addDocument(doc);\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1208\n  public void testExceptionJustBeforeFlush() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriter w = RandomIndexWriter.mockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMaxBufferedDocs(2), new TestPoint1());\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"a field\", Field.Store.YES));\n    w.addDocument(doc);\n\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    Document crashDoc = new Document();\n    crashDoc.add(newTextField(\"crash\", \"do it on token 4\", Field.Store.YES));\n    try {\n      w.addDocument(crashDoc, analyzer);\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n    }\n    w.addDocument(doc);\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionJustBeforeFlush().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionJustBeforeFlush().mjava","sourceNew":"  // LUCENE-1208\n  public void testExceptionJustBeforeFlush() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriter w = RandomIndexWriter.mockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMaxBufferedDocs(2), new TestPoint1());\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"a field\", Field.Store.YES));\n    w.addDocument(doc);\n\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    Document crashDoc = new Document();\n    crashDoc.add(newTextField(\"crash\", \"do it on token 4\", Field.Store.YES));\n    try {\n      w.addDocument(crashDoc, analyzer);\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n    }\n    w.addDocument(doc);\n    w.shutdown();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1208\n  public void testExceptionJustBeforeFlush() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriter w = RandomIndexWriter.mockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMaxBufferedDocs(2), new TestPoint1());\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"a field\", Field.Store.YES));\n    w.addDocument(doc);\n\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    Document crashDoc = new Document();\n    crashDoc.add(newTextField(\"crash\", \"do it on token 4\", Field.Store.YES));\n    try {\n      w.addDocument(crashDoc, analyzer);\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n    }\n    w.addDocument(doc);\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionJustBeforeFlush().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionJustBeforeFlush().mjava","sourceNew":"  // LUCENE-1208\n  public void testExceptionJustBeforeFlush() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriter w = RandomIndexWriter.mockIndexWriter(dir, \n                                                      newIndexWriterConfig(new MockAnalyzer(random()))\n                                                        .setMaxBufferedDocs(2), \n                                                      new TestPoint1());\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"a field\", Field.Store.YES));\n    w.addDocument(doc);\n\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    Document crashDoc = new Document();\n    crashDoc.add(newTextField(\"crash\", \"do it on token 4\", Field.Store.YES));\n    try {\n      w.addDocument(crashDoc, analyzer);\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n    }\n    w.addDocument(doc);\n    w.shutdown();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1208\n  public void testExceptionJustBeforeFlush() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriter w = RandomIndexWriter.mockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMaxBufferedDocs(2), new TestPoint1());\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"a field\", Field.Store.YES));\n    w.addDocument(doc);\n\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    Document crashDoc = new Document();\n    crashDoc.add(newTextField(\"crash\", \"do it on token 4\", Field.Store.YES));\n    try {\n      w.addDocument(crashDoc, analyzer);\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n    }\n    w.addDocument(doc);\n    w.shutdown();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionJustBeforeFlush().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionJustBeforeFlush().mjava","sourceNew":"  // LUCENE-1208\n  public void testExceptionJustBeforeFlush() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriter w = RandomIndexWriter.mockIndexWriter(dir, \n                                                      newIndexWriterConfig(new MockAnalyzer(random()))\n                                                        .setMaxBufferedDocs(2), \n                                                      new TestPoint1());\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"a field\", Field.Store.YES));\n    w.addDocument(doc);\n\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    Document crashDoc = new Document();\n    crashDoc.add(newTextField(\"crash\", \"do it on token 4\", Field.Store.YES));\n    try {\n      w.addDocument(crashDoc, analyzer);\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n    }\n    w.addDocument(doc);\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1208\n  public void testExceptionJustBeforeFlush() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriter w = RandomIndexWriter.mockIndexWriter(dir, \n                                                      newIndexWriterConfig(new MockAnalyzer(random()))\n                                                        .setMaxBufferedDocs(2), \n                                                      new TestPoint1());\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"a field\", Field.Store.YES));\n    w.addDocument(doc);\n\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    Document crashDoc = new Document();\n    crashDoc.add(newTextField(\"crash\", \"do it on token 4\", Field.Store.YES));\n    try {\n      w.addDocument(crashDoc, analyzer);\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n    }\n    w.addDocument(doc);\n    w.shutdown();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5c1c5aa8e88aa52c9e1cbfc696b611d3a56223c0","date":1422781929,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionJustBeforeFlush().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionJustBeforeFlush().mjava","sourceNew":"  // LUCENE-1208\n  public void testExceptionJustBeforeFlush() throws IOException {\n    Directory dir = newDirectory();\n\n    final AtomicBoolean doCrash = new AtomicBoolean();\n\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        TokenStream stream = tokenizer;\n        if (doCrash.get()) {\n          stream = new CrashingFilter(fieldName, stream);\n        }\n        return new TokenStreamComponents(tokenizer, stream);\n      }\n    };\n\n    IndexWriter w = RandomIndexWriter.mockIndexWriter(dir, \n                                                      newIndexWriterConfig(analyzer)\n                                                        .setMaxBufferedDocs(2), \n                                                      new TestPoint1());\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"a field\", Field.Store.YES));\n    w.addDocument(doc);\n\n    Document crashDoc = new Document();\n    crashDoc.add(newTextField(\"crash\", \"do it on token 4\", Field.Store.YES));\n    doCrash.set(true);\n    try {\n      w.addDocument(crashDoc);\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n    }\n    w.addDocument(doc);\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1208\n  public void testExceptionJustBeforeFlush() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriter w = RandomIndexWriter.mockIndexWriter(dir, \n                                                      newIndexWriterConfig(new MockAnalyzer(random()))\n                                                        .setMaxBufferedDocs(2), \n                                                      new TestPoint1());\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"a field\", Field.Store.YES));\n    w.addDocument(doc);\n\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    Document crashDoc = new Document();\n    crashDoc.add(newTextField(\"crash\", \"do it on token 4\", Field.Store.YES));\n    try {\n      w.addDocument(crashDoc, analyzer);\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n    }\n    w.addDocument(doc);\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a6b82a3644db30161c3cbd3e23aeefe19cb88113","date":1435478870,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionJustBeforeFlush().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionJustBeforeFlush().mjava","sourceNew":"  // LUCENE-1208\n  public void testExceptionJustBeforeFlush() throws IOException {\n    Directory dir = newDirectory();\n\n    final AtomicBoolean doCrash = new AtomicBoolean();\n\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        TokenStream stream = tokenizer;\n        if (doCrash.get()) {\n          stream = new CrashingFilter(fieldName, stream);\n        }\n        return new TokenStreamComponents(tokenizer, stream);\n      }\n    };\n\n    IndexWriter w = RandomIndexWriter.mockIndexWriter(random(), dir, \n                                                      newIndexWriterConfig(analyzer)\n                                                        .setMaxBufferedDocs(2), \n                                                      new TestPoint1());\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"a field\", Field.Store.YES));\n    w.addDocument(doc);\n\n    Document crashDoc = new Document();\n    crashDoc.add(newTextField(\"crash\", \"do it on token 4\", Field.Store.YES));\n    doCrash.set(true);\n    try {\n      w.addDocument(crashDoc);\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n    }\n    w.addDocument(doc);\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1208\n  public void testExceptionJustBeforeFlush() throws IOException {\n    Directory dir = newDirectory();\n\n    final AtomicBoolean doCrash = new AtomicBoolean();\n\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        TokenStream stream = tokenizer;\n        if (doCrash.get()) {\n          stream = new CrashingFilter(fieldName, stream);\n        }\n        return new TokenStreamComponents(tokenizer, stream);\n      }\n    };\n\n    IndexWriter w = RandomIndexWriter.mockIndexWriter(dir, \n                                                      newIndexWriterConfig(analyzer)\n                                                        .setMaxBufferedDocs(2), \n                                                      new TestPoint1());\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"a field\", Field.Store.YES));\n    w.addDocument(doc);\n\n    Document crashDoc = new Document();\n    crashDoc.add(newTextField(\"crash\", \"do it on token 4\", Field.Store.YES));\n    doCrash.set(true);\n    try {\n      w.addDocument(crashDoc);\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n    }\n    w.addDocument(doc);\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"05fe562aa248790944d43cdd478f512572835ba0","date":1455901667,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionJustBeforeFlush().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionJustBeforeFlush().mjava","sourceNew":"  // LUCENE-1208\n  public void testExceptionJustBeforeFlush() throws IOException {\n    Directory dir = newDirectory();\n\n    final AtomicBoolean doCrash = new AtomicBoolean();\n\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        TokenStream stream = tokenizer;\n        if (doCrash.get()) {\n          stream = new CrashingFilter(fieldName, stream);\n        }\n        return new TokenStreamComponents(tokenizer, stream);\n      }\n    };\n\n    IndexWriter w = RandomIndexWriter.mockIndexWriter(random(), dir, \n                                                      newIndexWriterConfig(analyzer)\n                                                        .setMaxBufferedDocs(2), \n                                                      new TestPoint1());\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"a field\", Field.Store.YES));\n    w.addDocument(doc);\n\n    Document crashDoc = new Document();\n    crashDoc.add(newTextField(\"crash\", \"do it on token 4\", Field.Store.YES));\n    doCrash.set(true);\n    expectThrows(IOException.class, () -> {\n      w.addDocument(crashDoc);\n    });\n\n    w.addDocument(doc);\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1208\n  public void testExceptionJustBeforeFlush() throws IOException {\n    Directory dir = newDirectory();\n\n    final AtomicBoolean doCrash = new AtomicBoolean();\n\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        TokenStream stream = tokenizer;\n        if (doCrash.get()) {\n          stream = new CrashingFilter(fieldName, stream);\n        }\n        return new TokenStreamComponents(tokenizer, stream);\n      }\n    };\n\n    IndexWriter w = RandomIndexWriter.mockIndexWriter(random(), dir, \n                                                      newIndexWriterConfig(analyzer)\n                                                        .setMaxBufferedDocs(2), \n                                                      new TestPoint1());\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"a field\", Field.Store.YES));\n    w.addDocument(doc);\n\n    Document crashDoc = new Document();\n    crashDoc.add(newTextField(\"crash\", \"do it on token 4\", Field.Store.YES));\n    doCrash.set(true);\n    try {\n      w.addDocument(crashDoc);\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n    }\n    w.addDocument(doc);\n    w.close();\n    dir.close();\n  }\n\n","bugFix":["878eedeaae8b281cc57edbb48be7876469cec585","b76f497be58114d897182bb3ac8d503e7b3dcd1d"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"978de4e2d23054c6624dd5928ddeb734dca68eec":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["38061899d760e06a12fe186bc1f09ca9ff0e64a6"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":["978de4e2d23054c6624dd5928ddeb734dca68eec","38061899d760e06a12fe186bc1f09ca9ff0e64a6"],"05fe562aa248790944d43cdd478f512572835ba0":["a6b82a3644db30161c3cbd3e23aeefe19cb88113"],"5c1c5aa8e88aa52c9e1cbfc696b611d3a56223c0":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"38061899d760e06a12fe186bc1f09ca9ff0e64a6":["978de4e2d23054c6624dd5928ddeb734dca68eec"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"a6b82a3644db30161c3cbd3e23aeefe19cb88113":["5c1c5aa8e88aa52c9e1cbfc696b611d3a56223c0"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["05fe562aa248790944d43cdd478f512572835ba0"]},"commit2Childs":{"978de4e2d23054c6624dd5928ddeb734dca68eec":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576","38061899d760e06a12fe186bc1f09ca9ff0e64a6"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":[],"05fe562aa248790944d43cdd478f512572835ba0":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"5c1c5aa8e88aa52c9e1cbfc696b611d3a56223c0":["a6b82a3644db30161c3cbd3e23aeefe19cb88113"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["978de4e2d23054c6624dd5928ddeb734dca68eec"],"38061899d760e06a12fe186bc1f09ca9ff0e64a6":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","3dffec77fb8f7d0e9ca4869dddd6af94528b4576"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["5c1c5aa8e88aa52c9e1cbfc696b611d3a56223c0"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"a6b82a3644db30161c3cbd3e23aeefe19cb88113":["05fe562aa248790944d43cdd478f512572835ba0"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}