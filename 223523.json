{"path":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#doSplit().mjava","commits":[{"id":"20c968c14aace7cf49843bf2c1fafc7fd3845659","date":1533133859,"type":0,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#doSplit().mjava","pathOld":"/dev/null","sourceNew":"  public void doSplit() throws IOException {\n\n    List<LeafReaderContext> leaves = searcher.getRawReader().leaves();\n    Directory parentDirectory = searcher.getRawReader().directory();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n    SolrIndexConfig parentConfig = searcher.getCore().getSolrConfig().indexConfig;\n    String timestamp = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\" + leaves.size());\n    RTimerTree t;\n\n    if (splitMethod != SplitMethod.LINK) {\n      t = timings.sub(\"findDocSetsPerLeaf\");\n      for (LeafReaderContext readerContext : leaves) {\n        assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n        FixedBitSet[] docSets = split(readerContext, numPieces, field, rangesArr, splitKey, hashRouter, false);\n        segmentDocSets.add(docSets);\n      }\n      t.stop();\n    }\n\n\n    Map<IndexReader.CacheKey, FixedBitSet[]> docsToDeleteCache = new ConcurrentHashMap<>();\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      String partitionName = \"SolrIndexSplitter:partition=\" + partitionNumber + \",partitionCount=\" + numPieces + (ranges != null ? \",range=\" + ranges.get(partitionNumber) : \"\");\n      log.info(partitionName);\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw;\n      if (cores != null && splitMethod != SplitMethod.LINK) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        if (splitMethod == SplitMethod.LINK) {\n          SolrCore subCore = cores.get(partitionNumber);\n          String path = subCore.getDataDir() + INDEX_PREFIX + timestamp;\n          t = timings.sub(\"hardLinkCopy\");\n          t.resume();\n          // copy by hard-linking\n          Directory splitDir = subCore.getDirectoryFactory().get(path, DirectoryFactory.DirContext.DEFAULT, subCore.getSolrConfig().indexConfig.lockType);\n          // the wrapper doesn't hold any resources itself so it doesn't need closing\n          HardlinkCopyDirectoryWrapper hardLinkedDir = new HardlinkCopyDirectoryWrapper(splitDir);\n          boolean copiedOk = false;\n          try {\n            for (String file : parentDirectory.listAll()) {\n              // we've closed the IndexWriter, so ignore write.lock\n              // its file may be present even when IndexWriter is closed but\n              // we've already checked that the lock is not held by anyone else\n              if (file.equals(IndexWriter.WRITE_LOCK_NAME)) {\n                continue;\n              }\n              hardLinkedDir.copyFrom(parentDirectory, file, file, IOContext.DEFAULT);\n            }\n            copiedOk = true;\n          } finally {\n            if (!copiedOk) {\n              subCore.getDirectoryFactory().doneWithDirectory(splitDir);\n              subCore.getDirectoryFactory().remove(splitDir);\n            }\n          }\n          t.pause();\n          IndexWriterConfig iwConfig = parentConfig.toIndexWriterConfig(subCore);\n          // don't run merges at this time\n          iwConfig.setMergePolicy(NoMergePolicy.INSTANCE);\n          t = timings.sub(\"createSubIW\");\n          t.resume();\n          iw = new SolrIndexWriter(partitionName, splitDir, iwConfig);\n          t.pause();\n        } else {\n          SolrCore core = searcher.getCore();\n          String path = paths.get(partitionNumber);\n          t = timings.sub(\"createSubIW\");\n          t.resume();\n          iw = SolrIndexWriter.create(core, partitionName, path,\n              core.getDirectoryFactory(), true, core.getLatestSchema(),\n              core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n          t.pause();\n        }\n      }\n\n      try {\n        if (splitMethod == SplitMethod.LINK) {\n          t = timings.sub(\"deleteDocuments\");\n          t.resume();\n          // apply deletions specific to this partition. As a side-effect on the first call this also populates\n          // a cache of docsets to delete per leaf reader per partition, which is reused for subsequent partitions.\n          iw.deleteDocuments(new SplittingQuery(partitionNumber, field, rangesArr, hashRouter, splitKey, docsToDeleteCache));\n          t.pause();\n        } else {\n          // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n          t = timings.sub(\"addIndexes\");\n          t.resume();\n          for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n            log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n            CodecReader subReader = SlowCodecReaderWrapper.wrap(leaves.get(segmentNumber).reader());\n            iw.addIndexes(new LiveDocsReader(subReader, segmentDocSets.get(segmentNumber)[partitionNumber]));\n          }\n          t.pause();\n        }\n        // we commit explicitly instead of sending a CommitUpdateCommand through the processor chain\n        // because the sub-shard cores will just ignore such a commit because the update log is not\n        // in active state at this time.\n        //TODO no commitUpdateCommand\n        SolrIndexWriter.setCommitData(iw, -1);\n        t = timings.sub(\"subIWCommit\");\n        t.resume();\n        iw.commit();\n        t.pause();\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            t = timings.sub(\"subIWClose\");\n            t.resume();\n            iw.close();\n            t.pause();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n          if (splitMethod == SplitMethod.LINK) {\n            SolrCore subCore = cores.get(partitionNumber);\n            subCore.getDirectoryFactory().release(iw.getDirectory());\n          }\n        }\n      }\n    }\n    // all sub-indexes created ok\n    // when using hard-linking switch directories & refresh cores\n    if (splitMethod == SplitMethod.LINK && cores != null) {\n      boolean switchOk = true;\n      t = timings.sub(\"switchSubIndexes\");\n      for (int partitionNumber = 0; partitionNumber < numPieces; partitionNumber++) {\n        SolrCore subCore = cores.get(partitionNumber);\n        String indexDirPath = subCore.getIndexDir();\n\n        log.debug(\"Switching directories\");\n        String hardLinkPath = subCore.getDataDir() + INDEX_PREFIX + timestamp;\n        subCore.modifyIndexProps(INDEX_PREFIX + timestamp);\n        try {\n          subCore.getUpdateHandler().newIndexWriter(false);\n          openNewSearcher(subCore);\n        } catch (Exception e) {\n          log.error(\"Failed to switch sub-core \" + indexDirPath + \" to \" + hardLinkPath + \", split will fail.\", e);\n          switchOk = false;\n          break;\n        }\n      }\n      t.stop();\n      if (!switchOk) {\n        t = timings.sub(\"rollbackSubIndexes\");\n        // rollback the switch\n        for (int partitionNumber = 0; partitionNumber < numPieces; partitionNumber++) {\n          SolrCore subCore = cores.get(partitionNumber);\n          Directory dir = null;\n          try {\n            dir = subCore.getDirectoryFactory().get(subCore.getDataDir(), DirectoryFactory.DirContext.META_DATA,\n                subCore.getSolrConfig().indexConfig.lockType);\n            dir.deleteFile(IndexFetcher.INDEX_PROPERTIES);\n          } finally {\n            if (dir != null) {\n              subCore.getDirectoryFactory().release(dir);\n            }\n          }\n          // switch back if necessary and remove the hardlinked dir\n          String hardLinkPath = subCore.getDataDir() + INDEX_PREFIX + timestamp;\n          try {\n            dir = subCore.getDirectoryFactory().get(hardLinkPath, DirectoryFactory.DirContext.DEFAULT,\n                subCore.getSolrConfig().indexConfig.lockType);\n            subCore.getDirectoryFactory().doneWithDirectory(dir);\n            subCore.getDirectoryFactory().remove(dir);\n          } finally {\n            if (dir != null) {\n              subCore.getDirectoryFactory().release(dir);\n            }\n          }\n          subCore.getUpdateHandler().newIndexWriter(false);\n          try {\n            openNewSearcher(subCore);\n          } catch (Exception e) {\n            log.warn(\"Error rolling back failed split of \" + hardLinkPath, e);\n          }\n        }\n        t.stop();\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"There were errors during index split\");\n      } else {\n        // complete the switch - remove original index\n        t = timings.sub(\"cleanSubIndex\");\n        for (int partitionNumber = 0; partitionNumber < numPieces; partitionNumber++) {\n          SolrCore subCore = cores.get(partitionNumber);\n          String oldIndexPath = subCore.getDataDir() + \"index\";\n          Directory indexDir = null;\n          try {\n            indexDir = subCore.getDirectoryFactory().get(oldIndexPath,\n                DirectoryFactory.DirContext.DEFAULT, subCore.getSolrConfig().indexConfig.lockType);\n            subCore.getDirectoryFactory().doneWithDirectory(indexDir);\n            subCore.getDirectoryFactory().remove(indexDir);\n          } finally {\n            if (indexDir != null) {\n              subCore.getDirectoryFactory().release(indexDir);\n            }\n          }\n        }\n        t.stop();\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6aafdd2b981170a4d391e52d7cfd3d53a626cc7c","date":1533237435,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#doSplit().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#doSplit().mjava","sourceNew":"  public void doSplit() throws IOException {\n\n    List<LeafReaderContext> leaves = searcher.getRawReader().leaves();\n    Directory parentDirectory = searcher.getRawReader().directory();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n    SolrIndexConfig parentConfig = searcher.getCore().getSolrConfig().indexConfig;\n    String timestamp = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\" + leaves.size());\n    RTimerTree t;\n\n    // this tracks round-robin assignment of docs to partitions\n    AtomicInteger currentPartition = new AtomicInteger();\n\n    if (splitMethod != SplitMethod.LINK) {\n      t = timings.sub(\"findDocSetsPerLeaf\");\n      for (LeafReaderContext readerContext : leaves) {\n        assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n        FixedBitSet[] docSets = split(readerContext, numPieces, field, rangesArr, splitKey, hashRouter, currentPartition, false);\n        segmentDocSets.add(docSets);\n      }\n      t.stop();\n    }\n\n\n    Map<IndexReader.CacheKey, FixedBitSet[]> docsToDeleteCache = new ConcurrentHashMap<>();\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      String partitionName = \"SolrIndexSplitter:partition=\" + partitionNumber + \",partitionCount=\" + numPieces + (ranges != null ? \",range=\" + ranges.get(partitionNumber) : \"\");\n      log.info(partitionName);\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw;\n      if (cores != null && splitMethod != SplitMethod.LINK) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        if (splitMethod == SplitMethod.LINK) {\n          SolrCore subCore = cores.get(partitionNumber);\n          String path = subCore.getDataDir() + INDEX_PREFIX + timestamp;\n          t = timings.sub(\"hardLinkCopy\");\n          t.resume();\n          // copy by hard-linking\n          Directory splitDir = subCore.getDirectoryFactory().get(path, DirectoryFactory.DirContext.DEFAULT, subCore.getSolrConfig().indexConfig.lockType);\n          // the wrapper doesn't hold any resources itself so it doesn't need closing\n          HardlinkCopyDirectoryWrapper hardLinkedDir = new HardlinkCopyDirectoryWrapper(splitDir);\n          boolean copiedOk = false;\n          try {\n            for (String file : parentDirectory.listAll()) {\n              // we've closed the IndexWriter, so ignore write.lock\n              // its file may be present even when IndexWriter is closed but\n              // we've already checked that the lock is not held by anyone else\n              if (file.equals(IndexWriter.WRITE_LOCK_NAME)) {\n                continue;\n              }\n              hardLinkedDir.copyFrom(parentDirectory, file, file, IOContext.DEFAULT);\n            }\n            copiedOk = true;\n          } finally {\n            if (!copiedOk) {\n              subCore.getDirectoryFactory().doneWithDirectory(splitDir);\n              subCore.getDirectoryFactory().remove(splitDir);\n            }\n          }\n          t.pause();\n          IndexWriterConfig iwConfig = parentConfig.toIndexWriterConfig(subCore);\n          // don't run merges at this time\n          iwConfig.setMergePolicy(NoMergePolicy.INSTANCE);\n          t = timings.sub(\"createSubIW\");\n          t.resume();\n          iw = new SolrIndexWriter(partitionName, splitDir, iwConfig);\n          t.pause();\n        } else {\n          SolrCore core = searcher.getCore();\n          String path = paths.get(partitionNumber);\n          t = timings.sub(\"createSubIW\");\n          t.resume();\n          iw = SolrIndexWriter.create(core, partitionName, path,\n              core.getDirectoryFactory(), true, core.getLatestSchema(),\n              core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n          t.pause();\n        }\n      }\n\n      try {\n        if (splitMethod == SplitMethod.LINK) {\n          t = timings.sub(\"deleteDocuments\");\n          t.resume();\n          // apply deletions specific to this partition. As a side-effect on the first call this also populates\n          // a cache of docsets to delete per leaf reader per partition, which is reused for subsequent partitions.\n          iw.deleteDocuments(new SplittingQuery(partitionNumber, field, rangesArr, hashRouter, splitKey, docsToDeleteCache, currentPartition));\n          t.pause();\n        } else {\n          // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n          t = timings.sub(\"addIndexes\");\n          t.resume();\n          for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n            log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n            CodecReader subReader = SlowCodecReaderWrapper.wrap(leaves.get(segmentNumber).reader());\n            iw.addIndexes(new LiveDocsReader(subReader, segmentDocSets.get(segmentNumber)[partitionNumber]));\n          }\n          t.pause();\n        }\n        // we commit explicitly instead of sending a CommitUpdateCommand through the processor chain\n        // because the sub-shard cores will just ignore such a commit because the update log is not\n        // in active state at this time.\n        //TODO no commitUpdateCommand\n        SolrIndexWriter.setCommitData(iw, -1);\n        t = timings.sub(\"subIWCommit\");\n        t.resume();\n        iw.commit();\n        t.pause();\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            t = timings.sub(\"subIWClose\");\n            t.resume();\n            iw.close();\n            t.pause();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n          if (splitMethod == SplitMethod.LINK) {\n            SolrCore subCore = cores.get(partitionNumber);\n            subCore.getDirectoryFactory().release(iw.getDirectory());\n          }\n        }\n      }\n    }\n    // all sub-indexes created ok\n    // when using hard-linking switch directories & refresh cores\n    if (splitMethod == SplitMethod.LINK && cores != null) {\n      boolean switchOk = true;\n      t = timings.sub(\"switchSubIndexes\");\n      for (int partitionNumber = 0; partitionNumber < numPieces; partitionNumber++) {\n        SolrCore subCore = cores.get(partitionNumber);\n        String indexDirPath = subCore.getIndexDir();\n\n        log.debug(\"Switching directories\");\n        String hardLinkPath = subCore.getDataDir() + INDEX_PREFIX + timestamp;\n        subCore.modifyIndexProps(INDEX_PREFIX + timestamp);\n        try {\n          subCore.getUpdateHandler().newIndexWriter(false);\n          openNewSearcher(subCore);\n        } catch (Exception e) {\n          log.error(\"Failed to switch sub-core \" + indexDirPath + \" to \" + hardLinkPath + \", split will fail.\", e);\n          switchOk = false;\n          break;\n        }\n      }\n      t.stop();\n      if (!switchOk) {\n        t = timings.sub(\"rollbackSubIndexes\");\n        // rollback the switch\n        for (int partitionNumber = 0; partitionNumber < numPieces; partitionNumber++) {\n          SolrCore subCore = cores.get(partitionNumber);\n          Directory dir = null;\n          try {\n            dir = subCore.getDirectoryFactory().get(subCore.getDataDir(), DirectoryFactory.DirContext.META_DATA,\n                subCore.getSolrConfig().indexConfig.lockType);\n            dir.deleteFile(IndexFetcher.INDEX_PROPERTIES);\n          } finally {\n            if (dir != null) {\n              subCore.getDirectoryFactory().release(dir);\n            }\n          }\n          // switch back if necessary and remove the hardlinked dir\n          String hardLinkPath = subCore.getDataDir() + INDEX_PREFIX + timestamp;\n          try {\n            dir = subCore.getDirectoryFactory().get(hardLinkPath, DirectoryFactory.DirContext.DEFAULT,\n                subCore.getSolrConfig().indexConfig.lockType);\n            subCore.getDirectoryFactory().doneWithDirectory(dir);\n            subCore.getDirectoryFactory().remove(dir);\n          } finally {\n            if (dir != null) {\n              subCore.getDirectoryFactory().release(dir);\n            }\n          }\n          subCore.getUpdateHandler().newIndexWriter(false);\n          try {\n            openNewSearcher(subCore);\n          } catch (Exception e) {\n            log.warn(\"Error rolling back failed split of \" + hardLinkPath, e);\n          }\n        }\n        t.stop();\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"There were errors during index split\");\n      } else {\n        // complete the switch - remove original index\n        t = timings.sub(\"cleanSubIndex\");\n        for (int partitionNumber = 0; partitionNumber < numPieces; partitionNumber++) {\n          SolrCore subCore = cores.get(partitionNumber);\n          String oldIndexPath = subCore.getDataDir() + \"index\";\n          Directory indexDir = null;\n          try {\n            indexDir = subCore.getDirectoryFactory().get(oldIndexPath,\n                DirectoryFactory.DirContext.DEFAULT, subCore.getSolrConfig().indexConfig.lockType);\n            subCore.getDirectoryFactory().doneWithDirectory(indexDir);\n            subCore.getDirectoryFactory().remove(indexDir);\n          } finally {\n            if (indexDir != null) {\n              subCore.getDirectoryFactory().release(indexDir);\n            }\n          }\n        }\n        t.stop();\n      }\n    }\n  }\n\n","sourceOld":"  public void doSplit() throws IOException {\n\n    List<LeafReaderContext> leaves = searcher.getRawReader().leaves();\n    Directory parentDirectory = searcher.getRawReader().directory();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n    SolrIndexConfig parentConfig = searcher.getCore().getSolrConfig().indexConfig;\n    String timestamp = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\" + leaves.size());\n    RTimerTree t;\n\n    if (splitMethod != SplitMethod.LINK) {\n      t = timings.sub(\"findDocSetsPerLeaf\");\n      for (LeafReaderContext readerContext : leaves) {\n        assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n        FixedBitSet[] docSets = split(readerContext, numPieces, field, rangesArr, splitKey, hashRouter, false);\n        segmentDocSets.add(docSets);\n      }\n      t.stop();\n    }\n\n\n    Map<IndexReader.CacheKey, FixedBitSet[]> docsToDeleteCache = new ConcurrentHashMap<>();\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      String partitionName = \"SolrIndexSplitter:partition=\" + partitionNumber + \",partitionCount=\" + numPieces + (ranges != null ? \",range=\" + ranges.get(partitionNumber) : \"\");\n      log.info(partitionName);\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw;\n      if (cores != null && splitMethod != SplitMethod.LINK) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        if (splitMethod == SplitMethod.LINK) {\n          SolrCore subCore = cores.get(partitionNumber);\n          String path = subCore.getDataDir() + INDEX_PREFIX + timestamp;\n          t = timings.sub(\"hardLinkCopy\");\n          t.resume();\n          // copy by hard-linking\n          Directory splitDir = subCore.getDirectoryFactory().get(path, DirectoryFactory.DirContext.DEFAULT, subCore.getSolrConfig().indexConfig.lockType);\n          // the wrapper doesn't hold any resources itself so it doesn't need closing\n          HardlinkCopyDirectoryWrapper hardLinkedDir = new HardlinkCopyDirectoryWrapper(splitDir);\n          boolean copiedOk = false;\n          try {\n            for (String file : parentDirectory.listAll()) {\n              // we've closed the IndexWriter, so ignore write.lock\n              // its file may be present even when IndexWriter is closed but\n              // we've already checked that the lock is not held by anyone else\n              if (file.equals(IndexWriter.WRITE_LOCK_NAME)) {\n                continue;\n              }\n              hardLinkedDir.copyFrom(parentDirectory, file, file, IOContext.DEFAULT);\n            }\n            copiedOk = true;\n          } finally {\n            if (!copiedOk) {\n              subCore.getDirectoryFactory().doneWithDirectory(splitDir);\n              subCore.getDirectoryFactory().remove(splitDir);\n            }\n          }\n          t.pause();\n          IndexWriterConfig iwConfig = parentConfig.toIndexWriterConfig(subCore);\n          // don't run merges at this time\n          iwConfig.setMergePolicy(NoMergePolicy.INSTANCE);\n          t = timings.sub(\"createSubIW\");\n          t.resume();\n          iw = new SolrIndexWriter(partitionName, splitDir, iwConfig);\n          t.pause();\n        } else {\n          SolrCore core = searcher.getCore();\n          String path = paths.get(partitionNumber);\n          t = timings.sub(\"createSubIW\");\n          t.resume();\n          iw = SolrIndexWriter.create(core, partitionName, path,\n              core.getDirectoryFactory(), true, core.getLatestSchema(),\n              core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n          t.pause();\n        }\n      }\n\n      try {\n        if (splitMethod == SplitMethod.LINK) {\n          t = timings.sub(\"deleteDocuments\");\n          t.resume();\n          // apply deletions specific to this partition. As a side-effect on the first call this also populates\n          // a cache of docsets to delete per leaf reader per partition, which is reused for subsequent partitions.\n          iw.deleteDocuments(new SplittingQuery(partitionNumber, field, rangesArr, hashRouter, splitKey, docsToDeleteCache));\n          t.pause();\n        } else {\n          // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n          t = timings.sub(\"addIndexes\");\n          t.resume();\n          for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n            log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n            CodecReader subReader = SlowCodecReaderWrapper.wrap(leaves.get(segmentNumber).reader());\n            iw.addIndexes(new LiveDocsReader(subReader, segmentDocSets.get(segmentNumber)[partitionNumber]));\n          }\n          t.pause();\n        }\n        // we commit explicitly instead of sending a CommitUpdateCommand through the processor chain\n        // because the sub-shard cores will just ignore such a commit because the update log is not\n        // in active state at this time.\n        //TODO no commitUpdateCommand\n        SolrIndexWriter.setCommitData(iw, -1);\n        t = timings.sub(\"subIWCommit\");\n        t.resume();\n        iw.commit();\n        t.pause();\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            t = timings.sub(\"subIWClose\");\n            t.resume();\n            iw.close();\n            t.pause();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n          if (splitMethod == SplitMethod.LINK) {\n            SolrCore subCore = cores.get(partitionNumber);\n            subCore.getDirectoryFactory().release(iw.getDirectory());\n          }\n        }\n      }\n    }\n    // all sub-indexes created ok\n    // when using hard-linking switch directories & refresh cores\n    if (splitMethod == SplitMethod.LINK && cores != null) {\n      boolean switchOk = true;\n      t = timings.sub(\"switchSubIndexes\");\n      for (int partitionNumber = 0; partitionNumber < numPieces; partitionNumber++) {\n        SolrCore subCore = cores.get(partitionNumber);\n        String indexDirPath = subCore.getIndexDir();\n\n        log.debug(\"Switching directories\");\n        String hardLinkPath = subCore.getDataDir() + INDEX_PREFIX + timestamp;\n        subCore.modifyIndexProps(INDEX_PREFIX + timestamp);\n        try {\n          subCore.getUpdateHandler().newIndexWriter(false);\n          openNewSearcher(subCore);\n        } catch (Exception e) {\n          log.error(\"Failed to switch sub-core \" + indexDirPath + \" to \" + hardLinkPath + \", split will fail.\", e);\n          switchOk = false;\n          break;\n        }\n      }\n      t.stop();\n      if (!switchOk) {\n        t = timings.sub(\"rollbackSubIndexes\");\n        // rollback the switch\n        for (int partitionNumber = 0; partitionNumber < numPieces; partitionNumber++) {\n          SolrCore subCore = cores.get(partitionNumber);\n          Directory dir = null;\n          try {\n            dir = subCore.getDirectoryFactory().get(subCore.getDataDir(), DirectoryFactory.DirContext.META_DATA,\n                subCore.getSolrConfig().indexConfig.lockType);\n            dir.deleteFile(IndexFetcher.INDEX_PROPERTIES);\n          } finally {\n            if (dir != null) {\n              subCore.getDirectoryFactory().release(dir);\n            }\n          }\n          // switch back if necessary and remove the hardlinked dir\n          String hardLinkPath = subCore.getDataDir() + INDEX_PREFIX + timestamp;\n          try {\n            dir = subCore.getDirectoryFactory().get(hardLinkPath, DirectoryFactory.DirContext.DEFAULT,\n                subCore.getSolrConfig().indexConfig.lockType);\n            subCore.getDirectoryFactory().doneWithDirectory(dir);\n            subCore.getDirectoryFactory().remove(dir);\n          } finally {\n            if (dir != null) {\n              subCore.getDirectoryFactory().release(dir);\n            }\n          }\n          subCore.getUpdateHandler().newIndexWriter(false);\n          try {\n            openNewSearcher(subCore);\n          } catch (Exception e) {\n            log.warn(\"Error rolling back failed split of \" + hardLinkPath, e);\n          }\n        }\n        t.stop();\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"There were errors during index split\");\n      } else {\n        // complete the switch - remove original index\n        t = timings.sub(\"cleanSubIndex\");\n        for (int partitionNumber = 0; partitionNumber < numPieces; partitionNumber++) {\n          SolrCore subCore = cores.get(partitionNumber);\n          String oldIndexPath = subCore.getDataDir() + \"index\";\n          Directory indexDir = null;\n          try {\n            indexDir = subCore.getDirectoryFactory().get(oldIndexPath,\n                DirectoryFactory.DirContext.DEFAULT, subCore.getSolrConfig().indexConfig.lockType);\n            subCore.getDirectoryFactory().doneWithDirectory(indexDir);\n            subCore.getDirectoryFactory().remove(indexDir);\n          } finally {\n            if (indexDir != null) {\n              subCore.getDirectoryFactory().release(indexDir);\n            }\n          }\n        }\n        t.stop();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"740d649f013f07efbeb73ca854f106c60166e7c0","date":1587431295,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#doSplit().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#doSplit().mjava","sourceNew":"  public void doSplit() throws IOException {\n\n    List<LeafReaderContext> leaves = searcher.getRawReader().leaves();\n    Directory parentDirectory = searcher.getRawReader().directory();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n    SolrIndexConfig parentConfig = searcher.getCore().getSolrConfig().indexConfig;\n    String timestamp = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date());\n\n    if (log.isInfoEnabled()) {\n      log.info(\"SolrIndexSplitter: partitions={} segments={}\", numPieces, leaves.size());\n    }\n    RTimerTree t;\n\n    // this tracks round-robin assignment of docs to partitions\n    AtomicInteger currentPartition = new AtomicInteger();\n\n    if (splitMethod != SplitMethod.LINK) {\n      t = timings.sub(\"findDocSetsPerLeaf\");\n      for (LeafReaderContext readerContext : leaves) {\n        assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n        FixedBitSet[] docSets = split(readerContext, numPieces, field, rangesArr, splitKey, hashRouter, currentPartition, false);\n        segmentDocSets.add(docSets);\n      }\n      t.stop();\n    }\n\n\n    Map<IndexReader.CacheKey, FixedBitSet[]> docsToDeleteCache = new ConcurrentHashMap<>();\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      String partitionName = \"SolrIndexSplitter:partition=\" + partitionNumber + \",partitionCount=\" + numPieces + (ranges != null ? \",range=\" + ranges.get(partitionNumber) : \"\");\n      log.info(partitionName);\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw;\n      if (cores != null && splitMethod != SplitMethod.LINK) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        if (splitMethod == SplitMethod.LINK) {\n          SolrCore subCore = cores.get(partitionNumber);\n          String path = subCore.getDataDir() + INDEX_PREFIX + timestamp;\n          t = timings.sub(\"hardLinkCopy\");\n          t.resume();\n          // copy by hard-linking\n          Directory splitDir = subCore.getDirectoryFactory().get(path, DirectoryFactory.DirContext.DEFAULT, subCore.getSolrConfig().indexConfig.lockType);\n          // the wrapper doesn't hold any resources itself so it doesn't need closing\n          HardlinkCopyDirectoryWrapper hardLinkedDir = new HardlinkCopyDirectoryWrapper(splitDir);\n          boolean copiedOk = false;\n          try {\n            for (String file : parentDirectory.listAll()) {\n              // we've closed the IndexWriter, so ignore write.lock\n              // its file may be present even when IndexWriter is closed but\n              // we've already checked that the lock is not held by anyone else\n              if (file.equals(IndexWriter.WRITE_LOCK_NAME)) {\n                continue;\n              }\n              hardLinkedDir.copyFrom(parentDirectory, file, file, IOContext.DEFAULT);\n            }\n            copiedOk = true;\n          } finally {\n            if (!copiedOk) {\n              subCore.getDirectoryFactory().doneWithDirectory(splitDir);\n              subCore.getDirectoryFactory().remove(splitDir);\n            }\n          }\n          t.pause();\n          IndexWriterConfig iwConfig = parentConfig.toIndexWriterConfig(subCore);\n          // don't run merges at this time\n          iwConfig.setMergePolicy(NoMergePolicy.INSTANCE);\n          t = timings.sub(\"createSubIW\");\n          t.resume();\n          iw = new SolrIndexWriter(partitionName, splitDir, iwConfig);\n          t.pause();\n        } else {\n          SolrCore core = searcher.getCore();\n          String path = paths.get(partitionNumber);\n          t = timings.sub(\"createSubIW\");\n          t.resume();\n          iw = SolrIndexWriter.create(core, partitionName, path,\n              core.getDirectoryFactory(), true, core.getLatestSchema(),\n              core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n          t.pause();\n        }\n      }\n\n      try {\n        if (splitMethod == SplitMethod.LINK) {\n          t = timings.sub(\"deleteDocuments\");\n          t.resume();\n          // apply deletions specific to this partition. As a side-effect on the first call this also populates\n          // a cache of docsets to delete per leaf reader per partition, which is reused for subsequent partitions.\n          iw.deleteDocuments(new SplittingQuery(partitionNumber, field, rangesArr, hashRouter, splitKey, docsToDeleteCache, currentPartition));\n          t.pause();\n        } else {\n          // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n          t = timings.sub(\"addIndexes\");\n          t.resume();\n          for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n            if (log.isInfoEnabled()) {\n              log.info(\"SolrIndexSplitter: partition #{} partitionCount={} segment #{} segmentCount={}\"\n              , partitionNumber, numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\")\n              , segmentNumber, leaves.size());\n            }\n            CodecReader subReader = SlowCodecReaderWrapper.wrap(leaves.get(segmentNumber).reader());\n            iw.addIndexes(new LiveDocsReader(subReader, segmentDocSets.get(segmentNumber)[partitionNumber]));\n          }\n          t.pause();\n        }\n        // we commit explicitly instead of sending a CommitUpdateCommand through the processor chain\n        // because the sub-shard cores will just ignore such a commit because the update log is not\n        // in active state at this time.\n        //TODO no commitUpdateCommand\n        SolrIndexWriter.setCommitData(iw, -1);\n        t = timings.sub(\"subIWCommit\");\n        t.resume();\n        iw.commit();\n        t.pause();\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            t = timings.sub(\"subIWClose\");\n            t.resume();\n            iw.close();\n            t.pause();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n          if (splitMethod == SplitMethod.LINK) {\n            SolrCore subCore = cores.get(partitionNumber);\n            subCore.getDirectoryFactory().release(iw.getDirectory());\n          }\n        }\n      }\n    }\n    // all sub-indexes created ok\n    // when using hard-linking switch directories & refresh cores\n    if (splitMethod == SplitMethod.LINK && cores != null) {\n      boolean switchOk = true;\n      t = timings.sub(\"switchSubIndexes\");\n      for (int partitionNumber = 0; partitionNumber < numPieces; partitionNumber++) {\n        SolrCore subCore = cores.get(partitionNumber);\n        String indexDirPath = subCore.getIndexDir();\n\n        log.debug(\"Switching directories\");\n        String hardLinkPath = subCore.getDataDir() + INDEX_PREFIX + timestamp;\n        subCore.modifyIndexProps(INDEX_PREFIX + timestamp);\n        try {\n          subCore.getUpdateHandler().newIndexWriter(false);\n          openNewSearcher(subCore);\n        } catch (Exception e) {\n          log.error(\"Failed to switch sub-core {} to {}, split will fail\", indexDirPath, hardLinkPath, e);\n          switchOk = false;\n          break;\n        }\n      }\n      t.stop();\n      if (!switchOk) {\n        t = timings.sub(\"rollbackSubIndexes\");\n        // rollback the switch\n        for (int partitionNumber = 0; partitionNumber < numPieces; partitionNumber++) {\n          SolrCore subCore = cores.get(partitionNumber);\n          Directory dir = null;\n          try {\n            dir = subCore.getDirectoryFactory().get(subCore.getDataDir(), DirectoryFactory.DirContext.META_DATA,\n                subCore.getSolrConfig().indexConfig.lockType);\n            dir.deleteFile(IndexFetcher.INDEX_PROPERTIES);\n          } finally {\n            if (dir != null) {\n              subCore.getDirectoryFactory().release(dir);\n            }\n          }\n          // switch back if necessary and remove the hardlinked dir\n          String hardLinkPath = subCore.getDataDir() + INDEX_PREFIX + timestamp;\n          try {\n            dir = subCore.getDirectoryFactory().get(hardLinkPath, DirectoryFactory.DirContext.DEFAULT,\n                subCore.getSolrConfig().indexConfig.lockType);\n            subCore.getDirectoryFactory().doneWithDirectory(dir);\n            subCore.getDirectoryFactory().remove(dir);\n          } finally {\n            if (dir != null) {\n              subCore.getDirectoryFactory().release(dir);\n            }\n          }\n          subCore.getUpdateHandler().newIndexWriter(false);\n          try {\n            openNewSearcher(subCore);\n          } catch (Exception e) {\n            log.warn(\"Error rolling back failed split of {}\", hardLinkPath, e);\n          }\n        }\n        t.stop();\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"There were errors during index split\");\n      } else {\n        // complete the switch - remove original index\n        t = timings.sub(\"cleanSubIndex\");\n        for (int partitionNumber = 0; partitionNumber < numPieces; partitionNumber++) {\n          SolrCore subCore = cores.get(partitionNumber);\n          String oldIndexPath = subCore.getDataDir() + \"index\";\n          Directory indexDir = null;\n          try {\n            indexDir = subCore.getDirectoryFactory().get(oldIndexPath,\n                DirectoryFactory.DirContext.DEFAULT, subCore.getSolrConfig().indexConfig.lockType);\n            subCore.getDirectoryFactory().doneWithDirectory(indexDir);\n            subCore.getDirectoryFactory().remove(indexDir);\n          } finally {\n            if (indexDir != null) {\n              subCore.getDirectoryFactory().release(indexDir);\n            }\n          }\n        }\n        t.stop();\n      }\n    }\n  }\n\n","sourceOld":"  public void doSplit() throws IOException {\n\n    List<LeafReaderContext> leaves = searcher.getRawReader().leaves();\n    Directory parentDirectory = searcher.getRawReader().directory();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n    SolrIndexConfig parentConfig = searcher.getCore().getSolrConfig().indexConfig;\n    String timestamp = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\" + leaves.size());\n    RTimerTree t;\n\n    // this tracks round-robin assignment of docs to partitions\n    AtomicInteger currentPartition = new AtomicInteger();\n\n    if (splitMethod != SplitMethod.LINK) {\n      t = timings.sub(\"findDocSetsPerLeaf\");\n      for (LeafReaderContext readerContext : leaves) {\n        assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n        FixedBitSet[] docSets = split(readerContext, numPieces, field, rangesArr, splitKey, hashRouter, currentPartition, false);\n        segmentDocSets.add(docSets);\n      }\n      t.stop();\n    }\n\n\n    Map<IndexReader.CacheKey, FixedBitSet[]> docsToDeleteCache = new ConcurrentHashMap<>();\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      String partitionName = \"SolrIndexSplitter:partition=\" + partitionNumber + \",partitionCount=\" + numPieces + (ranges != null ? \",range=\" + ranges.get(partitionNumber) : \"\");\n      log.info(partitionName);\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw;\n      if (cores != null && splitMethod != SplitMethod.LINK) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        if (splitMethod == SplitMethod.LINK) {\n          SolrCore subCore = cores.get(partitionNumber);\n          String path = subCore.getDataDir() + INDEX_PREFIX + timestamp;\n          t = timings.sub(\"hardLinkCopy\");\n          t.resume();\n          // copy by hard-linking\n          Directory splitDir = subCore.getDirectoryFactory().get(path, DirectoryFactory.DirContext.DEFAULT, subCore.getSolrConfig().indexConfig.lockType);\n          // the wrapper doesn't hold any resources itself so it doesn't need closing\n          HardlinkCopyDirectoryWrapper hardLinkedDir = new HardlinkCopyDirectoryWrapper(splitDir);\n          boolean copiedOk = false;\n          try {\n            for (String file : parentDirectory.listAll()) {\n              // we've closed the IndexWriter, so ignore write.lock\n              // its file may be present even when IndexWriter is closed but\n              // we've already checked that the lock is not held by anyone else\n              if (file.equals(IndexWriter.WRITE_LOCK_NAME)) {\n                continue;\n              }\n              hardLinkedDir.copyFrom(parentDirectory, file, file, IOContext.DEFAULT);\n            }\n            copiedOk = true;\n          } finally {\n            if (!copiedOk) {\n              subCore.getDirectoryFactory().doneWithDirectory(splitDir);\n              subCore.getDirectoryFactory().remove(splitDir);\n            }\n          }\n          t.pause();\n          IndexWriterConfig iwConfig = parentConfig.toIndexWriterConfig(subCore);\n          // don't run merges at this time\n          iwConfig.setMergePolicy(NoMergePolicy.INSTANCE);\n          t = timings.sub(\"createSubIW\");\n          t.resume();\n          iw = new SolrIndexWriter(partitionName, splitDir, iwConfig);\n          t.pause();\n        } else {\n          SolrCore core = searcher.getCore();\n          String path = paths.get(partitionNumber);\n          t = timings.sub(\"createSubIW\");\n          t.resume();\n          iw = SolrIndexWriter.create(core, partitionName, path,\n              core.getDirectoryFactory(), true, core.getLatestSchema(),\n              core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n          t.pause();\n        }\n      }\n\n      try {\n        if (splitMethod == SplitMethod.LINK) {\n          t = timings.sub(\"deleteDocuments\");\n          t.resume();\n          // apply deletions specific to this partition. As a side-effect on the first call this also populates\n          // a cache of docsets to delete per leaf reader per partition, which is reused for subsequent partitions.\n          iw.deleteDocuments(new SplittingQuery(partitionNumber, field, rangesArr, hashRouter, splitKey, docsToDeleteCache, currentPartition));\n          t.pause();\n        } else {\n          // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n          t = timings.sub(\"addIndexes\");\n          t.resume();\n          for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n            log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n            CodecReader subReader = SlowCodecReaderWrapper.wrap(leaves.get(segmentNumber).reader());\n            iw.addIndexes(new LiveDocsReader(subReader, segmentDocSets.get(segmentNumber)[partitionNumber]));\n          }\n          t.pause();\n        }\n        // we commit explicitly instead of sending a CommitUpdateCommand through the processor chain\n        // because the sub-shard cores will just ignore such a commit because the update log is not\n        // in active state at this time.\n        //TODO no commitUpdateCommand\n        SolrIndexWriter.setCommitData(iw, -1);\n        t = timings.sub(\"subIWCommit\");\n        t.resume();\n        iw.commit();\n        t.pause();\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            t = timings.sub(\"subIWClose\");\n            t.resume();\n            iw.close();\n            t.pause();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n          if (splitMethod == SplitMethod.LINK) {\n            SolrCore subCore = cores.get(partitionNumber);\n            subCore.getDirectoryFactory().release(iw.getDirectory());\n          }\n        }\n      }\n    }\n    // all sub-indexes created ok\n    // when using hard-linking switch directories & refresh cores\n    if (splitMethod == SplitMethod.LINK && cores != null) {\n      boolean switchOk = true;\n      t = timings.sub(\"switchSubIndexes\");\n      for (int partitionNumber = 0; partitionNumber < numPieces; partitionNumber++) {\n        SolrCore subCore = cores.get(partitionNumber);\n        String indexDirPath = subCore.getIndexDir();\n\n        log.debug(\"Switching directories\");\n        String hardLinkPath = subCore.getDataDir() + INDEX_PREFIX + timestamp;\n        subCore.modifyIndexProps(INDEX_PREFIX + timestamp);\n        try {\n          subCore.getUpdateHandler().newIndexWriter(false);\n          openNewSearcher(subCore);\n        } catch (Exception e) {\n          log.error(\"Failed to switch sub-core \" + indexDirPath + \" to \" + hardLinkPath + \", split will fail.\", e);\n          switchOk = false;\n          break;\n        }\n      }\n      t.stop();\n      if (!switchOk) {\n        t = timings.sub(\"rollbackSubIndexes\");\n        // rollback the switch\n        for (int partitionNumber = 0; partitionNumber < numPieces; partitionNumber++) {\n          SolrCore subCore = cores.get(partitionNumber);\n          Directory dir = null;\n          try {\n            dir = subCore.getDirectoryFactory().get(subCore.getDataDir(), DirectoryFactory.DirContext.META_DATA,\n                subCore.getSolrConfig().indexConfig.lockType);\n            dir.deleteFile(IndexFetcher.INDEX_PROPERTIES);\n          } finally {\n            if (dir != null) {\n              subCore.getDirectoryFactory().release(dir);\n            }\n          }\n          // switch back if necessary and remove the hardlinked dir\n          String hardLinkPath = subCore.getDataDir() + INDEX_PREFIX + timestamp;\n          try {\n            dir = subCore.getDirectoryFactory().get(hardLinkPath, DirectoryFactory.DirContext.DEFAULT,\n                subCore.getSolrConfig().indexConfig.lockType);\n            subCore.getDirectoryFactory().doneWithDirectory(dir);\n            subCore.getDirectoryFactory().remove(dir);\n          } finally {\n            if (dir != null) {\n              subCore.getDirectoryFactory().release(dir);\n            }\n          }\n          subCore.getUpdateHandler().newIndexWriter(false);\n          try {\n            openNewSearcher(subCore);\n          } catch (Exception e) {\n            log.warn(\"Error rolling back failed split of \" + hardLinkPath, e);\n          }\n        }\n        t.stop();\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"There were errors during index split\");\n      } else {\n        // complete the switch - remove original index\n        t = timings.sub(\"cleanSubIndex\");\n        for (int partitionNumber = 0; partitionNumber < numPieces; partitionNumber++) {\n          SolrCore subCore = cores.get(partitionNumber);\n          String oldIndexPath = subCore.getDataDir() + \"index\";\n          Directory indexDir = null;\n          try {\n            indexDir = subCore.getDirectoryFactory().get(oldIndexPath,\n                DirectoryFactory.DirContext.DEFAULT, subCore.getSolrConfig().indexConfig.lockType);\n            subCore.getDirectoryFactory().doneWithDirectory(indexDir);\n            subCore.getDirectoryFactory().remove(indexDir);\n          } finally {\n            if (indexDir != null) {\n              subCore.getDirectoryFactory().release(indexDir);\n            }\n          }\n        }\n        t.stop();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"575e66bd4b2349209027f6801184da7fc3cba13f","date":1587609169,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#doSplit().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#doSplit().mjava","sourceNew":"  public void doSplit() throws IOException {\n\n    List<LeafReaderContext> leaves = searcher.getRawReader().leaves();\n    Directory parentDirectory = searcher.getRawReader().directory();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n    SolrIndexConfig parentConfig = searcher.getCore().getSolrConfig().indexConfig;\n    String timestamp = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date());\n\n    if (log.isInfoEnabled()) {\n      log.info(\"SolrIndexSplitter: partitions={} segments={}\", numPieces, leaves.size());\n    }\n    RTimerTree t;\n\n    // this tracks round-robin assignment of docs to partitions\n    AtomicInteger currentPartition = new AtomicInteger();\n\n    if (splitMethod != SplitMethod.LINK) {\n      t = timings.sub(\"findDocSetsPerLeaf\");\n      for (LeafReaderContext readerContext : leaves) {\n        assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n        FixedBitSet[] docSets = split(readerContext, numPieces, field, rangesArr, splitKey, hashRouter, currentPartition, false);\n        segmentDocSets.add(docSets);\n      }\n      t.stop();\n    }\n\n\n    Map<IndexReader.CacheKey, FixedBitSet[]> docsToDeleteCache = new ConcurrentHashMap<>();\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      String partitionName = \"SolrIndexSplitter:partition=\" + partitionNumber + \",partitionCount=\" + numPieces + (ranges != null ? \",range=\" + ranges.get(partitionNumber) : \"\");\n      log.info(partitionName);\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw;\n      if (cores != null && splitMethod != SplitMethod.LINK) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        if (splitMethod == SplitMethod.LINK) {\n          SolrCore subCore = cores.get(partitionNumber);\n          String path = subCore.getDataDir() + INDEX_PREFIX + timestamp;\n          t = timings.sub(\"hardLinkCopy\");\n          t.resume();\n          // copy by hard-linking\n          Directory splitDir = subCore.getDirectoryFactory().get(path, DirectoryFactory.DirContext.DEFAULT, subCore.getSolrConfig().indexConfig.lockType);\n          // the wrapper doesn't hold any resources itself so it doesn't need closing\n          HardlinkCopyDirectoryWrapper hardLinkedDir = new HardlinkCopyDirectoryWrapper(splitDir);\n          boolean copiedOk = false;\n          try {\n            for (String file : parentDirectory.listAll()) {\n              // we've closed the IndexWriter, so ignore write.lock\n              // its file may be present even when IndexWriter is closed but\n              // we've already checked that the lock is not held by anyone else\n              if (file.equals(IndexWriter.WRITE_LOCK_NAME)) {\n                continue;\n              }\n              hardLinkedDir.copyFrom(parentDirectory, file, file, IOContext.DEFAULT);\n            }\n            copiedOk = true;\n          } finally {\n            if (!copiedOk) {\n              subCore.getDirectoryFactory().doneWithDirectory(splitDir);\n              subCore.getDirectoryFactory().remove(splitDir);\n            }\n          }\n          t.pause();\n          IndexWriterConfig iwConfig = parentConfig.toIndexWriterConfig(subCore);\n          // don't run merges at this time\n          iwConfig.setMergePolicy(NoMergePolicy.INSTANCE);\n          t = timings.sub(\"createSubIW\");\n          t.resume();\n          iw = new SolrIndexWriter(partitionName, splitDir, iwConfig);\n          t.pause();\n        } else {\n          SolrCore core = searcher.getCore();\n          String path = paths.get(partitionNumber);\n          t = timings.sub(\"createSubIW\");\n          t.resume();\n          iw = SolrIndexWriter.create(core, partitionName, path,\n              core.getDirectoryFactory(), true, core.getLatestSchema(),\n              core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n          t.pause();\n        }\n      }\n\n      try {\n        if (splitMethod == SplitMethod.LINK) {\n          t = timings.sub(\"deleteDocuments\");\n          t.resume();\n          // apply deletions specific to this partition. As a side-effect on the first call this also populates\n          // a cache of docsets to delete per leaf reader per partition, which is reused for subsequent partitions.\n          iw.deleteDocuments(new SplittingQuery(partitionNumber, field, rangesArr, hashRouter, splitKey, docsToDeleteCache, currentPartition));\n          t.pause();\n        } else {\n          // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n          t = timings.sub(\"addIndexes\");\n          t.resume();\n          for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n            if (log.isInfoEnabled()) {\n              log.info(\"SolrIndexSplitter: partition #{} partitionCount={} segment #{} segmentCount={}\"\n              , partitionNumber, numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\")\n              , segmentNumber, leaves.size()); //LOGOK\n            }\n            CodecReader subReader = SlowCodecReaderWrapper.wrap(leaves.get(segmentNumber).reader());\n            iw.addIndexes(new LiveDocsReader(subReader, segmentDocSets.get(segmentNumber)[partitionNumber]));\n          }\n          t.pause();\n        }\n        // we commit explicitly instead of sending a CommitUpdateCommand through the processor chain\n        // because the sub-shard cores will just ignore such a commit because the update log is not\n        // in active state at this time.\n        //TODO no commitUpdateCommand\n        SolrIndexWriter.setCommitData(iw, -1);\n        t = timings.sub(\"subIWCommit\");\n        t.resume();\n        iw.commit();\n        t.pause();\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            t = timings.sub(\"subIWClose\");\n            t.resume();\n            iw.close();\n            t.pause();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n          if (splitMethod == SplitMethod.LINK) {\n            SolrCore subCore = cores.get(partitionNumber);\n            subCore.getDirectoryFactory().release(iw.getDirectory());\n          }\n        }\n      }\n    }\n    // all sub-indexes created ok\n    // when using hard-linking switch directories & refresh cores\n    if (splitMethod == SplitMethod.LINK && cores != null) {\n      boolean switchOk = true;\n      t = timings.sub(\"switchSubIndexes\");\n      for (int partitionNumber = 0; partitionNumber < numPieces; partitionNumber++) {\n        SolrCore subCore = cores.get(partitionNumber);\n        String indexDirPath = subCore.getIndexDir();\n\n        log.debug(\"Switching directories\");\n        String hardLinkPath = subCore.getDataDir() + INDEX_PREFIX + timestamp;\n        subCore.modifyIndexProps(INDEX_PREFIX + timestamp);\n        try {\n          subCore.getUpdateHandler().newIndexWriter(false);\n          openNewSearcher(subCore);\n        } catch (Exception e) {\n          log.error(\"Failed to switch sub-core {} to {}, split will fail\", indexDirPath, hardLinkPath, e);\n          switchOk = false;\n          break;\n        }\n      }\n      t.stop();\n      if (!switchOk) {\n        t = timings.sub(\"rollbackSubIndexes\");\n        // rollback the switch\n        for (int partitionNumber = 0; partitionNumber < numPieces; partitionNumber++) {\n          SolrCore subCore = cores.get(partitionNumber);\n          Directory dir = null;\n          try {\n            dir = subCore.getDirectoryFactory().get(subCore.getDataDir(), DirectoryFactory.DirContext.META_DATA,\n                subCore.getSolrConfig().indexConfig.lockType);\n            dir.deleteFile(IndexFetcher.INDEX_PROPERTIES);\n          } finally {\n            if (dir != null) {\n              subCore.getDirectoryFactory().release(dir);\n            }\n          }\n          // switch back if necessary and remove the hardlinked dir\n          String hardLinkPath = subCore.getDataDir() + INDEX_PREFIX + timestamp;\n          try {\n            dir = subCore.getDirectoryFactory().get(hardLinkPath, DirectoryFactory.DirContext.DEFAULT,\n                subCore.getSolrConfig().indexConfig.lockType);\n            subCore.getDirectoryFactory().doneWithDirectory(dir);\n            subCore.getDirectoryFactory().remove(dir);\n          } finally {\n            if (dir != null) {\n              subCore.getDirectoryFactory().release(dir);\n            }\n          }\n          subCore.getUpdateHandler().newIndexWriter(false);\n          try {\n            openNewSearcher(subCore);\n          } catch (Exception e) {\n            log.warn(\"Error rolling back failed split of {}\", hardLinkPath, e);\n          }\n        }\n        t.stop();\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"There were errors during index split\");\n      } else {\n        // complete the switch - remove original index\n        t = timings.sub(\"cleanSubIndex\");\n        for (int partitionNumber = 0; partitionNumber < numPieces; partitionNumber++) {\n          SolrCore subCore = cores.get(partitionNumber);\n          String oldIndexPath = subCore.getDataDir() + \"index\";\n          Directory indexDir = null;\n          try {\n            indexDir = subCore.getDirectoryFactory().get(oldIndexPath,\n                DirectoryFactory.DirContext.DEFAULT, subCore.getSolrConfig().indexConfig.lockType);\n            subCore.getDirectoryFactory().doneWithDirectory(indexDir);\n            subCore.getDirectoryFactory().remove(indexDir);\n          } finally {\n            if (indexDir != null) {\n              subCore.getDirectoryFactory().release(indexDir);\n            }\n          }\n        }\n        t.stop();\n      }\n    }\n  }\n\n","sourceOld":"  public void doSplit() throws IOException {\n\n    List<LeafReaderContext> leaves = searcher.getRawReader().leaves();\n    Directory parentDirectory = searcher.getRawReader().directory();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n    SolrIndexConfig parentConfig = searcher.getCore().getSolrConfig().indexConfig;\n    String timestamp = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date());\n\n    if (log.isInfoEnabled()) {\n      log.info(\"SolrIndexSplitter: partitions={} segments={}\", numPieces, leaves.size());\n    }\n    RTimerTree t;\n\n    // this tracks round-robin assignment of docs to partitions\n    AtomicInteger currentPartition = new AtomicInteger();\n\n    if (splitMethod != SplitMethod.LINK) {\n      t = timings.sub(\"findDocSetsPerLeaf\");\n      for (LeafReaderContext readerContext : leaves) {\n        assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n        FixedBitSet[] docSets = split(readerContext, numPieces, field, rangesArr, splitKey, hashRouter, currentPartition, false);\n        segmentDocSets.add(docSets);\n      }\n      t.stop();\n    }\n\n\n    Map<IndexReader.CacheKey, FixedBitSet[]> docsToDeleteCache = new ConcurrentHashMap<>();\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      String partitionName = \"SolrIndexSplitter:partition=\" + partitionNumber + \",partitionCount=\" + numPieces + (ranges != null ? \",range=\" + ranges.get(partitionNumber) : \"\");\n      log.info(partitionName);\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw;\n      if (cores != null && splitMethod != SplitMethod.LINK) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        if (splitMethod == SplitMethod.LINK) {\n          SolrCore subCore = cores.get(partitionNumber);\n          String path = subCore.getDataDir() + INDEX_PREFIX + timestamp;\n          t = timings.sub(\"hardLinkCopy\");\n          t.resume();\n          // copy by hard-linking\n          Directory splitDir = subCore.getDirectoryFactory().get(path, DirectoryFactory.DirContext.DEFAULT, subCore.getSolrConfig().indexConfig.lockType);\n          // the wrapper doesn't hold any resources itself so it doesn't need closing\n          HardlinkCopyDirectoryWrapper hardLinkedDir = new HardlinkCopyDirectoryWrapper(splitDir);\n          boolean copiedOk = false;\n          try {\n            for (String file : parentDirectory.listAll()) {\n              // we've closed the IndexWriter, so ignore write.lock\n              // its file may be present even when IndexWriter is closed but\n              // we've already checked that the lock is not held by anyone else\n              if (file.equals(IndexWriter.WRITE_LOCK_NAME)) {\n                continue;\n              }\n              hardLinkedDir.copyFrom(parentDirectory, file, file, IOContext.DEFAULT);\n            }\n            copiedOk = true;\n          } finally {\n            if (!copiedOk) {\n              subCore.getDirectoryFactory().doneWithDirectory(splitDir);\n              subCore.getDirectoryFactory().remove(splitDir);\n            }\n          }\n          t.pause();\n          IndexWriterConfig iwConfig = parentConfig.toIndexWriterConfig(subCore);\n          // don't run merges at this time\n          iwConfig.setMergePolicy(NoMergePolicy.INSTANCE);\n          t = timings.sub(\"createSubIW\");\n          t.resume();\n          iw = new SolrIndexWriter(partitionName, splitDir, iwConfig);\n          t.pause();\n        } else {\n          SolrCore core = searcher.getCore();\n          String path = paths.get(partitionNumber);\n          t = timings.sub(\"createSubIW\");\n          t.resume();\n          iw = SolrIndexWriter.create(core, partitionName, path,\n              core.getDirectoryFactory(), true, core.getLatestSchema(),\n              core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n          t.pause();\n        }\n      }\n\n      try {\n        if (splitMethod == SplitMethod.LINK) {\n          t = timings.sub(\"deleteDocuments\");\n          t.resume();\n          // apply deletions specific to this partition. As a side-effect on the first call this also populates\n          // a cache of docsets to delete per leaf reader per partition, which is reused for subsequent partitions.\n          iw.deleteDocuments(new SplittingQuery(partitionNumber, field, rangesArr, hashRouter, splitKey, docsToDeleteCache, currentPartition));\n          t.pause();\n        } else {\n          // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n          t = timings.sub(\"addIndexes\");\n          t.resume();\n          for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n            if (log.isInfoEnabled()) {\n              log.info(\"SolrIndexSplitter: partition #{} partitionCount={} segment #{} segmentCount={}\"\n              , partitionNumber, numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\")\n              , segmentNumber, leaves.size());\n            }\n            CodecReader subReader = SlowCodecReaderWrapper.wrap(leaves.get(segmentNumber).reader());\n            iw.addIndexes(new LiveDocsReader(subReader, segmentDocSets.get(segmentNumber)[partitionNumber]));\n          }\n          t.pause();\n        }\n        // we commit explicitly instead of sending a CommitUpdateCommand through the processor chain\n        // because the sub-shard cores will just ignore such a commit because the update log is not\n        // in active state at this time.\n        //TODO no commitUpdateCommand\n        SolrIndexWriter.setCommitData(iw, -1);\n        t = timings.sub(\"subIWCommit\");\n        t.resume();\n        iw.commit();\n        t.pause();\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            t = timings.sub(\"subIWClose\");\n            t.resume();\n            iw.close();\n            t.pause();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n          if (splitMethod == SplitMethod.LINK) {\n            SolrCore subCore = cores.get(partitionNumber);\n            subCore.getDirectoryFactory().release(iw.getDirectory());\n          }\n        }\n      }\n    }\n    // all sub-indexes created ok\n    // when using hard-linking switch directories & refresh cores\n    if (splitMethod == SplitMethod.LINK && cores != null) {\n      boolean switchOk = true;\n      t = timings.sub(\"switchSubIndexes\");\n      for (int partitionNumber = 0; partitionNumber < numPieces; partitionNumber++) {\n        SolrCore subCore = cores.get(partitionNumber);\n        String indexDirPath = subCore.getIndexDir();\n\n        log.debug(\"Switching directories\");\n        String hardLinkPath = subCore.getDataDir() + INDEX_PREFIX + timestamp;\n        subCore.modifyIndexProps(INDEX_PREFIX + timestamp);\n        try {\n          subCore.getUpdateHandler().newIndexWriter(false);\n          openNewSearcher(subCore);\n        } catch (Exception e) {\n          log.error(\"Failed to switch sub-core {} to {}, split will fail\", indexDirPath, hardLinkPath, e);\n          switchOk = false;\n          break;\n        }\n      }\n      t.stop();\n      if (!switchOk) {\n        t = timings.sub(\"rollbackSubIndexes\");\n        // rollback the switch\n        for (int partitionNumber = 0; partitionNumber < numPieces; partitionNumber++) {\n          SolrCore subCore = cores.get(partitionNumber);\n          Directory dir = null;\n          try {\n            dir = subCore.getDirectoryFactory().get(subCore.getDataDir(), DirectoryFactory.DirContext.META_DATA,\n                subCore.getSolrConfig().indexConfig.lockType);\n            dir.deleteFile(IndexFetcher.INDEX_PROPERTIES);\n          } finally {\n            if (dir != null) {\n              subCore.getDirectoryFactory().release(dir);\n            }\n          }\n          // switch back if necessary and remove the hardlinked dir\n          String hardLinkPath = subCore.getDataDir() + INDEX_PREFIX + timestamp;\n          try {\n            dir = subCore.getDirectoryFactory().get(hardLinkPath, DirectoryFactory.DirContext.DEFAULT,\n                subCore.getSolrConfig().indexConfig.lockType);\n            subCore.getDirectoryFactory().doneWithDirectory(dir);\n            subCore.getDirectoryFactory().remove(dir);\n          } finally {\n            if (dir != null) {\n              subCore.getDirectoryFactory().release(dir);\n            }\n          }\n          subCore.getUpdateHandler().newIndexWriter(false);\n          try {\n            openNewSearcher(subCore);\n          } catch (Exception e) {\n            log.warn(\"Error rolling back failed split of {}\", hardLinkPath, e);\n          }\n        }\n        t.stop();\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"There were errors during index split\");\n      } else {\n        // complete the switch - remove original index\n        t = timings.sub(\"cleanSubIndex\");\n        for (int partitionNumber = 0; partitionNumber < numPieces; partitionNumber++) {\n          SolrCore subCore = cores.get(partitionNumber);\n          String oldIndexPath = subCore.getDataDir() + \"index\";\n          Directory indexDir = null;\n          try {\n            indexDir = subCore.getDirectoryFactory().get(oldIndexPath,\n                DirectoryFactory.DirContext.DEFAULT, subCore.getSolrConfig().indexConfig.lockType);\n            subCore.getDirectoryFactory().doneWithDirectory(indexDir);\n            subCore.getDirectoryFactory().remove(indexDir);\n          } finally {\n            if (indexDir != null) {\n              subCore.getDirectoryFactory().release(indexDir);\n            }\n          }\n        }\n        t.stop();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe9f2c4a0d7ac164e4bdd4eee7f87131aec83fd4","date":1588172214,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#doSplit().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#doSplit().mjava","sourceNew":"  public void doSplit() throws IOException {\n\n    List<LeafReaderContext> leaves = searcher.getRawReader().leaves();\n    Directory parentDirectory = searcher.getRawReader().directory();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n    SolrIndexConfig parentConfig = searcher.getCore().getSolrConfig().indexConfig;\n    String timestamp = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date());\n\n    if (log.isInfoEnabled()) {\n      log.info(\"SolrIndexSplitter: partitions={} segments={}\", numPieces, leaves.size());\n    }\n    RTimerTree t;\n\n    // this tracks round-robin assignment of docs to partitions\n    AtomicInteger currentPartition = new AtomicInteger();\n\n    if (splitMethod != SplitMethod.LINK) {\n      t = timings.sub(\"findDocSetsPerLeaf\");\n      for (LeafReaderContext readerContext : leaves) {\n        assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n        FixedBitSet[] docSets = split(readerContext, numPieces, field, rangesArr, splitKey, hashRouter, currentPartition, false);\n        segmentDocSets.add(docSets);\n      }\n      t.stop();\n    }\n\n\n    Map<IndexReader.CacheKey, FixedBitSet[]> docsToDeleteCache = new ConcurrentHashMap<>();\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      String partitionName = \"SolrIndexSplitter:partition=\" + partitionNumber + \",partitionCount=\" + numPieces + (ranges != null ? \",range=\" + ranges.get(partitionNumber) : \"\");\n      log.info(partitionName);\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw;\n      if (cores != null && splitMethod != SplitMethod.LINK) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        if (splitMethod == SplitMethod.LINK) {\n          SolrCore subCore = cores.get(partitionNumber);\n          String path = subCore.getDataDir() + INDEX_PREFIX + timestamp;\n          t = timings.sub(\"hardLinkCopy\");\n          t.resume();\n          // copy by hard-linking\n          Directory splitDir = subCore.getDirectoryFactory().get(path, DirectoryFactory.DirContext.DEFAULT, subCore.getSolrConfig().indexConfig.lockType);\n          // the wrapper doesn't hold any resources itself so it doesn't need closing\n          HardlinkCopyDirectoryWrapper hardLinkedDir = new HardlinkCopyDirectoryWrapper(splitDir);\n          boolean copiedOk = false;\n          try {\n            for (String file : parentDirectory.listAll()) {\n              // we've closed the IndexWriter, so ignore write.lock\n              // its file may be present even when IndexWriter is closed but\n              // we've already checked that the lock is not held by anyone else\n              if (file.equals(IndexWriter.WRITE_LOCK_NAME)) {\n                continue;\n              }\n              hardLinkedDir.copyFrom(parentDirectory, file, file, IOContext.DEFAULT);\n            }\n            copiedOk = true;\n          } finally {\n            if (!copiedOk) {\n              subCore.getDirectoryFactory().doneWithDirectory(splitDir);\n              subCore.getDirectoryFactory().remove(splitDir);\n            }\n          }\n          t.pause();\n          IndexWriterConfig iwConfig = parentConfig.toIndexWriterConfig(subCore);\n          // don't run merges at this time\n          iwConfig.setMergePolicy(NoMergePolicy.INSTANCE);\n          t = timings.sub(\"createSubIW\");\n          t.resume();\n          iw = new SolrIndexWriter(partitionName, splitDir, iwConfig);\n          t.pause();\n        } else {\n          SolrCore core = searcher.getCore();\n          String path = paths.get(partitionNumber);\n          t = timings.sub(\"createSubIW\");\n          t.resume();\n          iw = SolrIndexWriter.create(core, partitionName, path,\n              core.getDirectoryFactory(), true, core.getLatestSchema(),\n              core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n          t.pause();\n        }\n      }\n\n      try {\n        if (splitMethod == SplitMethod.LINK) {\n          t = timings.sub(\"deleteDocuments\");\n          t.resume();\n          // apply deletions specific to this partition. As a side-effect on the first call this also populates\n          // a cache of docsets to delete per leaf reader per partition, which is reused for subsequent partitions.\n          iw.deleteDocuments(new SplittingQuery(partitionNumber, field, rangesArr, hashRouter, splitKey, docsToDeleteCache, currentPartition));\n          t.pause();\n        } else {\n          // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n          t = timings.sub(\"addIndexes\");\n          t.resume();\n          for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n            if (log.isInfoEnabled()) {\n              log.info(\"SolrIndexSplitter: partition # {} partitionCount={} {} segment #={} segmentCount={}\", partitionNumber, numPieces\n                  , (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"), segmentNumber, leaves.size()); // logOk\n            }\n            CodecReader subReader = SlowCodecReaderWrapper.wrap(leaves.get(segmentNumber).reader());\n            iw.addIndexes(new LiveDocsReader(subReader, segmentDocSets.get(segmentNumber)[partitionNumber]));\n          }\n          t.pause();\n        }\n        // we commit explicitly instead of sending a CommitUpdateCommand through the processor chain\n        // because the sub-shard cores will just ignore such a commit because the update log is not\n        // in active state at this time.\n        //TODO no commitUpdateCommand\n        SolrIndexWriter.setCommitData(iw, -1);\n        t = timings.sub(\"subIWCommit\");\n        t.resume();\n        iw.commit();\n        t.pause();\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            t = timings.sub(\"subIWClose\");\n            t.resume();\n            iw.close();\n            t.pause();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n          if (splitMethod == SplitMethod.LINK) {\n            SolrCore subCore = cores.get(partitionNumber);\n            subCore.getDirectoryFactory().release(iw.getDirectory());\n          }\n        }\n      }\n    }\n    // all sub-indexes created ok\n    // when using hard-linking switch directories & refresh cores\n    if (splitMethod == SplitMethod.LINK && cores != null) {\n      boolean switchOk = true;\n      t = timings.sub(\"switchSubIndexes\");\n      for (int partitionNumber = 0; partitionNumber < numPieces; partitionNumber++) {\n        SolrCore subCore = cores.get(partitionNumber);\n        String indexDirPath = subCore.getIndexDir();\n\n        log.debug(\"Switching directories\");\n        String hardLinkPath = subCore.getDataDir() + INDEX_PREFIX + timestamp;\n        subCore.modifyIndexProps(INDEX_PREFIX + timestamp);\n        try {\n          subCore.getUpdateHandler().newIndexWriter(false);\n          openNewSearcher(subCore);\n        } catch (Exception e) {\n          log.error(\"Failed to switch sub-core {} to {}, split will fail\", indexDirPath, hardLinkPath, e);\n          switchOk = false;\n          break;\n        }\n      }\n      t.stop();\n      if (!switchOk) {\n        t = timings.sub(\"rollbackSubIndexes\");\n        // rollback the switch\n        for (int partitionNumber = 0; partitionNumber < numPieces; partitionNumber++) {\n          SolrCore subCore = cores.get(partitionNumber);\n          Directory dir = null;\n          try {\n            dir = subCore.getDirectoryFactory().get(subCore.getDataDir(), DirectoryFactory.DirContext.META_DATA,\n                subCore.getSolrConfig().indexConfig.lockType);\n            dir.deleteFile(IndexFetcher.INDEX_PROPERTIES);\n          } finally {\n            if (dir != null) {\n              subCore.getDirectoryFactory().release(dir);\n            }\n          }\n          // switch back if necessary and remove the hardlinked dir\n          String hardLinkPath = subCore.getDataDir() + INDEX_PREFIX + timestamp;\n          try {\n            dir = subCore.getDirectoryFactory().get(hardLinkPath, DirectoryFactory.DirContext.DEFAULT,\n                subCore.getSolrConfig().indexConfig.lockType);\n            subCore.getDirectoryFactory().doneWithDirectory(dir);\n            subCore.getDirectoryFactory().remove(dir);\n          } finally {\n            if (dir != null) {\n              subCore.getDirectoryFactory().release(dir);\n            }\n          }\n          subCore.getUpdateHandler().newIndexWriter(false);\n          try {\n            openNewSearcher(subCore);\n          } catch (Exception e) {\n            log.warn(\"Error rolling back failed split of {}\", hardLinkPath, e);\n          }\n        }\n        t.stop();\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"There were errors during index split\");\n      } else {\n        // complete the switch - remove original index\n        t = timings.sub(\"cleanSubIndex\");\n        for (int partitionNumber = 0; partitionNumber < numPieces; partitionNumber++) {\n          SolrCore subCore = cores.get(partitionNumber);\n          String oldIndexPath = subCore.getDataDir() + \"index\";\n          Directory indexDir = null;\n          try {\n            indexDir = subCore.getDirectoryFactory().get(oldIndexPath,\n                DirectoryFactory.DirContext.DEFAULT, subCore.getSolrConfig().indexConfig.lockType);\n            subCore.getDirectoryFactory().doneWithDirectory(indexDir);\n            subCore.getDirectoryFactory().remove(indexDir);\n          } finally {\n            if (indexDir != null) {\n              subCore.getDirectoryFactory().release(indexDir);\n            }\n          }\n        }\n        t.stop();\n      }\n    }\n  }\n\n","sourceOld":"  public void doSplit() throws IOException {\n\n    List<LeafReaderContext> leaves = searcher.getRawReader().leaves();\n    Directory parentDirectory = searcher.getRawReader().directory();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n    SolrIndexConfig parentConfig = searcher.getCore().getSolrConfig().indexConfig;\n    String timestamp = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date());\n\n    if (log.isInfoEnabled()) {\n      log.info(\"SolrIndexSplitter: partitions={} segments={}\", numPieces, leaves.size());\n    }\n    RTimerTree t;\n\n    // this tracks round-robin assignment of docs to partitions\n    AtomicInteger currentPartition = new AtomicInteger();\n\n    if (splitMethod != SplitMethod.LINK) {\n      t = timings.sub(\"findDocSetsPerLeaf\");\n      for (LeafReaderContext readerContext : leaves) {\n        assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n        FixedBitSet[] docSets = split(readerContext, numPieces, field, rangesArr, splitKey, hashRouter, currentPartition, false);\n        segmentDocSets.add(docSets);\n      }\n      t.stop();\n    }\n\n\n    Map<IndexReader.CacheKey, FixedBitSet[]> docsToDeleteCache = new ConcurrentHashMap<>();\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      String partitionName = \"SolrIndexSplitter:partition=\" + partitionNumber + \",partitionCount=\" + numPieces + (ranges != null ? \",range=\" + ranges.get(partitionNumber) : \"\");\n      log.info(partitionName);\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw;\n      if (cores != null && splitMethod != SplitMethod.LINK) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        if (splitMethod == SplitMethod.LINK) {\n          SolrCore subCore = cores.get(partitionNumber);\n          String path = subCore.getDataDir() + INDEX_PREFIX + timestamp;\n          t = timings.sub(\"hardLinkCopy\");\n          t.resume();\n          // copy by hard-linking\n          Directory splitDir = subCore.getDirectoryFactory().get(path, DirectoryFactory.DirContext.DEFAULT, subCore.getSolrConfig().indexConfig.lockType);\n          // the wrapper doesn't hold any resources itself so it doesn't need closing\n          HardlinkCopyDirectoryWrapper hardLinkedDir = new HardlinkCopyDirectoryWrapper(splitDir);\n          boolean copiedOk = false;\n          try {\n            for (String file : parentDirectory.listAll()) {\n              // we've closed the IndexWriter, so ignore write.lock\n              // its file may be present even when IndexWriter is closed but\n              // we've already checked that the lock is not held by anyone else\n              if (file.equals(IndexWriter.WRITE_LOCK_NAME)) {\n                continue;\n              }\n              hardLinkedDir.copyFrom(parentDirectory, file, file, IOContext.DEFAULT);\n            }\n            copiedOk = true;\n          } finally {\n            if (!copiedOk) {\n              subCore.getDirectoryFactory().doneWithDirectory(splitDir);\n              subCore.getDirectoryFactory().remove(splitDir);\n            }\n          }\n          t.pause();\n          IndexWriterConfig iwConfig = parentConfig.toIndexWriterConfig(subCore);\n          // don't run merges at this time\n          iwConfig.setMergePolicy(NoMergePolicy.INSTANCE);\n          t = timings.sub(\"createSubIW\");\n          t.resume();\n          iw = new SolrIndexWriter(partitionName, splitDir, iwConfig);\n          t.pause();\n        } else {\n          SolrCore core = searcher.getCore();\n          String path = paths.get(partitionNumber);\n          t = timings.sub(\"createSubIW\");\n          t.resume();\n          iw = SolrIndexWriter.create(core, partitionName, path,\n              core.getDirectoryFactory(), true, core.getLatestSchema(),\n              core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n          t.pause();\n        }\n      }\n\n      try {\n        if (splitMethod == SplitMethod.LINK) {\n          t = timings.sub(\"deleteDocuments\");\n          t.resume();\n          // apply deletions specific to this partition. As a side-effect on the first call this also populates\n          // a cache of docsets to delete per leaf reader per partition, which is reused for subsequent partitions.\n          iw.deleteDocuments(new SplittingQuery(partitionNumber, field, rangesArr, hashRouter, splitKey, docsToDeleteCache, currentPartition));\n          t.pause();\n        } else {\n          // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n          t = timings.sub(\"addIndexes\");\n          t.resume();\n          for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n            if (log.isInfoEnabled()) {\n              log.info(\"SolrIndexSplitter: partition #{} partitionCount={} segment #{} segmentCount={}\"\n              , partitionNumber, numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\")\n              , segmentNumber, leaves.size()); //LOGOK\n            }\n            CodecReader subReader = SlowCodecReaderWrapper.wrap(leaves.get(segmentNumber).reader());\n            iw.addIndexes(new LiveDocsReader(subReader, segmentDocSets.get(segmentNumber)[partitionNumber]));\n          }\n          t.pause();\n        }\n        // we commit explicitly instead of sending a CommitUpdateCommand through the processor chain\n        // because the sub-shard cores will just ignore such a commit because the update log is not\n        // in active state at this time.\n        //TODO no commitUpdateCommand\n        SolrIndexWriter.setCommitData(iw, -1);\n        t = timings.sub(\"subIWCommit\");\n        t.resume();\n        iw.commit();\n        t.pause();\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            t = timings.sub(\"subIWClose\");\n            t.resume();\n            iw.close();\n            t.pause();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n          if (splitMethod == SplitMethod.LINK) {\n            SolrCore subCore = cores.get(partitionNumber);\n            subCore.getDirectoryFactory().release(iw.getDirectory());\n          }\n        }\n      }\n    }\n    // all sub-indexes created ok\n    // when using hard-linking switch directories & refresh cores\n    if (splitMethod == SplitMethod.LINK && cores != null) {\n      boolean switchOk = true;\n      t = timings.sub(\"switchSubIndexes\");\n      for (int partitionNumber = 0; partitionNumber < numPieces; partitionNumber++) {\n        SolrCore subCore = cores.get(partitionNumber);\n        String indexDirPath = subCore.getIndexDir();\n\n        log.debug(\"Switching directories\");\n        String hardLinkPath = subCore.getDataDir() + INDEX_PREFIX + timestamp;\n        subCore.modifyIndexProps(INDEX_PREFIX + timestamp);\n        try {\n          subCore.getUpdateHandler().newIndexWriter(false);\n          openNewSearcher(subCore);\n        } catch (Exception e) {\n          log.error(\"Failed to switch sub-core {} to {}, split will fail\", indexDirPath, hardLinkPath, e);\n          switchOk = false;\n          break;\n        }\n      }\n      t.stop();\n      if (!switchOk) {\n        t = timings.sub(\"rollbackSubIndexes\");\n        // rollback the switch\n        for (int partitionNumber = 0; partitionNumber < numPieces; partitionNumber++) {\n          SolrCore subCore = cores.get(partitionNumber);\n          Directory dir = null;\n          try {\n            dir = subCore.getDirectoryFactory().get(subCore.getDataDir(), DirectoryFactory.DirContext.META_DATA,\n                subCore.getSolrConfig().indexConfig.lockType);\n            dir.deleteFile(IndexFetcher.INDEX_PROPERTIES);\n          } finally {\n            if (dir != null) {\n              subCore.getDirectoryFactory().release(dir);\n            }\n          }\n          // switch back if necessary and remove the hardlinked dir\n          String hardLinkPath = subCore.getDataDir() + INDEX_PREFIX + timestamp;\n          try {\n            dir = subCore.getDirectoryFactory().get(hardLinkPath, DirectoryFactory.DirContext.DEFAULT,\n                subCore.getSolrConfig().indexConfig.lockType);\n            subCore.getDirectoryFactory().doneWithDirectory(dir);\n            subCore.getDirectoryFactory().remove(dir);\n          } finally {\n            if (dir != null) {\n              subCore.getDirectoryFactory().release(dir);\n            }\n          }\n          subCore.getUpdateHandler().newIndexWriter(false);\n          try {\n            openNewSearcher(subCore);\n          } catch (Exception e) {\n            log.warn(\"Error rolling back failed split of {}\", hardLinkPath, e);\n          }\n        }\n        t.stop();\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"There were errors during index split\");\n      } else {\n        // complete the switch - remove original index\n        t = timings.sub(\"cleanSubIndex\");\n        for (int partitionNumber = 0; partitionNumber < numPieces; partitionNumber++) {\n          SolrCore subCore = cores.get(partitionNumber);\n          String oldIndexPath = subCore.getDataDir() + \"index\";\n          Directory indexDir = null;\n          try {\n            indexDir = subCore.getDirectoryFactory().get(oldIndexPath,\n                DirectoryFactory.DirContext.DEFAULT, subCore.getSolrConfig().indexConfig.lockType);\n            subCore.getDirectoryFactory().doneWithDirectory(indexDir);\n            subCore.getDirectoryFactory().remove(indexDir);\n          } finally {\n            if (indexDir != null) {\n              subCore.getDirectoryFactory().release(indexDir);\n            }\n          }\n        }\n        t.stop();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b2d19164145b2a65acf62a657c75f4a249b649c0","date":1601732857,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#doSplit().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#doSplit().mjava","sourceNew":"  public void doSplit() throws IOException {\n\n    List<LeafReaderContext> leaves = searcher.getRawReader().leaves();\n    Directory parentDirectory = searcher.getRawReader().directory();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n    SolrIndexConfig parentConfig = searcher.getCore().getSolrConfig().indexConfig;\n    String timestamp = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date());\n\n    if (log.isInfoEnabled()) {\n      log.info(\"SolrIndexSplitter: partitions={} segments={}\", numPieces, leaves.size());\n    }\n    RTimerTree t;\n\n    // this tracks round-robin assignment of docs to partitions\n    AtomicInteger currentPartition = new AtomicInteger();\n\n    if (splitMethod != SplitMethod.LINK) {\n      t = timings.sub(\"findDocSetsPerLeaf\");\n      for (LeafReaderContext readerContext : leaves) {\n        assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n        FixedBitSet[] docSets = split(readerContext, numPieces, field, rangesArr, splitKey, hashRouter, currentPartition, false);\n        segmentDocSets.add(docSets);\n      }\n      t.stop();\n    }\n\n\n    Map<IndexReader.CacheKey, FixedBitSet[]> docsToDeleteCache = new ConcurrentHashMap<>();\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      String partitionName = \"SolrIndexSplitter:partition=\" + partitionNumber + \",partitionCount=\" + numPieces + (ranges != null ? \",range=\" + ranges.get(partitionNumber) : \"\");\n      log.info(partitionName);\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw;\n      if (cores != null && splitMethod != SplitMethod.LINK) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        if (splitMethod == SplitMethod.LINK) {\n          SolrCore subCore = cores.get(partitionNumber);\n          String path = subCore.getDataDir() + INDEX_PREFIX + timestamp;\n          t = timings.sub(\"hardLinkCopy\");\n          t.resume();\n          // copy by hard-linking\n          Directory splitDir = subCore.getDirectoryFactory().get(path, DirectoryFactory.DirContext.DEFAULT, subCore.getSolrConfig().indexConfig.lockType);\n          // the wrapper doesn't hold any resources itself so it doesn't need closing\n          HardlinkCopyDirectoryWrapper hardLinkedDir = new HardlinkCopyDirectoryWrapper(splitDir);\n          boolean copiedOk = false;\n          try {\n            for (String file : parentDirectory.listAll()) {\n              // we've closed the IndexWriter, so ignore write.lock\n              // its file may be present even when IndexWriter is closed but\n              // we've already checked that the lock is not held by anyone else\n              if (file.equals(IndexWriter.WRITE_LOCK_NAME)) {\n                continue;\n              }\n              hardLinkedDir.copyFrom(parentDirectory, file, file, IOContext.DEFAULT);\n            }\n            copiedOk = true;\n          } finally {\n            if (!copiedOk) {\n              subCore.getDirectoryFactory().doneWithDirectory(splitDir);\n              subCore.getDirectoryFactory().remove(splitDir);\n            }\n          }\n          t.pause();\n          IndexWriterConfig iwConfig = parentConfig.toIndexWriterConfig(subCore);\n          // don't run merges at this time\n          iwConfig.setMergePolicy(NoMergePolicy.INSTANCE);\n          t = timings.sub(\"createSubIW\");\n          t.resume();\n          iw = new SolrIndexWriter(partitionName, splitDir, iwConfig);\n          t.pause();\n        } else {\n          SolrCore core = searcher.getCore();\n          String path = paths.get(partitionNumber);\n          t = timings.sub(\"createSubIW\");\n          t.resume();\n          iw = SolrIndexWriter.create(core, partitionName, path,\n              core.getDirectoryFactory(), true, core.getLatestSchema(),\n              core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n          t.pause();\n        }\n      }\n\n      try {\n        if (splitMethod == SplitMethod.LINK) {\n          t = timings.sub(\"deleteDocuments\");\n          t.resume();\n          // apply deletions specific to this partition. As a side-effect on the first call this also populates\n          // a cache of docsets to delete per leaf reader per partition, which is reused for subsequent partitions.\n          iw.deleteDocuments(new SplittingQuery(partitionNumber, field, rangesArr, hashRouter, splitKey, docsToDeleteCache, currentPartition));\n          t.pause();\n        } else {\n          // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n          t = timings.sub(\"addIndexes\");\n          t.resume();\n          for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n            if (log.isInfoEnabled()) {\n              log.info(\"SolrIndexSplitter: partition # {} partitionCount={} {} segment #={} segmentCount={}\", partitionNumber, numPieces\n                  , (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"), segmentNumber, leaves.size()); // nowarn\n            }\n            CodecReader subReader = SlowCodecReaderWrapper.wrap(leaves.get(segmentNumber).reader());\n            iw.addIndexes(new LiveDocsReader(subReader, segmentDocSets.get(segmentNumber)[partitionNumber]));\n          }\n          t.pause();\n        }\n        // we commit explicitly instead of sending a CommitUpdateCommand through the processor chain\n        // because the sub-shard cores will just ignore such a commit because the update log is not\n        // in active state at this time.\n        //TODO no commitUpdateCommand\n        SolrIndexWriter.setCommitData(iw, -1);\n        t = timings.sub(\"subIWCommit\");\n        t.resume();\n        iw.commit();\n        t.pause();\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            t = timings.sub(\"subIWClose\");\n            t.resume();\n            iw.close();\n            t.pause();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n          if (splitMethod == SplitMethod.LINK) {\n            SolrCore subCore = cores.get(partitionNumber);\n            subCore.getDirectoryFactory().release(iw.getDirectory());\n          }\n        }\n      }\n    }\n    // all sub-indexes created ok\n    // when using hard-linking switch directories & refresh cores\n    if (splitMethod == SplitMethod.LINK && cores != null) {\n      boolean switchOk = true;\n      t = timings.sub(\"switchSubIndexes\");\n      for (int partitionNumber = 0; partitionNumber < numPieces; partitionNumber++) {\n        SolrCore subCore = cores.get(partitionNumber);\n        String indexDirPath = subCore.getIndexDir();\n\n        log.debug(\"Switching directories\");\n        String hardLinkPath = subCore.getDataDir() + INDEX_PREFIX + timestamp;\n        subCore.modifyIndexProps(INDEX_PREFIX + timestamp);\n        try {\n          subCore.getUpdateHandler().newIndexWriter(false);\n          openNewSearcher(subCore);\n        } catch (Exception e) {\n          log.error(\"Failed to switch sub-core {} to {}, split will fail\", indexDirPath, hardLinkPath, e);\n          switchOk = false;\n          break;\n        }\n      }\n      t.stop();\n      if (!switchOk) {\n        t = timings.sub(\"rollbackSubIndexes\");\n        // rollback the switch\n        for (int partitionNumber = 0; partitionNumber < numPieces; partitionNumber++) {\n          SolrCore subCore = cores.get(partitionNumber);\n          Directory dir = null;\n          try {\n            dir = subCore.getDirectoryFactory().get(subCore.getDataDir(), DirectoryFactory.DirContext.META_DATA,\n                subCore.getSolrConfig().indexConfig.lockType);\n            dir.deleteFile(IndexFetcher.INDEX_PROPERTIES);\n          } finally {\n            if (dir != null) {\n              subCore.getDirectoryFactory().release(dir);\n            }\n          }\n          // switch back if necessary and remove the hardlinked dir\n          String hardLinkPath = subCore.getDataDir() + INDEX_PREFIX + timestamp;\n          try {\n            dir = subCore.getDirectoryFactory().get(hardLinkPath, DirectoryFactory.DirContext.DEFAULT,\n                subCore.getSolrConfig().indexConfig.lockType);\n            subCore.getDirectoryFactory().doneWithDirectory(dir);\n            subCore.getDirectoryFactory().remove(dir);\n          } finally {\n            if (dir != null) {\n              subCore.getDirectoryFactory().release(dir);\n            }\n          }\n          subCore.getUpdateHandler().newIndexWriter(false);\n          try {\n            openNewSearcher(subCore);\n          } catch (Exception e) {\n            log.warn(\"Error rolling back failed split of {}\", hardLinkPath, e);\n          }\n        }\n        t.stop();\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"There were errors during index split\");\n      } else {\n        // complete the switch - remove original index\n        t = timings.sub(\"cleanSubIndex\");\n        for (int partitionNumber = 0; partitionNumber < numPieces; partitionNumber++) {\n          SolrCore subCore = cores.get(partitionNumber);\n          String oldIndexPath = subCore.getDataDir() + \"index\";\n          Directory indexDir = null;\n          try {\n            indexDir = subCore.getDirectoryFactory().get(oldIndexPath,\n                DirectoryFactory.DirContext.DEFAULT, subCore.getSolrConfig().indexConfig.lockType);\n            subCore.getDirectoryFactory().doneWithDirectory(indexDir);\n            subCore.getDirectoryFactory().remove(indexDir);\n          } finally {\n            if (indexDir != null) {\n              subCore.getDirectoryFactory().release(indexDir);\n            }\n          }\n        }\n        t.stop();\n      }\n    }\n  }\n\n","sourceOld":"  public void doSplit() throws IOException {\n\n    List<LeafReaderContext> leaves = searcher.getRawReader().leaves();\n    Directory parentDirectory = searcher.getRawReader().directory();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n    SolrIndexConfig parentConfig = searcher.getCore().getSolrConfig().indexConfig;\n    String timestamp = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date());\n\n    if (log.isInfoEnabled()) {\n      log.info(\"SolrIndexSplitter: partitions={} segments={}\", numPieces, leaves.size());\n    }\n    RTimerTree t;\n\n    // this tracks round-robin assignment of docs to partitions\n    AtomicInteger currentPartition = new AtomicInteger();\n\n    if (splitMethod != SplitMethod.LINK) {\n      t = timings.sub(\"findDocSetsPerLeaf\");\n      for (LeafReaderContext readerContext : leaves) {\n        assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n        FixedBitSet[] docSets = split(readerContext, numPieces, field, rangesArr, splitKey, hashRouter, currentPartition, false);\n        segmentDocSets.add(docSets);\n      }\n      t.stop();\n    }\n\n\n    Map<IndexReader.CacheKey, FixedBitSet[]> docsToDeleteCache = new ConcurrentHashMap<>();\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      String partitionName = \"SolrIndexSplitter:partition=\" + partitionNumber + \",partitionCount=\" + numPieces + (ranges != null ? \",range=\" + ranges.get(partitionNumber) : \"\");\n      log.info(partitionName);\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw;\n      if (cores != null && splitMethod != SplitMethod.LINK) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        if (splitMethod == SplitMethod.LINK) {\n          SolrCore subCore = cores.get(partitionNumber);\n          String path = subCore.getDataDir() + INDEX_PREFIX + timestamp;\n          t = timings.sub(\"hardLinkCopy\");\n          t.resume();\n          // copy by hard-linking\n          Directory splitDir = subCore.getDirectoryFactory().get(path, DirectoryFactory.DirContext.DEFAULT, subCore.getSolrConfig().indexConfig.lockType);\n          // the wrapper doesn't hold any resources itself so it doesn't need closing\n          HardlinkCopyDirectoryWrapper hardLinkedDir = new HardlinkCopyDirectoryWrapper(splitDir);\n          boolean copiedOk = false;\n          try {\n            for (String file : parentDirectory.listAll()) {\n              // we've closed the IndexWriter, so ignore write.lock\n              // its file may be present even when IndexWriter is closed but\n              // we've already checked that the lock is not held by anyone else\n              if (file.equals(IndexWriter.WRITE_LOCK_NAME)) {\n                continue;\n              }\n              hardLinkedDir.copyFrom(parentDirectory, file, file, IOContext.DEFAULT);\n            }\n            copiedOk = true;\n          } finally {\n            if (!copiedOk) {\n              subCore.getDirectoryFactory().doneWithDirectory(splitDir);\n              subCore.getDirectoryFactory().remove(splitDir);\n            }\n          }\n          t.pause();\n          IndexWriterConfig iwConfig = parentConfig.toIndexWriterConfig(subCore);\n          // don't run merges at this time\n          iwConfig.setMergePolicy(NoMergePolicy.INSTANCE);\n          t = timings.sub(\"createSubIW\");\n          t.resume();\n          iw = new SolrIndexWriter(partitionName, splitDir, iwConfig);\n          t.pause();\n        } else {\n          SolrCore core = searcher.getCore();\n          String path = paths.get(partitionNumber);\n          t = timings.sub(\"createSubIW\");\n          t.resume();\n          iw = SolrIndexWriter.create(core, partitionName, path,\n              core.getDirectoryFactory(), true, core.getLatestSchema(),\n              core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n          t.pause();\n        }\n      }\n\n      try {\n        if (splitMethod == SplitMethod.LINK) {\n          t = timings.sub(\"deleteDocuments\");\n          t.resume();\n          // apply deletions specific to this partition. As a side-effect on the first call this also populates\n          // a cache of docsets to delete per leaf reader per partition, which is reused for subsequent partitions.\n          iw.deleteDocuments(new SplittingQuery(partitionNumber, field, rangesArr, hashRouter, splitKey, docsToDeleteCache, currentPartition));\n          t.pause();\n        } else {\n          // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n          t = timings.sub(\"addIndexes\");\n          t.resume();\n          for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n            if (log.isInfoEnabled()) {\n              log.info(\"SolrIndexSplitter: partition # {} partitionCount={} {} segment #={} segmentCount={}\", partitionNumber, numPieces\n                  , (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"), segmentNumber, leaves.size()); // logOk\n            }\n            CodecReader subReader = SlowCodecReaderWrapper.wrap(leaves.get(segmentNumber).reader());\n            iw.addIndexes(new LiveDocsReader(subReader, segmentDocSets.get(segmentNumber)[partitionNumber]));\n          }\n          t.pause();\n        }\n        // we commit explicitly instead of sending a CommitUpdateCommand through the processor chain\n        // because the sub-shard cores will just ignore such a commit because the update log is not\n        // in active state at this time.\n        //TODO no commitUpdateCommand\n        SolrIndexWriter.setCommitData(iw, -1);\n        t = timings.sub(\"subIWCommit\");\n        t.resume();\n        iw.commit();\n        t.pause();\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            t = timings.sub(\"subIWClose\");\n            t.resume();\n            iw.close();\n            t.pause();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n          if (splitMethod == SplitMethod.LINK) {\n            SolrCore subCore = cores.get(partitionNumber);\n            subCore.getDirectoryFactory().release(iw.getDirectory());\n          }\n        }\n      }\n    }\n    // all sub-indexes created ok\n    // when using hard-linking switch directories & refresh cores\n    if (splitMethod == SplitMethod.LINK && cores != null) {\n      boolean switchOk = true;\n      t = timings.sub(\"switchSubIndexes\");\n      for (int partitionNumber = 0; partitionNumber < numPieces; partitionNumber++) {\n        SolrCore subCore = cores.get(partitionNumber);\n        String indexDirPath = subCore.getIndexDir();\n\n        log.debug(\"Switching directories\");\n        String hardLinkPath = subCore.getDataDir() + INDEX_PREFIX + timestamp;\n        subCore.modifyIndexProps(INDEX_PREFIX + timestamp);\n        try {\n          subCore.getUpdateHandler().newIndexWriter(false);\n          openNewSearcher(subCore);\n        } catch (Exception e) {\n          log.error(\"Failed to switch sub-core {} to {}, split will fail\", indexDirPath, hardLinkPath, e);\n          switchOk = false;\n          break;\n        }\n      }\n      t.stop();\n      if (!switchOk) {\n        t = timings.sub(\"rollbackSubIndexes\");\n        // rollback the switch\n        for (int partitionNumber = 0; partitionNumber < numPieces; partitionNumber++) {\n          SolrCore subCore = cores.get(partitionNumber);\n          Directory dir = null;\n          try {\n            dir = subCore.getDirectoryFactory().get(subCore.getDataDir(), DirectoryFactory.DirContext.META_DATA,\n                subCore.getSolrConfig().indexConfig.lockType);\n            dir.deleteFile(IndexFetcher.INDEX_PROPERTIES);\n          } finally {\n            if (dir != null) {\n              subCore.getDirectoryFactory().release(dir);\n            }\n          }\n          // switch back if necessary and remove the hardlinked dir\n          String hardLinkPath = subCore.getDataDir() + INDEX_PREFIX + timestamp;\n          try {\n            dir = subCore.getDirectoryFactory().get(hardLinkPath, DirectoryFactory.DirContext.DEFAULT,\n                subCore.getSolrConfig().indexConfig.lockType);\n            subCore.getDirectoryFactory().doneWithDirectory(dir);\n            subCore.getDirectoryFactory().remove(dir);\n          } finally {\n            if (dir != null) {\n              subCore.getDirectoryFactory().release(dir);\n            }\n          }\n          subCore.getUpdateHandler().newIndexWriter(false);\n          try {\n            openNewSearcher(subCore);\n          } catch (Exception e) {\n            log.warn(\"Error rolling back failed split of {}\", hardLinkPath, e);\n          }\n        }\n        t.stop();\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"There were errors during index split\");\n      } else {\n        // complete the switch - remove original index\n        t = timings.sub(\"cleanSubIndex\");\n        for (int partitionNumber = 0; partitionNumber < numPieces; partitionNumber++) {\n          SolrCore subCore = cores.get(partitionNumber);\n          String oldIndexPath = subCore.getDataDir() + \"index\";\n          Directory indexDir = null;\n          try {\n            indexDir = subCore.getDirectoryFactory().get(oldIndexPath,\n                DirectoryFactory.DirContext.DEFAULT, subCore.getSolrConfig().indexConfig.lockType);\n            subCore.getDirectoryFactory().doneWithDirectory(indexDir);\n            subCore.getDirectoryFactory().remove(indexDir);\n          } finally {\n            if (indexDir != null) {\n              subCore.getDirectoryFactory().release(indexDir);\n            }\n          }\n        }\n        t.stop();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"740d649f013f07efbeb73ca854f106c60166e7c0":["6aafdd2b981170a4d391e52d7cfd3d53a626cc7c"],"6aafdd2b981170a4d391e52d7cfd3d53a626cc7c":["20c968c14aace7cf49843bf2c1fafc7fd3845659"],"fe9f2c4a0d7ac164e4bdd4eee7f87131aec83fd4":["575e66bd4b2349209027f6801184da7fc3cba13f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b2d19164145b2a65acf62a657c75f4a249b649c0":["fe9f2c4a0d7ac164e4bdd4eee7f87131aec83fd4"],"575e66bd4b2349209027f6801184da7fc3cba13f":["740d649f013f07efbeb73ca854f106c60166e7c0"],"20c968c14aace7cf49843bf2c1fafc7fd3845659":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b2d19164145b2a65acf62a657c75f4a249b649c0"]},"commit2Childs":{"740d649f013f07efbeb73ca854f106c60166e7c0":["575e66bd4b2349209027f6801184da7fc3cba13f"],"6aafdd2b981170a4d391e52d7cfd3d53a626cc7c":["740d649f013f07efbeb73ca854f106c60166e7c0"],"fe9f2c4a0d7ac164e4bdd4eee7f87131aec83fd4":["b2d19164145b2a65acf62a657c75f4a249b649c0"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["20c968c14aace7cf49843bf2c1fafc7fd3845659"],"b2d19164145b2a65acf62a657c75f4a249b649c0":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"575e66bd4b2349209027f6801184da7fc3cba13f":["fe9f2c4a0d7ac164e4bdd4eee7f87131aec83fd4"],"20c968c14aace7cf49843bf2c1fafc7fd3845659":["6aafdd2b981170a4d391e52d7cfd3d53a626cc7c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}