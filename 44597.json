{"path":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","commits":[{"id":"893269407f5c988a4b2ee13c7ffc01ec43136c1d","date":1268598046,"type":0,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getReader();\n    FieldType ft = schema.getFieldType(field);\n\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;    \n    final BoundedTreeSet<CountPair<String,Integer>> queue = (sort.equals(\"count\") || sort.equals(\"true\")) ? new BoundedTreeSet<CountPair<String,Integer>>(maxsize) : null;\n    final NamedList res = new NamedList();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    String startTerm = prefix==null ? \"\" : ft.toInternal(prefix);\n    TermEnum te = r.terms(new Term(field,startTerm));\n    TermDocs td = r.termDocs();\n\n    if (docs.size() >= mincount) { \n    do {\n      Term t = te.term();\n\n      if (null == t || ! t.field().equals(field))\n        break;\n\n      if (prefix!=null && !t.text().startsWith(prefix)) break;\n\n      int df = te.docFreq();\n\n      // If we are sorting, we can use df>min (rather than >=) since we\n      // are going in index order.  For certain term distributions this can\n      // make a large difference (for example, many terms with df=1).\n      if (df>0 && df>min) {\n        int c;\n\n        if (df >= minDfFilterCache) {\n          // use the filter cache\n          c = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          // iterate over TermDocs to calculate the intersection\n          td.seek(te);\n          c=0;\n          while (td.next()) {\n            if (docs.exists(td.doc())) c++;\n          }\n        }\n\n        if (sort.equals(\"count\") || sort.equals(\"true\")) {\n          if (c>min) {\n            queue.add(new CountPair<String,Integer>(t.text(), c));\n            if (queue.size()>=maxsize) min=queue.last().val;\n          }\n        } else {\n          if (c >= mincount && --off<0) {\n            if (--lim<0) break;\n            res.add(ft.indexedToReadable(t.text()), c);\n          }\n        }\n      }\n    } while (te.next());\n    }\n\n    if (sort.equals(\"count\") || sort.equals(\"true\")) {\n      for (CountPair<String,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        res.add(ft.indexedToReadable(p.key), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    te.close();\n    td.close();    \n\n    return res;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8","date":1268599006,"type":4,"author":"Mark Robert Miller","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":null,"sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getReader();\n    FieldType ft = schema.getFieldType(field);\n\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;    \n    final BoundedTreeSet<CountPair<String,Integer>> queue = (sort.equals(\"count\") || sort.equals(\"true\")) ? new BoundedTreeSet<CountPair<String,Integer>>(maxsize) : null;\n    final NamedList res = new NamedList();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    String startTerm = prefix==null ? \"\" : ft.toInternal(prefix);\n    TermEnum te = r.terms(new Term(field,startTerm));\n    TermDocs td = r.termDocs();\n\n    if (docs.size() >= mincount) { \n    do {\n      Term t = te.term();\n\n      if (null == t || ! t.field().equals(field))\n        break;\n\n      if (prefix!=null && !t.text().startsWith(prefix)) break;\n\n      int df = te.docFreq();\n\n      // If we are sorting, we can use df>min (rather than >=) since we\n      // are going in index order.  For certain term distributions this can\n      // make a large difference (for example, many terms with df=1).\n      if (df>0 && df>min) {\n        int c;\n\n        if (df >= minDfFilterCache) {\n          // use the filter cache\n          c = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          // iterate over TermDocs to calculate the intersection\n          td.seek(te);\n          c=0;\n          while (td.next()) {\n            if (docs.exists(td.doc())) c++;\n          }\n        }\n\n        if (sort.equals(\"count\") || sort.equals(\"true\")) {\n          if (c>min) {\n            queue.add(new CountPair<String,Integer>(t.text(), c));\n            if (queue.size()>=maxsize) min=queue.last().val;\n          }\n        } else {\n          if (c >= mincount && --off<0) {\n            if (--lim<0) break;\n            res.add(ft.indexedToReadable(t.text()), c);\n          }\n        }\n      }\n    } while (te.next());\n    }\n\n    if (sort.equals(\"count\") || sort.equals(\"true\")) {\n      for (CountPair<String,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        res.add(ft.indexedToReadable(p.key), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    te.close();\n    td.close();    \n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ad94625fb8d088209f46650c8097196fec67f00c","date":1453508319,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getReader();\n    FieldType ft = schema.getFieldType(field);\n\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;    \n    final BoundedTreeSet<CountPair<String,Integer>> queue = (sort.equals(\"count\") || sort.equals(\"true\")) ? new BoundedTreeSet<CountPair<String,Integer>>(maxsize) : null;\n    final NamedList res = new NamedList();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    String startTerm = prefix==null ? \"\" : ft.toInternal(prefix);\n    TermEnum te = r.terms(new Term(field,startTerm));\n    TermDocs td = r.termDocs();\n\n    if (docs.size() >= mincount) { \n    do {\n      Term t = te.term();\n\n      if (null == t || ! t.field().equals(field))\n        break;\n\n      if (prefix!=null && !t.text().startsWith(prefix)) break;\n\n      int df = te.docFreq();\n\n      // If we are sorting, we can use df>min (rather than >=) since we\n      // are going in index order.  For certain term distributions this can\n      // make a large difference (for example, many terms with df=1).\n      if (df>0 && df>min) {\n        int c;\n\n        if (df >= minDfFilterCache) {\n          // use the filter cache\n          c = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          // iterate over TermDocs to calculate the intersection\n          td.seek(te);\n          c=0;\n          while (td.next()) {\n            if (docs.exists(td.doc())) c++;\n          }\n        }\n\n        if (sort.equals(\"count\") || sort.equals(\"true\")) {\n          if (c>min) {\n            queue.add(new CountPair<String,Integer>(t.text(), c));\n            if (queue.size()>=maxsize) min=queue.last().val;\n          }\n        } else {\n          if (c >= mincount && --off<0) {\n            if (--lim<0) break;\n            res.add(ft.indexedToReadable(t.text()), c);\n          }\n        }\n      }\n    } while (te.next());\n    }\n\n    if (sort.equals(\"count\") || sort.equals(\"true\")) {\n      for (CountPair<String,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        res.add(ft.indexedToReadable(p.key), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    te.close();\n    td.close();    \n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getReader();\n    FieldType ft = schema.getFieldType(field);\n\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;    \n    final BoundedTreeSet<CountPair<String,Integer>> queue = (sort.equals(\"count\") || sort.equals(\"true\")) ? new BoundedTreeSet<CountPair<String,Integer>>(maxsize) : null;\n    final NamedList res = new NamedList();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    String startTerm = prefix==null ? \"\" : ft.toInternal(prefix);\n    TermEnum te = r.terms(new Term(field,startTerm));\n    TermDocs td = r.termDocs();\n\n    if (docs.size() >= mincount) { \n    do {\n      Term t = te.term();\n\n      if (null == t || ! t.field().equals(field))\n        break;\n\n      if (prefix!=null && !t.text().startsWith(prefix)) break;\n\n      int df = te.docFreq();\n\n      // If we are sorting, we can use df>min (rather than >=) since we\n      // are going in index order.  For certain term distributions this can\n      // make a large difference (for example, many terms with df=1).\n      if (df>0 && df>min) {\n        int c;\n\n        if (df >= minDfFilterCache) {\n          // use the filter cache\n          c = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          // iterate over TermDocs to calculate the intersection\n          td.seek(te);\n          c=0;\n          while (td.next()) {\n            if (docs.exists(td.doc())) c++;\n          }\n        }\n\n        if (sort.equals(\"count\") || sort.equals(\"true\")) {\n          if (c>min) {\n            queue.add(new CountPair<String,Integer>(t.text(), c));\n            if (queue.size()>=maxsize) min=queue.last().val;\n          }\n        } else {\n          if (c >= mincount && --off<0) {\n            if (--lim<0) break;\n            res.add(ft.indexedToReadable(t.text()), c);\n          }\n        }\n      }\n    } while (te.next());\n    }\n\n    if (sort.equals(\"count\") || sort.equals(\"true\")) {\n      for (CountPair<String,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        res.add(ft.indexedToReadable(p.key), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    te.close();\n    td.close();    \n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1da8d55113b689b06716246649de6f62430f15c0","date":1453508340,"type":0,"author":"Dawid Weiss","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getReader();\n    FieldType ft = schema.getFieldType(field);\n\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;    \n    final BoundedTreeSet<CountPair<String,Integer>> queue = (sort.equals(\"count\") || sort.equals(\"true\")) ? new BoundedTreeSet<CountPair<String,Integer>>(maxsize) : null;\n    final NamedList res = new NamedList();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    String startTerm = prefix==null ? \"\" : ft.toInternal(prefix);\n    TermEnum te = r.terms(new Term(field,startTerm));\n    TermDocs td = r.termDocs();\n\n    if (docs.size() >= mincount) { \n    do {\n      Term t = te.term();\n\n      if (null == t || ! t.field().equals(field))\n        break;\n\n      if (prefix!=null && !t.text().startsWith(prefix)) break;\n\n      int df = te.docFreq();\n\n      // If we are sorting, we can use df>min (rather than >=) since we\n      // are going in index order.  For certain term distributions this can\n      // make a large difference (for example, many terms with df=1).\n      if (df>0 && df>min) {\n        int c;\n\n        if (df >= minDfFilterCache) {\n          // use the filter cache\n          c = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          // iterate over TermDocs to calculate the intersection\n          td.seek(te);\n          c=0;\n          while (td.next()) {\n            if (docs.exists(td.doc())) c++;\n          }\n        }\n\n        if (sort.equals(\"count\") || sort.equals(\"true\")) {\n          if (c>min) {\n            queue.add(new CountPair<String,Integer>(t.text(), c));\n            if (queue.size()>=maxsize) min=queue.last().val;\n          }\n        } else {\n          if (c >= mincount && --off<0) {\n            if (--lim<0) break;\n            res.add(ft.indexedToReadable(t.text()), c);\n          }\n        }\n      }\n    } while (te.next());\n    }\n\n    if (sort.equals(\"count\") || sort.equals(\"true\")) {\n      for (CountPair<String,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        res.add(ft.indexedToReadable(p.key), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    te.close();\n    td.close();    \n\n    return res;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8e818b7a45246b9f62db1078bda5312b04b39a41","date":1272918796,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList res = new NamedList();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        }\n      } else {\n        // position termsEnum on first term\n        termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n\n\n    if (termsEnum != null && docs.size() >= mincount) {\n      for(;;) {\n        BytesRef term = termsEnum.term();\n        if (term == null)\n          break;\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: not a big deal, but there are prob more efficient ways to go from utf8 to string\n            // TODO: need a term query that takes a BytesRef\n            Term t = template.createTerm(new String(term.utf8ToString()));\n            c = searcher.numDocs(new TermQuery(t), docs);\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            docsEnum = termsEnum.docs(null, docsEnum);\n\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            c=0;\n            for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              for (int i=0; i<nDocs; i++) {\n                if (docs.exists(docArr[i])) c++;\n              }\n            }\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              BytesRef termCopy = new BytesRef(term);\n              String s = term.utf8ToString();\n              res.add(ft.indexedToReadable(s), c);\n            }\n          }\n        }\n\n        termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        String s = p.key.utf8ToString();        \n        res.add(ft.indexedToReadable(s), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getReader();\n    FieldType ft = schema.getFieldType(field);\n\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;    \n    final BoundedTreeSet<CountPair<String,Integer>> queue = (sort.equals(\"count\") || sort.equals(\"true\")) ? new BoundedTreeSet<CountPair<String,Integer>>(maxsize) : null;\n    final NamedList res = new NamedList();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    String startTerm = prefix==null ? \"\" : ft.toInternal(prefix);\n    TermEnum te = r.terms(new Term(field,startTerm));\n    TermDocs td = r.termDocs();\n\n    if (docs.size() >= mincount) { \n    do {\n      Term t = te.term();\n\n      if (null == t || ! t.field().equals(field))\n        break;\n\n      if (prefix!=null && !t.text().startsWith(prefix)) break;\n\n      int df = te.docFreq();\n\n      // If we are sorting, we can use df>min (rather than >=) since we\n      // are going in index order.  For certain term distributions this can\n      // make a large difference (for example, many terms with df=1).\n      if (df>0 && df>min) {\n        int c;\n\n        if (df >= minDfFilterCache) {\n          // use the filter cache\n          c = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          // iterate over TermDocs to calculate the intersection\n          td.seek(te);\n          c=0;\n          while (td.next()) {\n            if (docs.exists(td.doc())) c++;\n          }\n        }\n\n        if (sort.equals(\"count\") || sort.equals(\"true\")) {\n          if (c>min) {\n            queue.add(new CountPair<String,Integer>(t.text(), c));\n            if (queue.size()>=maxsize) min=queue.last().val;\n          }\n        } else {\n          if (c >= mincount && --off<0) {\n            if (--lim<0) break;\n            res.add(ft.indexedToReadable(t.text()), c);\n          }\n        }\n      }\n    } while (te.next());\n    }\n\n    if (sort.equals(\"count\") || sort.equals(\"true\")) {\n      for (CountPair<String,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        res.add(ft.indexedToReadable(p.key), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    te.close();\n    td.close();    \n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0ba886716298f7bc8ddb5d32220cdabc357c39c6","date":1272924029,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList res = new NamedList();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        }\n      } else {\n        // position termsEnum on first term\n        termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n\n\n    if (termsEnum != null && docs.size() >= mincount) {\n      for(;;) {\n        BytesRef term = termsEnum.term();\n        if (term == null)\n          break;\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: not a big deal, but there are prob more efficient ways to go from utf8 to string\n            // TODO: need a term query that takes a BytesRef\n            Term t = template.createTerm(new String(term.utf8ToString()));\n            c = searcher.numDocs(new TermQuery(t), docs);\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            docsEnum = termsEnum.docs(null, docsEnum);\n\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            c=0;\n            for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (docs.exists(docArr[i])) c++;\n              }\n            }\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              BytesRef termCopy = new BytesRef(term);\n              String s = term.utf8ToString();\n              res.add(ft.indexedToReadable(s), c);\n            }\n          }\n        }\n\n        termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        String s = p.key.utf8ToString();        \n        res.add(ft.indexedToReadable(s), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList res = new NamedList();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        }\n      } else {\n        // position termsEnum on first term\n        termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n\n\n    if (termsEnum != null && docs.size() >= mincount) {\n      for(;;) {\n        BytesRef term = termsEnum.term();\n        if (term == null)\n          break;\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: not a big deal, but there are prob more efficient ways to go from utf8 to string\n            // TODO: need a term query that takes a BytesRef\n            Term t = template.createTerm(new String(term.utf8ToString()));\n            c = searcher.numDocs(new TermQuery(t), docs);\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            docsEnum = termsEnum.docs(null, docsEnum);\n\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            c=0;\n            for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              for (int i=0; i<nDocs; i++) {\n                if (docs.exists(docArr[i])) c++;\n              }\n            }\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              BytesRef termCopy = new BytesRef(term);\n              String s = term.utf8ToString();\n              res.add(ft.indexedToReadable(s), c);\n            }\n          }\n        }\n\n        termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        String s = p.key.utf8ToString();        \n        res.add(ft.indexedToReadable(s), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"79f0559a3f76caeab0d8a1f765d02cd1e038e332","date":1272927065,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList res = new NamedList();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: not a big deal, but there are prob more efficient ways to go from utf8 to string\n            // TODO: need a term query that takes a BytesRef\n            Term t = template.createTerm(new String(term.utf8ToString()));\n            c = searcher.numDocs(new TermQuery(t), docs);\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            docsEnum = termsEnum.docs(null, docsEnum);\n\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            c=0;\n            for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (docs.exists(docArr[i])) c++;\n              }\n            }\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              BytesRef termCopy = new BytesRef(term);\n              String s = term.utf8ToString();\n              res.add(ft.indexedToReadable(s), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        String s = p.key.utf8ToString();        \n        res.add(ft.indexedToReadable(s), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList res = new NamedList();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        }\n      } else {\n        // position termsEnum on first term\n        termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n\n\n    if (termsEnum != null && docs.size() >= mincount) {\n      for(;;) {\n        BytesRef term = termsEnum.term();\n        if (term == null)\n          break;\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: not a big deal, but there are prob more efficient ways to go from utf8 to string\n            // TODO: need a term query that takes a BytesRef\n            Term t = template.createTerm(new String(term.utf8ToString()));\n            c = searcher.numDocs(new TermQuery(t), docs);\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            docsEnum = termsEnum.docs(null, docsEnum);\n\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            c=0;\n            for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (docs.exists(docArr[i])) c++;\n              }\n            }\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              BytesRef termCopy = new BytesRef(term);\n              String s = term.utf8ToString();\n              res.add(ft.indexedToReadable(s), c);\n            }\n          }\n        }\n\n        termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        String s = p.key.utf8ToString();        \n        res.add(ft.indexedToReadable(s), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2e7531cc801491d1779f5339f12b9035a7b12f3b","date":1273005578,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList res = new NamedList();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: not a big deal, but there are prob more efficient ways to go from utf8 to string\n            // TODO: need a term query that takes a BytesRef\n            Term t = template.createTerm(new String(term.utf8ToString()));\n            c = searcher.numDocs(new TermQuery(t), docs);\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            docsEnum = termsEnum.docs(null, docsEnum);\n\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            c=0;\n            for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) c++;\n              }\n            }\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              BytesRef termCopy = new BytesRef(term);\n              String s = term.utf8ToString();\n              res.add(ft.indexedToReadable(s), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        String s = p.key.utf8ToString();        \n        res.add(ft.indexedToReadable(s), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList res = new NamedList();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: not a big deal, but there are prob more efficient ways to go from utf8 to string\n            // TODO: need a term query that takes a BytesRef\n            Term t = template.createTerm(new String(term.utf8ToString()));\n            c = searcher.numDocs(new TermQuery(t), docs);\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            docsEnum = termsEnum.docs(null, docsEnum);\n\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            c=0;\n            for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (docs.exists(docArr[i])) c++;\n              }\n            }\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              BytesRef termCopy = new BytesRef(term);\n              String s = term.utf8ToString();\n              res.add(ft.indexedToReadable(s), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        String s = p.key.utf8ToString();        \n        res.add(ft.indexedToReadable(s), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0d9658ef4eb8aec58e8b20258934a1b2571df85b","date":1274053730,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList res = new NamedList();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: not a big deal, but there are prob more efficient ways to go from utf8 to string\n            // TODO: need a term query that takes a BytesRef\n            Term t = template.createTerm(new String(term.utf8ToString()));\n            c = searcher.numDocs(new TermQuery(t), docs);\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            c=0;\n            for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) c++;\n              }\n            }\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              BytesRef termCopy = new BytesRef(term);\n              String s = term.utf8ToString();\n              res.add(ft.indexedToReadable(s), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        String s = p.key.utf8ToString();        \n        res.add(ft.indexedToReadable(s), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList res = new NamedList();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: not a big deal, but there are prob more efficient ways to go from utf8 to string\n            // TODO: need a term query that takes a BytesRef\n            Term t = template.createTerm(new String(term.utf8ToString()));\n            c = searcher.numDocs(new TermQuery(t), docs);\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            docsEnum = termsEnum.docs(null, docsEnum);\n\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            c=0;\n            for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) c++;\n              }\n            }\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              BytesRef termCopy = new BytesRef(term);\n              String s = term.utf8ToString();\n              res.add(ft.indexedToReadable(s), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        String s = p.key.utf8ToString();        \n        res.add(ft.indexedToReadable(s), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae1b90c2e90761c4321d2a54b1ad7f69f943d82f","date":1274898664,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList res = new NamedList();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n    CharArr spare = new CharArr();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: need a term query that takes a BytesRef to handle binary terms\n            spare.reset();\n            ByteUtils.UTF8toUTF16(term, spare);\n            Term t = template.createTerm(spare.toString());\n            c = searcher.numDocs(new TermQuery(t), docs);\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            c=0;\n            for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) c++;\n              }\n            }\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              spare.reset();\n              ft.indexedToReadable(term, spare);\n              res.add(spare.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        spare.reset();\n        ft.indexedToReadable(p.key, spare);\n        res.add(spare.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList res = new NamedList();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: not a big deal, but there are prob more efficient ways to go from utf8 to string\n            // TODO: need a term query that takes a BytesRef\n            Term t = template.createTerm(new String(term.utf8ToString()));\n            c = searcher.numDocs(new TermQuery(t), docs);\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            c=0;\n            for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) c++;\n              }\n            }\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              BytesRef termCopy = new BytesRef(term);\n              String s = term.utf8ToString();\n              res.add(ft.indexedToReadable(s), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        String s = p.key.utf8ToString();        \n        res.add(ft.indexedToReadable(s), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8b5a8afdb4baa3bf62c1bf96d2b4057c54a9fdb9","date":1276896025,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList res = new NamedList();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n    CharArr spare = new CharArr();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: need a term query that takes a BytesRef to handle binary terms\n            spare.reset();\n            ByteUtils.UTF8toUTF16(term, spare);\n            Term t = template.createTerm(spare.toString());\n            c = searcher.numDocs(new TermQuery(t), docs);\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              spare.reset();\n              ft.indexedToReadable(term, spare);\n              res.add(spare.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        spare.reset();\n        ft.indexedToReadable(p.key, spare);\n        res.add(spare.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList res = new NamedList();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n    CharArr spare = new CharArr();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: need a term query that takes a BytesRef to handle binary terms\n            spare.reset();\n            ByteUtils.UTF8toUTF16(term, spare);\n            Term t = template.createTerm(spare.toString());\n            c = searcher.numDocs(new TermQuery(t), docs);\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            c=0;\n            for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) c++;\n              }\n            }\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              spare.reset();\n              ft.indexedToReadable(term, spare);\n              res.add(spare.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        spare.reset();\n        ft.indexedToReadable(p.key, spare);\n        res.add(spare.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b4cd50ad7936c0f3a0851313b4e0ab861c7a6b17","date":1277233255,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList res = new NamedList();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n    CharArr spare = new CharArr();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: need a term query that takes a BytesRef to handle binary terms\n            spare.reset();\n            ByteUtils.UTF8toUTF16(term, spare);\n            Term t = template.createTerm(spare.toString());\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.reuse = docsEnum;\n            }\n\n            c = searcher.numDocs(new TermQuery(t), docs, deState);\n\n            docsEnum = deState.reuse;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              spare.reset();\n              ft.indexedToReadable(term, spare);\n              res.add(spare.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        spare.reset();\n        ft.indexedToReadable(p.key, spare);\n        res.add(spare.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList res = new NamedList();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n    CharArr spare = new CharArr();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: need a term query that takes a BytesRef to handle binary terms\n            spare.reset();\n            ByteUtils.UTF8toUTF16(term, spare);\n            Term t = template.createTerm(spare.toString());\n            c = searcher.numDocs(new TermQuery(t), docs);\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              spare.reset();\n              ft.indexedToReadable(term, spare);\n              res.add(spare.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        spare.reset();\n        ft.indexedToReadable(p.key, spare);\n        res.add(spare.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5f4e87790277826a2aea119328600dfb07761f32","date":1279827275,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList res = new NamedList();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n    CharArr spare = new CharArr();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: need a term query that takes a BytesRef to handle binary terms\n            spare.reset();\n            ByteUtils.UTF8toUTF16(term, spare);\n            Term t = template.createTerm(spare.toString());\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.reuse = docsEnum;\n            }\n\n            c = searcher.numDocs(new TermQuery(t), docs, deState);\n\n            docsEnum = deState.reuse;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              spare.reset();\n              ft.indexedToReadable(term, spare);\n              res.add(spare.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        spare.reset();\n        ft.indexedToReadable(p.key, spare);\n        res.add(spare.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList res = new NamedList();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n    CharArr spare = new CharArr();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: need a term query that takes a BytesRef to handle binary terms\n            spare.reset();\n            ByteUtils.UTF8toUTF16(term, spare);\n            Term t = template.createTerm(spare.toString());\n            c = searcher.numDocs(new TermQuery(t), docs);\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            c=0;\n            for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) c++;\n              }\n            }\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              spare.reset();\n              ft.indexedToReadable(term, spare);\n              res.add(spare.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        spare.reset();\n        ft.indexedToReadable(p.key, spare);\n        res.add(spare.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"755f2f419306d7297c8feee10d1897addf4b2dd0","date":1294442354,"type":3,"author":"Chris M. Hostetter","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n    CharArr spare = new CharArr();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: need a term query that takes a BytesRef to handle binary terms\n            spare.reset();\n            ByteUtils.UTF8toUTF16(term, spare);\n            Term t = template.createTerm(spare.toString());\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.reuse = docsEnum;\n            }\n\n            c = searcher.numDocs(new TermQuery(t), docs, deState);\n\n            docsEnum = deState.reuse;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              spare.reset();\n              ft.indexedToReadable(term, spare);\n              res.add(spare.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        spare.reset();\n        ft.indexedToReadable(p.key, spare);\n        res.add(spare.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList res = new NamedList();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n    CharArr spare = new CharArr();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: need a term query that takes a BytesRef to handle binary terms\n            spare.reset();\n            ByteUtils.UTF8toUTF16(term, spare);\n            Term t = template.createTerm(spare.toString());\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.reuse = docsEnum;\n            }\n\n            c = searcher.numDocs(new TermQuery(t), docs, deState);\n\n            docsEnum = deState.reuse;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              spare.reset();\n              ft.indexedToReadable(term, spare);\n              res.add(spare.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        spare.reset();\n        ft.indexedToReadable(p.key, spare);\n        res.add(spare.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c220849f876de24a79f756f65b3eb045db59f63f","date":1294902803,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n    CharArr spare = new CharArr();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: need a term query that takes a BytesRef to handle binary terms\n            spare.reset();\n            ByteUtils.UTF8toUTF16(term, spare);\n            Term t = template.createTerm(spare.toString());\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.reuse = docsEnum;\n            }\n\n            c = searcher.numDocs(new TermQuery(t), docs, deState);\n\n            docsEnum = deState.reuse;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              spare.reset();\n              ft.indexedToReadable(term, spare);\n              res.add(spare.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        spare.reset();\n        ft.indexedToReadable(p.key, spare);\n        res.add(spare.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n    CharArr spare = new CharArr();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: need a term query that takes a BytesRef to handle binary terms\n            spare.reset();\n            ByteUtils.UTF8toUTF16(term, spare);\n            Term t = template.createTerm(spare.toString());\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.reuse = docsEnum;\n            }\n\n            c = searcher.numDocs(new TermQuery(t), docs, deState);\n\n            docsEnum = deState.reuse;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              spare.reset();\n              ft.indexedToReadable(term, spare);\n              res.add(spare.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        spare.reset();\n        ft.indexedToReadable(p.key, spare);\n        res.add(spare.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"868da859b43505d9d2a023bfeae6dd0c795f5295","date":1294948401,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n    CharArr spare = new CharArr();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: need a term query that takes a BytesRef to handle binary terms\n            spare.reset();\n            ByteUtils.UTF8toUTF16(term, spare);\n            Term t = template.createTerm(spare.toString());\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.reuse = docsEnum;\n            }\n\n            c = searcher.numDocs(new TermQuery(t), docs, deState);\n\n            docsEnum = deState.reuse;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              spare.reset();\n              ft.indexedToReadable(term, spare);\n              res.add(spare.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        spare.reset();\n        ft.indexedToReadable(p.key, spare);\n        res.add(spare.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList res = new NamedList();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n    CharArr spare = new CharArr();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: need a term query that takes a BytesRef to handle binary terms\n            spare.reset();\n            ByteUtils.UTF8toUTF16(term, spare);\n            Term t = template.createTerm(spare.toString());\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.reuse = docsEnum;\n            }\n\n            c = searcher.numDocs(new TermQuery(t), docs, deState);\n\n            docsEnum = deState.reuse;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              spare.reset();\n              ft.indexedToReadable(term, spare);\n              res.add(spare.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        spare.reset();\n        ft.indexedToReadable(p.key, spare);\n        res.add(spare.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n    CharArr spare = new CharArr();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: need a term query that takes a BytesRef to handle binary terms\n            spare.reset();\n            ByteUtils.UTF8toUTF16(term, spare);\n            Term t = template.createTerm(spare.toString());\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.reuse = docsEnum;\n            }\n\n            c = searcher.numDocs(new TermQuery(t), docs, deState);\n\n            docsEnum = deState.reuse;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              spare.reset();\n              ft.indexedToReadable(term, spare);\n              res.add(spare.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        spare.reset();\n        ft.indexedToReadable(p.key, spare);\n        res.add(spare.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList res = new NamedList();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n    CharArr spare = new CharArr();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: need a term query that takes a BytesRef to handle binary terms\n            spare.reset();\n            ByteUtils.UTF8toUTF16(term, spare);\n            Term t = template.createTerm(spare.toString());\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.reuse = docsEnum;\n            }\n\n            c = searcher.numDocs(new TermQuery(t), docs, deState);\n\n            docsEnum = deState.reuse;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              spare.reset();\n              ft.indexedToReadable(term, spare);\n              res.add(spare.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        spare.reset();\n        ft.indexedToReadable(p.key, spare);\n        res.add(spare.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f20bb72b0dfa147c6f1fcd7693102c63a2714eae","date":1303767270,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n    CharArr spare = new CharArr();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: need a term query that takes a BytesRef to handle binary terms\n            spare.reset();\n            ByteUtils.UTF8toUTF16(term, spare);\n            Term t = template.createTerm(spare.toString());\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = StringHelper.intern(field);\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              spare.reset();\n              ft.indexedToReadable(term, spare);\n              res.add(spare.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        spare.reset();\n        ft.indexedToReadable(p.key, spare);\n        res.add(spare.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n    CharArr spare = new CharArr();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: need a term query that takes a BytesRef to handle binary terms\n            spare.reset();\n            ByteUtils.UTF8toUTF16(term, spare);\n            Term t = template.createTerm(spare.toString());\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.reuse = docsEnum;\n            }\n\n            c = searcher.numDocs(new TermQuery(t), docs, deState);\n\n            docsEnum = deState.reuse;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              spare.reset();\n              ft.indexedToReadable(term, spare);\n              res.add(spare.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        spare.reset();\n        ft.indexedToReadable(p.key, spare);\n        res.add(spare.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0663cc678850ea2c51151f9fd217342ea35b8568","date":1303828523,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n    CharArr spare = new CharArr();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: need a term query that takes a BytesRef to handle binary terms\n            spare.reset();\n            ByteUtils.UTF8toUTF16(term, spare);\n            Term t = template.createTerm(spare.toString());\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.reuse = docsEnum;\n            }\n\n            c = searcher.numDocs(new TermQuery(t), docs, deState);\n\n            docsEnum = deState.reuse;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              spare.reset();\n              ft.indexedToReadable(term, spare);\n              res.add(spare.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        spare.reset();\n        ft.indexedToReadable(p.key, spare);\n        res.add(spare.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n    CharArr spare = new CharArr();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: need a term query that takes a BytesRef to handle binary terms\n            spare.reset();\n            ByteUtils.UTF8toUTF16(term, spare);\n            Term t = template.createTerm(spare.toString());\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = StringHelper.intern(field);\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              spare.reset();\n              ft.indexedToReadable(term, spare);\n              res.add(spare.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        spare.reset();\n        ft.indexedToReadable(p.key, spare);\n        res.add(spare.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"21486a8058ee8d7503c7d7a5e55b6c3a218d0942","date":1303841712,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n    CharArr spare = new CharArr();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: need a term query that takes a BytesRef to handle binary terms\n            spare.reset();\n            ByteUtils.UTF8toUTF16(term, spare);\n            Term t = template.createTerm(spare.toString());\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = StringHelper.intern(field);\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              spare.reset();\n              ft.indexedToReadable(term, spare);\n              res.add(spare.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        spare.reset();\n        ft.indexedToReadable(p.key, spare);\n        res.add(spare.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n    CharArr spare = new CharArr();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: need a term query that takes a BytesRef to handle binary terms\n            spare.reset();\n            ByteUtils.UTF8toUTF16(term, spare);\n            Term t = template.createTerm(spare.toString());\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.reuse = docsEnum;\n            }\n\n            c = searcher.numDocs(new TermQuery(t), docs, deState);\n\n            docsEnum = deState.reuse;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              spare.reset();\n              ft.indexedToReadable(term, spare);\n              res.add(spare.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        spare.reset();\n        ft.indexedToReadable(p.key, spare);\n        res.add(spare.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7a405e749df166cf8c456ac9381f77f6c99a6270","date":1303842176,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n    CharArr spare = new CharArr();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: need a term query that takes a BytesRef to handle binary terms\n            spare.reset();\n            ByteUtils.UTF8toUTF16(term, spare);\n            Term t = template.createTerm(spare.toString());\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.reuse = docsEnum;\n            }\n\n            c = searcher.numDocs(new TermQuery(t), docs, deState);\n\n            docsEnum = deState.reuse;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              spare.reset();\n              ft.indexedToReadable(term, spare);\n              res.add(spare.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        spare.reset();\n        ft.indexedToReadable(p.key, spare);\n        res.add(spare.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n    CharArr spare = new CharArr();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: need a term query that takes a BytesRef to handle binary terms\n            spare.reset();\n            ByteUtils.UTF8toUTF16(term, spare);\n            Term t = template.createTerm(spare.toString());\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = StringHelper.intern(field);\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              spare.reset();\n              ft.indexedToReadable(term, spare);\n              res.add(spare.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        spare.reset();\n        ft.indexedToReadable(p.key, spare);\n        res.add(spare.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f8f944ac3fe3f9d40d825177507fb381d2b106b3","date":1303868525,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n    CharArr spare = new CharArr();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: need a term query that takes a BytesRef to handle binary terms\n            spare.reset();\n            ByteUtils.UTF8toUTF16(term, spare);\n            Term t = template.createTerm(spare.toString());\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = StringHelper.intern(field);\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              spare.reset();\n              ft.indexedToReadable(term, spare);\n              res.add(spare.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        spare.reset();\n        ft.indexedToReadable(p.key, spare);\n        res.add(spare.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n    CharArr spare = new CharArr();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: need a term query that takes a BytesRef to handle binary terms\n            spare.reset();\n            ByteUtils.UTF8toUTF16(term, spare);\n            Term t = template.createTerm(spare.toString());\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.reuse = docsEnum;\n            }\n\n            c = searcher.numDocs(new TermQuery(t), docs, deState);\n\n            docsEnum = deState.reuse;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              spare.reset();\n              ft.indexedToReadable(term, spare);\n              res.add(spare.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        spare.reset();\n        ft.indexedToReadable(p.key, spare);\n        res.add(spare.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d4d5df8e07c035d62d982894b439322da40e0938","date":1303923139,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n    CharArr spare = new CharArr();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: need a term query that takes a BytesRef to handle binary terms\n            spare.reset();\n            ByteUtils.UTF8toUTF16(term, spare);\n            Term t = template.createTerm(spare.toString());\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = StringHelper.intern(field);\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              spare.reset();\n              ft.indexedToReadable(term, spare);\n              res.add(spare.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        spare.reset();\n        ft.indexedToReadable(p.key, spare);\n        res.add(spare.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n    CharArr spare = new CharArr();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: need a term query that takes a BytesRef to handle binary terms\n            spare.reset();\n            ByteUtils.UTF8toUTF16(term, spare);\n            Term t = template.createTerm(spare.toString());\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.reuse = docsEnum;\n            }\n\n            c = searcher.numDocs(new TermQuery(t), docs, deState);\n\n            docsEnum = deState.reuse;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              spare.reset();\n              ft.indexedToReadable(term, spare);\n              res.add(spare.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        spare.reset();\n        ft.indexedToReadable(p.key, spare);\n        res.add(spare.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n    CharArr spare = new CharArr();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: need a term query that takes a BytesRef to handle binary terms\n            spare.reset();\n            ByteUtils.UTF8toUTF16(term, spare);\n            Term t = template.createTerm(spare.toString());\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = StringHelper.intern(field);\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              spare.reset();\n              ft.indexedToReadable(term, spare);\n              res.add(spare.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        spare.reset();\n        ft.indexedToReadable(p.key, spare);\n        res.add(spare.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n    CharArr spare = new CharArr();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: need a term query that takes a BytesRef to handle binary terms\n            spare.reset();\n            ByteUtils.UTF8toUTF16(term, spare);\n            Term t = template.createTerm(spare.toString());\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.reuse = docsEnum;\n            }\n\n            c = searcher.numDocs(new TermQuery(t), docs, deState);\n\n            docsEnum = deState.reuse;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              spare.reset();\n              ft.indexedToReadable(term, spare);\n              res.add(spare.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        spare.reset();\n        ft.indexedToReadable(p.key, spare);\n        res.add(spare.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"74c180151db6adaa9157da90caae698271578aa7","date":1305826642,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharArr spare = new CharArr();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = StringHelper.intern(field);\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              spare.reset();\n              ft.indexedToReadable(term, spare);\n              res.add(spare.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        spare.reset();\n        ft.indexedToReadable(p.key, spare);\n        res.add(spare.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n    CharArr spare = new CharArr();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: need a term query that takes a BytesRef to handle binary terms\n            spare.reset();\n            ByteUtils.UTF8toUTF16(term, spare);\n            Term t = template.createTerm(spare.toString());\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = StringHelper.intern(field);\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              spare.reset();\n              ft.indexedToReadable(term, spare);\n              res.add(spare.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        spare.reset();\n        ft.indexedToReadable(p.key, spare);\n        res.add(spare.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharArr spare = new CharArr();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = StringHelper.intern(field);\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              spare.reset();\n              ft.indexedToReadable(term, spare);\n              res.add(spare.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        spare.reset();\n        ft.indexedToReadable(p.key, spare);\n        res.add(spare.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n    CharArr spare = new CharArr();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: need a term query that takes a BytesRef to handle binary terms\n            spare.reset();\n            ByteUtils.UTF8toUTF16(term, spare);\n            Term t = template.createTerm(spare.toString());\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.reuse = docsEnum;\n            }\n\n            c = searcher.numDocs(new TermQuery(t), docs, deState);\n\n            docsEnum = deState.reuse;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              spare.reset();\n              ft.indexedToReadable(term, spare);\n              res.add(spare.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        spare.reset();\n        ft.indexedToReadable(p.key, spare);\n        res.add(spare.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5c698c0cb88bac4bcd36a1b1001a0c6a2163ea2a","date":1306150983,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharArr spare = new CharArr();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = StringHelper.intern(field);\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              spare.reset();\n              ft.indexedToReadable(term, spare);\n              res.add(spare.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        spare.reset();\n        ft.indexedToReadable(p.key, spare);\n        res.add(spare.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    Term template = new Term(field);\n    DocsEnum docsEnum = null;\n    CharArr spare = new CharArr();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n            // TODO: need a term query that takes a BytesRef to handle binary terms\n            spare.reset();\n            ByteUtils.UTF8toUTF16(term, spare);\n            Term t = template.createTerm(spare.toString());\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = StringHelper.intern(field);\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              spare.reset();\n              ft.indexedToReadable(term, spare);\n              res.add(spare.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        spare.reset();\n        ft.indexedToReadable(p.key, spare);\n        res.add(spare.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a1b3a24d5d9b47345473ff564f5cc127a7b526b4","date":1306277076,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = StringHelper.intern(field);\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharArr spare = new CharArr();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = StringHelper.intern(field);\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              spare.reset();\n              ft.indexedToReadable(term, spare);\n              res.add(spare.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        spare.reset();\n        ft.indexedToReadable(p.key, spare);\n        res.add(spare.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":["7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50","7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50","7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","date":1306767085,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = StringHelper.intern(field);\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharArr spare = new CharArr();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = StringHelper.intern(field);\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              spare.reset();\n              ft.indexedToReadable(term, spare);\n              res.add(spare.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        spare.reset();\n        ft.indexedToReadable(p.key, spare);\n        res.add(spare.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2e10cb22a8bdb44339e282925a29182bb2f3174d","date":1306841137,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = StringHelper.intern(field);\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharArr spare = new CharArr();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = StringHelper.intern(field);\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              spare.reset();\n              ft.indexedToReadable(term, spare);\n              res.add(spare.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        spare.reset();\n        ft.indexedToReadable(p.key, spare);\n        res.add(spare.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fafef7c83fe8e0b3ca9298d5d75d6b943dc28153","date":1308670974,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = StringHelper.intern(field);\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fd9cc9d77712aba3662f24632df7539ab75e3667","date":1309095238,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2553b00f699380c64959ccb27991289aae87be2e","date":1309290151,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = StringHelper.intern(field);\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","date":1309960478,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = MultiFields.getLiveDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"817d8435e9135b756f08ce6710ab0baac51bdf88","date":1309986993,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = MultiFields.getLiveDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = MultiFields.getLiveDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = StringHelper.intern(field);\n              deState.deletedDocs = MultiFields.getDeletedDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c903c3d15906a3da96b8c0c2fb704491005fdbdb","date":1453508227,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = MultiFields.getLiveDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = MultiFields.getLiveDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a258fbb26824fd104ed795e5d9033d2d040049ee","date":1453508252,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = MultiFields.getLiveDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = MultiFields.getLiveDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c26f00b574427b55127e869b935845554afde1fa","date":1310252513,"type":5,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = MultiFields.getLiveDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = MultiFields.getLiveDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"b4cd50ad7936c0f3a0851313b4e0ab861c7a6b17":["8b5a8afdb4baa3bf62c1bf96d2b4057c54a9fdb9"],"5c698c0cb88bac4bcd36a1b1001a0c6a2163ea2a":["135621f3a0670a9394eb563224a3b76cc4dddc0f","74c180151db6adaa9157da90caae698271578aa7"],"8b5a8afdb4baa3bf62c1bf96d2b4057c54a9fdb9":["ae1b90c2e90761c4321d2a54b1ad7f69f943d82f"],"f20bb72b0dfa147c6f1fcd7693102c63a2714eae":["c220849f876de24a79f756f65b3eb045db59f63f"],"fafef7c83fe8e0b3ca9298d5d75d6b943dc28153":["a1b3a24d5d9b47345473ff564f5cc127a7b526b4"],"c26f00b574427b55127e869b935845554afde1fa":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb":["fd9cc9d77712aba3662f24632df7539ab75e3667"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":[],"ae1b90c2e90761c4321d2a54b1ad7f69f943d82f":["0d9658ef4eb8aec58e8b20258934a1b2571df85b"],"1da8d55113b689b06716246649de6f62430f15c0":["a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8","ad94625fb8d088209f46650c8097196fec67f00c"],"c220849f876de24a79f756f65b3eb045db59f63f":["755f2f419306d7297c8feee10d1897addf4b2dd0"],"74c180151db6adaa9157da90caae698271578aa7":["f8f944ac3fe3f9d40d825177507fb381d2b106b3"],"8e818b7a45246b9f62db1078bda5312b04b39a41":["1da8d55113b689b06716246649de6f62430f15c0"],"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91":["a3776dccca01c11e7046323cfad46a3b4a471233","a1b3a24d5d9b47345473ff564f5cc127a7b526b4"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["b4cd50ad7936c0f3a0851313b4e0ab861c7a6b17","c220849f876de24a79f756f65b3eb045db59f63f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"ad94625fb8d088209f46650c8097196fec67f00c":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"fd9cc9d77712aba3662f24632df7539ab75e3667":["fafef7c83fe8e0b3ca9298d5d75d6b943dc28153"],"21486a8058ee8d7503c7d7a5e55b6c3a218d0942":["0663cc678850ea2c51151f9fd217342ea35b8568"],"2e10cb22a8bdb44339e282925a29182bb2f3174d":["5c698c0cb88bac4bcd36a1b1001a0c6a2163ea2a","a1b3a24d5d9b47345473ff564f5cc127a7b526b4"],"2e7531cc801491d1779f5339f12b9035a7b12f3b":["79f0559a3f76caeab0d8a1f765d02cd1e038e332"],"893269407f5c988a4b2ee13c7ffc01ec43136c1d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"d4d5df8e07c035d62d982894b439322da40e0938":["868da859b43505d9d2a023bfeae6dd0c795f5295","f8f944ac3fe3f9d40d825177507fb381d2b106b3"],"7a405e749df166cf8c456ac9381f77f6c99a6270":["21486a8058ee8d7503c7d7a5e55b6c3a218d0942"],"a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8":["893269407f5c988a4b2ee13c7ffc01ec43136c1d"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["29ef99d61cda9641b6250bf9567329a6e65f901d","f8f944ac3fe3f9d40d825177507fb381d2b106b3"],"0663cc678850ea2c51151f9fd217342ea35b8568":["f20bb72b0dfa147c6f1fcd7693102c63a2714eae"],"5f4e87790277826a2aea119328600dfb07761f32":["ae1b90c2e90761c4321d2a54b1ad7f69f943d82f","b4cd50ad7936c0f3a0851313b4e0ab861c7a6b17"],"0d9658ef4eb8aec58e8b20258934a1b2571df85b":["2e7531cc801491d1779f5339f12b9035a7b12f3b"],"2553b00f699380c64959ccb27991289aae87be2e":["5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","fd9cc9d77712aba3662f24632df7539ab75e3667"],"f8f944ac3fe3f9d40d825177507fb381d2b106b3":["7a405e749df166cf8c456ac9381f77f6c99a6270"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["a1b3a24d5d9b47345473ff564f5cc127a7b526b4","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"817d8435e9135b756f08ce6710ab0baac51bdf88":["2553b00f699380c64959ccb27991289aae87be2e","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"79f0559a3f76caeab0d8a1f765d02cd1e038e332":["0ba886716298f7bc8ddb5d32220cdabc357c39c6"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["817d8435e9135b756f08ce6710ab0baac51bdf88"],"755f2f419306d7297c8feee10d1897addf4b2dd0":["b4cd50ad7936c0f3a0851313b4e0ab861c7a6b17"],"a3776dccca01c11e7046323cfad46a3b4a471233":["c220849f876de24a79f756f65b3eb045db59f63f","74c180151db6adaa9157da90caae698271578aa7"],"a258fbb26824fd104ed795e5d9033d2d040049ee":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"a1b3a24d5d9b47345473ff564f5cc127a7b526b4":["74c180151db6adaa9157da90caae698271578aa7"],"0ba886716298f7bc8ddb5d32220cdabc357c39c6":["8e818b7a45246b9f62db1078bda5312b04b39a41"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["5f4e87790277826a2aea119328600dfb07761f32","c220849f876de24a79f756f65b3eb045db59f63f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["c26f00b574427b55127e869b935845554afde1fa"]},"commit2Childs":{"b4cd50ad7936c0f3a0851313b4e0ab861c7a6b17":["29ef99d61cda9641b6250bf9567329a6e65f901d","5f4e87790277826a2aea119328600dfb07761f32","755f2f419306d7297c8feee10d1897addf4b2dd0"],"5c698c0cb88bac4bcd36a1b1001a0c6a2163ea2a":["2e10cb22a8bdb44339e282925a29182bb2f3174d"],"8b5a8afdb4baa3bf62c1bf96d2b4057c54a9fdb9":["b4cd50ad7936c0f3a0851313b4e0ab861c7a6b17"],"f20bb72b0dfa147c6f1fcd7693102c63a2714eae":["0663cc678850ea2c51151f9fd217342ea35b8568"],"fafef7c83fe8e0b3ca9298d5d75d6b943dc28153":["fd9cc9d77712aba3662f24632df7539ab75e3667"],"c26f00b574427b55127e869b935845554afde1fa":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb":["c26f00b574427b55127e869b935845554afde1fa","d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88","a258fbb26824fd104ed795e5d9033d2d040049ee"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":["ad94625fb8d088209f46650c8097196fec67f00c"],"ae1b90c2e90761c4321d2a54b1ad7f69f943d82f":["8b5a8afdb4baa3bf62c1bf96d2b4057c54a9fdb9","5f4e87790277826a2aea119328600dfb07761f32"],"1da8d55113b689b06716246649de6f62430f15c0":["8e818b7a45246b9f62db1078bda5312b04b39a41"],"c220849f876de24a79f756f65b3eb045db59f63f":["f20bb72b0dfa147c6f1fcd7693102c63a2714eae","29ef99d61cda9641b6250bf9567329a6e65f901d","a3776dccca01c11e7046323cfad46a3b4a471233","868da859b43505d9d2a023bfeae6dd0c795f5295"],"74c180151db6adaa9157da90caae698271578aa7":["5c698c0cb88bac4bcd36a1b1001a0c6a2163ea2a","a3776dccca01c11e7046323cfad46a3b4a471233","a1b3a24d5d9b47345473ff564f5cc127a7b526b4"],"8e818b7a45246b9f62db1078bda5312b04b39a41":["0ba886716298f7bc8ddb5d32220cdabc357c39c6"],"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91":["2553b00f699380c64959ccb27991289aae87be2e"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["893269407f5c988a4b2ee13c7ffc01ec43136c1d"],"ad94625fb8d088209f46650c8097196fec67f00c":["1da8d55113b689b06716246649de6f62430f15c0"],"fd9cc9d77712aba3662f24632df7539ab75e3667":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","2553b00f699380c64959ccb27991289aae87be2e"],"21486a8058ee8d7503c7d7a5e55b6c3a218d0942":["7a405e749df166cf8c456ac9381f77f6c99a6270"],"2e10cb22a8bdb44339e282925a29182bb2f3174d":[],"2e7531cc801491d1779f5339f12b9035a7b12f3b":["0d9658ef4eb8aec58e8b20258934a1b2571df85b"],"893269407f5c988a4b2ee13c7ffc01ec43136c1d":["a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8"],"d4d5df8e07c035d62d982894b439322da40e0938":[],"7a405e749df166cf8c456ac9381f77f6c99a6270":["f8f944ac3fe3f9d40d825177507fb381d2b106b3"],"a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8":["1da8d55113b689b06716246649de6f62430f15c0"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["5c698c0cb88bac4bcd36a1b1001a0c6a2163ea2a"],"0663cc678850ea2c51151f9fd217342ea35b8568":["21486a8058ee8d7503c7d7a5e55b6c3a218d0942"],"5f4e87790277826a2aea119328600dfb07761f32":["868da859b43505d9d2a023bfeae6dd0c795f5295"],"0d9658ef4eb8aec58e8b20258934a1b2571df85b":["ae1b90c2e90761c4321d2a54b1ad7f69f943d82f"],"2553b00f699380c64959ccb27991289aae87be2e":["817d8435e9135b756f08ce6710ab0baac51bdf88"],"f8f944ac3fe3f9d40d825177507fb381d2b106b3":["74c180151db6adaa9157da90caae698271578aa7","d4d5df8e07c035d62d982894b439322da40e0938","135621f3a0670a9394eb563224a3b76cc4dddc0f"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":[],"817d8435e9135b756f08ce6710ab0baac51bdf88":["c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["c26f00b574427b55127e869b935845554afde1fa"],"79f0559a3f76caeab0d8a1f765d02cd1e038e332":["2e7531cc801491d1779f5339f12b9035a7b12f3b"],"755f2f419306d7297c8feee10d1897addf4b2dd0":["c220849f876de24a79f756f65b3eb045db59f63f"],"a3776dccca01c11e7046323cfad46a3b4a471233":["5128b7b3b73fedff05fdc5ea2e6be53c1020bb91"],"a258fbb26824fd104ed795e5d9033d2d040049ee":[],"a1b3a24d5d9b47345473ff564f5cc127a7b526b4":["fafef7c83fe8e0b3ca9298d5d75d6b943dc28153","5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","2e10cb22a8bdb44339e282925a29182bb2f3174d","d083e83f225b11e5fdd900e83d26ddb385b6955c"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["d4d5df8e07c035d62d982894b439322da40e0938"],"0ba886716298f7bc8ddb5d32220cdabc357c39c6":["79f0559a3f76caeab0d8a1f765d02cd1e038e332"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["2e10cb22a8bdb44339e282925a29182bb2f3174d","d4d5df8e07c035d62d982894b439322da40e0938","d083e83f225b11e5fdd900e83d26ddb385b6955c","a258fbb26824fd104ed795e5d9033d2d040049ee","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}