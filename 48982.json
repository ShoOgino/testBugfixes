{"path":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","commits":[{"id":"84b590669deb3d3a471cec6cb13b104b2ee94418","date":1288889547,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"/dev/null","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = 50;\n    int END_COUNT = START_COUNT + NUM_DIR*25;\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = new IndexSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+100;\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(new RAMDirectory(startDir));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n        IOException err = null;\n        \n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate, diskFree);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = new IndexSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n              \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n              \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n              \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n              (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0, 0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with 5000 more bytes of free space:\n        diskFree += 5000;\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["d19974432be9aed28ee7dca73bdf01d139e763a9","0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"85a883878c0af761245ab048babc63d099f835f3","date":1289553330,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"/dev/null","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = 50;\n    int END_COUNT = START_COUNT + NUM_DIR*25;\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = new IndexSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+100;\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(new RAMDirectory(startDir));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n        IOException err = null;\n        \n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate, diskFree);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = new IndexSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n              \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n              \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n              \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n              (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0, 0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with 5000 more bytes of free space:\n        diskFree += 5000;\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"53a31399f2471493d67b19a95c028a74e0113b6a","date":1289817072,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = 50;\n    int END_COUNT = START_COUNT + NUM_DIR*25;\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = new IndexSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+100;\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n        IOException err = null;\n        \n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = new IndexSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n              \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n              \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n              \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n              (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with 5000 more bytes of free space:\n        diskFree += 5000;\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = 50;\n    int END_COUNT = START_COUNT + NUM_DIR*25;\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = new IndexSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+100;\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(new RAMDirectory(startDir));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n        IOException err = null;\n        \n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate, diskFree);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = new IndexSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n              \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n              \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n              \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n              (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0, 0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with 5000 more bytes of free space:\n        diskFree += 5000;\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9ab1f5591dc05f1f2b5407d809c9699f75554a32","date":1290008586,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = 50;\n    int END_COUNT = START_COUNT + NUM_DIR*25;\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = new IndexSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+100;\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n        IOException err = null;\n        \n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = new IndexSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n              \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n              \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n              \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n              (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with 5000 more bytes of free space:\n        diskFree += 5000;\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = 50;\n    int END_COUNT = START_COUNT + NUM_DIR*25;\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = new IndexSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+100;\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(new RAMDirectory(startDir));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n        IOException err = null;\n        \n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate, diskFree);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = new IndexSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n              \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n              \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n              \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n              (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0, 0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with 5000 more bytes of free space:\n        diskFree += 5000;\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2ed5989ef2bc7bbd155dbff1e023ea849003dbf7","date":1292688238,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = 50;\n    int END_COUNT = START_COUNT + NUM_DIR*25;\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = new IndexSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n        IOException err = null;\n        writer.setInfoStream(VERBOSE ? System.out : null);\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = new IndexSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n              \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n              \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n              \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n              (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with 5000 more bytes of free space:\n        diskFree += 5000;\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = 50;\n    int END_COUNT = START_COUNT + NUM_DIR*25;\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = new IndexSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+100;\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n        IOException err = null;\n        \n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = new IndexSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n              \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n              \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n              \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n              (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with 5000 more bytes of free space:\n        diskFree += 5000;\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ab5cb6a74aefb78aa0569857970b9151dfe2e787","date":1292842407,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = 50;\n    int END_COUNT = START_COUNT + NUM_DIR*25;\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = new IndexSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n        IOException err = null;\n        writer.setInfoStream(VERBOSE ? System.out : null);\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = new IndexSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n              \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n              \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n              \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n              (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with 5000 more bytes of free space:\n        diskFree += 5000;\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = 50;\n    int END_COUNT = START_COUNT + NUM_DIR*25;\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = new IndexSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+100;\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n        IOException err = null;\n        \n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = new IndexSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n              \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n              \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n              \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n              (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with 5000 more bytes of free space:\n        diskFree += 5000;\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":0,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"/dev/null","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = 50;\n    int END_COUNT = START_COUNT + NUM_DIR*25;\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = new IndexSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n        IOException err = null;\n        writer.setInfoStream(VERBOSE ? System.out : null);\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = new IndexSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n              \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n              \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n              \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n              (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with 5000 more bytes of free space:\n        diskFree += 5000;\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c19f985e36a65cc969e8e564fe337a0d41512075","date":1296330536,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = 50;\n    int END_COUNT = START_COUNT + NUM_DIR*25;\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = new IndexSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n        writer.setInfoStream(VERBOSE ? System.out : null);\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = new IndexSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with 5000 more bytes of free space:\n        diskFree += 5000;\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = 50;\n    int END_COUNT = START_COUNT + NUM_DIR*25;\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = new IndexSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n        IOException err = null;\n        writer.setInfoStream(VERBOSE ? System.out : null);\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = new IndexSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n              \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n              \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n              \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n              (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with 5000 more bytes of free space:\n        diskFree += 5000;\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"790e1fde4caa765b3faaad3fbcd25c6973450336","date":1296689245,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = 50;\n    int END_COUNT = START_COUNT + NUM_DIR*25;\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n        writer.setInfoStream(VERBOSE ? System.out : null);\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with 5000 more bytes of free space:\n        diskFree += 5000;\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = 50;\n    int END_COUNT = START_COUNT + NUM_DIR*25;\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = new IndexSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n        writer.setInfoStream(VERBOSE ? System.out : null);\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = new IndexSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with 5000 more bytes of free space:\n        diskFree += 5000;\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = 50;\n    int END_COUNT = START_COUNT + NUM_DIR*25;\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n        writer.setInfoStream(VERBOSE ? System.out : null);\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with 5000 more bytes of free space:\n        diskFree += 5000;\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = 50;\n    int END_COUNT = START_COUNT + NUM_DIR*25;\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = new IndexSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n        IOException err = null;\n        writer.setInfoStream(VERBOSE ? System.out : null);\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = new IndexSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n              \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n              \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n              \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n              (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with 5000 more bytes of free space:\n        diskFree += 5000;\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bde51b089eb7f86171eb3406e38a274743f9b7ac","date":1298336439,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = 50;\n    int END_COUNT = START_COUNT + NUM_DIR*25;\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n        writer.setInfoStream(VERBOSE ? System.out : null);\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with 5000 more bytes of free space:\n        diskFree += 5000;\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = 50;\n    int END_COUNT = START_COUNT + NUM_DIR*25;\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = new IndexSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n        IOException err = null;\n        writer.setInfoStream(VERBOSE ? System.out : null);\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = new IndexSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n              \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n              \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n              \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n              (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with 5000 more bytes of free space:\n        diskFree += 5000;\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"63b5d57e38bf07e9421a6309dc1f255446de9f22","date":1302191539,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = 50;\n    int END_COUNT = START_COUNT + NUM_DIR*25;\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n        writer.setInfoStream(VERBOSE ? System.out : null);\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = 50;\n    int END_COUNT = START_COUNT + NUM_DIR*25;\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n        writer.setInfoStream(VERBOSE ? System.out : null);\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with 5000 more bytes of free space:\n        diskFree += 5000;\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f2c5f0cb44df114db4228c8f77861714b5cabaea","date":1302542431,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = 50;\n    int END_COUNT = START_COUNT + NUM_DIR*25;\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n        writer.setInfoStream(VERBOSE ? System.out : null);\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = 50;\n    int END_COUNT = START_COUNT + NUM_DIR*25;\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n        writer.setInfoStream(VERBOSE ? System.out : null);\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"962d04139994fce5193143ef35615499a9a96d78","date":1302693744,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = 50;\n    int END_COUNT = START_COUNT + NUM_DIR*25;\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n        writer.setInfoStream(VERBOSE ? System.out : null);\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = 50;\n    int END_COUNT = START_COUNT + NUM_DIR*25;\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n        writer.setInfoStream(VERBOSE ? System.out : null);\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with 5000 more bytes of free space:\n        diskFree += 5000;\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = 50;\n    int END_COUNT = START_COUNT + NUM_DIR*25;\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n        writer.setInfoStream(VERBOSE ? System.out : null);\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = 50;\n    int END_COUNT = START_COUNT + NUM_DIR*25;\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n        writer.setInfoStream(VERBOSE ? System.out : null);\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with 5000 more bytes of free space:\n        diskFree += 5000;\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = 50;\n    int END_COUNT = START_COUNT + NUM_DIR*25;\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n        writer.setInfoStream(VERBOSE ? System.out : null);\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = 50;\n    int END_COUNT = START_COUNT + NUM_DIR*25;\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n        writer.setInfoStream(VERBOSE ? System.out : null);\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with 5000 more bytes of free space:\n        diskFree += 5000;\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"639c36565ce03aed5b0fce7c9e4448e53a1f7efd","date":1308580104,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = 50;\n    int END_COUNT = START_COUNT + NUM_DIR*25;\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, IOContext.DEFAULT));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n        writer.setInfoStream(VERBOSE ? System.out : null);\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = 50;\n    int END_COUNT = START_COUNT + NUM_DIR*25;\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n        writer.setInfoStream(VERBOSE ? System.out : null);\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a37d7952ff54064a735708748444570f9963683e","date":1309331473,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n        writer.setInfoStream(VERBOSE ? System.out : null);\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = 50;\n    int END_COUNT = START_COUNT + NUM_DIR*25;\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n        writer.setInfoStream(VERBOSE ? System.out : null);\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b6f9be74ca7baaef11857ad002cad40419979516","date":1309449808,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = 50;\n    int END_COUNT = START_COUNT + NUM_DIR*25;\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n        writer.setInfoStream(VERBOSE ? System.out : null);\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = 50;\n    int END_COUNT = START_COUNT + NUM_DIR*25;\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, IOContext.DEFAULT));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n        writer.setInfoStream(VERBOSE ? System.out : null);\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"817d8435e9135b756f08ce6710ab0baac51bdf88","date":1309986993,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n        writer.setInfoStream(VERBOSE ? System.out : null);\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = 50;\n    int END_COUNT = START_COUNT + NUM_DIR*25;\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n        writer.setInfoStream(VERBOSE ? System.out : null);\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n        writer.setInfoStream(VERBOSE ? System.out : null);\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = 50;\n    int END_COUNT = START_COUNT + NUM_DIR*25;\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n        writer.setInfoStream(VERBOSE ? System.out : null);\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ddc4c914be86e34b54f70023f45a60fa7f04e929","date":1310115160,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n        writer.setInfoStream(VERBOSE ? System.out : null);\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n        writer.setInfoStream(VERBOSE ? System.out : null);\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5d004d0e0b3f65bb40da76d476d659d7888270e8","date":1310158940,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n        writer.setInfoStream(VERBOSE ? System.out : null);\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n        writer.setInfoStream(VERBOSE ? System.out : null);\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"44d6f0ab53c1962856b9f48dedb7a2a6cc18905c","date":1310389132,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n    assumeFalse(\"This test cannot run with Memory codec\", CodecProvider.getDefault().getFieldCodec(\"id\").equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with Memory codec\", CodecProvider.getDefault().getFieldCodec(\"content\").equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n        writer.setInfoStream(VERBOSE ? System.out : null);\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n        writer.setInfoStream(VERBOSE ? System.out : null);\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7b91922b55d15444d554721b352861d028eb8278","date":1320421415,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n        writer.setInfoStream(VERBOSE ? System.out : null);\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n    assumeFalse(\"This test cannot run with Memory codec\", CodecProvider.getDefault().getFieldCodec(\"id\").equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with Memory codec\", CodecProvider.getDefault().getFieldCodec(\"content\").equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n        writer.setInfoStream(VERBOSE ? System.out : null);\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"06584e6e98d592b34e1329b384182f368d2025e8","date":1320850353,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n        writer.setInfoStream(VERBOSE ? System.out : null);\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d14e8d18c0e3970c20354dbeeb49da11bd587fbd","date":1321041051,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexesNoOptimize into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + optimize()\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.optimize();\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1c5b026d03cbbb03ca4c0b97d14e9839682281dc","date":1323049298,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3615ce4a1f785ae1b779244de52c6a7d99227e60","date":1323422019,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","date":1323437438,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir, true);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i], true);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir, true);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0e7c2454a6a8237bfd0e953f5b940838408c9055","date":1323649300,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"93ccd971aca7fb61b7f1b946e44714cfc80bfc7c","date":1323720782,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    searcher.close();\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          searcher.close();\n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7118b8e5d127b58ad37740f4fa0881259a362090","date":1327618211,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n        writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"78a55f24d9b493c2a1cecf79f1d78279062b545b","date":1327688152,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n        writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fd92b8bcc88e969302510acf77bd6970da3994c4","date":1327839530,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n        writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n    \n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              writer.addIndexes(dirs);\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n        writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n        writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"7118b8e5d127b58ad37740f4fa0881259a362090":["0e7c2454a6a8237bfd0e953f5b940838408c9055"],"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd","3615ce4a1f785ae1b779244de52c6a7d99227e60"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["7118b8e5d127b58ad37740f4fa0881259a362090"],"93ccd971aca7fb61b7f1b946e44714cfc80bfc7c":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","0e7c2454a6a8237bfd0e953f5b940838408c9055"],"fd92b8bcc88e969302510acf77bd6970da3994c4":["0e7c2454a6a8237bfd0e953f5b940838408c9055","7118b8e5d127b58ad37740f4fa0881259a362090"],"639c36565ce03aed5b0fce7c9e4448e53a1f7efd":["f2c5f0cb44df114db4228c8f77861714b5cabaea"],"1c5b026d03cbbb03ca4c0b97d14e9839682281dc":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd"],"d14e8d18c0e3970c20354dbeeb49da11bd587fbd":["06584e6e98d592b34e1329b384182f368d2025e8"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","2ed5989ef2bc7bbd155dbff1e023ea849003dbf7"],"c19f985e36a65cc969e8e564fe337a0d41512075":["2ed5989ef2bc7bbd155dbff1e023ea849003dbf7"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["63b5d57e38bf07e9421a6309dc1f255446de9f22"],"53a31399f2471493d67b19a95c028a74e0113b6a":["84b590669deb3d3a471cec6cb13b104b2ee94418"],"84b590669deb3d3a471cec6cb13b104b2ee94418":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"0e7c2454a6a8237bfd0e953f5b940838408c9055":["3615ce4a1f785ae1b779244de52c6a7d99227e60"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["ab5cb6a74aefb78aa0569857970b9151dfe2e787","790e1fde4caa765b3faaad3fbcd25c6973450336"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b6f9be74ca7baaef11857ad002cad40419979516":["639c36565ce03aed5b0fce7c9e4448e53a1f7efd"],"5d004d0e0b3f65bb40da76d476d659d7888270e8":["817d8435e9135b756f08ce6710ab0baac51bdf88","ddc4c914be86e34b54f70023f45a60fa7f04e929"],"ddc4c914be86e34b54f70023f45a60fa7f04e929":["a37d7952ff54064a735708748444570f9963683e","d083e83f225b11e5fdd900e83d26ddb385b6955c"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","790e1fde4caa765b3faaad3fbcd25c6973450336"],"78a55f24d9b493c2a1cecf79f1d78279062b545b":["0e7c2454a6a8237bfd0e953f5b940838408c9055","7118b8e5d127b58ad37740f4fa0881259a362090"],"a37d7952ff54064a735708748444570f9963683e":["f2c5f0cb44df114db4228c8f77861714b5cabaea"],"9ab1f5591dc05f1f2b5407d809c9699f75554a32":["85a883878c0af761245ab048babc63d099f835f3","53a31399f2471493d67b19a95c028a74e0113b6a"],"2ed5989ef2bc7bbd155dbff1e023ea849003dbf7":["53a31399f2471493d67b19a95c028a74e0113b6a"],"06584e6e98d592b34e1329b384182f368d2025e8":["7b91922b55d15444d554721b352861d028eb8278"],"85a883878c0af761245ab048babc63d099f835f3":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","84b590669deb3d3a471cec6cb13b104b2ee94418"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["29ef99d61cda9641b6250bf9567329a6e65f901d","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"63b5d57e38bf07e9421a6309dc1f255446de9f22":["790e1fde4caa765b3faaad3fbcd25c6973450336"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["9ab1f5591dc05f1f2b5407d809c9699f75554a32","2ed5989ef2bc7bbd155dbff1e023ea849003dbf7"],"962d04139994fce5193143ef35615499a9a96d78":["bde51b089eb7f86171eb3406e38a274743f9b7ac","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["b6f9be74ca7baaef11857ad002cad40419979516","a37d7952ff54064a735708748444570f9963683e"],"817d8435e9135b756f08ce6710ab0baac51bdf88":["a3776dccca01c11e7046323cfad46a3b4a471233","a37d7952ff54064a735708748444570f9963683e"],"790e1fde4caa765b3faaad3fbcd25c6973450336":["c19f985e36a65cc969e8e564fe337a0d41512075"],"7b91922b55d15444d554721b352861d028eb8278":["44d6f0ab53c1962856b9f48dedb7a2a6cc18905c"],"a3776dccca01c11e7046323cfad46a3b4a471233":["790e1fde4caa765b3faaad3fbcd25c6973450336","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"3615ce4a1f785ae1b779244de52c6a7d99227e60":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd","1c5b026d03cbbb03ca4c0b97d14e9839682281dc"],"44d6f0ab53c1962856b9f48dedb7a2a6cc18905c":["ddc4c914be86e34b54f70023f45a60fa7f04e929"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"7118b8e5d127b58ad37740f4fa0881259a362090":["3a119bbc8703c10faa329ec201c654b3a35a1e3e","fd92b8bcc88e969302510acf77bd6970da3994c4","78a55f24d9b493c2a1cecf79f1d78279062b545b"],"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00":["93ccd971aca7fb61b7f1b946e44714cfc80bfc7c"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"93ccd971aca7fb61b7f1b946e44714cfc80bfc7c":[],"fd92b8bcc88e969302510acf77bd6970da3994c4":[],"639c36565ce03aed5b0fce7c9e4448e53a1f7efd":["b6f9be74ca7baaef11857ad002cad40419979516"],"1c5b026d03cbbb03ca4c0b97d14e9839682281dc":["3615ce4a1f785ae1b779244de52c6a7d99227e60"],"d14e8d18c0e3970c20354dbeeb49da11bd587fbd":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","1c5b026d03cbbb03ca4c0b97d14e9839682281dc","3615ce4a1f785ae1b779244de52c6a7d99227e60"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["bde51b089eb7f86171eb3406e38a274743f9b7ac"],"c19f985e36a65cc969e8e564fe337a0d41512075":["790e1fde4caa765b3faaad3fbcd25c6973450336"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["639c36565ce03aed5b0fce7c9e4448e53a1f7efd","a37d7952ff54064a735708748444570f9963683e","135621f3a0670a9394eb563224a3b76cc4dddc0f","962d04139994fce5193143ef35615499a9a96d78","a3776dccca01c11e7046323cfad46a3b4a471233"],"53a31399f2471493d67b19a95c028a74e0113b6a":["9ab1f5591dc05f1f2b5407d809c9699f75554a32","2ed5989ef2bc7bbd155dbff1e023ea849003dbf7"],"84b590669deb3d3a471cec6cb13b104b2ee94418":["53a31399f2471493d67b19a95c028a74e0113b6a","85a883878c0af761245ab048babc63d099f835f3"],"0e7c2454a6a8237bfd0e953f5b940838408c9055":["7118b8e5d127b58ad37740f4fa0881259a362090","93ccd971aca7fb61b7f1b946e44714cfc80bfc7c","fd92b8bcc88e969302510acf77bd6970da3994c4","78a55f24d9b493c2a1cecf79f1d78279062b545b"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","84b590669deb3d3a471cec6cb13b104b2ee94418","85a883878c0af761245ab048babc63d099f835f3"],"b6f9be74ca7baaef11857ad002cad40419979516":["d083e83f225b11e5fdd900e83d26ddb385b6955c"],"5d004d0e0b3f65bb40da76d476d659d7888270e8":[],"ddc4c914be86e34b54f70023f45a60fa7f04e929":["5d004d0e0b3f65bb40da76d476d659d7888270e8","44d6f0ab53c1962856b9f48dedb7a2a6cc18905c"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["962d04139994fce5193143ef35615499a9a96d78"],"78a55f24d9b493c2a1cecf79f1d78279062b545b":[],"a37d7952ff54064a735708748444570f9963683e":["ddc4c914be86e34b54f70023f45a60fa7f04e929","d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88"],"9ab1f5591dc05f1f2b5407d809c9699f75554a32":["ab5cb6a74aefb78aa0569857970b9151dfe2e787"],"2ed5989ef2bc7bbd155dbff1e023ea849003dbf7":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","c19f985e36a65cc969e8e564fe337a0d41512075","ab5cb6a74aefb78aa0569857970b9151dfe2e787"],"06584e6e98d592b34e1329b384182f368d2025e8":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd"],"85a883878c0af761245ab048babc63d099f835f3":["9ab1f5591dc05f1f2b5407d809c9699f75554a32"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":[],"63b5d57e38bf07e9421a6309dc1f255446de9f22":["f2c5f0cb44df114db4228c8f77861714b5cabaea"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["29ef99d61cda9641b6250bf9567329a6e65f901d"],"962d04139994fce5193143ef35615499a9a96d78":[],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["ddc4c914be86e34b54f70023f45a60fa7f04e929"],"817d8435e9135b756f08ce6710ab0baac51bdf88":["5d004d0e0b3f65bb40da76d476d659d7888270e8"],"790e1fde4caa765b3faaad3fbcd25c6973450336":["29ef99d61cda9641b6250bf9567329a6e65f901d","bde51b089eb7f86171eb3406e38a274743f9b7ac","63b5d57e38bf07e9421a6309dc1f255446de9f22","a3776dccca01c11e7046323cfad46a3b4a471233"],"7b91922b55d15444d554721b352861d028eb8278":["06584e6e98d592b34e1329b384182f368d2025e8"],"a3776dccca01c11e7046323cfad46a3b4a471233":["817d8435e9135b756f08ce6710ab0baac51bdf88"],"3615ce4a1f785ae1b779244de52c6a7d99227e60":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","0e7c2454a6a8237bfd0e953f5b940838408c9055"],"44d6f0ab53c1962856b9f48dedb7a2a6cc18905c":["7b91922b55d15444d554721b352861d028eb8278"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["93ccd971aca7fb61b7f1b946e44714cfc80bfc7c","fd92b8bcc88e969302510acf77bd6970da3994c4","5d004d0e0b3f65bb40da76d476d659d7888270e8","78a55f24d9b493c2a1cecf79f1d78279062b545b","135621f3a0670a9394eb563224a3b76cc4dddc0f","962d04139994fce5193143ef35615499a9a96d78","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}