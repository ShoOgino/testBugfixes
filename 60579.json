{"path":"lucene/analysis/common/src/test/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerTest#testLucene1133().mjava","commits":[{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerTest#testLucene1133().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerTest#testLucene1133().mjava","sourceNew":"  public void testLucene1133() throws Exception {\n    Set<String> untoks = new HashSet<String>();\n    untoks.add(WikipediaTokenizer.CATEGORY);\n    untoks.add(WikipediaTokenizer.ITALICS);\n    //should be exactly the same, regardless of untoks\n    WikipediaTokenizer tf = new WikipediaTokenizer(new StringReader(LINK_PHRASES), WikipediaTokenizer.TOKENS_ONLY, untoks);\n    checkLinkPhrases(tf);\n    String test = \"[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]\";\n    tf = new WikipediaTokenizer(new StringReader(test), WikipediaTokenizer.UNTOKENIZED_ONLY, untoks);\n    assertTokenStreamContents(tf,\n        new String[] { \"a b c d\", \"e f g\", \"link\", \"here\", \"link\",\n          \"there\", \"italics here\", \"something\", \"more italics\", \"h   i   j\" },\n        new int[] { 11, 32, 42, 47, 56, 61, 71, 86, 98, 124 },\n        new int[] { 18, 37, 46, 51, 60, 66, 83, 95, 110, 133 },\n        new int[] { 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 }\n       );\n  }\n\n","sourceOld":"  public void testLucene1133() throws Exception {\n    Set<String> untoks = new HashSet<String>();\n    untoks.add(WikipediaTokenizer.CATEGORY);\n    untoks.add(WikipediaTokenizer.ITALICS);\n    //should be exactly the same, regardless of untoks\n    WikipediaTokenizer tf = new WikipediaTokenizer(new StringReader(LINK_PHRASES), WikipediaTokenizer.TOKENS_ONLY, untoks);\n    checkLinkPhrases(tf);\n    String test = \"[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]\";\n    tf = new WikipediaTokenizer(new StringReader(test), WikipediaTokenizer.UNTOKENIZED_ONLY, untoks);\n    assertTokenStreamContents(tf,\n        new String[] { \"a b c d\", \"e f g\", \"link\", \"here\", \"link\",\n          \"there\", \"italics here\", \"something\", \"more italics\", \"h   i   j\" },\n        new int[] { 11, 32, 42, 47, 56, 61, 71, 86, 98, 124 },\n        new int[] { 18, 37, 46, 51, 60, 66, 83, 95, 110, 133 },\n        new int[] { 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 }\n       );\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","date":1389274049,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerTest#testLucene1133().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerTest#testLucene1133().mjava","sourceNew":"  public void testLucene1133() throws Exception {\n    Set<String> untoks = new HashSet<String>();\n    untoks.add(WikipediaTokenizer.CATEGORY);\n    untoks.add(WikipediaTokenizer.ITALICS);\n    //should be exactly the same, regardless of untoks\n    WikipediaTokenizer tf = new WikipediaTokenizer(WikipediaTokenizer.TOKENS_ONLY, untoks);\n    tf.setReader(new StringReader(LINK_PHRASES));\n    checkLinkPhrases(tf);\n    String test = \"[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]\";\n    tf = new WikipediaTokenizer(WikipediaTokenizer.UNTOKENIZED_ONLY, untoks);\n    tf.setReader(new StringReader(test));\n    assertTokenStreamContents(tf,\n        new String[] { \"a b c d\", \"e f g\", \"link\", \"here\", \"link\",\n          \"there\", \"italics here\", \"something\", \"more italics\", \"h   i   j\" },\n        new int[] { 11, 32, 42, 47, 56, 61, 71, 86, 98, 124 },\n        new int[] { 18, 37, 46, 51, 60, 66, 83, 95, 110, 133 },\n        new int[] { 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 }\n       );\n  }\n\n","sourceOld":"  public void testLucene1133() throws Exception {\n    Set<String> untoks = new HashSet<String>();\n    untoks.add(WikipediaTokenizer.CATEGORY);\n    untoks.add(WikipediaTokenizer.ITALICS);\n    //should be exactly the same, regardless of untoks\n    WikipediaTokenizer tf = new WikipediaTokenizer(new StringReader(LINK_PHRASES), WikipediaTokenizer.TOKENS_ONLY, untoks);\n    checkLinkPhrases(tf);\n    String test = \"[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]\";\n    tf = new WikipediaTokenizer(new StringReader(test), WikipediaTokenizer.UNTOKENIZED_ONLY, untoks);\n    assertTokenStreamContents(tf,\n        new String[] { \"a b c d\", \"e f g\", \"link\", \"here\", \"link\",\n          \"there\", \"italics here\", \"something\", \"more italics\", \"h   i   j\" },\n        new int[] { 11, 32, 42, 47, 56, 61, 71, 86, 98, 124 },\n        new int[] { 18, 37, 46, 51, 60, 66, 83, 95, 110, 133 },\n        new int[] { 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 }\n       );\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerTest#testLucene1133().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerTest#testLucene1133().mjava","sourceNew":"  public void testLucene1133() throws Exception {\n    Set<String> untoks = new HashSet<>();\n    untoks.add(WikipediaTokenizer.CATEGORY);\n    untoks.add(WikipediaTokenizer.ITALICS);\n    //should be exactly the same, regardless of untoks\n    WikipediaTokenizer tf = new WikipediaTokenizer(WikipediaTokenizer.TOKENS_ONLY, untoks);\n    tf.setReader(new StringReader(LINK_PHRASES));\n    checkLinkPhrases(tf);\n    String test = \"[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]\";\n    tf = new WikipediaTokenizer(WikipediaTokenizer.UNTOKENIZED_ONLY, untoks);\n    tf.setReader(new StringReader(test));\n    assertTokenStreamContents(tf,\n        new String[] { \"a b c d\", \"e f g\", \"link\", \"here\", \"link\",\n          \"there\", \"italics here\", \"something\", \"more italics\", \"h   i   j\" },\n        new int[] { 11, 32, 42, 47, 56, 61, 71, 86, 98, 124 },\n        new int[] { 18, 37, 46, 51, 60, 66, 83, 95, 110, 133 },\n        new int[] { 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 }\n       );\n  }\n\n","sourceOld":"  public void testLucene1133() throws Exception {\n    Set<String> untoks = new HashSet<String>();\n    untoks.add(WikipediaTokenizer.CATEGORY);\n    untoks.add(WikipediaTokenizer.ITALICS);\n    //should be exactly the same, regardless of untoks\n    WikipediaTokenizer tf = new WikipediaTokenizer(WikipediaTokenizer.TOKENS_ONLY, untoks);\n    tf.setReader(new StringReader(LINK_PHRASES));\n    checkLinkPhrases(tf);\n    String test = \"[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]\";\n    tf = new WikipediaTokenizer(WikipediaTokenizer.UNTOKENIZED_ONLY, untoks);\n    tf.setReader(new StringReader(test));\n    assertTokenStreamContents(tf,\n        new String[] { \"a b c d\", \"e f g\", \"link\", \"here\", \"link\",\n          \"there\", \"italics here\", \"something\", \"more italics\", \"h   i   j\" },\n        new int[] { 11, 32, 42, 47, 56, 61, 71, 86, 98, 124 },\n        new int[] { 18, 37, 46, 51, 60, 66, 83, 95, 110, 133 },\n        new int[] { 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 }\n       );\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"923f36bb0db6f793cf62dbb68723ae3bfbaf1d75","date":1399205975,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerTest#testLucene1133().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerTest#testLucene1133().mjava","sourceNew":"  public void testLucene1133() throws Exception {\n    Set<String> untoks = new HashSet<>();\n    untoks.add(WikipediaTokenizer.CATEGORY);\n    untoks.add(WikipediaTokenizer.ITALICS);\n    //should be exactly the same, regardless of untoks\n    WikipediaTokenizer tf = new WikipediaTokenizer(newAttributeFactory(), WikipediaTokenizer.TOKENS_ONLY, untoks);\n    tf.setReader(new StringReader(LINK_PHRASES));\n    checkLinkPhrases(tf);\n    String test = \"[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]\";\n    tf = new WikipediaTokenizer(WikipediaTokenizer.UNTOKENIZED_ONLY, untoks);\n    tf.setReader(new StringReader(test));\n    assertTokenStreamContents(tf,\n        new String[] { \"a b c d\", \"e f g\", \"link\", \"here\", \"link\",\n          \"there\", \"italics here\", \"something\", \"more italics\", \"h   i   j\" },\n        new int[] { 11, 32, 42, 47, 56, 61, 71, 86, 98, 124 },\n        new int[] { 18, 37, 46, 51, 60, 66, 83, 95, 110, 133 },\n        new int[] { 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 }\n       );\n  }\n\n","sourceOld":"  public void testLucene1133() throws Exception {\n    Set<String> untoks = new HashSet<>();\n    untoks.add(WikipediaTokenizer.CATEGORY);\n    untoks.add(WikipediaTokenizer.ITALICS);\n    //should be exactly the same, regardless of untoks\n    WikipediaTokenizer tf = new WikipediaTokenizer(WikipediaTokenizer.TOKENS_ONLY, untoks);\n    tf.setReader(new StringReader(LINK_PHRASES));\n    checkLinkPhrases(tf);\n    String test = \"[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]\";\n    tf = new WikipediaTokenizer(WikipediaTokenizer.UNTOKENIZED_ONLY, untoks);\n    tf.setReader(new StringReader(test));\n    assertTokenStreamContents(tf,\n        new String[] { \"a b c d\", \"e f g\", \"link\", \"here\", \"link\",\n          \"there\", \"italics here\", \"something\", \"more italics\", \"h   i   j\" },\n        new int[] { 11, 32, 42, 47, 56, 61, 71, 86, 98, 124 },\n        new int[] { 18, 37, 46, 51, 60, 66, 83, 95, 110, 133 },\n        new int[] { 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 }\n       );\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"923f36bb0db6f793cf62dbb68723ae3bfbaf1d75":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["923f36bb0db6f793cf62dbb68723ae3bfbaf1d75"]},"commit2Childs":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["923f36bb0db6f793cf62dbb68723ae3bfbaf1d75"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"923f36bb0db6f793cf62dbb68723ae3bfbaf1d75":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}