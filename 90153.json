{"path":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findMerges(MergeTrigger,SegmentInfos,MergeContext).mjava","commits":[{"id":"1d28f215464f76024caf026606f8ea51a5319c53","date":1527226629,"type":1,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findMerges(MergeTrigger,SegmentInfos,MergeContext).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findMerges(MergeTrigger,SegmentInfos,IndexWriter).mjava","sourceNew":"  @Override\n  public MergeSpecification findMerges(MergeTrigger mergeTrigger, SegmentInfos infos, MergeContext mergeContext) throws IOException {\n    if (verbose(mergeContext)) {\n      message(\"findMerges: \" + infos.size() + \" segments\", mergeContext);\n    }\n    if (infos.size() == 0) {\n      return null;\n    }\n    final Set<SegmentCommitInfo> merging = mergeContext.getMergingSegments();\n    final Set<SegmentCommitInfo> toBeMerged = new HashSet<>();\n\n    final List<SegmentCommitInfo> infosSorted = new ArrayList<>(infos.asList());\n\n    // The size can change concurrently while we are running here, because deletes\n    // are now applied concurrently, and this can piss off TimSort!  So we\n    // call size() once per segment and sort by that:\n    Map<SegmentCommitInfo,Long> sizeInBytes = getSegmentSizes(mergeContext, infos.asList());\n    \n    infosSorted.sort(new SegmentByteSizeDescending(sizeInBytes));\n\n    // Compute total index bytes & print details about the index\n    long totIndexBytes = 0;\n    long minSegmentBytes = Long.MAX_VALUE;\n    for(SegmentCommitInfo info : infosSorted) {\n      final long segBytes = sizeInBytes.get(info);\n      if (verbose(mergeContext)) {\n        String extra = merging.contains(info) ? \" [merging]\" : \"\";\n        if (segBytes >= maxMergedSegmentBytes/2.0) {\n          extra += \" [skip: too large]\";\n        } else if (segBytes < floorSegmentBytes) {\n          extra += \" [floored]\";\n        }\n        message(\"  seg=\" + segString(mergeContext, Collections.singleton(info)) + \" size=\" + String.format(Locale.ROOT, \"%.3f\", segBytes/1024/1024.) + \" MB\" + extra, mergeContext);\n      }\n\n      minSegmentBytes = Math.min(segBytes, minSegmentBytes);\n      // Accum total byte size\n      totIndexBytes += segBytes;\n    }\n\n    // If we have too-large segments, grace them out\n    // of the maxSegmentCount:\n    int tooBigCount = 0;\n    while (tooBigCount < infosSorted.size()) {\n      long segBytes = sizeInBytes.get(infosSorted.get(tooBigCount));\n      if (segBytes < maxMergedSegmentBytes/2.0) {\n        break;\n      }\n      totIndexBytes -= segBytes;\n      tooBigCount++;\n    }\n\n    minSegmentBytes = floorSize(minSegmentBytes);\n\n    // Compute max allowed segs in the index\n    long levelSize = minSegmentBytes;\n    long bytesLeft = totIndexBytes;\n    double allowedSegCount = 0;\n    while(true) {\n      final double segCountLevel = bytesLeft / (double) levelSize;\n      if (segCountLevel < segsPerTier) {\n        allowedSegCount += Math.ceil(segCountLevel);\n        break;\n      }\n      allowedSegCount += segsPerTier;\n      bytesLeft -= segsPerTier * levelSize;\n      levelSize *= maxMergeAtOnce;\n    }\n    int allowedSegCountInt = (int) allowedSegCount;\n\n    MergeSpecification spec = null;\n\n    // Cycle to possibly select more than one merge:\n    while(true) {\n\n      long mergingBytes = 0;\n\n      // Gather eligible segments for merging, ie segments\n      // not already being merged and not already picked (by\n      // prior iteration of this loop) for merging:\n      final List<SegmentCommitInfo> eligible = new ArrayList<>();\n      for(int idx = tooBigCount; idx<infosSorted.size(); idx++) {\n        final SegmentCommitInfo info = infosSorted.get(idx);\n        if (merging.contains(info)) {\n          mergingBytes += sizeInBytes.get(info);\n        } else if (!toBeMerged.contains(info)) {\n          eligible.add(info);\n        }\n      }\n\n      final boolean maxMergeIsRunning = mergingBytes >= maxMergedSegmentBytes;\n\n      if (verbose(mergeContext)) {\n        message(\"  allowedSegmentCount=\" + allowedSegCountInt + \" vs count=\" + infosSorted.size() + \" (eligible count=\" + eligible.size() + \") tooBigCount=\" + tooBigCount, mergeContext);\n      }\n\n      if (eligible.size() == 0) {\n        return spec;\n      }\n\n      if (eligible.size() > allowedSegCountInt) {\n\n        // OK we are over budget -- find best merge!\n        MergeScore bestScore = null;\n        List<SegmentCommitInfo> best = null;\n        boolean bestTooLarge = false;\n        long bestMergeBytes = 0;\n\n        // Consider all merge starts:\n        for(int startIdx = 0;startIdx <= eligible.size()-maxMergeAtOnce; startIdx++) {\n\n          long totAfterMergeBytes = 0;\n\n          final List<SegmentCommitInfo> candidate = new ArrayList<>();\n          boolean hitTooLarge = false;\n          for(int idx = startIdx;idx<eligible.size() && candidate.size() < maxMergeAtOnce;idx++) {\n            final SegmentCommitInfo info = eligible.get(idx);\n            final long segBytes = sizeInBytes.get(info);\n\n            if (totAfterMergeBytes + segBytes > maxMergedSegmentBytes) {\n              hitTooLarge = true;\n              // NOTE: we continue, so that we can try\n              // \"packing\" smaller segments into this merge\n              // to see if we can get closer to the max\n              // size; this in general is not perfect since\n              // this is really \"bin packing\" and we'd have\n              // to try different permutations.\n              continue;\n            }\n            candidate.add(info);\n            totAfterMergeBytes += segBytes;\n          }\n\n          // We should never see an empty candidate: we iterated over maxMergeAtOnce\n          // segments, and already pre-excluded the too-large segments:\n          assert candidate.size() > 0;\n\n          final MergeScore score = score(candidate, hitTooLarge, sizeInBytes);\n          if (verbose(mergeContext)) {\n            message(\"  maybe=\" + segString(mergeContext, candidate) + \" score=\" + score.getScore() + \" \" + score.getExplanation() + \" tooLarge=\" + hitTooLarge + \" size=\" + String.format(Locale.ROOT, \"%.3f MB\", totAfterMergeBytes/1024./1024.), mergeContext);\n          }\n\n          // If we are already running a max sized merge\n          // (maxMergeIsRunning), don't allow another max\n          // sized merge to kick off:\n          if ((bestScore == null || score.getScore() < bestScore.getScore()) && (!hitTooLarge || !maxMergeIsRunning)) {\n            best = candidate;\n            bestScore = score;\n            bestTooLarge = hitTooLarge;\n            bestMergeBytes = totAfterMergeBytes;\n          }\n        }\n        \n        if (best != null) {\n          if (spec == null) {\n            spec = new MergeSpecification();\n          }\n          final OneMerge merge = new OneMerge(best);\n          spec.add(merge);\n          toBeMerged.addAll(merge.segments);\n\n          if (verbose(mergeContext)) {\n            message(\"  add merge=\" + segString(mergeContext, merge.segments) + \" size=\" + String.format(Locale.ROOT, \"%.3f MB\", bestMergeBytes/1024./1024.) + \" score=\" + String.format(Locale.ROOT, \"%.3f\", bestScore.getScore()) + \" \" + bestScore.getExplanation() + (bestTooLarge ? \" [max merge]\" : \"\"), mergeContext);\n          }\n        } else {\n          return spec;\n        }\n      } else {\n        return spec;\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public MergeSpecification findMerges(MergeTrigger mergeTrigger, SegmentInfos infos, IndexWriter writer) throws IOException {\n    if (verbose(writer)) {\n      message(\"findMerges: \" + infos.size() + \" segments\", writer);\n    }\n    if (infos.size() == 0) {\n      return null;\n    }\n    final Set<SegmentCommitInfo> merging = writer.getMergingSegments();\n    final Set<SegmentCommitInfo> toBeMerged = new HashSet<>();\n\n    final List<SegmentCommitInfo> infosSorted = new ArrayList<>(infos.asList());\n\n    // The size can change concurrently while we are running here, because deletes\n    // are now applied concurrently, and this can piss off TimSort!  So we\n    // call size() once per segment and sort by that:\n    Map<SegmentCommitInfo,Long> sizeInBytes = getSegmentSizes(writer, infos.asList());\n    \n    infosSorted.sort(new SegmentByteSizeDescending(sizeInBytes));\n\n    // Compute total index bytes & print details about the index\n    long totIndexBytes = 0;\n    long minSegmentBytes = Long.MAX_VALUE;\n    for(SegmentCommitInfo info : infosSorted) {\n      final long segBytes = sizeInBytes.get(info);\n      if (verbose(writer)) {\n        String extra = merging.contains(info) ? \" [merging]\" : \"\";\n        if (segBytes >= maxMergedSegmentBytes/2.0) {\n          extra += \" [skip: too large]\";\n        } else if (segBytes < floorSegmentBytes) {\n          extra += \" [floored]\";\n        }\n        message(\"  seg=\" + writer.segString(info) + \" size=\" + String.format(Locale.ROOT, \"%.3f\", segBytes/1024/1024.) + \" MB\" + extra, writer);\n      }\n\n      minSegmentBytes = Math.min(segBytes, minSegmentBytes);\n      // Accum total byte size\n      totIndexBytes += segBytes;\n    }\n\n    // If we have too-large segments, grace them out\n    // of the maxSegmentCount:\n    int tooBigCount = 0;\n    while (tooBigCount < infosSorted.size()) {\n      long segBytes = sizeInBytes.get(infosSorted.get(tooBigCount));\n      if (segBytes < maxMergedSegmentBytes/2.0) {\n        break;\n      }\n      totIndexBytes -= segBytes;\n      tooBigCount++;\n    }\n\n    minSegmentBytes = floorSize(minSegmentBytes);\n\n    // Compute max allowed segs in the index\n    long levelSize = minSegmentBytes;\n    long bytesLeft = totIndexBytes;\n    double allowedSegCount = 0;\n    while(true) {\n      final double segCountLevel = bytesLeft / (double) levelSize;\n      if (segCountLevel < segsPerTier) {\n        allowedSegCount += Math.ceil(segCountLevel);\n        break;\n      }\n      allowedSegCount += segsPerTier;\n      bytesLeft -= segsPerTier * levelSize;\n      levelSize *= maxMergeAtOnce;\n    }\n    int allowedSegCountInt = (int) allowedSegCount;\n\n    MergeSpecification spec = null;\n\n    // Cycle to possibly select more than one merge:\n    while(true) {\n\n      long mergingBytes = 0;\n\n      // Gather eligible segments for merging, ie segments\n      // not already being merged and not already picked (by\n      // prior iteration of this loop) for merging:\n      final List<SegmentCommitInfo> eligible = new ArrayList<>();\n      for(int idx = tooBigCount; idx<infosSorted.size(); idx++) {\n        final SegmentCommitInfo info = infosSorted.get(idx);\n        if (merging.contains(info)) {\n          mergingBytes += sizeInBytes.get(info);\n        } else if (!toBeMerged.contains(info)) {\n          eligible.add(info);\n        }\n      }\n\n      final boolean maxMergeIsRunning = mergingBytes >= maxMergedSegmentBytes;\n\n      if (verbose(writer)) {\n        message(\"  allowedSegmentCount=\" + allowedSegCountInt + \" vs count=\" + infosSorted.size() + \" (eligible count=\" + eligible.size() + \") tooBigCount=\" + tooBigCount, writer);\n      }\n\n      if (eligible.size() == 0) {\n        return spec;\n      }\n\n      if (eligible.size() > allowedSegCountInt) {\n\n        // OK we are over budget -- find best merge!\n        MergeScore bestScore = null;\n        List<SegmentCommitInfo> best = null;\n        boolean bestTooLarge = false;\n        long bestMergeBytes = 0;\n\n        // Consider all merge starts:\n        for(int startIdx = 0;startIdx <= eligible.size()-maxMergeAtOnce; startIdx++) {\n\n          long totAfterMergeBytes = 0;\n\n          final List<SegmentCommitInfo> candidate = new ArrayList<>();\n          boolean hitTooLarge = false;\n          for(int idx = startIdx;idx<eligible.size() && candidate.size() < maxMergeAtOnce;idx++) {\n            final SegmentCommitInfo info = eligible.get(idx);\n            final long segBytes = sizeInBytes.get(info);\n\n            if (totAfterMergeBytes + segBytes > maxMergedSegmentBytes) {\n              hitTooLarge = true;\n              // NOTE: we continue, so that we can try\n              // \"packing\" smaller segments into this merge\n              // to see if we can get closer to the max\n              // size; this in general is not perfect since\n              // this is really \"bin packing\" and we'd have\n              // to try different permutations.\n              continue;\n            }\n            candidate.add(info);\n            totAfterMergeBytes += segBytes;\n          }\n\n          // We should never see an empty candidate: we iterated over maxMergeAtOnce\n          // segments, and already pre-excluded the too-large segments:\n          assert candidate.size() > 0;\n\n          final MergeScore score = score(candidate, hitTooLarge, mergingBytes, writer, sizeInBytes);\n          if (verbose(writer)) {\n            message(\"  maybe=\" + writer.segString(candidate) + \" score=\" + score.getScore() + \" \" + score.getExplanation() + \" tooLarge=\" + hitTooLarge + \" size=\" + String.format(Locale.ROOT, \"%.3f MB\", totAfterMergeBytes/1024./1024.), writer);\n          }\n\n          // If we are already running a max sized merge\n          // (maxMergeIsRunning), don't allow another max\n          // sized merge to kick off:\n          if ((bestScore == null || score.getScore() < bestScore.getScore()) && (!hitTooLarge || !maxMergeIsRunning)) {\n            best = candidate;\n            bestScore = score;\n            bestTooLarge = hitTooLarge;\n            bestMergeBytes = totAfterMergeBytes;\n          }\n        }\n        \n        if (best != null) {\n          if (spec == null) {\n            spec = new MergeSpecification();\n          }\n          final OneMerge merge = new OneMerge(best);\n          spec.add(merge);\n          toBeMerged.addAll(merge.segments);\n\n          if (verbose(writer)) {\n            message(\"  add merge=\" + writer.segString(merge.segments) + \" size=\" + String.format(Locale.ROOT, \"%.3f MB\", bestMergeBytes/1024./1024.) + \" score=\" + String.format(Locale.ROOT, \"%.3f\", bestScore.getScore()) + \" \" + bestScore.getExplanation() + (bestTooLarge ? \" [max merge]\" : \"\"), writer);\n          }\n        } else {\n          return spec;\n        }\n      } else {\n        return spec;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"56fb5e4e4b239474721e13b4cd9542ea2d215451","date":1529091182,"type":3,"author":"Erick","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findMerges(MergeTrigger,SegmentInfos,MergeContext).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findMerges(MergeTrigger,SegmentInfos,MergeContext).mjava","sourceNew":"  @Override\n  public MergeSpecification findMerges(MergeTrigger mergeTrigger, SegmentInfos infos, MergeContext mergeContext) throws IOException {\n    final Set<SegmentCommitInfo> merging = mergeContext.getMergingSegments();\n    // Compute total index bytes & print details about the index\n    long totIndexBytes = 0;\n    long minSegmentBytes = Long.MAX_VALUE;\n\n    int totalDelDocs = 0;\n    int totalMaxDoc = 0;\n\n    List<SegmentSizeAndDocs> sortedInfos = getSortedBySegmentSize(infos, mergeContext);\n    Iterator<SegmentSizeAndDocs> iter = sortedInfos.iterator();\n    while (iter.hasNext()) {\n      SegmentSizeAndDocs segSizeDocs = iter.next();\n      final long segBytes = segSizeDocs.sizeInBytes;\n      if (verbose(mergeContext)) {\n        String extra = merging.contains(segSizeDocs.segInfo) ? \" [merging]\" : \"\";\n        if (segBytes >= maxMergedSegmentBytes) {\n          extra += \" [skip: too large]\";\n        } else if (segBytes < floorSegmentBytes) {\n          extra += \" [floored]\";\n        }\n        message(\"  seg=\" + segString(mergeContext, Collections.singleton(segSizeDocs.segInfo)) + \" size=\" + String.format(Locale.ROOT, \"%.3f\", segBytes / 1024 / 1024.) + \" MB\" + extra, mergeContext);\n      }\n      if (merging.contains(segSizeDocs.segInfo)) {\n        iter.remove();\n      } else {\n        totalDelDocs += segSizeDocs.delCount;\n        totalMaxDoc += segSizeDocs.maxDoc;\n      }\n\n      minSegmentBytes = Math.min(segBytes, minSegmentBytes);\n      totIndexBytes += segBytes;\n    }\n    assert totalMaxDoc >= 0;\n    assert totalDelDocs >= 0;\n\n    // Compute max allowed segments in the index\n    long levelSize = Math.max(minSegmentBytes, floorSegmentBytes);\n    long bytesLeft = totIndexBytes;\n    double allowedSegCount = 0;\n    while (true) {\n      final double segCountLevel = bytesLeft / (double) levelSize;\n      if (segCountLevel < segsPerTier) {\n        allowedSegCount += Math.ceil(segCountLevel);\n        break;\n      }\n      allowedSegCount += segsPerTier;\n      bytesLeft -= segsPerTier * levelSize;\n      levelSize *= maxMergeAtOnce;\n    }\n\n    // If we're above certain thresholds, we can merge very large segments.\n    double totalDelPct = (double) totalDelDocs / (double) totalMaxDoc;\n    //TODO: See LUCENE-8263\n    //double targetAsPct = indexPctDeletedTarget / 100.0;\n    double targetAsPct = 0.5;\n    int tooBigCount = 0;\n    iter = sortedInfos.iterator();\n\n    // remove large segments from consideration under two conditions.\n    // 1> Overall percent deleted docs relatively small and this segment is larger than 50% maxSegSize\n    // 2> overall percent deleted docs large and this segment is large and has few deleted docs\n\n    long mergingBytes = 0L;\n\n    while (iter.hasNext()) {\n      SegmentSizeAndDocs segSizeDocs = iter.next();\n      double segDelPct = (double) segSizeDocs.delCount / (double) segSizeDocs.maxDoc;\n      if (segSizeDocs.sizeInBytes > maxMergedSegmentBytes / 2 && (totalDelPct < targetAsPct || segDelPct < targetAsPct)) {\n        iter.remove();\n        tooBigCount++; // Just for reporting purposes.\n      } else {\n        mergingBytes += segSizeDocs.sizeInBytes;\n      }\n\n    }\n    if (verbose(mergeContext) && tooBigCount > 0) {\n      message(\"  allowedSegmentCount=\" + allowedSegCount + \" vs count=\" + infos.size() +\n          \" (eligible count=\" + sortedInfos.size() + \") tooBigCount= \" + tooBigCount, mergeContext);\n    }\n    return doFindMerges(sortedInfos, maxMergedSegmentBytes, maxMergeAtOnce, (int) allowedSegCount, MERGE_TYPE.NATURAL,\n        mergeContext, mergingBytes >= maxMergedSegmentBytes);\n  }\n\n","sourceOld":"  @Override\n  public MergeSpecification findMerges(MergeTrigger mergeTrigger, SegmentInfos infos, MergeContext mergeContext) throws IOException {\n    if (verbose(mergeContext)) {\n      message(\"findMerges: \" + infos.size() + \" segments\", mergeContext);\n    }\n    if (infos.size() == 0) {\n      return null;\n    }\n    final Set<SegmentCommitInfo> merging = mergeContext.getMergingSegments();\n    final Set<SegmentCommitInfo> toBeMerged = new HashSet<>();\n\n    final List<SegmentCommitInfo> infosSorted = new ArrayList<>(infos.asList());\n\n    // The size can change concurrently while we are running here, because deletes\n    // are now applied concurrently, and this can piss off TimSort!  So we\n    // call size() once per segment and sort by that:\n    Map<SegmentCommitInfo,Long> sizeInBytes = getSegmentSizes(mergeContext, infos.asList());\n    \n    infosSorted.sort(new SegmentByteSizeDescending(sizeInBytes));\n\n    // Compute total index bytes & print details about the index\n    long totIndexBytes = 0;\n    long minSegmentBytes = Long.MAX_VALUE;\n    for(SegmentCommitInfo info : infosSorted) {\n      final long segBytes = sizeInBytes.get(info);\n      if (verbose(mergeContext)) {\n        String extra = merging.contains(info) ? \" [merging]\" : \"\";\n        if (segBytes >= maxMergedSegmentBytes/2.0) {\n          extra += \" [skip: too large]\";\n        } else if (segBytes < floorSegmentBytes) {\n          extra += \" [floored]\";\n        }\n        message(\"  seg=\" + segString(mergeContext, Collections.singleton(info)) + \" size=\" + String.format(Locale.ROOT, \"%.3f\", segBytes/1024/1024.) + \" MB\" + extra, mergeContext);\n      }\n\n      minSegmentBytes = Math.min(segBytes, minSegmentBytes);\n      // Accum total byte size\n      totIndexBytes += segBytes;\n    }\n\n    // If we have too-large segments, grace them out\n    // of the maxSegmentCount:\n    int tooBigCount = 0;\n    while (tooBigCount < infosSorted.size()) {\n      long segBytes = sizeInBytes.get(infosSorted.get(tooBigCount));\n      if (segBytes < maxMergedSegmentBytes/2.0) {\n        break;\n      }\n      totIndexBytes -= segBytes;\n      tooBigCount++;\n    }\n\n    minSegmentBytes = floorSize(minSegmentBytes);\n\n    // Compute max allowed segs in the index\n    long levelSize = minSegmentBytes;\n    long bytesLeft = totIndexBytes;\n    double allowedSegCount = 0;\n    while(true) {\n      final double segCountLevel = bytesLeft / (double) levelSize;\n      if (segCountLevel < segsPerTier) {\n        allowedSegCount += Math.ceil(segCountLevel);\n        break;\n      }\n      allowedSegCount += segsPerTier;\n      bytesLeft -= segsPerTier * levelSize;\n      levelSize *= maxMergeAtOnce;\n    }\n    int allowedSegCountInt = (int) allowedSegCount;\n\n    MergeSpecification spec = null;\n\n    // Cycle to possibly select more than one merge:\n    while(true) {\n\n      long mergingBytes = 0;\n\n      // Gather eligible segments for merging, ie segments\n      // not already being merged and not already picked (by\n      // prior iteration of this loop) for merging:\n      final List<SegmentCommitInfo> eligible = new ArrayList<>();\n      for(int idx = tooBigCount; idx<infosSorted.size(); idx++) {\n        final SegmentCommitInfo info = infosSorted.get(idx);\n        if (merging.contains(info)) {\n          mergingBytes += sizeInBytes.get(info);\n        } else if (!toBeMerged.contains(info)) {\n          eligible.add(info);\n        }\n      }\n\n      final boolean maxMergeIsRunning = mergingBytes >= maxMergedSegmentBytes;\n\n      if (verbose(mergeContext)) {\n        message(\"  allowedSegmentCount=\" + allowedSegCountInt + \" vs count=\" + infosSorted.size() + \" (eligible count=\" + eligible.size() + \") tooBigCount=\" + tooBigCount, mergeContext);\n      }\n\n      if (eligible.size() == 0) {\n        return spec;\n      }\n\n      if (eligible.size() > allowedSegCountInt) {\n\n        // OK we are over budget -- find best merge!\n        MergeScore bestScore = null;\n        List<SegmentCommitInfo> best = null;\n        boolean bestTooLarge = false;\n        long bestMergeBytes = 0;\n\n        // Consider all merge starts:\n        for(int startIdx = 0;startIdx <= eligible.size()-maxMergeAtOnce; startIdx++) {\n\n          long totAfterMergeBytes = 0;\n\n          final List<SegmentCommitInfo> candidate = new ArrayList<>();\n          boolean hitTooLarge = false;\n          for(int idx = startIdx;idx<eligible.size() && candidate.size() < maxMergeAtOnce;idx++) {\n            final SegmentCommitInfo info = eligible.get(idx);\n            final long segBytes = sizeInBytes.get(info);\n\n            if (totAfterMergeBytes + segBytes > maxMergedSegmentBytes) {\n              hitTooLarge = true;\n              // NOTE: we continue, so that we can try\n              // \"packing\" smaller segments into this merge\n              // to see if we can get closer to the max\n              // size; this in general is not perfect since\n              // this is really \"bin packing\" and we'd have\n              // to try different permutations.\n              continue;\n            }\n            candidate.add(info);\n            totAfterMergeBytes += segBytes;\n          }\n\n          // We should never see an empty candidate: we iterated over maxMergeAtOnce\n          // segments, and already pre-excluded the too-large segments:\n          assert candidate.size() > 0;\n\n          final MergeScore score = score(candidate, hitTooLarge, sizeInBytes);\n          if (verbose(mergeContext)) {\n            message(\"  maybe=\" + segString(mergeContext, candidate) + \" score=\" + score.getScore() + \" \" + score.getExplanation() + \" tooLarge=\" + hitTooLarge + \" size=\" + String.format(Locale.ROOT, \"%.3f MB\", totAfterMergeBytes/1024./1024.), mergeContext);\n          }\n\n          // If we are already running a max sized merge\n          // (maxMergeIsRunning), don't allow another max\n          // sized merge to kick off:\n          if ((bestScore == null || score.getScore() < bestScore.getScore()) && (!hitTooLarge || !maxMergeIsRunning)) {\n            best = candidate;\n            bestScore = score;\n            bestTooLarge = hitTooLarge;\n            bestMergeBytes = totAfterMergeBytes;\n          }\n        }\n        \n        if (best != null) {\n          if (spec == null) {\n            spec = new MergeSpecification();\n          }\n          final OneMerge merge = new OneMerge(best);\n          spec.add(merge);\n          toBeMerged.addAll(merge.segments);\n\n          if (verbose(mergeContext)) {\n            message(\"  add merge=\" + segString(mergeContext, merge.segments) + \" size=\" + String.format(Locale.ROOT, \"%.3f MB\", bestMergeBytes/1024./1024.) + \" score=\" + String.format(Locale.ROOT, \"%.3f\", bestScore.getScore()) + \" \" + bestScore.getExplanation() + (bestTooLarge ? \" [max merge]\" : \"\"), mergeContext);\n          }\n        } else {\n          return spec;\n        }\n      } else {\n        return spec;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["404dafe26b816e6ed478486e26abd62d8607b123"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"404dafe26b816e6ed478486e26abd62d8607b123","date":1531142470,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findMerges(MergeTrigger,SegmentInfos,MergeContext).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findMerges(MergeTrigger,SegmentInfos,MergeContext).mjava","sourceNew":"  @Override\n  public MergeSpecification findMerges(MergeTrigger mergeTrigger, SegmentInfos infos, MergeContext mergeContext) throws IOException {\n    final Set<SegmentCommitInfo> merging = mergeContext.getMergingSegments();\n    // Compute total index bytes & print details about the index\n    long totIndexBytes = 0;\n    long minSegmentBytes = Long.MAX_VALUE;\n\n    int totalDelDocs = 0;\n    int totalMaxDoc = 0;\n\n    List<SegmentSizeAndDocs> sortedInfos = getSortedBySegmentSize(infos, mergeContext);\n    Iterator<SegmentSizeAndDocs> iter = sortedInfos.iterator();\n    while (iter.hasNext()) {\n      SegmentSizeAndDocs segSizeDocs = iter.next();\n      final long segBytes = segSizeDocs.sizeInBytes;\n      if (verbose(mergeContext)) {\n        String extra = merging.contains(segSizeDocs.segInfo) ? \" [merging]\" : \"\";\n        if (segBytes >= maxMergedSegmentBytes) {\n          extra += \" [skip: too large]\";\n        } else if (segBytes < floorSegmentBytes) {\n          extra += \" [floored]\";\n        }\n        message(\"  seg=\" + segString(mergeContext, Collections.singleton(segSizeDocs.segInfo)) + \" size=\" + String.format(Locale.ROOT, \"%.3f\", segBytes / 1024 / 1024.) + \" MB\" + extra, mergeContext);\n      }\n      if (merging.contains(segSizeDocs.segInfo)) {\n        iter.remove();\n      } else {\n        totalDelDocs += segSizeDocs.delCount;\n        totalMaxDoc += segSizeDocs.maxDoc;\n      }\n\n      minSegmentBytes = Math.min(segBytes, minSegmentBytes);\n      totIndexBytes += segBytes;\n    }\n    assert totalMaxDoc >= 0;\n    assert totalDelDocs >= 0;\n\n    // If we have too-large segments, grace them out of the maximum segment count\n    // If we're above certain thresholds, we can merge very large segments.\n    double totalDelPct = (double) totalDelDocs / (double) totalMaxDoc;\n    //TODO: See LUCENE-8263\n    //double targetAsPct = indexPctDeletedTarget / 100.0;\n    double targetAsPct = 0.5;\n    int tooBigCount = 0;\n    iter = sortedInfos.iterator();\n\n    // remove large segments from consideration under two conditions.\n    // 1> Overall percent deleted docs relatively small and this segment is larger than 50% maxSegSize\n    // 2> overall percent deleted docs large and this segment is large and has few deleted docs\n\n    long mergingBytes = 0L;\n\n    while (iter.hasNext()) {\n      SegmentSizeAndDocs segSizeDocs = iter.next();\n      double segDelPct = (double) segSizeDocs.delCount / (double) segSizeDocs.maxDoc;\n      if (segSizeDocs.sizeInBytes > maxMergedSegmentBytes / 2 && (totalDelPct < targetAsPct || segDelPct < targetAsPct)) {\n        iter.remove();\n        tooBigCount++; // Just for reporting purposes.\n        totIndexBytes -= segSizeDocs.sizeInBytes;\n      } else {\n        mergingBytes += segSizeDocs.sizeInBytes;\n      }\n    }\n\n    // Compute max allowed segments in the index\n    long levelSize = Math.max(minSegmentBytes, floorSegmentBytes);\n    long bytesLeft = totIndexBytes;\n    double allowedSegCount = 0;\n    while (true) {\n      final double segCountLevel = bytesLeft / (double) levelSize;\n      if (segCountLevel < segsPerTier) {\n        allowedSegCount += Math.ceil(segCountLevel);\n        break;\n      }\n      allowedSegCount += segsPerTier;\n      bytesLeft -= segsPerTier * levelSize;\n      levelSize *= maxMergeAtOnce;\n    }\n\n    if (verbose(mergeContext) && tooBigCount > 0) {\n      message(\"  allowedSegmentCount=\" + allowedSegCount + \" vs count=\" + infos.size() +\n          \" (eligible count=\" + sortedInfos.size() + \") tooBigCount= \" + tooBigCount, mergeContext);\n    }\n    return doFindMerges(sortedInfos, maxMergedSegmentBytes, maxMergeAtOnce, (int) allowedSegCount, MERGE_TYPE.NATURAL,\n        mergeContext, mergingBytes >= maxMergedSegmentBytes);\n  }\n\n","sourceOld":"  @Override\n  public MergeSpecification findMerges(MergeTrigger mergeTrigger, SegmentInfos infos, MergeContext mergeContext) throws IOException {\n    final Set<SegmentCommitInfo> merging = mergeContext.getMergingSegments();\n    // Compute total index bytes & print details about the index\n    long totIndexBytes = 0;\n    long minSegmentBytes = Long.MAX_VALUE;\n\n    int totalDelDocs = 0;\n    int totalMaxDoc = 0;\n\n    List<SegmentSizeAndDocs> sortedInfos = getSortedBySegmentSize(infos, mergeContext);\n    Iterator<SegmentSizeAndDocs> iter = sortedInfos.iterator();\n    while (iter.hasNext()) {\n      SegmentSizeAndDocs segSizeDocs = iter.next();\n      final long segBytes = segSizeDocs.sizeInBytes;\n      if (verbose(mergeContext)) {\n        String extra = merging.contains(segSizeDocs.segInfo) ? \" [merging]\" : \"\";\n        if (segBytes >= maxMergedSegmentBytes) {\n          extra += \" [skip: too large]\";\n        } else if (segBytes < floorSegmentBytes) {\n          extra += \" [floored]\";\n        }\n        message(\"  seg=\" + segString(mergeContext, Collections.singleton(segSizeDocs.segInfo)) + \" size=\" + String.format(Locale.ROOT, \"%.3f\", segBytes / 1024 / 1024.) + \" MB\" + extra, mergeContext);\n      }\n      if (merging.contains(segSizeDocs.segInfo)) {\n        iter.remove();\n      } else {\n        totalDelDocs += segSizeDocs.delCount;\n        totalMaxDoc += segSizeDocs.maxDoc;\n      }\n\n      minSegmentBytes = Math.min(segBytes, minSegmentBytes);\n      totIndexBytes += segBytes;\n    }\n    assert totalMaxDoc >= 0;\n    assert totalDelDocs >= 0;\n\n    // Compute max allowed segments in the index\n    long levelSize = Math.max(minSegmentBytes, floorSegmentBytes);\n    long bytesLeft = totIndexBytes;\n    double allowedSegCount = 0;\n    while (true) {\n      final double segCountLevel = bytesLeft / (double) levelSize;\n      if (segCountLevel < segsPerTier) {\n        allowedSegCount += Math.ceil(segCountLevel);\n        break;\n      }\n      allowedSegCount += segsPerTier;\n      bytesLeft -= segsPerTier * levelSize;\n      levelSize *= maxMergeAtOnce;\n    }\n\n    // If we're above certain thresholds, we can merge very large segments.\n    double totalDelPct = (double) totalDelDocs / (double) totalMaxDoc;\n    //TODO: See LUCENE-8263\n    //double targetAsPct = indexPctDeletedTarget / 100.0;\n    double targetAsPct = 0.5;\n    int tooBigCount = 0;\n    iter = sortedInfos.iterator();\n\n    // remove large segments from consideration under two conditions.\n    // 1> Overall percent deleted docs relatively small and this segment is larger than 50% maxSegSize\n    // 2> overall percent deleted docs large and this segment is large and has few deleted docs\n\n    long mergingBytes = 0L;\n\n    while (iter.hasNext()) {\n      SegmentSizeAndDocs segSizeDocs = iter.next();\n      double segDelPct = (double) segSizeDocs.delCount / (double) segSizeDocs.maxDoc;\n      if (segSizeDocs.sizeInBytes > maxMergedSegmentBytes / 2 && (totalDelPct < targetAsPct || segDelPct < targetAsPct)) {\n        iter.remove();\n        tooBigCount++; // Just for reporting purposes.\n      } else {\n        mergingBytes += segSizeDocs.sizeInBytes;\n      }\n\n    }\n    if (verbose(mergeContext) && tooBigCount > 0) {\n      message(\"  allowedSegmentCount=\" + allowedSegCount + \" vs count=\" + infos.size() +\n          \" (eligible count=\" + sortedInfos.size() + \") tooBigCount= \" + tooBigCount, mergeContext);\n    }\n    return doFindMerges(sortedInfos, maxMergedSegmentBytes, maxMergeAtOnce, (int) allowedSegCount, MERGE_TYPE.NATURAL,\n        mergeContext, mergingBytes >= maxMergedSegmentBytes);\n  }\n\n","bugFix":["01e5948db9a07144112d2f08f28ca2e3cd880348","56fb5e4e4b239474721e13b4cd9542ea2d215451"],"bugIntro":["890efdcaafbf652afef683f55f47f84e415bb292"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f9abddc2db59050e915640d3c6835e9b8fe5f47d","date":1531142616,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findMerges(MergeTrigger,SegmentInfos,MergeContext).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findMerges(MergeTrigger,SegmentInfos,MergeContext).mjava","sourceNew":"  @Override\n  public MergeSpecification findMerges(MergeTrigger mergeTrigger, SegmentInfos infos, MergeContext mergeContext) throws IOException {\n    final Set<SegmentCommitInfo> merging = mergeContext.getMergingSegments();\n    // Compute total index bytes & print details about the index\n    long totIndexBytes = 0;\n    long minSegmentBytes = Long.MAX_VALUE;\n\n    int totalDelDocs = 0;\n    int totalMaxDoc = 0;\n\n    long mergingBytes = 0;\n\n    List<SegmentSizeAndDocs> sortedInfos = getSortedBySegmentSize(infos, mergeContext);\n    Iterator<SegmentSizeAndDocs> iter = sortedInfos.iterator();\n    while (iter.hasNext()) {\n      SegmentSizeAndDocs segSizeDocs = iter.next();\n      final long segBytes = segSizeDocs.sizeInBytes;\n      if (verbose(mergeContext)) {\n        String extra = merging.contains(segSizeDocs.segInfo) ? \" [merging]\" : \"\";\n        if (segBytes >= maxMergedSegmentBytes) {\n          extra += \" [skip: too large]\";\n        } else if (segBytes < floorSegmentBytes) {\n          extra += \" [floored]\";\n        }\n        message(\"  seg=\" + segString(mergeContext, Collections.singleton(segSizeDocs.segInfo)) + \" size=\" + String.format(Locale.ROOT, \"%.3f\", segBytes / 1024 / 1024.) + \" MB\" + extra, mergeContext);\n      }\n      if (merging.contains(segSizeDocs.segInfo)) {\n        mergingBytes += segSizeDocs.sizeInBytes;\n        iter.remove();\n      } else {\n        totalDelDocs += segSizeDocs.delCount;\n        totalMaxDoc += segSizeDocs.maxDoc;\n      }\n\n      minSegmentBytes = Math.min(segBytes, minSegmentBytes);\n      totIndexBytes += segBytes;\n    }\n    assert totalMaxDoc >= 0;\n    assert totalDelDocs >= 0;\n\n    // If we have too-large segments, grace them out of the maximum segment count\n    // If we're above certain thresholds, we can merge very large segments.\n    double totalDelPct = (double) totalDelDocs / (double) totalMaxDoc;\n    //TODO: See LUCENE-8263\n    //double targetAsPct = indexPctDeletedTarget / 100.0;\n    double targetAsPct = 0.5;\n    int tooBigCount = 0;\n    iter = sortedInfos.iterator();\n\n    // remove large segments from consideration under two conditions.\n    // 1> Overall percent deleted docs relatively small and this segment is larger than 50% maxSegSize\n    // 2> overall percent deleted docs large and this segment is large and has few deleted docs\n\n    while (iter.hasNext()) {\n      SegmentSizeAndDocs segSizeDocs = iter.next();\n      double segDelPct = (double) segSizeDocs.delCount / (double) segSizeDocs.maxDoc;\n      if (segSizeDocs.sizeInBytes > maxMergedSegmentBytes / 2 && (totalDelPct < targetAsPct || segDelPct < targetAsPct)) {\n        iter.remove();\n        tooBigCount++; // Just for reporting purposes.\n        totIndexBytes -= segSizeDocs.sizeInBytes;\n      }\n    }\n\n    // Compute max allowed segments in the index\n    long levelSize = Math.max(minSegmentBytes, floorSegmentBytes);\n    long bytesLeft = totIndexBytes;\n    double allowedSegCount = 0;\n    while (true) {\n      final double segCountLevel = bytesLeft / (double) levelSize;\n      if (segCountLevel < segsPerTier) {\n        allowedSegCount += Math.ceil(segCountLevel);\n        break;\n      }\n      allowedSegCount += segsPerTier;\n      bytesLeft -= segsPerTier * levelSize;\n      levelSize *= maxMergeAtOnce;\n    }\n\n    if (verbose(mergeContext) && tooBigCount > 0) {\n      message(\"  allowedSegmentCount=\" + allowedSegCount + \" vs count=\" + infos.size() +\n          \" (eligible count=\" + sortedInfos.size() + \") tooBigCount= \" + tooBigCount, mergeContext);\n    }\n    return doFindMerges(sortedInfos, maxMergedSegmentBytes, maxMergeAtOnce, (int) allowedSegCount, MERGE_TYPE.NATURAL,\n        mergeContext, mergingBytes >= maxMergedSegmentBytes);\n  }\n\n","sourceOld":"  @Override\n  public MergeSpecification findMerges(MergeTrigger mergeTrigger, SegmentInfos infos, MergeContext mergeContext) throws IOException {\n    final Set<SegmentCommitInfo> merging = mergeContext.getMergingSegments();\n    // Compute total index bytes & print details about the index\n    long totIndexBytes = 0;\n    long minSegmentBytes = Long.MAX_VALUE;\n\n    int totalDelDocs = 0;\n    int totalMaxDoc = 0;\n\n    List<SegmentSizeAndDocs> sortedInfos = getSortedBySegmentSize(infos, mergeContext);\n    Iterator<SegmentSizeAndDocs> iter = sortedInfos.iterator();\n    while (iter.hasNext()) {\n      SegmentSizeAndDocs segSizeDocs = iter.next();\n      final long segBytes = segSizeDocs.sizeInBytes;\n      if (verbose(mergeContext)) {\n        String extra = merging.contains(segSizeDocs.segInfo) ? \" [merging]\" : \"\";\n        if (segBytes >= maxMergedSegmentBytes) {\n          extra += \" [skip: too large]\";\n        } else if (segBytes < floorSegmentBytes) {\n          extra += \" [floored]\";\n        }\n        message(\"  seg=\" + segString(mergeContext, Collections.singleton(segSizeDocs.segInfo)) + \" size=\" + String.format(Locale.ROOT, \"%.3f\", segBytes / 1024 / 1024.) + \" MB\" + extra, mergeContext);\n      }\n      if (merging.contains(segSizeDocs.segInfo)) {\n        iter.remove();\n      } else {\n        totalDelDocs += segSizeDocs.delCount;\n        totalMaxDoc += segSizeDocs.maxDoc;\n      }\n\n      minSegmentBytes = Math.min(segBytes, minSegmentBytes);\n      totIndexBytes += segBytes;\n    }\n    assert totalMaxDoc >= 0;\n    assert totalDelDocs >= 0;\n\n    // If we have too-large segments, grace them out of the maximum segment count\n    // If we're above certain thresholds, we can merge very large segments.\n    double totalDelPct = (double) totalDelDocs / (double) totalMaxDoc;\n    //TODO: See LUCENE-8263\n    //double targetAsPct = indexPctDeletedTarget / 100.0;\n    double targetAsPct = 0.5;\n    int tooBigCount = 0;\n    iter = sortedInfos.iterator();\n\n    // remove large segments from consideration under two conditions.\n    // 1> Overall percent deleted docs relatively small and this segment is larger than 50% maxSegSize\n    // 2> overall percent deleted docs large and this segment is large and has few deleted docs\n\n    long mergingBytes = 0L;\n\n    while (iter.hasNext()) {\n      SegmentSizeAndDocs segSizeDocs = iter.next();\n      double segDelPct = (double) segSizeDocs.delCount / (double) segSizeDocs.maxDoc;\n      if (segSizeDocs.sizeInBytes > maxMergedSegmentBytes / 2 && (totalDelPct < targetAsPct || segDelPct < targetAsPct)) {\n        iter.remove();\n        tooBigCount++; // Just for reporting purposes.\n        totIndexBytes -= segSizeDocs.sizeInBytes;\n      } else {\n        mergingBytes += segSizeDocs.sizeInBytes;\n      }\n    }\n\n    // Compute max allowed segments in the index\n    long levelSize = Math.max(minSegmentBytes, floorSegmentBytes);\n    long bytesLeft = totIndexBytes;\n    double allowedSegCount = 0;\n    while (true) {\n      final double segCountLevel = bytesLeft / (double) levelSize;\n      if (segCountLevel < segsPerTier) {\n        allowedSegCount += Math.ceil(segCountLevel);\n        break;\n      }\n      allowedSegCount += segsPerTier;\n      bytesLeft -= segsPerTier * levelSize;\n      levelSize *= maxMergeAtOnce;\n    }\n\n    if (verbose(mergeContext) && tooBigCount > 0) {\n      message(\"  allowedSegmentCount=\" + allowedSegCount + \" vs count=\" + infos.size() +\n          \" (eligible count=\" + sortedInfos.size() + \") tooBigCount= \" + tooBigCount, mergeContext);\n    }\n    return doFindMerges(sortedInfos, maxMergedSegmentBytes, maxMergeAtOnce, (int) allowedSegCount, MERGE_TYPE.NATURAL,\n        mergeContext, mergingBytes >= maxMergedSegmentBytes);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9707a68fe260631e514201dbf24e9afc9a3a4ba1","date":1531207054,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findMerges(MergeTrigger,SegmentInfos,MergeContext).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findMerges(MergeTrigger,SegmentInfos,MergeContext).mjava","sourceNew":"  @Override\n  public MergeSpecification findMerges(MergeTrigger mergeTrigger, SegmentInfos infos, MergeContext mergeContext) throws IOException {\n    final Set<SegmentCommitInfo> merging = mergeContext.getMergingSegments();\n    // Compute total index bytes & print details about the index\n    long totIndexBytes = 0;\n    long minSegmentBytes = Long.MAX_VALUE;\n\n    int totalDelDocs = 0;\n    int totalMaxDoc = 0;\n\n    long mergingBytes = 0;\n\n    List<SegmentSizeAndDocs> sortedInfos = getSortedBySegmentSize(infos, mergeContext);\n    Iterator<SegmentSizeAndDocs> iter = sortedInfos.iterator();\n    while (iter.hasNext()) {\n      SegmentSizeAndDocs segSizeDocs = iter.next();\n      final long segBytes = segSizeDocs.sizeInBytes;\n      if (verbose(mergeContext)) {\n        String extra = merging.contains(segSizeDocs.segInfo) ? \" [merging]\" : \"\";\n        if (segBytes >= maxMergedSegmentBytes) {\n          extra += \" [skip: too large]\";\n        } else if (segBytes < floorSegmentBytes) {\n          extra += \" [floored]\";\n        }\n        message(\"  seg=\" + segString(mergeContext, Collections.singleton(segSizeDocs.segInfo)) + \" size=\" + String.format(Locale.ROOT, \"%.3f\", segBytes / 1024 / 1024.) + \" MB\" + extra, mergeContext);\n      }\n      if (merging.contains(segSizeDocs.segInfo)) {\n        mergingBytes += segSizeDocs.sizeInBytes;\n        iter.remove();\n      } else {\n        totalDelDocs += segSizeDocs.delCount;\n        totalMaxDoc += segSizeDocs.maxDoc;\n      }\n\n      minSegmentBytes = Math.min(segBytes, minSegmentBytes);\n      totIndexBytes += segBytes;\n    }\n    assert totalMaxDoc >= 0;\n    assert totalDelDocs >= 0;\n\n    // If we have too-large segments, grace them out of the maximum segment count\n    // If we're above certain thresholds, we can merge very large segments.\n    double totalDelPct = (double) totalDelDocs / (double) totalMaxDoc;\n    //TODO: See LUCENE-8263\n    //double targetAsPct = indexPctDeletedTarget / 100.0;\n    double targetAsPct = 0.5;\n    int tooBigCount = 0;\n    iter = sortedInfos.iterator();\n\n    // remove large segments from consideration under two conditions.\n    // 1> Overall percent deleted docs relatively small and this segment is larger than 50% maxSegSize\n    // 2> overall percent deleted docs large and this segment is large and has few deleted docs\n\n    while (iter.hasNext()) {\n      SegmentSizeAndDocs segSizeDocs = iter.next();\n      double segDelPct = (double) segSizeDocs.delCount / (double) segSizeDocs.maxDoc;\n      if (segSizeDocs.sizeInBytes > maxMergedSegmentBytes / 2 && (totalDelPct < targetAsPct || segDelPct < targetAsPct)) {\n        iter.remove();\n        tooBigCount++; // Just for reporting purposes.\n        totIndexBytes -= segSizeDocs.sizeInBytes;\n      }\n    }\n\n    final int mergeFactor = (int) Math.min(maxMergeAtOnce, segsPerTier);\n    // Compute max allowed segments in the index\n    long levelSize = Math.max(minSegmentBytes, floorSegmentBytes);\n    long bytesLeft = totIndexBytes;\n    double allowedSegCount = 0;\n    while (true) {\n      final double segCountLevel = bytesLeft / (double) levelSize;\n      if (segCountLevel < segsPerTier) {\n        allowedSegCount += Math.ceil(segCountLevel);\n        break;\n      }\n      allowedSegCount += segsPerTier;\n      bytesLeft -= segsPerTier * levelSize;\n      levelSize *= mergeFactor;\n    }\n\n    if (verbose(mergeContext) && tooBigCount > 0) {\n      message(\"  allowedSegmentCount=\" + allowedSegCount + \" vs count=\" + infos.size() +\n          \" (eligible count=\" + sortedInfos.size() + \") tooBigCount= \" + tooBigCount, mergeContext);\n    }\n    return doFindMerges(sortedInfos, maxMergedSegmentBytes, mergeFactor, (int) allowedSegCount, MERGE_TYPE.NATURAL,\n        mergeContext, mergingBytes >= maxMergedSegmentBytes);\n  }\n\n","sourceOld":"  @Override\n  public MergeSpecification findMerges(MergeTrigger mergeTrigger, SegmentInfos infos, MergeContext mergeContext) throws IOException {\n    final Set<SegmentCommitInfo> merging = mergeContext.getMergingSegments();\n    // Compute total index bytes & print details about the index\n    long totIndexBytes = 0;\n    long minSegmentBytes = Long.MAX_VALUE;\n\n    int totalDelDocs = 0;\n    int totalMaxDoc = 0;\n\n    long mergingBytes = 0;\n\n    List<SegmentSizeAndDocs> sortedInfos = getSortedBySegmentSize(infos, mergeContext);\n    Iterator<SegmentSizeAndDocs> iter = sortedInfos.iterator();\n    while (iter.hasNext()) {\n      SegmentSizeAndDocs segSizeDocs = iter.next();\n      final long segBytes = segSizeDocs.sizeInBytes;\n      if (verbose(mergeContext)) {\n        String extra = merging.contains(segSizeDocs.segInfo) ? \" [merging]\" : \"\";\n        if (segBytes >= maxMergedSegmentBytes) {\n          extra += \" [skip: too large]\";\n        } else if (segBytes < floorSegmentBytes) {\n          extra += \" [floored]\";\n        }\n        message(\"  seg=\" + segString(mergeContext, Collections.singleton(segSizeDocs.segInfo)) + \" size=\" + String.format(Locale.ROOT, \"%.3f\", segBytes / 1024 / 1024.) + \" MB\" + extra, mergeContext);\n      }\n      if (merging.contains(segSizeDocs.segInfo)) {\n        mergingBytes += segSizeDocs.sizeInBytes;\n        iter.remove();\n      } else {\n        totalDelDocs += segSizeDocs.delCount;\n        totalMaxDoc += segSizeDocs.maxDoc;\n      }\n\n      minSegmentBytes = Math.min(segBytes, minSegmentBytes);\n      totIndexBytes += segBytes;\n    }\n    assert totalMaxDoc >= 0;\n    assert totalDelDocs >= 0;\n\n    // If we have too-large segments, grace them out of the maximum segment count\n    // If we're above certain thresholds, we can merge very large segments.\n    double totalDelPct = (double) totalDelDocs / (double) totalMaxDoc;\n    //TODO: See LUCENE-8263\n    //double targetAsPct = indexPctDeletedTarget / 100.0;\n    double targetAsPct = 0.5;\n    int tooBigCount = 0;\n    iter = sortedInfos.iterator();\n\n    // remove large segments from consideration under two conditions.\n    // 1> Overall percent deleted docs relatively small and this segment is larger than 50% maxSegSize\n    // 2> overall percent deleted docs large and this segment is large and has few deleted docs\n\n    while (iter.hasNext()) {\n      SegmentSizeAndDocs segSizeDocs = iter.next();\n      double segDelPct = (double) segSizeDocs.delCount / (double) segSizeDocs.maxDoc;\n      if (segSizeDocs.sizeInBytes > maxMergedSegmentBytes / 2 && (totalDelPct < targetAsPct || segDelPct < targetAsPct)) {\n        iter.remove();\n        tooBigCount++; // Just for reporting purposes.\n        totIndexBytes -= segSizeDocs.sizeInBytes;\n      }\n    }\n\n    // Compute max allowed segments in the index\n    long levelSize = Math.max(minSegmentBytes, floorSegmentBytes);\n    long bytesLeft = totIndexBytes;\n    double allowedSegCount = 0;\n    while (true) {\n      final double segCountLevel = bytesLeft / (double) levelSize;\n      if (segCountLevel < segsPerTier) {\n        allowedSegCount += Math.ceil(segCountLevel);\n        break;\n      }\n      allowedSegCount += segsPerTier;\n      bytesLeft -= segsPerTier * levelSize;\n      levelSize *= maxMergeAtOnce;\n    }\n\n    if (verbose(mergeContext) && tooBigCount > 0) {\n      message(\"  allowedSegmentCount=\" + allowedSegCount + \" vs count=\" + infos.size() +\n          \" (eligible count=\" + sortedInfos.size() + \") tooBigCount= \" + tooBigCount, mergeContext);\n    }\n    return doFindMerges(sortedInfos, maxMergedSegmentBytes, maxMergeAtOnce, (int) allowedSegCount, MERGE_TYPE.NATURAL,\n        mergeContext, mergingBytes >= maxMergedSegmentBytes);\n  }\n\n","bugFix":null,"bugIntro":["890efdcaafbf652afef683f55f47f84e415bb292"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"890efdcaafbf652afef683f55f47f84e415bb292","date":1531207054,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findMerges(MergeTrigger,SegmentInfos,MergeContext).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findMerges(MergeTrigger,SegmentInfos,MergeContext).mjava","sourceNew":"  @Override\n  public MergeSpecification findMerges(MergeTrigger mergeTrigger, SegmentInfos infos, MergeContext mergeContext) throws IOException {\n    final Set<SegmentCommitInfo> merging = mergeContext.getMergingSegments();\n    // Compute total index bytes & print details about the index\n    long totIndexBytes = 0;\n    long minSegmentBytes = Long.MAX_VALUE;\n\n    int totalDelDocs = 0;\n    int totalMaxDoc = 0;\n\n    long mergingBytes = 0;\n\n    List<SegmentSizeAndDocs> sortedInfos = getSortedBySegmentSize(infos, mergeContext);\n    Iterator<SegmentSizeAndDocs> iter = sortedInfos.iterator();\n    while (iter.hasNext()) {\n      SegmentSizeAndDocs segSizeDocs = iter.next();\n      final long segBytes = segSizeDocs.sizeInBytes;\n      if (verbose(mergeContext)) {\n        String extra = merging.contains(segSizeDocs.segInfo) ? \" [merging]\" : \"\";\n        if (segBytes >= maxMergedSegmentBytes) {\n          extra += \" [skip: too large]\";\n        } else if (segBytes < floorSegmentBytes) {\n          extra += \" [floored]\";\n        }\n        message(\"  seg=\" + segString(mergeContext, Collections.singleton(segSizeDocs.segInfo)) + \" size=\" + String.format(Locale.ROOT, \"%.3f\", segBytes / 1024 / 1024.) + \" MB\" + extra, mergeContext);\n      }\n      if (merging.contains(segSizeDocs.segInfo)) {\n        mergingBytes += segSizeDocs.sizeInBytes;\n        iter.remove();\n      } else {\n        totalDelDocs += segSizeDocs.delCount;\n        totalMaxDoc += segSizeDocs.maxDoc;\n      }\n\n      minSegmentBytes = Math.min(segBytes, minSegmentBytes);\n      totIndexBytes += segBytes;\n    }\n    assert totalMaxDoc >= 0;\n    assert totalDelDocs >= 0;\n\n    // If we have too-large segments, grace them out of the maximum segment count\n    // If we're above certain thresholds, we can merge very large segments.\n    double totalDelPct = (double) totalDelDocs / (double) totalMaxDoc;\n    //TODO: See LUCENE-8263\n    //double targetAsPct = indexPctDeletedTarget / 100.0;\n    double targetAsPct = 0.5;\n    int tooBigCount = 0;\n    iter = sortedInfos.iterator();\n\n    // remove large segments from consideration under two conditions.\n    // 1> Overall percent deleted docs relatively small and this segment is larger than 50% maxSegSize\n    // 2> overall percent deleted docs large and this segment is large and has few deleted docs\n\n    while (iter.hasNext()) {\n      SegmentSizeAndDocs segSizeDocs = iter.next();\n      double segDelPct = (double) segSizeDocs.delCount / (double) segSizeDocs.maxDoc;\n      if (segSizeDocs.sizeInBytes > maxMergedSegmentBytes / 2 && (totalDelPct < targetAsPct || segDelPct < targetAsPct)) {\n        iter.remove();\n        tooBigCount++; // Just for reporting purposes.\n        totIndexBytes -= segSizeDocs.sizeInBytes;\n      }\n    }\n\n    final int mergeFactor = (int) Math.min(maxMergeAtOnce, segsPerTier);\n    // Compute max allowed segments in the index\n    long levelSize = Math.max(minSegmentBytes, floorSegmentBytes);\n    long bytesLeft = totIndexBytes;\n    double allowedSegCount = 0;\n    while (true) {\n      final double segCountLevel = bytesLeft / (double) levelSize;\n      if (segCountLevel < segsPerTier || levelSize == maxMergedSegmentBytes) {\n        allowedSegCount += Math.ceil(segCountLevel);\n        break;\n      }\n      allowedSegCount += segsPerTier;\n      bytesLeft -= segsPerTier * levelSize;\n      levelSize = Math.min(maxMergedSegmentBytes, levelSize * mergeFactor);\n    }\n\n    if (verbose(mergeContext) && tooBigCount > 0) {\n      message(\"  allowedSegmentCount=\" + allowedSegCount + \" vs count=\" + infos.size() +\n          \" (eligible count=\" + sortedInfos.size() + \") tooBigCount= \" + tooBigCount, mergeContext);\n    }\n    return doFindMerges(sortedInfos, maxMergedSegmentBytes, mergeFactor, (int) allowedSegCount, MERGE_TYPE.NATURAL,\n        mergeContext, mergingBytes >= maxMergedSegmentBytes);\n  }\n\n","sourceOld":"  @Override\n  public MergeSpecification findMerges(MergeTrigger mergeTrigger, SegmentInfos infos, MergeContext mergeContext) throws IOException {\n    final Set<SegmentCommitInfo> merging = mergeContext.getMergingSegments();\n    // Compute total index bytes & print details about the index\n    long totIndexBytes = 0;\n    long minSegmentBytes = Long.MAX_VALUE;\n\n    int totalDelDocs = 0;\n    int totalMaxDoc = 0;\n\n    long mergingBytes = 0;\n\n    List<SegmentSizeAndDocs> sortedInfos = getSortedBySegmentSize(infos, mergeContext);\n    Iterator<SegmentSizeAndDocs> iter = sortedInfos.iterator();\n    while (iter.hasNext()) {\n      SegmentSizeAndDocs segSizeDocs = iter.next();\n      final long segBytes = segSizeDocs.sizeInBytes;\n      if (verbose(mergeContext)) {\n        String extra = merging.contains(segSizeDocs.segInfo) ? \" [merging]\" : \"\";\n        if (segBytes >= maxMergedSegmentBytes) {\n          extra += \" [skip: too large]\";\n        } else if (segBytes < floorSegmentBytes) {\n          extra += \" [floored]\";\n        }\n        message(\"  seg=\" + segString(mergeContext, Collections.singleton(segSizeDocs.segInfo)) + \" size=\" + String.format(Locale.ROOT, \"%.3f\", segBytes / 1024 / 1024.) + \" MB\" + extra, mergeContext);\n      }\n      if (merging.contains(segSizeDocs.segInfo)) {\n        mergingBytes += segSizeDocs.sizeInBytes;\n        iter.remove();\n      } else {\n        totalDelDocs += segSizeDocs.delCount;\n        totalMaxDoc += segSizeDocs.maxDoc;\n      }\n\n      minSegmentBytes = Math.min(segBytes, minSegmentBytes);\n      totIndexBytes += segBytes;\n    }\n    assert totalMaxDoc >= 0;\n    assert totalDelDocs >= 0;\n\n    // If we have too-large segments, grace them out of the maximum segment count\n    // If we're above certain thresholds, we can merge very large segments.\n    double totalDelPct = (double) totalDelDocs / (double) totalMaxDoc;\n    //TODO: See LUCENE-8263\n    //double targetAsPct = indexPctDeletedTarget / 100.0;\n    double targetAsPct = 0.5;\n    int tooBigCount = 0;\n    iter = sortedInfos.iterator();\n\n    // remove large segments from consideration under two conditions.\n    // 1> Overall percent deleted docs relatively small and this segment is larger than 50% maxSegSize\n    // 2> overall percent deleted docs large and this segment is large and has few deleted docs\n\n    while (iter.hasNext()) {\n      SegmentSizeAndDocs segSizeDocs = iter.next();\n      double segDelPct = (double) segSizeDocs.delCount / (double) segSizeDocs.maxDoc;\n      if (segSizeDocs.sizeInBytes > maxMergedSegmentBytes / 2 && (totalDelPct < targetAsPct || segDelPct < targetAsPct)) {\n        iter.remove();\n        tooBigCount++; // Just for reporting purposes.\n        totIndexBytes -= segSizeDocs.sizeInBytes;\n      }\n    }\n\n    final int mergeFactor = (int) Math.min(maxMergeAtOnce, segsPerTier);\n    // Compute max allowed segments in the index\n    long levelSize = Math.max(minSegmentBytes, floorSegmentBytes);\n    long bytesLeft = totIndexBytes;\n    double allowedSegCount = 0;\n    while (true) {\n      final double segCountLevel = bytesLeft / (double) levelSize;\n      if (segCountLevel < segsPerTier) {\n        allowedSegCount += Math.ceil(segCountLevel);\n        break;\n      }\n      allowedSegCount += segsPerTier;\n      bytesLeft -= segsPerTier * levelSize;\n      levelSize *= mergeFactor;\n    }\n\n    if (verbose(mergeContext) && tooBigCount > 0) {\n      message(\"  allowedSegmentCount=\" + allowedSegCount + \" vs count=\" + infos.size() +\n          \" (eligible count=\" + sortedInfos.size() + \") tooBigCount= \" + tooBigCount, mergeContext);\n    }\n    return doFindMerges(sortedInfos, maxMergedSegmentBytes, mergeFactor, (int) allowedSegCount, MERGE_TYPE.NATURAL,\n        mergeContext, mergingBytes >= maxMergedSegmentBytes);\n  }\n\n","bugFix":["9707a68fe260631e514201dbf24e9afc9a3a4ba1","404dafe26b816e6ed478486e26abd62d8607b123"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bf644d067afb7311fd40d2122a6c772f8635d65a","date":1531207054,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findMerges(MergeTrigger,SegmentInfos,MergeContext).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findMerges(MergeTrigger,SegmentInfos,MergeContext).mjava","sourceNew":"  @Override\n  public MergeSpecification findMerges(MergeTrigger mergeTrigger, SegmentInfos infos, MergeContext mergeContext) throws IOException {\n    final Set<SegmentCommitInfo> merging = mergeContext.getMergingSegments();\n    // Compute total index bytes & print details about the index\n    long totIndexBytes = 0;\n    long minSegmentBytes = Long.MAX_VALUE;\n\n    int totalDelDocs = 0;\n    int totalMaxDoc = 0;\n\n    long mergingBytes = 0;\n\n    List<SegmentSizeAndDocs> sortedInfos = getSortedBySegmentSize(infos, mergeContext);\n    Iterator<SegmentSizeAndDocs> iter = sortedInfos.iterator();\n    while (iter.hasNext()) {\n      SegmentSizeAndDocs segSizeDocs = iter.next();\n      final long segBytes = segSizeDocs.sizeInBytes;\n      if (verbose(mergeContext)) {\n        String extra = merging.contains(segSizeDocs.segInfo) ? \" [merging]\" : \"\";\n        if (segBytes >= maxMergedSegmentBytes) {\n          extra += \" [skip: too large]\";\n        } else if (segBytes < floorSegmentBytes) {\n          extra += \" [floored]\";\n        }\n        message(\"  seg=\" + segString(mergeContext, Collections.singleton(segSizeDocs.segInfo)) + \" size=\" + String.format(Locale.ROOT, \"%.3f\", segBytes / 1024 / 1024.) + \" MB\" + extra, mergeContext);\n      }\n      if (merging.contains(segSizeDocs.segInfo)) {\n        mergingBytes += segSizeDocs.sizeInBytes;\n        iter.remove();\n      } else {\n        totalDelDocs += segSizeDocs.delCount;\n        totalMaxDoc += segSizeDocs.maxDoc;\n      }\n\n      minSegmentBytes = Math.min(segBytes, minSegmentBytes);\n      totIndexBytes += segBytes;\n    }\n    assert totalMaxDoc >= 0;\n    assert totalDelDocs >= 0;\n\n    // If we have too-large segments, grace them out of the maximum segment count\n    // If we're above certain thresholds, we can merge very large segments.\n    double totalDelPct = (double) totalDelDocs / (double) totalMaxDoc;\n    //TODO: See LUCENE-8263\n    //double targetAsPct = indexPctDeletedTarget / 100.0;\n    double targetAsPct = 0.5;\n    int tooBigCount = 0;\n    iter = sortedInfos.iterator();\n\n    // remove large segments from consideration under two conditions.\n    // 1> Overall percent deleted docs relatively small and this segment is larger than 50% maxSegSize\n    // 2> overall percent deleted docs large and this segment is large and has few deleted docs\n\n    while (iter.hasNext()) {\n      SegmentSizeAndDocs segSizeDocs = iter.next();\n      double segDelPct = (double) segSizeDocs.delCount / (double) segSizeDocs.maxDoc;\n      if (segSizeDocs.sizeInBytes > maxMergedSegmentBytes / 2 && (totalDelPct < targetAsPct || segDelPct < targetAsPct)) {\n        iter.remove();\n        tooBigCount++; // Just for reporting purposes.\n        totIndexBytes -= segSizeDocs.sizeInBytes;\n      }\n    }\n\n    final int mergeFactor = (int) Math.min(maxMergeAtOnce, segsPerTier);\n    // Compute max allowed segments in the index\n    long levelSize = Math.max(minSegmentBytes, floorSegmentBytes);\n    long bytesLeft = totIndexBytes;\n    double allowedSegCount = 0;\n    while (true) {\n      final double segCountLevel = bytesLeft / (double) levelSize;\n      if (segCountLevel < segsPerTier || levelSize == maxMergedSegmentBytes) {\n        allowedSegCount += Math.ceil(segCountLevel);\n        break;\n      }\n      allowedSegCount += segsPerTier;\n      bytesLeft -= segsPerTier * levelSize;\n      levelSize = Math.min(maxMergedSegmentBytes, levelSize * mergeFactor);\n    }\n    // allowedSegCount may occasionally be less than segsPerTier\n    // if segment sizes are below the floor size\n    allowedSegCount = Math.max(allowedSegCount, segsPerTier);\n\n    if (verbose(mergeContext) && tooBigCount > 0) {\n      message(\"  allowedSegmentCount=\" + allowedSegCount + \" vs count=\" + infos.size() +\n          \" (eligible count=\" + sortedInfos.size() + \") tooBigCount= \" + tooBigCount, mergeContext);\n    }\n    return doFindMerges(sortedInfos, maxMergedSegmentBytes, mergeFactor, (int) allowedSegCount, MERGE_TYPE.NATURAL,\n        mergeContext, mergingBytes >= maxMergedSegmentBytes);\n  }\n\n","sourceOld":"  @Override\n  public MergeSpecification findMerges(MergeTrigger mergeTrigger, SegmentInfos infos, MergeContext mergeContext) throws IOException {\n    final Set<SegmentCommitInfo> merging = mergeContext.getMergingSegments();\n    // Compute total index bytes & print details about the index\n    long totIndexBytes = 0;\n    long minSegmentBytes = Long.MAX_VALUE;\n\n    int totalDelDocs = 0;\n    int totalMaxDoc = 0;\n\n    long mergingBytes = 0;\n\n    List<SegmentSizeAndDocs> sortedInfos = getSortedBySegmentSize(infos, mergeContext);\n    Iterator<SegmentSizeAndDocs> iter = sortedInfos.iterator();\n    while (iter.hasNext()) {\n      SegmentSizeAndDocs segSizeDocs = iter.next();\n      final long segBytes = segSizeDocs.sizeInBytes;\n      if (verbose(mergeContext)) {\n        String extra = merging.contains(segSizeDocs.segInfo) ? \" [merging]\" : \"\";\n        if (segBytes >= maxMergedSegmentBytes) {\n          extra += \" [skip: too large]\";\n        } else if (segBytes < floorSegmentBytes) {\n          extra += \" [floored]\";\n        }\n        message(\"  seg=\" + segString(mergeContext, Collections.singleton(segSizeDocs.segInfo)) + \" size=\" + String.format(Locale.ROOT, \"%.3f\", segBytes / 1024 / 1024.) + \" MB\" + extra, mergeContext);\n      }\n      if (merging.contains(segSizeDocs.segInfo)) {\n        mergingBytes += segSizeDocs.sizeInBytes;\n        iter.remove();\n      } else {\n        totalDelDocs += segSizeDocs.delCount;\n        totalMaxDoc += segSizeDocs.maxDoc;\n      }\n\n      minSegmentBytes = Math.min(segBytes, minSegmentBytes);\n      totIndexBytes += segBytes;\n    }\n    assert totalMaxDoc >= 0;\n    assert totalDelDocs >= 0;\n\n    // If we have too-large segments, grace them out of the maximum segment count\n    // If we're above certain thresholds, we can merge very large segments.\n    double totalDelPct = (double) totalDelDocs / (double) totalMaxDoc;\n    //TODO: See LUCENE-8263\n    //double targetAsPct = indexPctDeletedTarget / 100.0;\n    double targetAsPct = 0.5;\n    int tooBigCount = 0;\n    iter = sortedInfos.iterator();\n\n    // remove large segments from consideration under two conditions.\n    // 1> Overall percent deleted docs relatively small and this segment is larger than 50% maxSegSize\n    // 2> overall percent deleted docs large and this segment is large and has few deleted docs\n\n    while (iter.hasNext()) {\n      SegmentSizeAndDocs segSizeDocs = iter.next();\n      double segDelPct = (double) segSizeDocs.delCount / (double) segSizeDocs.maxDoc;\n      if (segSizeDocs.sizeInBytes > maxMergedSegmentBytes / 2 && (totalDelPct < targetAsPct || segDelPct < targetAsPct)) {\n        iter.remove();\n        tooBigCount++; // Just for reporting purposes.\n        totIndexBytes -= segSizeDocs.sizeInBytes;\n      }\n    }\n\n    final int mergeFactor = (int) Math.min(maxMergeAtOnce, segsPerTier);\n    // Compute max allowed segments in the index\n    long levelSize = Math.max(minSegmentBytes, floorSegmentBytes);\n    long bytesLeft = totIndexBytes;\n    double allowedSegCount = 0;\n    while (true) {\n      final double segCountLevel = bytesLeft / (double) levelSize;\n      if (segCountLevel < segsPerTier || levelSize == maxMergedSegmentBytes) {\n        allowedSegCount += Math.ceil(segCountLevel);\n        break;\n      }\n      allowedSegCount += segsPerTier;\n      bytesLeft -= segsPerTier * levelSize;\n      levelSize = Math.min(maxMergedSegmentBytes, levelSize * mergeFactor);\n    }\n\n    if (verbose(mergeContext) && tooBigCount > 0) {\n      message(\"  allowedSegmentCount=\" + allowedSegCount + \" vs count=\" + infos.size() +\n          \" (eligible count=\" + sortedInfos.size() + \") tooBigCount= \" + tooBigCount, mergeContext);\n    }\n    return doFindMerges(sortedInfos, maxMergedSegmentBytes, mergeFactor, (int) allowedSegCount, MERGE_TYPE.NATURAL,\n        mergeContext, mergingBytes >= maxMergedSegmentBytes);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26","date":1531589977,"type":3,"author":"Michael Braun","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findMerges(MergeTrigger,SegmentInfos,MergeContext).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findMerges(MergeTrigger,SegmentInfos,MergeContext).mjava","sourceNew":"  @Override\n  public MergeSpecification findMerges(MergeTrigger mergeTrigger, SegmentInfos infos, MergeContext mergeContext) throws IOException {\n    final Set<SegmentCommitInfo> merging = mergeContext.getMergingSegments();\n    // Compute total index bytes & print details about the index\n    long totIndexBytes = 0;\n    long minSegmentBytes = Long.MAX_VALUE;\n\n    int totalDelDocs = 0;\n    int totalMaxDoc = 0;\n\n    long mergingBytes = 0;\n\n    List<SegmentSizeAndDocs> sortedInfos = getSortedBySegmentSize(infos, mergeContext);\n    Iterator<SegmentSizeAndDocs> iter = sortedInfos.iterator();\n    while (iter.hasNext()) {\n      SegmentSizeAndDocs segSizeDocs = iter.next();\n      final long segBytes = segSizeDocs.sizeInBytes;\n      if (verbose(mergeContext)) {\n        String extra = merging.contains(segSizeDocs.segInfo) ? \" [merging]\" : \"\";\n        if (segBytes >= maxMergedSegmentBytes) {\n          extra += \" [skip: too large]\";\n        } else if (segBytes < floorSegmentBytes) {\n          extra += \" [floored]\";\n        }\n        message(\"  seg=\" + segString(mergeContext, Collections.singleton(segSizeDocs.segInfo)) + \" size=\" + String.format(Locale.ROOT, \"%.3f\", segBytes / 1024 / 1024.) + \" MB\" + extra, mergeContext);\n      }\n      if (merging.contains(segSizeDocs.segInfo)) {\n        mergingBytes += segSizeDocs.sizeInBytes;\n        iter.remove();\n      } else {\n        totalDelDocs += segSizeDocs.delCount;\n        totalMaxDoc += segSizeDocs.maxDoc;\n      }\n\n      minSegmentBytes = Math.min(segBytes, minSegmentBytes);\n      totIndexBytes += segBytes;\n    }\n    assert totalMaxDoc >= 0;\n    assert totalDelDocs >= 0;\n\n    // If we have too-large segments, grace them out of the maximum segment count\n    // If we're above certain thresholds, we can merge very large segments.\n    double totalDelPct = (double) totalDelDocs / (double) totalMaxDoc;\n    //TODO: See LUCENE-8263\n    //double targetAsPct = indexPctDeletedTarget / 100.0;\n    double targetAsPct = 0.5;\n    int tooBigCount = 0;\n    iter = sortedInfos.iterator();\n\n    // remove large segments from consideration under two conditions.\n    // 1> Overall percent deleted docs relatively small and this segment is larger than 50% maxSegSize\n    // 2> overall percent deleted docs large and this segment is large and has few deleted docs\n\n    while (iter.hasNext()) {\n      SegmentSizeAndDocs segSizeDocs = iter.next();\n      double segDelPct = (double) segSizeDocs.delCount / (double) segSizeDocs.maxDoc;\n      if (segSizeDocs.sizeInBytes > maxMergedSegmentBytes / 2 && (totalDelPct < targetAsPct || segDelPct < targetAsPct)) {\n        iter.remove();\n        tooBigCount++; // Just for reporting purposes.\n        totIndexBytes -= segSizeDocs.sizeInBytes;\n      }\n    }\n\n    final int mergeFactor = (int) Math.min(maxMergeAtOnce, segsPerTier);\n    // Compute max allowed segments in the index\n    long levelSize = Math.max(minSegmentBytes, floorSegmentBytes);\n    long bytesLeft = totIndexBytes;\n    double allowedSegCount = 0;\n    while (true) {\n      final double segCountLevel = bytesLeft / (double) levelSize;\n      if (segCountLevel < segsPerTier || levelSize == maxMergedSegmentBytes) {\n        allowedSegCount += Math.ceil(segCountLevel);\n        break;\n      }\n      allowedSegCount += segsPerTier;\n      bytesLeft -= segsPerTier * levelSize;\n      levelSize = Math.min(maxMergedSegmentBytes, levelSize * mergeFactor);\n    }\n    // allowedSegCount may occasionally be less than segsPerTier\n    // if segment sizes are below the floor size\n    allowedSegCount = Math.max(allowedSegCount, segsPerTier);\n\n    if (verbose(mergeContext) && tooBigCount > 0) {\n      message(\"  allowedSegmentCount=\" + allowedSegCount + \" vs count=\" + infos.size() +\n          \" (eligible count=\" + sortedInfos.size() + \") tooBigCount= \" + tooBigCount, mergeContext);\n    }\n    return doFindMerges(sortedInfos, maxMergedSegmentBytes, mergeFactor, (int) allowedSegCount, MERGE_TYPE.NATURAL,\n        mergeContext, mergingBytes >= maxMergedSegmentBytes);\n  }\n\n","sourceOld":"  @Override\n  public MergeSpecification findMerges(MergeTrigger mergeTrigger, SegmentInfos infos, MergeContext mergeContext) throws IOException {\n    if (verbose(mergeContext)) {\n      message(\"findMerges: \" + infos.size() + \" segments\", mergeContext);\n    }\n    if (infos.size() == 0) {\n      return null;\n    }\n    final Set<SegmentCommitInfo> merging = mergeContext.getMergingSegments();\n    final Set<SegmentCommitInfo> toBeMerged = new HashSet<>();\n\n    final List<SegmentCommitInfo> infosSorted = new ArrayList<>(infos.asList());\n\n    // The size can change concurrently while we are running here, because deletes\n    // are now applied concurrently, and this can piss off TimSort!  So we\n    // call size() once per segment and sort by that:\n    Map<SegmentCommitInfo,Long> sizeInBytes = getSegmentSizes(mergeContext, infos.asList());\n    \n    infosSorted.sort(new SegmentByteSizeDescending(sizeInBytes));\n\n    // Compute total index bytes & print details about the index\n    long totIndexBytes = 0;\n    long minSegmentBytes = Long.MAX_VALUE;\n    for(SegmentCommitInfo info : infosSorted) {\n      final long segBytes = sizeInBytes.get(info);\n      if (verbose(mergeContext)) {\n        String extra = merging.contains(info) ? \" [merging]\" : \"\";\n        if (segBytes >= maxMergedSegmentBytes/2.0) {\n          extra += \" [skip: too large]\";\n        } else if (segBytes < floorSegmentBytes) {\n          extra += \" [floored]\";\n        }\n        message(\"  seg=\" + segString(mergeContext, Collections.singleton(info)) + \" size=\" + String.format(Locale.ROOT, \"%.3f\", segBytes/1024/1024.) + \" MB\" + extra, mergeContext);\n      }\n\n      minSegmentBytes = Math.min(segBytes, minSegmentBytes);\n      // Accum total byte size\n      totIndexBytes += segBytes;\n    }\n\n    // If we have too-large segments, grace them out\n    // of the maxSegmentCount:\n    int tooBigCount = 0;\n    while (tooBigCount < infosSorted.size()) {\n      long segBytes = sizeInBytes.get(infosSorted.get(tooBigCount));\n      if (segBytes < maxMergedSegmentBytes/2.0) {\n        break;\n      }\n      totIndexBytes -= segBytes;\n      tooBigCount++;\n    }\n\n    minSegmentBytes = floorSize(minSegmentBytes);\n\n    // Compute max allowed segs in the index\n    long levelSize = minSegmentBytes;\n    long bytesLeft = totIndexBytes;\n    double allowedSegCount = 0;\n    while(true) {\n      final double segCountLevel = bytesLeft / (double) levelSize;\n      if (segCountLevel < segsPerTier) {\n        allowedSegCount += Math.ceil(segCountLevel);\n        break;\n      }\n      allowedSegCount += segsPerTier;\n      bytesLeft -= segsPerTier * levelSize;\n      levelSize *= maxMergeAtOnce;\n    }\n    int allowedSegCountInt = (int) allowedSegCount;\n\n    MergeSpecification spec = null;\n\n    // Cycle to possibly select more than one merge:\n    while(true) {\n\n      long mergingBytes = 0;\n\n      // Gather eligible segments for merging, ie segments\n      // not already being merged and not already picked (by\n      // prior iteration of this loop) for merging:\n      final List<SegmentCommitInfo> eligible = new ArrayList<>();\n      for(int idx = tooBigCount; idx<infosSorted.size(); idx++) {\n        final SegmentCommitInfo info = infosSorted.get(idx);\n        if (merging.contains(info)) {\n          mergingBytes += sizeInBytes.get(info);\n        } else if (!toBeMerged.contains(info)) {\n          eligible.add(info);\n        }\n      }\n\n      final boolean maxMergeIsRunning = mergingBytes >= maxMergedSegmentBytes;\n\n      if (verbose(mergeContext)) {\n        message(\"  allowedSegmentCount=\" + allowedSegCountInt + \" vs count=\" + infosSorted.size() + \" (eligible count=\" + eligible.size() + \") tooBigCount=\" + tooBigCount, mergeContext);\n      }\n\n      if (eligible.size() == 0) {\n        return spec;\n      }\n\n      if (eligible.size() > allowedSegCountInt) {\n\n        // OK we are over budget -- find best merge!\n        MergeScore bestScore = null;\n        List<SegmentCommitInfo> best = null;\n        boolean bestTooLarge = false;\n        long bestMergeBytes = 0;\n\n        // Consider all merge starts:\n        for(int startIdx = 0;startIdx <= eligible.size()-maxMergeAtOnce; startIdx++) {\n\n          long totAfterMergeBytes = 0;\n\n          final List<SegmentCommitInfo> candidate = new ArrayList<>();\n          boolean hitTooLarge = false;\n          for(int idx = startIdx;idx<eligible.size() && candidate.size() < maxMergeAtOnce;idx++) {\n            final SegmentCommitInfo info = eligible.get(idx);\n            final long segBytes = sizeInBytes.get(info);\n\n            if (totAfterMergeBytes + segBytes > maxMergedSegmentBytes) {\n              hitTooLarge = true;\n              // NOTE: we continue, so that we can try\n              // \"packing\" smaller segments into this merge\n              // to see if we can get closer to the max\n              // size; this in general is not perfect since\n              // this is really \"bin packing\" and we'd have\n              // to try different permutations.\n              continue;\n            }\n            candidate.add(info);\n            totAfterMergeBytes += segBytes;\n          }\n\n          // We should never see an empty candidate: we iterated over maxMergeAtOnce\n          // segments, and already pre-excluded the too-large segments:\n          assert candidate.size() > 0;\n\n          final MergeScore score = score(candidate, hitTooLarge, sizeInBytes);\n          if (verbose(mergeContext)) {\n            message(\"  maybe=\" + segString(mergeContext, candidate) + \" score=\" + score.getScore() + \" \" + score.getExplanation() + \" tooLarge=\" + hitTooLarge + \" size=\" + String.format(Locale.ROOT, \"%.3f MB\", totAfterMergeBytes/1024./1024.), mergeContext);\n          }\n\n          // If we are already running a max sized merge\n          // (maxMergeIsRunning), don't allow another max\n          // sized merge to kick off:\n          if ((bestScore == null || score.getScore() < bestScore.getScore()) && (!hitTooLarge || !maxMergeIsRunning)) {\n            best = candidate;\n            bestScore = score;\n            bestTooLarge = hitTooLarge;\n            bestMergeBytes = totAfterMergeBytes;\n          }\n        }\n        \n        if (best != null) {\n          if (spec == null) {\n            spec = new MergeSpecification();\n          }\n          final OneMerge merge = new OneMerge(best);\n          spec.add(merge);\n          toBeMerged.addAll(merge.segments);\n\n          if (verbose(mergeContext)) {\n            message(\"  add merge=\" + segString(mergeContext, merge.segments) + \" size=\" + String.format(Locale.ROOT, \"%.3f MB\", bestMergeBytes/1024./1024.) + \" score=\" + String.format(Locale.ROOT, \"%.3f\", bestScore.getScore()) + \" \" + bestScore.getExplanation() + (bestTooLarge ? \" [max merge]\" : \"\"), mergeContext);\n          }\n        } else {\n          return spec;\n        }\n      } else {\n        return spec;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4a90cc8c90aa53ddf51fbd15019989ac269514a3","date":1531845066,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findMerges(MergeTrigger,SegmentInfos,MergeContext).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findMerges(MergeTrigger,SegmentInfos,MergeContext).mjava","sourceNew":"  @Override\n  public MergeSpecification findMerges(MergeTrigger mergeTrigger, SegmentInfos infos, MergeContext mergeContext) throws IOException {\n    final Set<SegmentCommitInfo> merging = mergeContext.getMergingSegments();\n    // Compute total index bytes & print details about the index\n    long totIndexBytes = 0;\n    long minSegmentBytes = Long.MAX_VALUE;\n\n    int totalDelDocs = 0;\n    int totalMaxDoc = 0;\n\n    long mergingBytes = 0;\n\n    List<SegmentSizeAndDocs> sortedInfos = getSortedBySegmentSize(infos, mergeContext);\n    Iterator<SegmentSizeAndDocs> iter = sortedInfos.iterator();\n    while (iter.hasNext()) {\n      SegmentSizeAndDocs segSizeDocs = iter.next();\n      final long segBytes = segSizeDocs.sizeInBytes;\n      if (verbose(mergeContext)) {\n        String extra = merging.contains(segSizeDocs.segInfo) ? \" [merging]\" : \"\";\n        if (segBytes >= maxMergedSegmentBytes) {\n          extra += \" [skip: too large]\";\n        } else if (segBytes < floorSegmentBytes) {\n          extra += \" [floored]\";\n        }\n        message(\"  seg=\" + segString(mergeContext, Collections.singleton(segSizeDocs.segInfo)) + \" size=\" + String.format(Locale.ROOT, \"%.3f\", segBytes / 1024 / 1024.) + \" MB\" + extra, mergeContext);\n      }\n      if (merging.contains(segSizeDocs.segInfo)) {\n        mergingBytes += segSizeDocs.sizeInBytes;\n        iter.remove();\n        // if this segment is merging, then its deletes are being reclaimed already.\n        // only count live docs in the total max doc\n        totalMaxDoc += segSizeDocs.maxDoc - segSizeDocs.delCount;\n      } else {\n        totalDelDocs += segSizeDocs.delCount;\n        totalMaxDoc += segSizeDocs.maxDoc;\n      }\n\n      minSegmentBytes = Math.min(segBytes, minSegmentBytes);\n      totIndexBytes += segBytes;\n    }\n    assert totalMaxDoc >= 0;\n    assert totalDelDocs >= 0;\n\n    final double totalDelPct = 100 * (double) totalDelDocs / totalMaxDoc;\n    int allowedDelCount = (int) (deletesPctAllowed * totalMaxDoc / 100);\n\n    // If we have too-large segments, grace them out of the maximum segment count\n    // If we're above certain thresholds of deleted docs, we can merge very large segments.\n    int tooBigCount = 0;\n    iter = sortedInfos.iterator();\n\n    // remove large segments from consideration under two conditions.\n    // 1> Overall percent deleted docs relatively small and this segment is larger than 50% maxSegSize\n    // 2> overall percent deleted docs large and this segment is large and has few deleted docs\n\n    while (iter.hasNext()) {\n      SegmentSizeAndDocs segSizeDocs = iter.next();\n      double segDelPct = 100 * (double) segSizeDocs.delCount / (double) segSizeDocs.maxDoc;\n      if (segSizeDocs.sizeInBytes > maxMergedSegmentBytes / 2 && (totalDelPct <= deletesPctAllowed || segDelPct <= deletesPctAllowed)) {\n        iter.remove();\n        tooBigCount++; // Just for reporting purposes.\n        totIndexBytes -= segSizeDocs.sizeInBytes;\n        allowedDelCount -= segSizeDocs.delCount;\n      }\n    }\n    allowedDelCount = Math.max(0, allowedDelCount);\n\n    final int mergeFactor = (int) Math.min(maxMergeAtOnce, segsPerTier);\n    // Compute max allowed segments in the index\n    long levelSize = Math.max(minSegmentBytes, floorSegmentBytes);\n    long bytesLeft = totIndexBytes;\n    double allowedSegCount = 0;\n    while (true) {\n      final double segCountLevel = bytesLeft / (double) levelSize;\n      if (segCountLevel < segsPerTier || levelSize == maxMergedSegmentBytes) {\n        allowedSegCount += Math.ceil(segCountLevel);\n        break;\n      }\n      allowedSegCount += segsPerTier;\n      bytesLeft -= segsPerTier * levelSize;\n      levelSize = Math.min(maxMergedSegmentBytes, levelSize * mergeFactor);\n    }\n    // allowedSegCount may occasionally be less than segsPerTier\n    // if segment sizes are below the floor size\n    allowedSegCount = Math.max(allowedSegCount, segsPerTier);\n\n    if (verbose(mergeContext) && tooBigCount > 0) {\n      message(\"  allowedSegmentCount=\" + allowedSegCount + \" vs count=\" + infos.size() +\n          \" (eligible count=\" + sortedInfos.size() + \") tooBigCount= \" + tooBigCount, mergeContext);\n    }\n    return doFindMerges(sortedInfos, maxMergedSegmentBytes, mergeFactor, (int) allowedSegCount, allowedDelCount, MERGE_TYPE.NATURAL,\n        mergeContext, mergingBytes >= maxMergedSegmentBytes);\n  }\n\n","sourceOld":"  @Override\n  public MergeSpecification findMerges(MergeTrigger mergeTrigger, SegmentInfos infos, MergeContext mergeContext) throws IOException {\n    final Set<SegmentCommitInfo> merging = mergeContext.getMergingSegments();\n    // Compute total index bytes & print details about the index\n    long totIndexBytes = 0;\n    long minSegmentBytes = Long.MAX_VALUE;\n\n    int totalDelDocs = 0;\n    int totalMaxDoc = 0;\n\n    long mergingBytes = 0;\n\n    List<SegmentSizeAndDocs> sortedInfos = getSortedBySegmentSize(infos, mergeContext);\n    Iterator<SegmentSizeAndDocs> iter = sortedInfos.iterator();\n    while (iter.hasNext()) {\n      SegmentSizeAndDocs segSizeDocs = iter.next();\n      final long segBytes = segSizeDocs.sizeInBytes;\n      if (verbose(mergeContext)) {\n        String extra = merging.contains(segSizeDocs.segInfo) ? \" [merging]\" : \"\";\n        if (segBytes >= maxMergedSegmentBytes) {\n          extra += \" [skip: too large]\";\n        } else if (segBytes < floorSegmentBytes) {\n          extra += \" [floored]\";\n        }\n        message(\"  seg=\" + segString(mergeContext, Collections.singleton(segSizeDocs.segInfo)) + \" size=\" + String.format(Locale.ROOT, \"%.3f\", segBytes / 1024 / 1024.) + \" MB\" + extra, mergeContext);\n      }\n      if (merging.contains(segSizeDocs.segInfo)) {\n        mergingBytes += segSizeDocs.sizeInBytes;\n        iter.remove();\n      } else {\n        totalDelDocs += segSizeDocs.delCount;\n        totalMaxDoc += segSizeDocs.maxDoc;\n      }\n\n      minSegmentBytes = Math.min(segBytes, minSegmentBytes);\n      totIndexBytes += segBytes;\n    }\n    assert totalMaxDoc >= 0;\n    assert totalDelDocs >= 0;\n\n    // If we have too-large segments, grace them out of the maximum segment count\n    // If we're above certain thresholds, we can merge very large segments.\n    double totalDelPct = (double) totalDelDocs / (double) totalMaxDoc;\n    //TODO: See LUCENE-8263\n    //double targetAsPct = indexPctDeletedTarget / 100.0;\n    double targetAsPct = 0.5;\n    int tooBigCount = 0;\n    iter = sortedInfos.iterator();\n\n    // remove large segments from consideration under two conditions.\n    // 1> Overall percent deleted docs relatively small and this segment is larger than 50% maxSegSize\n    // 2> overall percent deleted docs large and this segment is large and has few deleted docs\n\n    while (iter.hasNext()) {\n      SegmentSizeAndDocs segSizeDocs = iter.next();\n      double segDelPct = (double) segSizeDocs.delCount / (double) segSizeDocs.maxDoc;\n      if (segSizeDocs.sizeInBytes > maxMergedSegmentBytes / 2 && (totalDelPct < targetAsPct || segDelPct < targetAsPct)) {\n        iter.remove();\n        tooBigCount++; // Just for reporting purposes.\n        totIndexBytes -= segSizeDocs.sizeInBytes;\n      }\n    }\n\n    final int mergeFactor = (int) Math.min(maxMergeAtOnce, segsPerTier);\n    // Compute max allowed segments in the index\n    long levelSize = Math.max(minSegmentBytes, floorSegmentBytes);\n    long bytesLeft = totIndexBytes;\n    double allowedSegCount = 0;\n    while (true) {\n      final double segCountLevel = bytesLeft / (double) levelSize;\n      if (segCountLevel < segsPerTier || levelSize == maxMergedSegmentBytes) {\n        allowedSegCount += Math.ceil(segCountLevel);\n        break;\n      }\n      allowedSegCount += segsPerTier;\n      bytesLeft -= segsPerTier * levelSize;\n      levelSize = Math.min(maxMergedSegmentBytes, levelSize * mergeFactor);\n    }\n    // allowedSegCount may occasionally be less than segsPerTier\n    // if segment sizes are below the floor size\n    allowedSegCount = Math.max(allowedSegCount, segsPerTier);\n\n    if (verbose(mergeContext) && tooBigCount > 0) {\n      message(\"  allowedSegmentCount=\" + allowedSegCount + \" vs count=\" + infos.size() +\n          \" (eligible count=\" + sortedInfos.size() + \") tooBigCount= \" + tooBigCount, mergeContext);\n    }\n    return doFindMerges(sortedInfos, maxMergedSegmentBytes, mergeFactor, (int) allowedSegCount, MERGE_TYPE.NATURAL,\n        mergeContext, mergingBytes >= maxMergedSegmentBytes);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","date":1531905561,"type":3,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findMerges(MergeTrigger,SegmentInfos,MergeContext).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findMerges(MergeTrigger,SegmentInfos,MergeContext).mjava","sourceNew":"  @Override\n  public MergeSpecification findMerges(MergeTrigger mergeTrigger, SegmentInfos infos, MergeContext mergeContext) throws IOException {\n    final Set<SegmentCommitInfo> merging = mergeContext.getMergingSegments();\n    // Compute total index bytes & print details about the index\n    long totIndexBytes = 0;\n    long minSegmentBytes = Long.MAX_VALUE;\n\n    int totalDelDocs = 0;\n    int totalMaxDoc = 0;\n\n    long mergingBytes = 0;\n\n    List<SegmentSizeAndDocs> sortedInfos = getSortedBySegmentSize(infos, mergeContext);\n    Iterator<SegmentSizeAndDocs> iter = sortedInfos.iterator();\n    while (iter.hasNext()) {\n      SegmentSizeAndDocs segSizeDocs = iter.next();\n      final long segBytes = segSizeDocs.sizeInBytes;\n      if (verbose(mergeContext)) {\n        String extra = merging.contains(segSizeDocs.segInfo) ? \" [merging]\" : \"\";\n        if (segBytes >= maxMergedSegmentBytes) {\n          extra += \" [skip: too large]\";\n        } else if (segBytes < floorSegmentBytes) {\n          extra += \" [floored]\";\n        }\n        message(\"  seg=\" + segString(mergeContext, Collections.singleton(segSizeDocs.segInfo)) + \" size=\" + String.format(Locale.ROOT, \"%.3f\", segBytes / 1024 / 1024.) + \" MB\" + extra, mergeContext);\n      }\n      if (merging.contains(segSizeDocs.segInfo)) {\n        mergingBytes += segSizeDocs.sizeInBytes;\n        iter.remove();\n        // if this segment is merging, then its deletes are being reclaimed already.\n        // only count live docs in the total max doc\n        totalMaxDoc += segSizeDocs.maxDoc - segSizeDocs.delCount;\n      } else {\n        totalDelDocs += segSizeDocs.delCount;\n        totalMaxDoc += segSizeDocs.maxDoc;\n      }\n\n      minSegmentBytes = Math.min(segBytes, minSegmentBytes);\n      totIndexBytes += segBytes;\n    }\n    assert totalMaxDoc >= 0;\n    assert totalDelDocs >= 0;\n\n    final double totalDelPct = 100 * (double) totalDelDocs / totalMaxDoc;\n    int allowedDelCount = (int) (deletesPctAllowed * totalMaxDoc / 100);\n\n    // If we have too-large segments, grace them out of the maximum segment count\n    // If we're above certain thresholds of deleted docs, we can merge very large segments.\n    int tooBigCount = 0;\n    iter = sortedInfos.iterator();\n\n    // remove large segments from consideration under two conditions.\n    // 1> Overall percent deleted docs relatively small and this segment is larger than 50% maxSegSize\n    // 2> overall percent deleted docs large and this segment is large and has few deleted docs\n\n    while (iter.hasNext()) {\n      SegmentSizeAndDocs segSizeDocs = iter.next();\n      double segDelPct = 100 * (double) segSizeDocs.delCount / (double) segSizeDocs.maxDoc;\n      if (segSizeDocs.sizeInBytes > maxMergedSegmentBytes / 2 && (totalDelPct <= deletesPctAllowed || segDelPct <= deletesPctAllowed)) {\n        iter.remove();\n        tooBigCount++; // Just for reporting purposes.\n        totIndexBytes -= segSizeDocs.sizeInBytes;\n        allowedDelCount -= segSizeDocs.delCount;\n      }\n    }\n    allowedDelCount = Math.max(0, allowedDelCount);\n\n    final int mergeFactor = (int) Math.min(maxMergeAtOnce, segsPerTier);\n    // Compute max allowed segments in the index\n    long levelSize = Math.max(minSegmentBytes, floorSegmentBytes);\n    long bytesLeft = totIndexBytes;\n    double allowedSegCount = 0;\n    while (true) {\n      final double segCountLevel = bytesLeft / (double) levelSize;\n      if (segCountLevel < segsPerTier || levelSize == maxMergedSegmentBytes) {\n        allowedSegCount += Math.ceil(segCountLevel);\n        break;\n      }\n      allowedSegCount += segsPerTier;\n      bytesLeft -= segsPerTier * levelSize;\n      levelSize = Math.min(maxMergedSegmentBytes, levelSize * mergeFactor);\n    }\n    // allowedSegCount may occasionally be less than segsPerTier\n    // if segment sizes are below the floor size\n    allowedSegCount = Math.max(allowedSegCount, segsPerTier);\n\n    if (verbose(mergeContext) && tooBigCount > 0) {\n      message(\"  allowedSegmentCount=\" + allowedSegCount + \" vs count=\" + infos.size() +\n          \" (eligible count=\" + sortedInfos.size() + \") tooBigCount= \" + tooBigCount, mergeContext);\n    }\n    return doFindMerges(sortedInfos, maxMergedSegmentBytes, mergeFactor, (int) allowedSegCount, allowedDelCount, MERGE_TYPE.NATURAL,\n        mergeContext, mergingBytes >= maxMergedSegmentBytes);\n  }\n\n","sourceOld":"  @Override\n  public MergeSpecification findMerges(MergeTrigger mergeTrigger, SegmentInfos infos, MergeContext mergeContext) throws IOException {\n    if (verbose(mergeContext)) {\n      message(\"findMerges: \" + infos.size() + \" segments\", mergeContext);\n    }\n    if (infos.size() == 0) {\n      return null;\n    }\n    final Set<SegmentCommitInfo> merging = mergeContext.getMergingSegments();\n    final Set<SegmentCommitInfo> toBeMerged = new HashSet<>();\n\n    final List<SegmentCommitInfo> infosSorted = new ArrayList<>(infos.asList());\n\n    // The size can change concurrently while we are running here, because deletes\n    // are now applied concurrently, and this can piss off TimSort!  So we\n    // call size() once per segment and sort by that:\n    Map<SegmentCommitInfo,Long> sizeInBytes = getSegmentSizes(mergeContext, infos.asList());\n    \n    infosSorted.sort(new SegmentByteSizeDescending(sizeInBytes));\n\n    // Compute total index bytes & print details about the index\n    long totIndexBytes = 0;\n    long minSegmentBytes = Long.MAX_VALUE;\n    for(SegmentCommitInfo info : infosSorted) {\n      final long segBytes = sizeInBytes.get(info);\n      if (verbose(mergeContext)) {\n        String extra = merging.contains(info) ? \" [merging]\" : \"\";\n        if (segBytes >= maxMergedSegmentBytes/2.0) {\n          extra += \" [skip: too large]\";\n        } else if (segBytes < floorSegmentBytes) {\n          extra += \" [floored]\";\n        }\n        message(\"  seg=\" + segString(mergeContext, Collections.singleton(info)) + \" size=\" + String.format(Locale.ROOT, \"%.3f\", segBytes/1024/1024.) + \" MB\" + extra, mergeContext);\n      }\n\n      minSegmentBytes = Math.min(segBytes, minSegmentBytes);\n      // Accum total byte size\n      totIndexBytes += segBytes;\n    }\n\n    // If we have too-large segments, grace them out\n    // of the maxSegmentCount:\n    int tooBigCount = 0;\n    while (tooBigCount < infosSorted.size()) {\n      long segBytes = sizeInBytes.get(infosSorted.get(tooBigCount));\n      if (segBytes < maxMergedSegmentBytes/2.0) {\n        break;\n      }\n      totIndexBytes -= segBytes;\n      tooBigCount++;\n    }\n\n    minSegmentBytes = floorSize(minSegmentBytes);\n\n    // Compute max allowed segs in the index\n    long levelSize = minSegmentBytes;\n    long bytesLeft = totIndexBytes;\n    double allowedSegCount = 0;\n    while(true) {\n      final double segCountLevel = bytesLeft / (double) levelSize;\n      if (segCountLevel < segsPerTier) {\n        allowedSegCount += Math.ceil(segCountLevel);\n        break;\n      }\n      allowedSegCount += segsPerTier;\n      bytesLeft -= segsPerTier * levelSize;\n      levelSize *= maxMergeAtOnce;\n    }\n    int allowedSegCountInt = (int) allowedSegCount;\n\n    MergeSpecification spec = null;\n\n    // Cycle to possibly select more than one merge:\n    while(true) {\n\n      long mergingBytes = 0;\n\n      // Gather eligible segments for merging, ie segments\n      // not already being merged and not already picked (by\n      // prior iteration of this loop) for merging:\n      final List<SegmentCommitInfo> eligible = new ArrayList<>();\n      for(int idx = tooBigCount; idx<infosSorted.size(); idx++) {\n        final SegmentCommitInfo info = infosSorted.get(idx);\n        if (merging.contains(info)) {\n          mergingBytes += sizeInBytes.get(info);\n        } else if (!toBeMerged.contains(info)) {\n          eligible.add(info);\n        }\n      }\n\n      final boolean maxMergeIsRunning = mergingBytes >= maxMergedSegmentBytes;\n\n      if (verbose(mergeContext)) {\n        message(\"  allowedSegmentCount=\" + allowedSegCountInt + \" vs count=\" + infosSorted.size() + \" (eligible count=\" + eligible.size() + \") tooBigCount=\" + tooBigCount, mergeContext);\n      }\n\n      if (eligible.size() == 0) {\n        return spec;\n      }\n\n      if (eligible.size() > allowedSegCountInt) {\n\n        // OK we are over budget -- find best merge!\n        MergeScore bestScore = null;\n        List<SegmentCommitInfo> best = null;\n        boolean bestTooLarge = false;\n        long bestMergeBytes = 0;\n\n        // Consider all merge starts:\n        for(int startIdx = 0;startIdx <= eligible.size()-maxMergeAtOnce; startIdx++) {\n\n          long totAfterMergeBytes = 0;\n\n          final List<SegmentCommitInfo> candidate = new ArrayList<>();\n          boolean hitTooLarge = false;\n          for(int idx = startIdx;idx<eligible.size() && candidate.size() < maxMergeAtOnce;idx++) {\n            final SegmentCommitInfo info = eligible.get(idx);\n            final long segBytes = sizeInBytes.get(info);\n\n            if (totAfterMergeBytes + segBytes > maxMergedSegmentBytes) {\n              hitTooLarge = true;\n              // NOTE: we continue, so that we can try\n              // \"packing\" smaller segments into this merge\n              // to see if we can get closer to the max\n              // size; this in general is not perfect since\n              // this is really \"bin packing\" and we'd have\n              // to try different permutations.\n              continue;\n            }\n            candidate.add(info);\n            totAfterMergeBytes += segBytes;\n          }\n\n          // We should never see an empty candidate: we iterated over maxMergeAtOnce\n          // segments, and already pre-excluded the too-large segments:\n          assert candidate.size() > 0;\n\n          final MergeScore score = score(candidate, hitTooLarge, sizeInBytes);\n          if (verbose(mergeContext)) {\n            message(\"  maybe=\" + segString(mergeContext, candidate) + \" score=\" + score.getScore() + \" \" + score.getExplanation() + \" tooLarge=\" + hitTooLarge + \" size=\" + String.format(Locale.ROOT, \"%.3f MB\", totAfterMergeBytes/1024./1024.), mergeContext);\n          }\n\n          // If we are already running a max sized merge\n          // (maxMergeIsRunning), don't allow another max\n          // sized merge to kick off:\n          if ((bestScore == null || score.getScore() < bestScore.getScore()) && (!hitTooLarge || !maxMergeIsRunning)) {\n            best = candidate;\n            bestScore = score;\n            bestTooLarge = hitTooLarge;\n            bestMergeBytes = totAfterMergeBytes;\n          }\n        }\n        \n        if (best != null) {\n          if (spec == null) {\n            spec = new MergeSpecification();\n          }\n          final OneMerge merge = new OneMerge(best);\n          spec.add(merge);\n          toBeMerged.addAll(merge.segments);\n\n          if (verbose(mergeContext)) {\n            message(\"  add merge=\" + segString(mergeContext, merge.segments) + \" size=\" + String.format(Locale.ROOT, \"%.3f MB\", bestMergeBytes/1024./1024.) + \" score=\" + String.format(Locale.ROOT, \"%.3f\", bestScore.getScore()) + \" \" + bestScore.getExplanation() + (bestTooLarge ? \" [max merge]\" : \"\"), mergeContext);\n          }\n        } else {\n          return spec;\n        }\n      } else {\n        return spec;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"890efdcaafbf652afef683f55f47f84e415bb292":["9707a68fe260631e514201dbf24e9afc9a3a4ba1"],"9707a68fe260631e514201dbf24e9afc9a3a4ba1":["f9abddc2db59050e915640d3c6835e9b8fe5f47d"],"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":["1d28f215464f76024caf026606f8ea51a5319c53","4a90cc8c90aa53ddf51fbd15019989ac269514a3"],"56fb5e4e4b239474721e13b4cd9542ea2d215451":["1d28f215464f76024caf026606f8ea51a5319c53"],"404dafe26b816e6ed478486e26abd62d8607b123":["56fb5e4e4b239474721e13b4cd9542ea2d215451"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"1d28f215464f76024caf026606f8ea51a5319c53":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"4a90cc8c90aa53ddf51fbd15019989ac269514a3":["bf644d067afb7311fd40d2122a6c772f8635d65a"],"f9abddc2db59050e915640d3c6835e9b8fe5f47d":["404dafe26b816e6ed478486e26abd62d8607b123"],"bf644d067afb7311fd40d2122a6c772f8635d65a":["890efdcaafbf652afef683f55f47f84e415bb292"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["4a90cc8c90aa53ddf51fbd15019989ac269514a3"],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":["1d28f215464f76024caf026606f8ea51a5319c53","bf644d067afb7311fd40d2122a6c772f8635d65a"]},"commit2Childs":{"890efdcaafbf652afef683f55f47f84e415bb292":["bf644d067afb7311fd40d2122a6c772f8635d65a"],"9707a68fe260631e514201dbf24e9afc9a3a4ba1":["890efdcaafbf652afef683f55f47f84e415bb292"],"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":[],"56fb5e4e4b239474721e13b4cd9542ea2d215451":["404dafe26b816e6ed478486e26abd62d8607b123"],"404dafe26b816e6ed478486e26abd62d8607b123":["f9abddc2db59050e915640d3c6835e9b8fe5f47d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["1d28f215464f76024caf026606f8ea51a5319c53"],"1d28f215464f76024caf026606f8ea51a5319c53":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","56fb5e4e4b239474721e13b4cd9542ea2d215451","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"4a90cc8c90aa53ddf51fbd15019989ac269514a3":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"f9abddc2db59050e915640d3c6835e9b8fe5f47d":["9707a68fe260631e514201dbf24e9afc9a3a4ba1"],"bf644d067afb7311fd40d2122a6c772f8635d65a":["4a90cc8c90aa53ddf51fbd15019989ac269514a3","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":[]},"heads":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","cd5edd1f2b162a5cfa08efd17851a07373a96817","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}