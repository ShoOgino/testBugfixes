{"path":"contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","commits":[{"id":"d3c3c2404d1200c39220fa15054fae854db4e1ee","date":1140827958,"type":0,"author":"Mark Harwood","isMerge":false,"pathNew":"contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"/dev/null","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\r\n    {\r\n        if(f.queryString==null) return;\r\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\r\n        Token token=ts.next();\r\n        int corpusNumDocs=reader.numDocs();\r\n        Term internSavingTemplateTerm =new Term(f.fieldName,\"\"); //optimization to avoid constructing new Term() objects\r\n        \r\n        while(token!=null)\r\n        {            \r\n            ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\r\n            float minScore=0;\r\n            Term startTerm=internSavingTemplateTerm.createTerm(token.termText());\r\n            FuzzyTermEnum fe=new FuzzyTermEnum(reader,startTerm,f.minSimilarity,f.prefixLength);\r\n            TermEnum origEnum = reader.terms(startTerm);\r\n            int df=0;\r\n            if(startTerm.equals(origEnum.term()))\r\n            {\r\n                df=origEnum.docFreq(); //store the df so all variants use same idf\r\n            }\r\n            int numVariants=0;\r\n            int totalVariantDocFreqs=0;\r\n            do\r\n            {\r\n                Term possibleMatch=fe.term();\r\n                if(possibleMatch!=null)\r\n                {\r\n\t                numVariants++;\r\n\t                totalVariantDocFreqs+=fe.docFreq();\r\n\t                float score=fe.difference();\r\n\t                if(variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\r\n\t                    ScoreTerm st=new ScoreTerm(possibleMatch,score,startTerm);                    \r\n\t                    variantsQ.insert(st);\r\n\t                    minScore = ((ScoreTerm)variantsQ.top()).score; // maintain minScore\r\n\t                }\r\n                }\r\n            }\r\n            while(fe.next());\r\n            if(numVariants==0)\r\n            {\r\n                //no variants to rank here\r\n                break;\r\n            }\r\n            int avgDf=totalVariantDocFreqs/numVariants;\r\n            if(df==0)//no direct match we can use as df for all variants \r\n            {\r\n                df=avgDf; //use avg df of all variants\r\n            }\r\n            \r\n            // take the top variants (scored by edit distance) and reset the score\r\n            // to include an IDF factor then add to the global queue for ranking overall top query terms\r\n            int size = variantsQ.size();\r\n            for(int i = 0; i < size; i++)\r\n            {\r\n              ScoreTerm st = (ScoreTerm) variantsQ.pop();\r\n              st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\r\n              q.insert(st);\r\n            }                            \r\n            token=ts.next();\r\n        }        \r\n    }\r\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e26cca6058accb5c7c98a2f936b511ad41db848d","date":1150141116,"type":3,"author":"Mark Harwood","isMerge":false,"pathNew":"contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\r\n    {\r\n        if(f.queryString==null) return;\r\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\r\n        Token token=ts.next();\r\n        int corpusNumDocs=reader.numDocs();\r\n        Term internSavingTemplateTerm =new Term(f.fieldName,\"\"); //optimization to avoid constructing new Term() objects\r\n        HashSet processedTerms=new HashSet();\r\n        while(token!=null)\r\n        {            \r\n        \tif(!processedTerms.contains(token.termText()))\r\n        \t{\r\n        \t\tprocessedTerms.add(token.termText());\r\n                ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\r\n                float minScore=0;\r\n                Term startTerm=internSavingTemplateTerm.createTerm(token.termText());\r\n                FuzzyTermEnum fe=new FuzzyTermEnum(reader,startTerm,f.minSimilarity,f.prefixLength);\r\n                TermEnum origEnum = reader.terms(startTerm);\r\n                int df=0;\r\n                if(startTerm.equals(origEnum.term()))\r\n                {\r\n                    df=origEnum.docFreq(); //store the df so all variants use same idf\r\n                }\r\n                int numVariants=0;\r\n                int totalVariantDocFreqs=0;\r\n                do\r\n                {\r\n                    Term possibleMatch=fe.term();\r\n                    if(possibleMatch!=null)\r\n                    {\r\n    \t                numVariants++;\r\n    \t                totalVariantDocFreqs+=fe.docFreq();\r\n    \t                float score=fe.difference();\r\n    \t                if(variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\r\n    \t                    ScoreTerm st=new ScoreTerm(possibleMatch,score,startTerm);                    \r\n    \t                    variantsQ.insert(st);\r\n    \t                    minScore = ((ScoreTerm)variantsQ.top()).score; // maintain minScore\r\n    \t                }\r\n                    }\r\n                }\r\n                while(fe.next());\r\n                if(numVariants==0)\r\n                {\r\n                    //no variants to rank here\r\n                    break;\r\n                }\r\n                int avgDf=totalVariantDocFreqs/numVariants;\r\n                if(df==0)//no direct match we can use as df for all variants \r\n                {\r\n                    df=avgDf; //use avg df of all variants\r\n                }\r\n                \r\n                // take the top variants (scored by edit distance) and reset the score\r\n                // to include an IDF factor then add to the global queue for ranking overall top query terms\r\n                int size = variantsQ.size();\r\n                for(int i = 0; i < size; i++)\r\n                {\r\n                  ScoreTerm st = (ScoreTerm) variantsQ.pop();\r\n                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\r\n                  q.insert(st);\r\n                }                            \r\n        \t}\r\n            token=ts.next();\r\n        }        \r\n    }\r\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\r\n    {\r\n        if(f.queryString==null) return;\r\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\r\n        Token token=ts.next();\r\n        int corpusNumDocs=reader.numDocs();\r\n        Term internSavingTemplateTerm =new Term(f.fieldName,\"\"); //optimization to avoid constructing new Term() objects\r\n        \r\n        while(token!=null)\r\n        {            \r\n            ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\r\n            float minScore=0;\r\n            Term startTerm=internSavingTemplateTerm.createTerm(token.termText());\r\n            FuzzyTermEnum fe=new FuzzyTermEnum(reader,startTerm,f.minSimilarity,f.prefixLength);\r\n            TermEnum origEnum = reader.terms(startTerm);\r\n            int df=0;\r\n            if(startTerm.equals(origEnum.term()))\r\n            {\r\n                df=origEnum.docFreq(); //store the df so all variants use same idf\r\n            }\r\n            int numVariants=0;\r\n            int totalVariantDocFreqs=0;\r\n            do\r\n            {\r\n                Term possibleMatch=fe.term();\r\n                if(possibleMatch!=null)\r\n                {\r\n\t                numVariants++;\r\n\t                totalVariantDocFreqs+=fe.docFreq();\r\n\t                float score=fe.difference();\r\n\t                if(variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\r\n\t                    ScoreTerm st=new ScoreTerm(possibleMatch,score,startTerm);                    \r\n\t                    variantsQ.insert(st);\r\n\t                    minScore = ((ScoreTerm)variantsQ.top()).score; // maintain minScore\r\n\t                }\r\n                }\r\n            }\r\n            while(fe.next());\r\n            if(numVariants==0)\r\n            {\r\n                //no variants to rank here\r\n                break;\r\n            }\r\n            int avgDf=totalVariantDocFreqs/numVariants;\r\n            if(df==0)//no direct match we can use as df for all variants \r\n            {\r\n                df=avgDf; //use avg df of all variants\r\n            }\r\n            \r\n            // take the top variants (scored by edit distance) and reset the score\r\n            // to include an IDF factor then add to the global queue for ranking overall top query terms\r\n            int size = variantsQ.size();\r\n            for(int i = 0; i < size; i++)\r\n            {\r\n              ScoreTerm st = (ScoreTerm) variantsQ.pop();\r\n              st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\r\n              q.insert(st);\r\n            }                            \r\n            token=ts.next();\r\n        }        \r\n    }\r\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9d73678020862536617f065bb3d28a71d8c4020c","date":1219142439,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\r\n    {\r\n        if(f.queryString==null) return;\r\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\r\n        Token token=ts.next();\r\n        int corpusNumDocs=reader.numDocs();\r\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\r\n        HashSet processedTerms=new HashSet();\r\n        while(token!=null)\r\n        {            \r\n        \tif(!processedTerms.contains(token.termText()))\r\n        \t{\r\n        \t\tprocessedTerms.add(token.termText());\r\n                ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\r\n                float minScore=0;\r\n                Term startTerm=internSavingTemplateTerm.createTerm(token.termText());\r\n                FuzzyTermEnum fe=new FuzzyTermEnum(reader,startTerm,f.minSimilarity,f.prefixLength);\r\n                TermEnum origEnum = reader.terms(startTerm);\r\n                int df=0;\r\n                if(startTerm.equals(origEnum.term()))\r\n                {\r\n                    df=origEnum.docFreq(); //store the df so all variants use same idf\r\n                }\r\n                int numVariants=0;\r\n                int totalVariantDocFreqs=0;\r\n                do\r\n                {\r\n                    Term possibleMatch=fe.term();\r\n                    if(possibleMatch!=null)\r\n                    {\r\n    \t                numVariants++;\r\n    \t                totalVariantDocFreqs+=fe.docFreq();\r\n    \t                float score=fe.difference();\r\n    \t                if(variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\r\n    \t                    ScoreTerm st=new ScoreTerm(possibleMatch,score,startTerm);                    \r\n    \t                    variantsQ.insert(st);\r\n    \t                    minScore = ((ScoreTerm)variantsQ.top()).score; // maintain minScore\r\n    \t                }\r\n                    }\r\n                }\r\n                while(fe.next());\r\n                if(numVariants==0)\r\n                {\r\n                    //no variants to rank here\r\n                    break;\r\n                }\r\n                int avgDf=totalVariantDocFreqs/numVariants;\r\n                if(df==0)//no direct match we can use as df for all variants \r\n                {\r\n                    df=avgDf; //use avg df of all variants\r\n                }\r\n                \r\n                // take the top variants (scored by edit distance) and reset the score\r\n                // to include an IDF factor then add to the global queue for ranking overall top query terms\r\n                int size = variantsQ.size();\r\n                for(int i = 0; i < size; i++)\r\n                {\r\n                  ScoreTerm st = (ScoreTerm) variantsQ.pop();\r\n                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\r\n                  q.insert(st);\r\n                }                            \r\n        \t}\r\n            token=ts.next();\r\n        }        \r\n    }\r\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\r\n    {\r\n        if(f.queryString==null) return;\r\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\r\n        Token token=ts.next();\r\n        int corpusNumDocs=reader.numDocs();\r\n        Term internSavingTemplateTerm =new Term(f.fieldName,\"\"); //optimization to avoid constructing new Term() objects\r\n        HashSet processedTerms=new HashSet();\r\n        while(token!=null)\r\n        {            \r\n        \tif(!processedTerms.contains(token.termText()))\r\n        \t{\r\n        \t\tprocessedTerms.add(token.termText());\r\n                ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\r\n                float minScore=0;\r\n                Term startTerm=internSavingTemplateTerm.createTerm(token.termText());\r\n                FuzzyTermEnum fe=new FuzzyTermEnum(reader,startTerm,f.minSimilarity,f.prefixLength);\r\n                TermEnum origEnum = reader.terms(startTerm);\r\n                int df=0;\r\n                if(startTerm.equals(origEnum.term()))\r\n                {\r\n                    df=origEnum.docFreq(); //store the df so all variants use same idf\r\n                }\r\n                int numVariants=0;\r\n                int totalVariantDocFreqs=0;\r\n                do\r\n                {\r\n                    Term possibleMatch=fe.term();\r\n                    if(possibleMatch!=null)\r\n                    {\r\n    \t                numVariants++;\r\n    \t                totalVariantDocFreqs+=fe.docFreq();\r\n    \t                float score=fe.difference();\r\n    \t                if(variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\r\n    \t                    ScoreTerm st=new ScoreTerm(possibleMatch,score,startTerm);                    \r\n    \t                    variantsQ.insert(st);\r\n    \t                    minScore = ((ScoreTerm)variantsQ.top()).score; // maintain minScore\r\n    \t                }\r\n                    }\r\n                }\r\n                while(fe.next());\r\n                if(numVariants==0)\r\n                {\r\n                    //no variants to rank here\r\n                    break;\r\n                }\r\n                int avgDf=totalVariantDocFreqs/numVariants;\r\n                if(df==0)//no direct match we can use as df for all variants \r\n                {\r\n                    df=avgDf; //use avg df of all variants\r\n                }\r\n                \r\n                // take the top variants (scored by edit distance) and reset the score\r\n                // to include an IDF factor then add to the global queue for ranking overall top query terms\r\n                int size = variantsQ.size();\r\n                for(int i = 0; i < size; i++)\r\n                {\r\n                  ScoreTerm st = (ScoreTerm) variantsQ.pop();\r\n                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\r\n                  q.insert(st);\r\n                }                            \r\n        \t}\r\n            token=ts.next();\r\n        }        \r\n    }\r\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7e2cb543b41c145f33390f460ee743d6693c9c6c","date":1219243087,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\r\n    {\r\n        if(f.queryString==null) return;\r\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\r\n        final Token reusableToken = new Token();\r\n        int corpusNumDocs=reader.numDocs();\r\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\r\n        HashSet processedTerms=new HashSet();\r\n        for (Token nextToken = ts.next(reusableToken); nextToken!=null; nextToken = ts.next(reusableToken))\r\n        {\r\n                String term = nextToken.term();\r\n        \tif(!processedTerms.contains(term))\r\n        \t{\r\n        \t\tprocessedTerms.add(term);\r\n                ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\r\n                float minScore=0;\r\n                Term startTerm=internSavingTemplateTerm.createTerm(term);\r\n                FuzzyTermEnum fe=new FuzzyTermEnum(reader,startTerm,f.minSimilarity,f.prefixLength);\r\n                TermEnum origEnum = reader.terms(startTerm);\r\n                int df=0;\r\n                if(startTerm.equals(origEnum.term()))\r\n                {\r\n                    df=origEnum.docFreq(); //store the df so all variants use same idf\r\n                }\r\n                int numVariants=0;\r\n                int totalVariantDocFreqs=0;\r\n                do\r\n                {\r\n                    Term possibleMatch=fe.term();\r\n                    if(possibleMatch!=null)\r\n                    {\r\n    \t                numVariants++;\r\n    \t                totalVariantDocFreqs+=fe.docFreq();\r\n    \t                float score=fe.difference();\r\n    \t                if(variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\r\n    \t                    ScoreTerm st=new ScoreTerm(possibleMatch,score,startTerm);                    \r\n    \t                    variantsQ.insert(st);\r\n    \t                    minScore = ((ScoreTerm)variantsQ.top()).score; // maintain minScore\r\n    \t                }\r\n                    }\r\n                }\r\n                while(fe.next());\r\n                if(numVariants==0)\r\n                {\r\n                    //no variants to rank here\r\n                    break;\r\n                }\r\n                int avgDf=totalVariantDocFreqs/numVariants;\r\n                if(df==0)//no direct match we can use as df for all variants \r\n                {\r\n                    df=avgDf; //use avg df of all variants\r\n                }\r\n                \r\n                // take the top variants (scored by edit distance) and reset the score\r\n                // to include an IDF factor then add to the global queue for ranking overall top query terms\r\n                int size = variantsQ.size();\r\n                for(int i = 0; i < size; i++)\r\n                {\r\n                  ScoreTerm st = (ScoreTerm) variantsQ.pop();\r\n                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\r\n                  q.insert(st);\r\n                }                            \r\n        \t}\r\n        }     \r\n    }\r\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\r\n    {\r\n        if(f.queryString==null) return;\r\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\r\n        Token token=ts.next();\r\n        int corpusNumDocs=reader.numDocs();\r\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\r\n        HashSet processedTerms=new HashSet();\r\n        while(token!=null)\r\n        {            \r\n        \tif(!processedTerms.contains(token.termText()))\r\n        \t{\r\n        \t\tprocessedTerms.add(token.termText());\r\n                ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\r\n                float minScore=0;\r\n                Term startTerm=internSavingTemplateTerm.createTerm(token.termText());\r\n                FuzzyTermEnum fe=new FuzzyTermEnum(reader,startTerm,f.minSimilarity,f.prefixLength);\r\n                TermEnum origEnum = reader.terms(startTerm);\r\n                int df=0;\r\n                if(startTerm.equals(origEnum.term()))\r\n                {\r\n                    df=origEnum.docFreq(); //store the df so all variants use same idf\r\n                }\r\n                int numVariants=0;\r\n                int totalVariantDocFreqs=0;\r\n                do\r\n                {\r\n                    Term possibleMatch=fe.term();\r\n                    if(possibleMatch!=null)\r\n                    {\r\n    \t                numVariants++;\r\n    \t                totalVariantDocFreqs+=fe.docFreq();\r\n    \t                float score=fe.difference();\r\n    \t                if(variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\r\n    \t                    ScoreTerm st=new ScoreTerm(possibleMatch,score,startTerm);                    \r\n    \t                    variantsQ.insert(st);\r\n    \t                    minScore = ((ScoreTerm)variantsQ.top()).score; // maintain minScore\r\n    \t                }\r\n                    }\r\n                }\r\n                while(fe.next());\r\n                if(numVariants==0)\r\n                {\r\n                    //no variants to rank here\r\n                    break;\r\n                }\r\n                int avgDf=totalVariantDocFreqs/numVariants;\r\n                if(df==0)//no direct match we can use as df for all variants \r\n                {\r\n                    df=avgDf; //use avg df of all variants\r\n                }\r\n                \r\n                // take the top variants (scored by edit distance) and reset the score\r\n                // to include an IDF factor then add to the global queue for ranking overall top query terms\r\n                int size = variantsQ.size();\r\n                for(int i = 0; i < size; i++)\r\n                {\r\n                  ScoreTerm st = (ScoreTerm) variantsQ.pop();\r\n                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\r\n                  q.insert(st);\r\n                }                            \r\n        \t}\r\n            token=ts.next();\r\n        }        \r\n    }\r\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f3055a6d1420d229175009d777f2b781b4e3e97f","date":1222465495,"type":3,"author":"Mark Harwood","isMerge":false,"pathNew":"contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\r\n    {\r\n        if(f.queryString==null) return;\r\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\r\n        final Token reusableToken = new Token();\r\n        int corpusNumDocs=reader.numDocs();\r\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\r\n        HashSet processedTerms=new HashSet();\r\n        for (Token nextToken = ts.next(reusableToken); nextToken!=null; nextToken = ts.next(reusableToken))\r\n        {\r\n                String term = nextToken.term();\r\n        \tif(!processedTerms.contains(term))\r\n        \t{\r\n        \t\tprocessedTerms.add(term);\r\n                ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\r\n                float minScore=0;\r\n                Term startTerm=internSavingTemplateTerm.createTerm(term);\r\n                FuzzyTermEnum fe=new FuzzyTermEnum(reader,startTerm,f.minSimilarity,f.prefixLength);\r\n                TermEnum origEnum = reader.terms(startTerm);\r\n                int df=0;\r\n                if(startTerm.equals(origEnum.term()))\r\n                {\r\n                    df=origEnum.docFreq(); //store the df so all variants use same idf\r\n                }\r\n                int numVariants=0;\r\n                int totalVariantDocFreqs=0;\r\n                do\r\n                {\r\n                    Term possibleMatch=fe.term();\r\n                    if(possibleMatch!=null)\r\n                    {\r\n    \t                numVariants++;\r\n    \t                totalVariantDocFreqs+=fe.docFreq();\r\n    \t                float score=fe.difference();\r\n    \t                if(variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\r\n    \t                    ScoreTerm st=new ScoreTerm(possibleMatch,score,startTerm);                    \r\n    \t                    variantsQ.insert(st);\r\n    \t                    minScore = ((ScoreTerm)variantsQ.top()).score; // maintain minScore\r\n    \t                }\r\n                    }\r\n                }\r\n                while(fe.next());\r\n                if(numVariants>0)\r\n                {\r\n\t                int avgDf=totalVariantDocFreqs/numVariants;\r\n\t                if(df==0)//no direct match we can use as df for all variants \r\n\t                {\r\n\t                    df=avgDf; //use avg df of all variants\r\n\t                }\r\n\t                \r\n\t                // take the top variants (scored by edit distance) and reset the score\r\n\t                // to include an IDF factor then add to the global queue for ranking \r\n\t                // overall top query terms\r\n\t                int size = variantsQ.size();\r\n\t                for(int i = 0; i < size; i++)\r\n\t                {\r\n\t                  ScoreTerm st = (ScoreTerm) variantsQ.pop();\r\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\r\n\t                  q.insert(st);\r\n\t                }                            \r\n                }\r\n        \t}\r\n        }     \r\n    }\r\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\r\n    {\r\n        if(f.queryString==null) return;\r\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\r\n        final Token reusableToken = new Token();\r\n        int corpusNumDocs=reader.numDocs();\r\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\r\n        HashSet processedTerms=new HashSet();\r\n        for (Token nextToken = ts.next(reusableToken); nextToken!=null; nextToken = ts.next(reusableToken))\r\n        {\r\n                String term = nextToken.term();\r\n        \tif(!processedTerms.contains(term))\r\n        \t{\r\n        \t\tprocessedTerms.add(term);\r\n                ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\r\n                float minScore=0;\r\n                Term startTerm=internSavingTemplateTerm.createTerm(term);\r\n                FuzzyTermEnum fe=new FuzzyTermEnum(reader,startTerm,f.minSimilarity,f.prefixLength);\r\n                TermEnum origEnum = reader.terms(startTerm);\r\n                int df=0;\r\n                if(startTerm.equals(origEnum.term()))\r\n                {\r\n                    df=origEnum.docFreq(); //store the df so all variants use same idf\r\n                }\r\n                int numVariants=0;\r\n                int totalVariantDocFreqs=0;\r\n                do\r\n                {\r\n                    Term possibleMatch=fe.term();\r\n                    if(possibleMatch!=null)\r\n                    {\r\n    \t                numVariants++;\r\n    \t                totalVariantDocFreqs+=fe.docFreq();\r\n    \t                float score=fe.difference();\r\n    \t                if(variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\r\n    \t                    ScoreTerm st=new ScoreTerm(possibleMatch,score,startTerm);                    \r\n    \t                    variantsQ.insert(st);\r\n    \t                    minScore = ((ScoreTerm)variantsQ.top()).score; // maintain minScore\r\n    \t                }\r\n                    }\r\n                }\r\n                while(fe.next());\r\n                if(numVariants==0)\r\n                {\r\n                    //no variants to rank here\r\n                    break;\r\n                }\r\n                int avgDf=totalVariantDocFreqs/numVariants;\r\n                if(df==0)//no direct match we can use as df for all variants \r\n                {\r\n                    df=avgDf; //use avg df of all variants\r\n                }\r\n                \r\n                // take the top variants (scored by edit distance) and reset the score\r\n                // to include an IDF factor then add to the global queue for ranking overall top query terms\r\n                int size = variantsQ.size();\r\n                for(int i = 0; i < size; i++)\r\n                {\r\n                  ScoreTerm st = (ScoreTerm) variantsQ.pop();\r\n                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\r\n                  q.insert(st);\r\n                }                            \r\n        \t}\r\n        }     \r\n    }\r\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6a361a621b184d9b73c9c9a37323a9845b8f8260","date":1226370946,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        final Token reusableToken = new Token();\n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet processedTerms=new HashSet();\n        for (Token nextToken = ts.next(reusableToken); nextToken!=null; nextToken = ts.next(reusableToken))\n        {\n                String term = nextToken.term();\n        \tif(!processedTerms.contains(term))\n        \t{\n        \t\tprocessedTerms.add(term);\n                ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                float minScore=0;\n                Term startTerm=internSavingTemplateTerm.createTerm(term);\n                FuzzyTermEnum fe=new FuzzyTermEnum(reader,startTerm,f.minSimilarity,f.prefixLength);\n                TermEnum origEnum = reader.terms(startTerm);\n                int df=0;\n                if(startTerm.equals(origEnum.term()))\n                {\n                    df=origEnum.docFreq(); //store the df so all variants use same idf\n                }\n                int numVariants=0;\n                int totalVariantDocFreqs=0;\n                do\n                {\n                    Term possibleMatch=fe.term();\n                    if(possibleMatch!=null)\n                    {\n    \t                numVariants++;\n    \t                totalVariantDocFreqs+=fe.docFreq();\n    \t                float score=fe.difference();\n    \t                if(variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n    \t                    ScoreTerm st=new ScoreTerm(possibleMatch,score,startTerm);                    \n    \t                    variantsQ.insert(st);\n    \t                    minScore = ((ScoreTerm)variantsQ.top()).score; // maintain minScore\n    \t                }\n                    }\n                }\n                while(fe.next());\n                if(numVariants>0)\n                {\n\t                int avgDf=totalVariantDocFreqs/numVariants;\n\t                if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n\t                // take the top variants (scored by edit distance) and reset the score\n\t                // to include an IDF factor then add to the global queue for ranking \n\t                // overall top query terms\n\t                int size = variantsQ.size();\n\t                for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = (ScoreTerm) variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insert(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\r\n    {\r\n        if(f.queryString==null) return;\r\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\r\n        final Token reusableToken = new Token();\r\n        int corpusNumDocs=reader.numDocs();\r\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\r\n        HashSet processedTerms=new HashSet();\r\n        for (Token nextToken = ts.next(reusableToken); nextToken!=null; nextToken = ts.next(reusableToken))\r\n        {\r\n                String term = nextToken.term();\r\n        \tif(!processedTerms.contains(term))\r\n        \t{\r\n        \t\tprocessedTerms.add(term);\r\n                ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\r\n                float minScore=0;\r\n                Term startTerm=internSavingTemplateTerm.createTerm(term);\r\n                FuzzyTermEnum fe=new FuzzyTermEnum(reader,startTerm,f.minSimilarity,f.prefixLength);\r\n                TermEnum origEnum = reader.terms(startTerm);\r\n                int df=0;\r\n                if(startTerm.equals(origEnum.term()))\r\n                {\r\n                    df=origEnum.docFreq(); //store the df so all variants use same idf\r\n                }\r\n                int numVariants=0;\r\n                int totalVariantDocFreqs=0;\r\n                do\r\n                {\r\n                    Term possibleMatch=fe.term();\r\n                    if(possibleMatch!=null)\r\n                    {\r\n    \t                numVariants++;\r\n    \t                totalVariantDocFreqs+=fe.docFreq();\r\n    \t                float score=fe.difference();\r\n    \t                if(variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\r\n    \t                    ScoreTerm st=new ScoreTerm(possibleMatch,score,startTerm);                    \r\n    \t                    variantsQ.insert(st);\r\n    \t                    minScore = ((ScoreTerm)variantsQ.top()).score; // maintain minScore\r\n    \t                }\r\n                    }\r\n                }\r\n                while(fe.next());\r\n                if(numVariants>0)\r\n                {\r\n\t                int avgDf=totalVariantDocFreqs/numVariants;\r\n\t                if(df==0)//no direct match we can use as df for all variants \r\n\t                {\r\n\t                    df=avgDf; //use avg df of all variants\r\n\t                }\r\n\t                \r\n\t                // take the top variants (scored by edit distance) and reset the score\r\n\t                // to include an IDF factor then add to the global queue for ranking \r\n\t                // overall top query terms\r\n\t                int size = variantsQ.size();\r\n\t                for(int i = 0; i < size; i++)\r\n\t                {\r\n\t                  ScoreTerm st = (ScoreTerm) variantsQ.pop();\r\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\r\n\t                  q.insert(st);\r\n\t                }                            \r\n                }\r\n        \t}\r\n        }     \r\n    }\r\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9b5756469957918cac40a831acec9cf01c8c2bb3","date":1249167152,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        TermAttribute termAtt = (TermAttribute) ts.addAttribute(TermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet processedTerms=new HashSet();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.term();\n        \tif(!processedTerms.contains(term))\n        \t{\n        \t\tprocessedTerms.add(term);\n                ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                float minScore=0;\n                Term startTerm=internSavingTemplateTerm.createTerm(term);\n                FuzzyTermEnum fe=new FuzzyTermEnum(reader,startTerm,f.minSimilarity,f.prefixLength);\n                TermEnum origEnum = reader.terms(startTerm);\n                int df=0;\n                if(startTerm.equals(origEnum.term()))\n                {\n                    df=origEnum.docFreq(); //store the df so all variants use same idf\n                }\n                int numVariants=0;\n                int totalVariantDocFreqs=0;\n                do\n                {\n                    Term possibleMatch=fe.term();\n                    if(possibleMatch!=null)\n                    {\n    \t                numVariants++;\n    \t                totalVariantDocFreqs+=fe.docFreq();\n    \t                float score=fe.difference();\n    \t                if(variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n    \t                    ScoreTerm st=new ScoreTerm(possibleMatch,score,startTerm);                    \n    \t                    variantsQ.insert(st);\n    \t                    minScore = ((ScoreTerm)variantsQ.top()).score; // maintain minScore\n    \t                }\n                    }\n                }\n                while(fe.next());\n                if(numVariants>0)\n                {\n\t                int avgDf=totalVariantDocFreqs/numVariants;\n\t                if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n\t                // take the top variants (scored by edit distance) and reset the score\n\t                // to include an IDF factor then add to the global queue for ranking \n\t                // overall top query terms\n\t                int size = variantsQ.size();\n\t                for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = (ScoreTerm) variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insert(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        final Token reusableToken = new Token();\n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet processedTerms=new HashSet();\n        for (Token nextToken = ts.next(reusableToken); nextToken!=null; nextToken = ts.next(reusableToken))\n        {\n                String term = nextToken.term();\n        \tif(!processedTerms.contains(term))\n        \t{\n        \t\tprocessedTerms.add(term);\n                ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                float minScore=0;\n                Term startTerm=internSavingTemplateTerm.createTerm(term);\n                FuzzyTermEnum fe=new FuzzyTermEnum(reader,startTerm,f.minSimilarity,f.prefixLength);\n                TermEnum origEnum = reader.terms(startTerm);\n                int df=0;\n                if(startTerm.equals(origEnum.term()))\n                {\n                    df=origEnum.docFreq(); //store the df so all variants use same idf\n                }\n                int numVariants=0;\n                int totalVariantDocFreqs=0;\n                do\n                {\n                    Term possibleMatch=fe.term();\n                    if(possibleMatch!=null)\n                    {\n    \t                numVariants++;\n    \t                totalVariantDocFreqs+=fe.docFreq();\n    \t                float score=fe.difference();\n    \t                if(variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n    \t                    ScoreTerm st=new ScoreTerm(possibleMatch,score,startTerm);                    \n    \t                    variantsQ.insert(st);\n    \t                    minScore = ((ScoreTerm)variantsQ.top()).score; // maintain minScore\n    \t                }\n                    }\n                }\n                while(fe.next());\n                if(numVariants>0)\n                {\n\t                int avgDf=totalVariantDocFreqs/numVariants;\n\t                if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n\t                // take the top variants (scored by edit distance) and reset the score\n\t                // to include an IDF factor then add to the global queue for ranking \n\t                // overall top query terms\n\t                int size = variantsQ.size();\n\t                for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = (ScoreTerm) variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insert(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8d78f014fded44fbde905f4f84cdc21907b371e8","date":1254383623,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        TermAttribute termAtt = ts.addAttribute(TermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet processedTerms=new HashSet();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.term();\n        \tif(!processedTerms.contains(term))\n        \t{\n        \t\tprocessedTerms.add(term);\n                ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                float minScore=0;\n                Term startTerm=internSavingTemplateTerm.createTerm(term);\n                FuzzyTermEnum fe=new FuzzyTermEnum(reader,startTerm,f.minSimilarity,f.prefixLength);\n                TermEnum origEnum = reader.terms(startTerm);\n                int df=0;\n                if(startTerm.equals(origEnum.term()))\n                {\n                    df=origEnum.docFreq(); //store the df so all variants use same idf\n                }\n                int numVariants=0;\n                int totalVariantDocFreqs=0;\n                do\n                {\n                    Term possibleMatch=fe.term();\n                    if(possibleMatch!=null)\n                    {\n    \t                numVariants++;\n    \t                totalVariantDocFreqs+=fe.docFreq();\n    \t                float score=fe.difference();\n    \t                if(variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n    \t                    ScoreTerm st=new ScoreTerm(possibleMatch,score,startTerm);                    \n    \t                    variantsQ.insert(st);\n    \t                    minScore = ((ScoreTerm)variantsQ.top()).score; // maintain minScore\n    \t                }\n                    }\n                }\n                while(fe.next());\n                if(numVariants>0)\n                {\n\t                int avgDf=totalVariantDocFreqs/numVariants;\n\t                if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n\t                // take the top variants (scored by edit distance) and reset the score\n\t                // to include an IDF factor then add to the global queue for ranking \n\t                // overall top query terms\n\t                int size = variantsQ.size();\n\t                for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = (ScoreTerm) variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insert(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        TermAttribute termAtt = (TermAttribute) ts.addAttribute(TermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet processedTerms=new HashSet();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.term();\n        \tif(!processedTerms.contains(term))\n        \t{\n        \t\tprocessedTerms.add(term);\n                ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                float minScore=0;\n                Term startTerm=internSavingTemplateTerm.createTerm(term);\n                FuzzyTermEnum fe=new FuzzyTermEnum(reader,startTerm,f.minSimilarity,f.prefixLength);\n                TermEnum origEnum = reader.terms(startTerm);\n                int df=0;\n                if(startTerm.equals(origEnum.term()))\n                {\n                    df=origEnum.docFreq(); //store the df so all variants use same idf\n                }\n                int numVariants=0;\n                int totalVariantDocFreqs=0;\n                do\n                {\n                    Term possibleMatch=fe.term();\n                    if(possibleMatch!=null)\n                    {\n    \t                numVariants++;\n    \t                totalVariantDocFreqs+=fe.docFreq();\n    \t                float score=fe.difference();\n    \t                if(variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n    \t                    ScoreTerm st=new ScoreTerm(possibleMatch,score,startTerm);                    \n    \t                    variantsQ.insert(st);\n    \t                    minScore = ((ScoreTerm)variantsQ.top()).score; // maintain minScore\n    \t                }\n                    }\n                }\n                while(fe.next());\n                if(numVariants>0)\n                {\n\t                int avgDf=totalVariantDocFreqs/numVariants;\n\t                if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n\t                // take the top variants (scored by edit distance) and reset the score\n\t                // to include an IDF factor then add to the global queue for ranking \n\t                // overall top query terms\n\t                int size = variantsQ.size();\n\t                for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = (ScoreTerm) variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insert(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0731e137bcbb58121034de6ddaa67332fbe6e5d1","date":1255233265,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        TermAttribute termAtt = ts.addAttribute(TermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet processedTerms=new HashSet();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.term();\n        \tif(!processedTerms.contains(term))\n        \t{\n        \t\tprocessedTerms.add(term);\n                ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                float minScore=0;\n                Term startTerm=internSavingTemplateTerm.createTerm(term);\n                FuzzyTermEnum fe=new FuzzyTermEnum(reader,startTerm,f.minSimilarity,f.prefixLength);\n                TermEnum origEnum = reader.terms(startTerm);\n                int df=0;\n                if(startTerm.equals(origEnum.term()))\n                {\n                    df=origEnum.docFreq(); //store the df so all variants use same idf\n                }\n                int numVariants=0;\n                int totalVariantDocFreqs=0;\n                do\n                {\n                    Term possibleMatch=fe.term();\n                    if(possibleMatch!=null)\n                    {\n    \t                numVariants++;\n    \t                totalVariantDocFreqs+=fe.docFreq();\n    \t                float score=fe.difference();\n    \t                if(variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n    \t                    ScoreTerm st=new ScoreTerm(possibleMatch,score,startTerm);                    \n    \t                    variantsQ.insertWithOverflow(st);\n    \t                    minScore = ((ScoreTerm)variantsQ.top()).score; // maintain minScore\n    \t                }\n                    }\n                }\n                while(fe.next());\n                if(numVariants>0)\n                {\n\t                int avgDf=totalVariantDocFreqs/numVariants;\n\t                if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n\t                // take the top variants (scored by edit distance) and reset the score\n\t                // to include an IDF factor then add to the global queue for ranking \n\t                // overall top query terms\n\t                int size = variantsQ.size();\n\t                for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = (ScoreTerm) variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        TermAttribute termAtt = ts.addAttribute(TermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet processedTerms=new HashSet();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.term();\n        \tif(!processedTerms.contains(term))\n        \t{\n        \t\tprocessedTerms.add(term);\n                ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                float minScore=0;\n                Term startTerm=internSavingTemplateTerm.createTerm(term);\n                FuzzyTermEnum fe=new FuzzyTermEnum(reader,startTerm,f.minSimilarity,f.prefixLength);\n                TermEnum origEnum = reader.terms(startTerm);\n                int df=0;\n                if(startTerm.equals(origEnum.term()))\n                {\n                    df=origEnum.docFreq(); //store the df so all variants use same idf\n                }\n                int numVariants=0;\n                int totalVariantDocFreqs=0;\n                do\n                {\n                    Term possibleMatch=fe.term();\n                    if(possibleMatch!=null)\n                    {\n    \t                numVariants++;\n    \t                totalVariantDocFreqs+=fe.docFreq();\n    \t                float score=fe.difference();\n    \t                if(variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n    \t                    ScoreTerm st=new ScoreTerm(possibleMatch,score,startTerm);                    \n    \t                    variantsQ.insert(st);\n    \t                    minScore = ((ScoreTerm)variantsQ.top()).score; // maintain minScore\n    \t                }\n                    }\n                }\n                while(fe.next());\n                if(numVariants>0)\n                {\n\t                int avgDf=totalVariantDocFreqs/numVariants;\n\t                if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n\t                // take the top variants (scored by edit distance) and reset the score\n\t                // to include an IDF factor then add to the global queue for ranking \n\t                // overall top query terms\n\t                int size = variantsQ.size();\n\t                for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = (ScoreTerm) variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insert(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"60cdc0e643184821eb066795a8791cd82559f46e","date":1257941914,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        TermAttribute termAtt = ts.addAttribute(TermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.term();\n        \tif(!processedTerms.contains(term))\n        \t{\n        \t\tprocessedTerms.add(term);\n                ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                float minScore=0;\n                Term startTerm=internSavingTemplateTerm.createTerm(term);\n                FuzzyTermEnum fe=new FuzzyTermEnum(reader,startTerm,f.minSimilarity,f.prefixLength);\n                TermEnum origEnum = reader.terms(startTerm);\n                int df=0;\n                if(startTerm.equals(origEnum.term()))\n                {\n                    df=origEnum.docFreq(); //store the df so all variants use same idf\n                }\n                int numVariants=0;\n                int totalVariantDocFreqs=0;\n                do\n                {\n                    Term possibleMatch=fe.term();\n                    if(possibleMatch!=null)\n                    {\n    \t                numVariants++;\n    \t                totalVariantDocFreqs+=fe.docFreq();\n    \t                float score=fe.difference();\n    \t                if(variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n    \t                    ScoreTerm st=new ScoreTerm(possibleMatch,score,startTerm);                    \n    \t                    variantsQ.insertWithOverflow(st);\n    \t                    minScore = variantsQ.top().score; // maintain minScore\n    \t                }\n                    }\n                }\n                while(fe.next());\n                if(numVariants>0)\n                {\n\t                int avgDf=totalVariantDocFreqs/numVariants;\n\t                if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n\t                // take the top variants (scored by edit distance) and reset the score\n\t                // to include an IDF factor then add to the global queue for ranking \n\t                // overall top query terms\n\t                int size = variantsQ.size();\n\t                for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        TermAttribute termAtt = ts.addAttribute(TermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet processedTerms=new HashSet();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.term();\n        \tif(!processedTerms.contains(term))\n        \t{\n        \t\tprocessedTerms.add(term);\n                ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                float minScore=0;\n                Term startTerm=internSavingTemplateTerm.createTerm(term);\n                FuzzyTermEnum fe=new FuzzyTermEnum(reader,startTerm,f.minSimilarity,f.prefixLength);\n                TermEnum origEnum = reader.terms(startTerm);\n                int df=0;\n                if(startTerm.equals(origEnum.term()))\n                {\n                    df=origEnum.docFreq(); //store the df so all variants use same idf\n                }\n                int numVariants=0;\n                int totalVariantDocFreqs=0;\n                do\n                {\n                    Term possibleMatch=fe.term();\n                    if(possibleMatch!=null)\n                    {\n    \t                numVariants++;\n    \t                totalVariantDocFreqs+=fe.docFreq();\n    \t                float score=fe.difference();\n    \t                if(variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n    \t                    ScoreTerm st=new ScoreTerm(possibleMatch,score,startTerm);                    \n    \t                    variantsQ.insertWithOverflow(st);\n    \t                    minScore = ((ScoreTerm)variantsQ.top()).score; // maintain minScore\n    \t                }\n                    }\n                }\n                while(fe.next());\n                if(numVariants>0)\n                {\n\t                int avgDf=totalVariantDocFreqs/numVariants;\n\t                if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n\t                // take the top variants (scored by edit distance) and reset the score\n\t                // to include an IDF factor then add to the global queue for ranking \n\t                // overall top query terms\n\t                int size = variantsQ.size();\n\t                for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = (ScoreTerm) variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        TermAttribute termAtt = ts.addAttribute(TermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.term();\n        \tif(!processedTerms.contains(term))\n        \t{\n        \t\tprocessedTerms.add(term);\n                ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                float minScore=0;\n                Term startTerm=internSavingTemplateTerm.createTerm(term);\n                FuzzyTermEnum fe=new FuzzyTermEnum(reader,startTerm,f.minSimilarity,f.prefixLength);\n                TermEnum origEnum = reader.terms(startTerm);\n                int df=0;\n                if(startTerm.equals(origEnum.term()))\n                {\n                    df=origEnum.docFreq(); //store the df so all variants use same idf\n                }\n                int numVariants=0;\n                int totalVariantDocFreqs=0;\n                do\n                {\n                    Term possibleMatch=fe.term();\n                    if(possibleMatch!=null)\n                    {\n    \t                numVariants++;\n    \t                totalVariantDocFreqs+=fe.docFreq();\n    \t                float score=fe.difference();\n    \t                if(variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n    \t                    ScoreTerm st=new ScoreTerm(possibleMatch,score,startTerm);                    \n    \t                    variantsQ.insertWithOverflow(st);\n    \t                    minScore = variantsQ.top().score; // maintain minScore\n    \t                }\n                    }\n                }\n                while(fe.next());\n                if(numVariants>0)\n                {\n\t                int avgDf=totalVariantDocFreqs/numVariants;\n\t                if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n\t                // take the top variants (scored by edit distance) and reset the score\n\t                // to include an IDF factor then add to the global queue for ranking \n\t                // overall top query terms\n\t                int size = variantsQ.size();\n\t                for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        TermAttribute termAtt = ts.addAttribute(TermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.term();\n        \tif(!processedTerms.contains(term))\n        \t{\n        \t\tprocessedTerms.add(term);\n                ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                float minScore=0;\n                Term startTerm=internSavingTemplateTerm.createTerm(term);\n                FuzzyTermEnum fe=new FuzzyTermEnum(reader,startTerm,f.minSimilarity,f.prefixLength);\n                TermEnum origEnum = reader.terms(startTerm);\n                int df=0;\n                if(startTerm.equals(origEnum.term()))\n                {\n                    df=origEnum.docFreq(); //store the df so all variants use same idf\n                }\n                int numVariants=0;\n                int totalVariantDocFreqs=0;\n                do\n                {\n                    Term possibleMatch=fe.term();\n                    if(possibleMatch!=null)\n                    {\n    \t                numVariants++;\n    \t                totalVariantDocFreqs+=fe.docFreq();\n    \t                float score=fe.difference();\n    \t                if(variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n    \t                    ScoreTerm st=new ScoreTerm(possibleMatch,score,startTerm);                    \n    \t                    variantsQ.insertWithOverflow(st);\n    \t                    minScore = variantsQ.top().score; // maintain minScore\n    \t                }\n                    }\n                }\n                while(fe.next());\n                if(numVariants>0)\n                {\n\t                int avgDf=totalVariantDocFreqs/numVariants;\n\t                if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n\t                // take the top variants (scored by edit distance) and reset the score\n\t                // to include an IDF factor then add to the global queue for ranking \n\t                // overall top query terms\n\t                int size = variantsQ.size();\n\t                for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"60cdc0e643184821eb066795a8791cd82559f46e":["0731e137bcbb58121034de6ddaa67332fbe6e5d1"],"7e2cb543b41c145f33390f460ee743d6693c9c6c":["9d73678020862536617f065bb3d28a71d8c4020c"],"e26cca6058accb5c7c98a2f936b511ad41db848d":["d3c3c2404d1200c39220fa15054fae854db4e1ee"],"f3055a6d1420d229175009d777f2b781b4e3e97f":["7e2cb543b41c145f33390f460ee743d6693c9c6c"],"6a361a621b184d9b73c9c9a37323a9845b8f8260":["f3055a6d1420d229175009d777f2b781b4e3e97f"],"9d73678020862536617f065bb3d28a71d8c4020c":["e26cca6058accb5c7c98a2f936b511ad41db848d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"0731e137bcbb58121034de6ddaa67332fbe6e5d1":["8d78f014fded44fbde905f4f84cdc21907b371e8"],"9b5756469957918cac40a831acec9cf01c8c2bb3":["6a361a621b184d9b73c9c9a37323a9845b8f8260"],"d3c3c2404d1200c39220fa15054fae854db4e1ee":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"8d78f014fded44fbde905f4f84cdc21907b371e8":["9b5756469957918cac40a831acec9cf01c8c2bb3"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["60cdc0e643184821eb066795a8791cd82559f46e"]},"commit2Childs":{"60cdc0e643184821eb066795a8791cd82559f46e":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"7e2cb543b41c145f33390f460ee743d6693c9c6c":["f3055a6d1420d229175009d777f2b781b4e3e97f"],"e26cca6058accb5c7c98a2f936b511ad41db848d":["9d73678020862536617f065bb3d28a71d8c4020c"],"f3055a6d1420d229175009d777f2b781b4e3e97f":["6a361a621b184d9b73c9c9a37323a9845b8f8260"],"6a361a621b184d9b73c9c9a37323a9845b8f8260":["9b5756469957918cac40a831acec9cf01c8c2bb3"],"9d73678020862536617f065bb3d28a71d8c4020c":["7e2cb543b41c145f33390f460ee743d6693c9c6c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d3c3c2404d1200c39220fa15054fae854db4e1ee"],"0731e137bcbb58121034de6ddaa67332fbe6e5d1":["60cdc0e643184821eb066795a8791cd82559f46e"],"9b5756469957918cac40a831acec9cf01c8c2bb3":["8d78f014fded44fbde905f4f84cdc21907b371e8"],"d3c3c2404d1200c39220fa15054fae854db4e1ee":["e26cca6058accb5c7c98a2f936b511ad41db848d"],"8d78f014fded44fbde905f4f84cdc21907b371e8":["0731e137bcbb58121034de6ddaa67332fbe6e5d1"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}