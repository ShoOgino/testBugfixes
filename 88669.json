{"path":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","commits":[{"id":"c903c3d15906a3da96b8c0c2fb704491005fdbdb","date":1453508227,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","pathOld":"solr/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","sourceNew":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // TODO: this is not always necessary - eventually we would like to avoid this wrap\n        //       when it is not needed.\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","sourceOld":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // TODO: this is not always necessary - eventually we would like to avoid this wrap\n        //       when it is not needed.\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a258fbb26824fd104ed795e5d9033d2d040049ee","date":1453508252,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","pathOld":"solr/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","sourceNew":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","sourceOld":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f0b9507caf22f292ac0e5e59f62db4275adf4511","date":1310107283,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","sourceNew":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","sourceOld":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // TODO: this is not always necessary - eventually we would like to avoid this wrap\n        //       when it is not needed.\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c26f00b574427b55127e869b935845554afde1fa","date":1310252513,"type":1,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","pathOld":"solr/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","sourceNew":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","sourceOld":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1509f151d7692d84fae414b2b799ac06ba60fcb4","date":1314451621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","sourceNew":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    IndexableField[] docFields = doc.getFields(fieldName);\n    List<String> listFields = new ArrayList<String>();\n    for (IndexableField field : docFields) {\n      listFields.add(field.stringValue());\n    }\n\n    String[] docTexts = (String[]) listFields.toArray(new String[listFields.size()]);\n   \n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","sourceOld":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a7e4907084808af8fdb14b9809e6dceaccf6867b","date":1343473006,"type":5,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","sourceNew":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    StorableField[] docFields = doc.getFields(fieldName);\n    List<String> listFields = new ArrayList<String>();\n    for (StorableField field : docFields) {\n      listFields.add(field.stringValue());\n    }\n\n    String[] docTexts = (String[]) listFields.toArray(new String[listFields.size()]);\n   \n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","sourceOld":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    IndexableField[] docFields = doc.getFields(fieldName);\n    List<String> listFields = new ArrayList<String>();\n    for (IndexableField field : docFields) {\n      listFields.add(field.stringValue());\n    }\n\n    String[] docTexts = (String[]) listFields.toArray(new String[listFields.size()]);\n   \n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1d028314cced5858683a1bb4741423d0f934257b","date":1346596535,"type":5,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,StoredDocument,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","sourceNew":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    StorableField[] docFields = doc.getFields(fieldName);\n    List<String> listFields = new ArrayList<String>();\n    for (StorableField field : docFields) {\n      listFields.add(field.stringValue());\n    }\n\n    String[] docTexts = (String[]) listFields.toArray(new String[listFields.size()]);\n   \n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","sourceOld":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    IndexableField[] docFields = doc.getFields(fieldName);\n    List<String> listFields = new ArrayList<String>();\n    for (IndexableField field : docFields) {\n      listFields.add(field.stringValue());\n    }\n\n    String[] docTexts = (String[]) listFields.toArray(new String[listFields.size()]);\n   \n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"1d028314cced5858683a1bb4741423d0f934257b":["1509f151d7692d84fae414b2b799ac06ba60fcb4","a7e4907084808af8fdb14b9809e6dceaccf6867b"],"f0b9507caf22f292ac0e5e59f62db4275adf4511":["c903c3d15906a3da96b8c0c2fb704491005fdbdb","a258fbb26824fd104ed795e5d9033d2d040049ee"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"c26f00b574427b55127e869b935845554afde1fa":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f0b9507caf22f292ac0e5e59f62db4275adf4511"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a258fbb26824fd104ed795e5d9033d2d040049ee":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["c26f00b574427b55127e869b935845554afde1fa"],"a7e4907084808af8fdb14b9809e6dceaccf6867b":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["1d028314cced5858683a1bb4741423d0f934257b"]},"commit2Childs":{"1d028314cced5858683a1bb4741423d0f934257b":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"f0b9507caf22f292ac0e5e59f62db4275adf4511":["c26f00b574427b55127e869b935845554afde1fa"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["f0b9507caf22f292ac0e5e59f62db4275adf4511"],"c26f00b574427b55127e869b935845554afde1fa":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c903c3d15906a3da96b8c0c2fb704491005fdbdb","c26f00b574427b55127e869b935845554afde1fa","a258fbb26824fd104ed795e5d9033d2d040049ee"],"a258fbb26824fd104ed795e5d9033d2d040049ee":["f0b9507caf22f292ac0e5e59f62db4275adf4511"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["1d028314cced5858683a1bb4741423d0f934257b","a7e4907084808af8fdb14b9809e6dceaccf6867b"],"a7e4907084808af8fdb14b9809e6dceaccf6867b":["1d028314cced5858683a1bb4741423d0f934257b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}