{"path":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,Term[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","commits":[{"id":"4b3d16cba9355e2e97962eb1c441bbd0b6735c15","date":1357426290,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,Term[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","pathOld":"lucene/sandbox/src/java/org/apache/lucene/sandbox/postingshighlight/PostingsHighlighter#highlightDoc(String,Term[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","sourceNew":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, Term terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<OffsetsEnum>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      DocsAndPositionsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i].bytes(), true)) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<Passage>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (right.score == left.score) {\n          return right.startOffset - left.endOffset;\n        } else {\n          return right.score > left.score ? 1 : -1;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final DocsAndPositionsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      if (start > current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        current.addMatch(start, end, terms[off.id]);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n    return new Passage[0];\n  }\n\n","sourceOld":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, Term terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<OffsetsEnum>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      DocsAndPositionsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i].bytes(), true)) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<Passage>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (right.score == left.score) {\n          return right.startOffset - left.endOffset;\n        } else {\n          return right.score > left.score ? 1 : -1;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final DocsAndPositionsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      if (start > current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        current.addMatch(start, end, terms[off.id]);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n    return new Passage[0];\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d3fcb70cf561547c7bb1506e0cf32ca7b1287064","date":1357616416,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,Term[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","pathOld":"/dev/null","sourceNew":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, Term terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<OffsetsEnum>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      DocsAndPositionsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i].bytes(), true)) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<Passage>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (right.score == left.score) {\n          return right.startOffset - left.endOffset;\n        } else {\n          return right.score > left.score ? 1 : -1;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final DocsAndPositionsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      if (start > current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        current.addMatch(start, end, terms[off.id]);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n    return new Passage[0];\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e4c0b17b8a1d791b23f2f7ee90448d718112841e","date":1361891420,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,Term[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,Term[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","sourceNew":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, Term terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<OffsetsEnum>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      DocsAndPositionsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i].bytes(), true)) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<Passage>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (right.score == left.score) {\n          return right.startOffset - left.endOffset;\n        } else {\n          return right.score > left.score ? 1 : -1;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final DocsAndPositionsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      if (start > current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        current.addMatch(start, end, terms[off.id]);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n    return new Passage[0];\n  }\n\n","sourceOld":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, Term terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<OffsetsEnum>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      DocsAndPositionsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i].bytes(), true)) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<Passage>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (right.score == left.score) {\n          return right.startOffset - left.endOffset;\n        } else {\n          return right.score > left.score ? 1 : -1;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final DocsAndPositionsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      if (start > current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        current.addMatch(start, end, terms[off.id]);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n    return new Passage[0];\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"86b90b0ece36c02b9f8b3a26374109d04b76274d","date":1361915620,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,Term[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,Term[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","sourceNew":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, Term terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<OffsetsEnum>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      DocsAndPositionsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i].bytes(), true)) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<Passage>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (right.score == left.score) {\n          return right.startOffset - left.endOffset;\n        } else {\n          return right.score > left.score ? 1 : -1;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final DocsAndPositionsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        current.addMatch(start, end, terms[off.id]);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n    return new Passage[0];\n  }\n\n","sourceOld":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, Term terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<OffsetsEnum>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      DocsAndPositionsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i].bytes(), true)) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<Passage>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (right.score == left.score) {\n          return right.startOffset - left.endOffset;\n        } else {\n          return right.score > left.score ? 1 : -1;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final DocsAndPositionsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      if (start > current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        current.addMatch(start, end, terms[off.id]);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n    return new Passage[0];\n  }\n\n","bugFix":["ffabe030a2b84ad50adb7265da07ee78f1c58f6a"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7dee6430c86436180446149d4f8425de750bf23","date":1363118976,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,Term[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,Term[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","sourceNew":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, Term terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<OffsetsEnum>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      DocsAndPositionsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i].bytes(), true)) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<Passage>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final DocsAndPositionsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        current.addMatch(start, end, terms[off.id]);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n    return new Passage[0];\n  }\n\n","sourceOld":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, Term terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<OffsetsEnum>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      DocsAndPositionsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i].bytes(), true)) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<Passage>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (right.score == left.score) {\n          return right.startOffset - left.endOffset;\n        } else {\n          return right.score > left.score ? 1 : -1;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final DocsAndPositionsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        current.addMatch(start, end, terms[off.id]);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n    return new Passage[0];\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"77fc0eb4b8857a9f5235049cdfe6f678a3ddae55","date":1363791725,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,Term[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,Term[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","sourceNew":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, Term terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<OffsetsEnum>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      DocsAndPositionsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i].bytes(), true)) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<Passage>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final DocsAndPositionsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        current.addMatch(start, end, terms[off.id]);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n\n    // Dead code but compiler disagrees:\n    assert false;\n    return null;\n  }\n\n","sourceOld":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, Term terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<OffsetsEnum>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      DocsAndPositionsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i].bytes(), true)) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<Passage>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final DocsAndPositionsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        current.addMatch(start, end, terms[off.id]);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n    return new Passage[0];\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a385683d8ce32386bb71e8c427cb78573debda2b","date":1363792009,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,Term[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,Term[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","sourceNew":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, Term terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<OffsetsEnum>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      DocsAndPositionsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i].bytes(), true)) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<Passage>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final DocsAndPositionsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        current.addMatch(start, end, terms[off.id]);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n    return new Passage[0];\n  }\n\n","sourceOld":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, Term terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<OffsetsEnum>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      DocsAndPositionsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i].bytes(), true)) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<Passage>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final DocsAndPositionsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        current.addMatch(start, end, terms[off.id]);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n\n    // Dead code but compiler disagrees:\n    assert false;\n    return null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4ca2025fc6d81ec43c276473ba49e4fbcb15ccb1","date":1363793774,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,Term[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,Term[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","sourceNew":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, Term terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<OffsetsEnum>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      DocsAndPositionsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i].bytes(), true)) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<Passage>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final DocsAndPositionsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        current.addMatch(start, end, terms[off.id]);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n\n    // Dead code but compiler disagrees:\n    assert false;\n    return null;\n  }\n\n","sourceOld":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, Term terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<OffsetsEnum>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      DocsAndPositionsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i].bytes(), true)) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<Passage>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final DocsAndPositionsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        current.addMatch(start, end, terms[off.id]);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n    return new Passage[0];\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4831dd345148fcd7c33877b449ade21fc45459d8","date":1363963811,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,BytesRef[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,Term[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","sourceNew":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, BytesRef terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {\n    PassageScorer scorer = getScorer(field);\n    if (scorer == null) {\n      throw new NullPointerException(\"PassageScorer cannot be null\");\n    }\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<OffsetsEnum>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      DocsAndPositionsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i], true)) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<Passage>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final DocsAndPositionsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        current.addMatch(start, end, terms[off.id]);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n\n    // Dead code but compiler disagrees:\n    assert false;\n    return null;\n  }\n\n","sourceOld":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, Term terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<OffsetsEnum>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      DocsAndPositionsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i].bytes(), true)) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<Passage>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final DocsAndPositionsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        current.addMatch(start, end, terms[off.id]);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n\n    // Dead code but compiler disagrees:\n    assert false;\n    return null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","4b3d16cba9355e2e97962eb1c441bbd0b6735c15"],"4b3d16cba9355e2e97962eb1c441bbd0b6735c15":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a385683d8ce32386bb71e8c427cb78573debda2b":["77fc0eb4b8857a9f5235049cdfe6f678a3ddae55"],"e7dee6430c86436180446149d4f8425de750bf23":["86b90b0ece36c02b9f8b3a26374109d04b76274d"],"4831dd345148fcd7c33877b449ade21fc45459d8":["4ca2025fc6d81ec43c276473ba49e4fbcb15ccb1"],"e4c0b17b8a1d791b23f2f7ee90448d718112841e":["4b3d16cba9355e2e97962eb1c441bbd0b6735c15"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"86b90b0ece36c02b9f8b3a26374109d04b76274d":["e4c0b17b8a1d791b23f2f7ee90448d718112841e"],"77fc0eb4b8857a9f5235049cdfe6f678a3ddae55":["e7dee6430c86436180446149d4f8425de750bf23"],"4ca2025fc6d81ec43c276473ba49e4fbcb15ccb1":["a385683d8ce32386bb71e8c427cb78573debda2b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["4831dd345148fcd7c33877b449ade21fc45459d8"]},"commit2Childs":{"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":[],"4b3d16cba9355e2e97962eb1c441bbd0b6735c15":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","e4c0b17b8a1d791b23f2f7ee90448d718112841e"],"a385683d8ce32386bb71e8c427cb78573debda2b":["4ca2025fc6d81ec43c276473ba49e4fbcb15ccb1"],"e7dee6430c86436180446149d4f8425de750bf23":["77fc0eb4b8857a9f5235049cdfe6f678a3ddae55"],"4831dd345148fcd7c33877b449ade21fc45459d8":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","4b3d16cba9355e2e97962eb1c441bbd0b6735c15"],"e4c0b17b8a1d791b23f2f7ee90448d718112841e":["86b90b0ece36c02b9f8b3a26374109d04b76274d"],"86b90b0ece36c02b9f8b3a26374109d04b76274d":["e7dee6430c86436180446149d4f8425de750bf23"],"77fc0eb4b8857a9f5235049cdfe6f678a3ddae55":["a385683d8ce32386bb71e8c427cb78573debda2b"],"4ca2025fc6d81ec43c276473ba49e4fbcb15ccb1":["4831dd345148fcd7c33877b449ade21fc45459d8"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}