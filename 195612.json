{"path":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionOnCommitTest#multiShardTest().mjava","commits":[{"id":"84f20f331d8001864545c7021812d8c6509c7593","date":1517216128,"type":1,"author":"Cao Manh Dat","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionOnCommitTest#multiShardTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/LeaderInitiatedRecoveryOnCommitTest#multiShardTest().mjava","sourceNew":"  private void multiShardTest() throws Exception {\n\n    log.info(\"Running multiShardTest\");\n\n    // create a collection that has 2 shard and 2 replicas\n    String testCollectionName = \"c8n_2x2_commits\";\n    createCollection(testCollectionName, \"conf1\", 2, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n\n    List<Replica> notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 2, 2, 30);\n    assertTrue(\"Expected 1 replicas for collection \" + testCollectionName\n            + \" but found \" + notLeaders.size() + \"; clusterState: \"\n            + printClusterStateInfo(),\n        notLeaders.size() == 1);\n\n    log.info(\"All replicas active for \"+testCollectionName);\n\n    // let's put the leader in its own partition, no replicas can contact it now\n    Replica leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    log.info(\"Creating partition to leader at \"+leader.getCoreUrl());\n    SocketProxy leaderProxy = getProxyForReplica(leader);\n    leaderProxy.close();\n\n    // let's find the leader of shard2 and ask him to commit\n    Replica shard2Leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard2\");\n    sendCommitWithRetry(shard2Leader);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    cloudClient.getZkStateReader().forceUpdateCollection(testCollectionName); // get the latest state\n    leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    assertSame(\"Leader was not active\", Replica.State.ACTIVE, leader.getState());\n\n    log.info(\"Healing partitioned replica at \"+leader.getCoreUrl());\n    leaderProxy.reopen();\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n\n    log.info(\"multiShardTest completed OK\");\n  }\n\n","sourceOld":"  private void multiShardTest() throws Exception {\n\n    log.info(\"Running multiShardTest\");\n\n    // create a collection that has 2 shard and 2 replicas\n    String testCollectionName = \"c8n_2x2_commits\";\n    createCollection(testCollectionName, \"conf1\", 2, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n\n    List<Replica> notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 2, 2, 30);\n    assertTrue(\"Expected 1 replicas for collection \" + testCollectionName\n            + \" but found \" + notLeaders.size() + \"; clusterState: \"\n            + printClusterStateInfo(),\n        notLeaders.size() == 1);\n\n    log.info(\"All replicas active for \"+testCollectionName);\n\n    // let's put the leader in its own partition, no replicas can contact it now\n    Replica leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    log.info(\"Creating partition to leader at \"+leader.getCoreUrl());\n    SocketProxy leaderProxy = getProxyForReplica(leader);\n    leaderProxy.close();\n\n    // let's find the leader of shard2 and ask him to commit\n    Replica shard2Leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard2\");\n    sendCommitWithRetry(shard2Leader);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    cloudClient.getZkStateReader().forceUpdateCollection(testCollectionName); // get the latest state\n    leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    assertSame(\"Leader was not active\", Replica.State.ACTIVE, leader.getState());\n\n    log.info(\"Healing partitioned replica at \"+leader.getCoreUrl());\n    leaderProxy.reopen();\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n\n    log.info(\"multiShardTest completed OK\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a966532d92cf9ba2856f15a8140151bb6b518e4b","date":1588290631,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionOnCommitTest#multiShardTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionOnCommitTest#multiShardTest().mjava","sourceNew":"  private void multiShardTest() throws Exception {\n\n    log.info(\"Running multiShardTest\");\n\n    // create a collection that has 2 shard and 2 replicas\n    String testCollectionName = \"c8n_2x2_commits\";\n    createCollection(testCollectionName, \"conf1\", 2, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n\n    List<Replica> notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 2, 2, 30);\n    assertTrue(\"Expected 1 replicas for collection \" + testCollectionName\n            + \" but found \" + notLeaders.size() + \"; clusterState: \"\n            + printClusterStateInfo(),\n        notLeaders.size() == 1);\n\n    if (log.isInfoEnabled()) {\n      log.info(\"All replicas active for {}\", testCollectionName);\n    }\n\n    // let's put the leader in its own partition, no replicas can contact it now\n    Replica leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    if (log.isInfoEnabled()) {\n      log.info(\"Creating partition to leader at {}\", leader.getCoreUrl());\n    }\n    SocketProxy leaderProxy = getProxyForReplica(leader);\n    leaderProxy.close();\n\n    // let's find the leader of shard2 and ask him to commit\n    Replica shard2Leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard2\");\n    sendCommitWithRetry(shard2Leader);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    cloudClient.getZkStateReader().forceUpdateCollection(testCollectionName); // get the latest state\n    leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    assertSame(\"Leader was not active\", Replica.State.ACTIVE, leader.getState());\n\n    if (log.isInfoEnabled()) {\n      log.info(\"Healing partitioned replica at {}\", leader.getCoreUrl());\n    }\n    leaderProxy.reopen();\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n\n    log.info(\"multiShardTest completed OK\");\n  }\n\n","sourceOld":"  private void multiShardTest() throws Exception {\n\n    log.info(\"Running multiShardTest\");\n\n    // create a collection that has 2 shard and 2 replicas\n    String testCollectionName = \"c8n_2x2_commits\";\n    createCollection(testCollectionName, \"conf1\", 2, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n\n    List<Replica> notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 2, 2, 30);\n    assertTrue(\"Expected 1 replicas for collection \" + testCollectionName\n            + \" but found \" + notLeaders.size() + \"; clusterState: \"\n            + printClusterStateInfo(),\n        notLeaders.size() == 1);\n\n    log.info(\"All replicas active for \"+testCollectionName);\n\n    // let's put the leader in its own partition, no replicas can contact it now\n    Replica leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    log.info(\"Creating partition to leader at \"+leader.getCoreUrl());\n    SocketProxy leaderProxy = getProxyForReplica(leader);\n    leaderProxy.close();\n\n    // let's find the leader of shard2 and ask him to commit\n    Replica shard2Leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard2\");\n    sendCommitWithRetry(shard2Leader);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    cloudClient.getZkStateReader().forceUpdateCollection(testCollectionName); // get the latest state\n    leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    assertSame(\"Leader was not active\", Replica.State.ACTIVE, leader.getState());\n\n    log.info(\"Healing partitioned replica at \"+leader.getCoreUrl());\n    leaderProxy.reopen();\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n\n    log.info(\"multiShardTest completed OK\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e","date":1594223844,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionOnCommitTest#multiShardTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionOnCommitTest#multiShardTest().mjava","sourceNew":"  private void multiShardTest() throws Exception {\n\n    log.info(\"Running multiShardTest\");\n\n    // create a collection that has 2 shard and 2 replicas\n    String testCollectionName = \"c8n_2x2_commits\";\n    createCollection(testCollectionName, \"conf1\", 2, 2);\n    cloudClient.setDefaultCollection(testCollectionName);\n\n    List<Replica> notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 2, 2, 30);\n    assertTrue(\"Expected 1 replicas for collection \" + testCollectionName\n            + \" but found \" + notLeaders.size() + \"; clusterState: \"\n            + printClusterStateInfo(),\n        notLeaders.size() == 1);\n\n    if (log.isInfoEnabled()) {\n      log.info(\"All replicas active for {}\", testCollectionName);\n    }\n\n    // let's put the leader in its own partition, no replicas can contact it now\n    Replica leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    if (log.isInfoEnabled()) {\n      log.info(\"Creating partition to leader at {}\", leader.getCoreUrl());\n    }\n    SocketProxy leaderProxy = getProxyForReplica(leader);\n    leaderProxy.close();\n\n    // let's find the leader of shard2 and ask him to commit\n    Replica shard2Leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard2\");\n    sendCommitWithRetry(shard2Leader);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    cloudClient.getZkStateReader().forceUpdateCollection(testCollectionName); // get the latest state\n    leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    assertSame(\"Leader was not active\", Replica.State.ACTIVE, leader.getState());\n\n    if (log.isInfoEnabled()) {\n      log.info(\"Healing partitioned replica at {}\", leader.getCoreUrl());\n    }\n    leaderProxy.reopen();\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n\n    log.info(\"multiShardTest completed OK\");\n  }\n\n","sourceOld":"  private void multiShardTest() throws Exception {\n\n    log.info(\"Running multiShardTest\");\n\n    // create a collection that has 2 shard and 2 replicas\n    String testCollectionName = \"c8n_2x2_commits\";\n    createCollection(testCollectionName, \"conf1\", 2, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n\n    List<Replica> notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 2, 2, 30);\n    assertTrue(\"Expected 1 replicas for collection \" + testCollectionName\n            + \" but found \" + notLeaders.size() + \"; clusterState: \"\n            + printClusterStateInfo(),\n        notLeaders.size() == 1);\n\n    if (log.isInfoEnabled()) {\n      log.info(\"All replicas active for {}\", testCollectionName);\n    }\n\n    // let's put the leader in its own partition, no replicas can contact it now\n    Replica leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    if (log.isInfoEnabled()) {\n      log.info(\"Creating partition to leader at {}\", leader.getCoreUrl());\n    }\n    SocketProxy leaderProxy = getProxyForReplica(leader);\n    leaderProxy.close();\n\n    // let's find the leader of shard2 and ask him to commit\n    Replica shard2Leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard2\");\n    sendCommitWithRetry(shard2Leader);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    cloudClient.getZkStateReader().forceUpdateCollection(testCollectionName); // get the latest state\n    leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    assertSame(\"Leader was not active\", Replica.State.ACTIVE, leader.getState());\n\n    if (log.isInfoEnabled()) {\n      log.info(\"Healing partitioned replica at {}\", leader.getCoreUrl());\n    }\n    leaderProxy.reopen();\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n\n    log.info(\"multiShardTest completed OK\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"84f20f331d8001864545c7021812d8c6509c7593":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a966532d92cf9ba2856f15a8140151bb6b518e4b":["84f20f331d8001864545c7021812d8c6509c7593"],"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e":["a966532d92cf9ba2856f15a8140151bb6b518e4b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["e46a76bb135597b8bf35930cfdb3702bdd1cbe6e"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["84f20f331d8001864545c7021812d8c6509c7593"],"84f20f331d8001864545c7021812d8c6509c7593":["a966532d92cf9ba2856f15a8140151bb6b518e4b"],"a966532d92cf9ba2856f15a8140151bb6b518e4b":["e46a76bb135597b8bf35930cfdb3702bdd1cbe6e"],"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}