{"path":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","commits":[{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","pathOld":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","sourceNew":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        CharacterRunAutomaton stopWords = new CharacterRunAutomaton(BasicAutomata.makeString(\"stoppedtoken\"));\n        // we disable MockTokenizer checks because we will forcefully limit the \n        // tokenstream and call end() before incrementToken() returns false.\n        final MockAnalyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true, stopWords, true);\n        analyzer.setEnableChecks(false);\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(\"stoppedtoken\");\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(analyzer, \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(analyzer, \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","sourceOld":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        CharacterRunAutomaton stopWords = new CharacterRunAutomaton(BasicAutomata.makeString(\"stoppedtoken\"));\n        // we disable MockTokenizer checks because we will forcefully limit the \n        // tokenstream and call end() before incrementToken() returns false.\n        final MockAnalyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true, stopWords, true);\n        analyzer.setEnableChecks(false);\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(\"stoppedtoken\");\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(analyzer, \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(analyzer, \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"eafa8c5eabc3dacd34680054e6a33bda024080ac","date":1367691488,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","sourceNew":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        CharacterRunAutomaton stopWords = new CharacterRunAutomaton(BasicAutomata.makeString(\"stoppedtoken\"));\n        // we disable MockTokenizer checks because we will forcefully limit the \n        // tokenstream and call end() before incrementToken() returns false.\n        final MockAnalyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true, stopWords);\n        analyzer.setEnableChecks(false);\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(\"stoppedtoken\");\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(analyzer, \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(analyzer, \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","sourceOld":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        CharacterRunAutomaton stopWords = new CharacterRunAutomaton(BasicAutomata.makeString(\"stoppedtoken\"));\n        // we disable MockTokenizer checks because we will forcefully limit the \n        // tokenstream and call end() before incrementToken() returns false.\n        final MockAnalyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true, stopWords, true);\n        analyzer.setEnableChecks(false);\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(\"stoppedtoken\");\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(analyzer, \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(analyzer, \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7f4ef381bf0c2d618c6db830d3dd668c6901c05a","date":1402592253,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","sourceNew":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        CharacterRunAutomaton stopWords = new CharacterRunAutomaton(BasicAutomata.makeStringLight(\"stoppedtoken\"));\n        // we disable MockTokenizer checks because we will forcefully limit the \n        // tokenstream and call end() before incrementToken() returns false.\n        final MockAnalyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true, stopWords);\n        analyzer.setEnableChecks(false);\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(\"stoppedtoken\");\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(analyzer, \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(analyzer, \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","sourceOld":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        CharacterRunAutomaton stopWords = new CharacterRunAutomaton(BasicAutomata.makeString(\"stoppedtoken\"));\n        // we disable MockTokenizer checks because we will forcefully limit the \n        // tokenstream and call end() before incrementToken() returns false.\n        final MockAnalyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true, stopWords);\n        analyzer.setEnableChecks(false);\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(\"stoppedtoken\");\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(analyzer, \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(analyzer, \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4ca1c732df8923f5624f6c06b1dcca9e69d98c96","date":1402957391,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","sourceNew":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        CharacterRunAutomaton stopWords = new CharacterRunAutomaton(Automata.makeString(\"stoppedtoken\"));\n        // we disable MockTokenizer checks because we will forcefully limit the \n        // tokenstream and call end() before incrementToken() returns false.\n        final MockAnalyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true, stopWords);\n        analyzer.setEnableChecks(false);\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(\"stoppedtoken\");\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(analyzer, \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(analyzer, \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","sourceOld":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        CharacterRunAutomaton stopWords = new CharacterRunAutomaton(BasicAutomata.makeStringLight(\"stoppedtoken\"));\n        // we disable MockTokenizer checks because we will forcefully limit the \n        // tokenstream and call end() before incrementToken() returns false.\n        final MockAnalyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true, stopWords);\n        analyzer.setEnableChecks(false);\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(\"stoppedtoken\");\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(analyzer, \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(analyzer, \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5c84485629d80d203608e8975a1139de9933cc38","date":1403166128,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","sourceNew":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        CharacterRunAutomaton stopWords = new CharacterRunAutomaton(Automata.makeString(\"stoppedtoken\"));\n        // we disable MockTokenizer checks because we will forcefully limit the \n        // tokenstream and call end() before incrementToken() returns false.\n        final MockAnalyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true, stopWords);\n        analyzer.setEnableChecks(false);\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(\"stoppedtoken\");\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(analyzer, \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(analyzer, \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","sourceOld":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        CharacterRunAutomaton stopWords = new CharacterRunAutomaton(BasicAutomata.makeString(\"stoppedtoken\"));\n        // we disable MockTokenizer checks because we will forcefully limit the \n        // tokenstream and call end() before incrementToken() returns false.\n        final MockAnalyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true, stopWords);\n        analyzer.setEnableChecks(false);\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(\"stoppedtoken\");\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(analyzer, \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(analyzer, \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"4ca1c732df8923f5624f6c06b1dcca9e69d98c96":["7f4ef381bf0c2d618c6db830d3dd668c6901c05a"],"eafa8c5eabc3dacd34680054e6a33bda024080ac":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"7f4ef381bf0c2d618c6db830d3dd668c6901c05a":["eafa8c5eabc3dacd34680054e6a33bda024080ac"],"5c84485629d80d203608e8975a1139de9933cc38":["eafa8c5eabc3dacd34680054e6a33bda024080ac","4ca1c732df8923f5624f6c06b1dcca9e69d98c96"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["5c84485629d80d203608e8975a1139de9933cc38"]},"commit2Childs":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["eafa8c5eabc3dacd34680054e6a33bda024080ac"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"4ca1c732df8923f5624f6c06b1dcca9e69d98c96":["5c84485629d80d203608e8975a1139de9933cc38"],"eafa8c5eabc3dacd34680054e6a33bda024080ac":["7f4ef381bf0c2d618c6db830d3dd668c6901c05a","5c84485629d80d203608e8975a1139de9933cc38"],"7f4ef381bf0c2d618c6db830d3dd668c6901c05a":["4ca1c732df8923f5624f6c06b1dcca9e69d98c96"],"5c84485629d80d203608e8975a1139de9933cc38":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}