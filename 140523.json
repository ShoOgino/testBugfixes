{"path":"src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","commits":[{"id":"68df8db3f6c0c0ebbd1e40ba638115a748fb6a2a","date":1240390408,"type":0,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n    tokenList = indexPart.get(\"org.apache.solr.analysis.EnglishPorterFilter\");\n    assertNotNull(\"Expcting EnglishPorterFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.solr.analysis.EnglishPorterFilter\");\n    assertNotNull(\"Expcting EnglishPorterFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, null, false));\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ad94625fb8d088209f46650c8097196fec67f00c","date":1453508319,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","pathOld":"src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","sourceNew":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n    tokenList = indexPart.get(\"org.apache.solr.analysis.EnglishPorterFilter\");\n    assertNotNull(\"Expcting EnglishPorterFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.solr.analysis.EnglishPorterFilter\");\n    assertNotNull(\"Expcting EnglishPorterFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, null, false));\n\n  }\n\n","sourceOld":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n    tokenList = indexPart.get(\"org.apache.solr.analysis.EnglishPorterFilter\");\n    assertNotNull(\"Expcting EnglishPorterFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.solr.analysis.EnglishPorterFilter\");\n    assertNotNull(\"Expcting EnglishPorterFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, null, false));\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"68df8db3f6c0c0ebbd1e40ba638115a748fb6a2a":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":[],"ad94625fb8d088209f46650c8097196fec67f00c":["68df8db3f6c0c0ebbd1e40ba638115a748fb6a2a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"68df8db3f6c0c0ebbd1e40ba638115a748fb6a2a":["ad94625fb8d088209f46650c8097196fec67f00c"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":["68df8db3f6c0c0ebbd1e40ba638115a748fb6a2a"],"ad94625fb8d088209f46650c8097196fec67f00c":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["ad94625fb8d088209f46650c8097196fec67f00c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"pathCommit":null}