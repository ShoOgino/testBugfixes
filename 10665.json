{"path":"lucene/suggest/src/test/org/apache/lucene/search/suggest/DocumentValueSourceDictionaryTest#testLongValuesSourceWithDeletions().mjava","commits":[{"id":"7abbd4266bb36de7f2181eb6d9f3df3ea45ff382","date":1483789945,"type":2,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/suggest/src/test/org/apache/lucene/search/suggest/DocumentValueSourceDictionaryTest#testLongValuesSourceWithDeletions().mjava","pathOld":"lucene/suggest/src/test/org/apache/lucene/search/suggest/DocumentValueSourceDictionaryTest#testWithDeletions().mjava","sourceNew":"  @Test\n  public void testLongValuesSourceWithDeletions() throws IOException {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Map<String, Document> docs = generateIndexDocuments(atLeast(100));\n    Random rand = random();\n    List<String> termsToDel = new ArrayList<>();\n    for(Document doc : docs.values()) {\n      if(rand.nextBoolean() && termsToDel.size() < docs.size()-1) {\n        termsToDel.add(doc.get(FIELD_NAME));\n      }\n      writer.addDocument(doc);\n    }\n    writer.commit();\n\n    Term[] delTerms = new Term[termsToDel.size()];\n    for(int i=0; i < termsToDel.size() ; i++) {\n      delTerms[i] = new Term(FIELD_NAME, termsToDel.get(i));\n    }\n\n    for(Term delTerm: delTerms) {\n      writer.deleteDocuments(delTerm);\n    }\n    writer.commit();\n    writer.close();\n\n    for(String termToDel: termsToDel) {\n      assertTrue(null!=docs.remove(termToDel));\n    }\n\n    IndexReader ir = DirectoryReader.open(dir);\n    assertTrue(\"NumDocs should be > 0 but was \" + ir.numDocs(), ir.numDocs() > 0);\n    assertEquals(ir.numDocs(), docs.size());\n    LongValuesSource sumValues = sum(WEIGHT_FIELD_NAME_1, WEIGHT_FIELD_NAME_2);\n    Dictionary dictionary = new DocumentValueSourceDictionary(ir, FIELD_NAME, sumValues, PAYLOAD_FIELD_NAME);\n    InputIterator inputIterator = dictionary.getEntryIterator();\n    BytesRef f;\n    while((f = inputIterator.next())!=null) {\n      Document doc = docs.remove(f.utf8ToString());\n      long w1 = doc.getField(WEIGHT_FIELD_NAME_1).numericValue().longValue();\n      long w2 = doc.getField(WEIGHT_FIELD_NAME_2).numericValue().longValue();\n      assertTrue(f.equals(new BytesRef(doc.get(FIELD_NAME))));\n      assertEquals(inputIterator.weight(), w2+w1);\n      IndexableField payloadField = doc.getField(PAYLOAD_FIELD_NAME);\n      if (payloadField == null) assertTrue(inputIterator.payload().length == 0);\n      else assertEquals(inputIterator.payload(), payloadField.binaryValue());\n    }\n    assertTrue(docs.isEmpty());\n    IOUtils.close(ir, analyzer, dir);\n  }\n\n","sourceOld":"  @Test\n  public void testWithDeletions() throws IOException {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Map<String, Document> docs = generateIndexDocuments(atLeast(100));\n    Random rand = random();\n    List<String> termsToDel = new ArrayList<>();\n    for(Document doc : docs.values()) {\n      if(rand.nextBoolean() && termsToDel.size() < docs.size()-1) {\n        termsToDel.add(doc.get(FIELD_NAME));\n      }\n      writer.addDocument(doc);\n    }\n    writer.commit();\n    \n    Term[] delTerms = new Term[termsToDel.size()];\n    for(int i=0; i < termsToDel.size() ; i++) {\n      delTerms[i] = new Term(FIELD_NAME, termsToDel.get(i));\n    }\n    \n    for(Term delTerm: delTerms) {\n      writer.deleteDocuments(delTerm);  \n    }\n    writer.commit();\n    writer.close();\n    \n    for(String termToDel: termsToDel) {\n      assertTrue(null!=docs.remove(termToDel));\n    }\n    \n    IndexReader ir = DirectoryReader.open(dir);\n    assertTrue(\"NumDocs should be > 0 but was \" + ir.numDocs(), ir.numDocs() > 0);\n    assertEquals(ir.numDocs(), docs.size());\n    ValueSource[] toAdd = new ValueSource[] {new LongFieldSource(WEIGHT_FIELD_NAME_1), new LongFieldSource(WEIGHT_FIELD_NAME_2)};\n\n    Dictionary dictionary = new DocumentValueSourceDictionary(ir, FIELD_NAME,  new SumFloatFunction(toAdd), PAYLOAD_FIELD_NAME);\n    InputIterator inputIterator = dictionary.getEntryIterator();\n    BytesRef f;\n    while((f = inputIterator.next())!=null) {\n      Document doc = docs.remove(f.utf8ToString());\n      long w1 = doc.getField(WEIGHT_FIELD_NAME_1).numericValue().longValue();\n      long w2 = doc.getField(WEIGHT_FIELD_NAME_2).numericValue().longValue();\n      assertTrue(f.equals(new BytesRef(doc.get(FIELD_NAME))));\n      assertEquals(inputIterator.weight(), w2+w1);\n      IndexableField payloadField = doc.getField(PAYLOAD_FIELD_NAME);\n      if (payloadField == null) assertTrue(inputIterator.payload().length == 0);\n      else assertEquals(inputIterator.payload(), payloadField.binaryValue());\n    }\n    assertTrue(docs.isEmpty());\n    IOUtils.close(ir, analyzer, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"09ab8ee44ca898536770d0106a7c0ee4be4f0eb7","date":1484239864,"type":2,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/suggest/src/test/org/apache/lucene/search/suggest/DocumentValueSourceDictionaryTest#testLongValuesSourceWithDeletions().mjava","pathOld":"lucene/suggest/src/test/org/apache/lucene/search/suggest/DocumentValueSourceDictionaryTest#testWithDeletions().mjava","sourceNew":"  @Test\n  public void testLongValuesSourceWithDeletions() throws IOException {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Map<String, Document> docs = generateIndexDocuments(atLeast(100));\n    Random rand = random();\n    List<String> termsToDel = new ArrayList<>();\n    for(Document doc : docs.values()) {\n      if(rand.nextBoolean() && termsToDel.size() < docs.size()-1) {\n        termsToDel.add(doc.get(FIELD_NAME));\n      }\n      writer.addDocument(doc);\n    }\n    writer.commit();\n\n    Term[] delTerms = new Term[termsToDel.size()];\n    for(int i=0; i < termsToDel.size() ; i++) {\n      delTerms[i] = new Term(FIELD_NAME, termsToDel.get(i));\n    }\n\n    for(Term delTerm: delTerms) {\n      writer.deleteDocuments(delTerm);\n    }\n    writer.commit();\n    writer.close();\n\n    for(String termToDel: termsToDel) {\n      assertTrue(null!=docs.remove(termToDel));\n    }\n\n    IndexReader ir = DirectoryReader.open(dir);\n    assertTrue(\"NumDocs should be > 0 but was \" + ir.numDocs(), ir.numDocs() > 0);\n    assertEquals(ir.numDocs(), docs.size());\n    LongValuesSource sumValues = sum(WEIGHT_FIELD_NAME_1, WEIGHT_FIELD_NAME_2);\n    Dictionary dictionary = new DocumentValueSourceDictionary(ir, FIELD_NAME, sumValues, PAYLOAD_FIELD_NAME);\n    InputIterator inputIterator = dictionary.getEntryIterator();\n    BytesRef f;\n    while((f = inputIterator.next())!=null) {\n      Document doc = docs.remove(f.utf8ToString());\n      long w1 = doc.getField(WEIGHT_FIELD_NAME_1).numericValue().longValue();\n      long w2 = doc.getField(WEIGHT_FIELD_NAME_2).numericValue().longValue();\n      assertTrue(f.equals(new BytesRef(doc.get(FIELD_NAME))));\n      assertEquals(inputIterator.weight(), w2+w1);\n      IndexableField payloadField = doc.getField(PAYLOAD_FIELD_NAME);\n      if (payloadField == null) assertTrue(inputIterator.payload().length == 0);\n      else assertEquals(inputIterator.payload(), payloadField.binaryValue());\n    }\n    assertTrue(docs.isEmpty());\n    IOUtils.close(ir, analyzer, dir);\n  }\n\n","sourceOld":"  @Test\n  public void testWithDeletions() throws IOException {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Map<String, Document> docs = generateIndexDocuments(atLeast(100));\n    Random rand = random();\n    List<String> termsToDel = new ArrayList<>();\n    for(Document doc : docs.values()) {\n      if(rand.nextBoolean() && termsToDel.size() < docs.size()-1) {\n        termsToDel.add(doc.get(FIELD_NAME));\n      }\n      writer.addDocument(doc);\n    }\n    writer.commit();\n    \n    Term[] delTerms = new Term[termsToDel.size()];\n    for(int i=0; i < termsToDel.size() ; i++) {\n      delTerms[i] = new Term(FIELD_NAME, termsToDel.get(i));\n    }\n    \n    for(Term delTerm: delTerms) {\n      writer.deleteDocuments(delTerm);  \n    }\n    writer.commit();\n    writer.close();\n    \n    for(String termToDel: termsToDel) {\n      assertTrue(null!=docs.remove(termToDel));\n    }\n    \n    IndexReader ir = DirectoryReader.open(dir);\n    assertTrue(\"NumDocs should be > 0 but was \" + ir.numDocs(), ir.numDocs() > 0);\n    assertEquals(ir.numDocs(), docs.size());\n    ValueSource[] toAdd = new ValueSource[] {new LongFieldSource(WEIGHT_FIELD_NAME_1), new LongFieldSource(WEIGHT_FIELD_NAME_2)};\n\n    Dictionary dictionary = new DocumentValueSourceDictionary(ir, FIELD_NAME,  new SumFloatFunction(toAdd), PAYLOAD_FIELD_NAME);\n    InputIterator inputIterator = dictionary.getEntryIterator();\n    BytesRef f;\n    while((f = inputIterator.next())!=null) {\n      Document doc = docs.remove(f.utf8ToString());\n      long w1 = doc.getField(WEIGHT_FIELD_NAME_1).numericValue().longValue();\n      long w2 = doc.getField(WEIGHT_FIELD_NAME_2).numericValue().longValue();\n      assertTrue(f.equals(new BytesRef(doc.get(FIELD_NAME))));\n      assertEquals(inputIterator.weight(), w2+w1);\n      IndexableField payloadField = doc.getField(PAYLOAD_FIELD_NAME);\n      if (payloadField == null) assertTrue(inputIterator.payload().length == 0);\n      else assertEquals(inputIterator.payload(), payloadField.binaryValue());\n    }\n    assertTrue(docs.isEmpty());\n    IOUtils.close(ir, analyzer, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"09ab8ee44ca898536770d0106a7c0ee4be4f0eb7":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","7abbd4266bb36de7f2181eb6d9f3df3ea45ff382"],"7abbd4266bb36de7f2181eb6d9f3df3ea45ff382":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["7abbd4266bb36de7f2181eb6d9f3df3ea45ff382"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["09ab8ee44ca898536770d0106a7c0ee4be4f0eb7","7abbd4266bb36de7f2181eb6d9f3df3ea45ff382"],"09ab8ee44ca898536770d0106a7c0ee4be4f0eb7":[],"7abbd4266bb36de7f2181eb6d9f3df3ea45ff382":["09ab8ee44ca898536770d0106a7c0ee4be4f0eb7","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["09ab8ee44ca898536770d0106a7c0ee4be4f0eb7","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}