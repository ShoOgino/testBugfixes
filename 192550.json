{"path":"src/java/org/apache/lucene/index/SegmentReader#initialize(SegmentInfo).mjava","commits":[{"id":"f653aecb322b74d99e6ecdb93765e453a3d7aa71","date":1082107025,"type":0,"author":"Christoph Goller","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentReader#initialize(SegmentInfo).mjava","pathOld":"/dev/null","sourceNew":"   private void initialize(SegmentInfo si) throws IOException\n   {\n    segment = si.name;\n\n    // Use compound file directory for some files, if it exists\n    Directory cfsDir = directory();\n    if (directory().fileExists(segment + \".cfs\")) {\n      cfsReader = new CompoundFileReader(directory(), segment + \".cfs\");\n      cfsDir = cfsReader;\n    }\n\n    // No compound file exists - use the multi-file format\n    fieldInfos = new FieldInfos(cfsDir, segment + \".fnm\");\n    fieldsReader = new FieldsReader(cfsDir, segment, fieldInfos);\n\n    tis = new TermInfosReader(cfsDir, segment, fieldInfos);\n\n    // NOTE: the bitvector is stored using the regular directory, not cfs\n    if (hasDeletions(si))\n      deletedDocs = new BitVector(directory(), segment + \".del\");\n\n    // make sure that all index files have been read or are kept open\n    // so that if an index update removes them we'll still have them\n    freqStream = cfsDir.openFile(segment + \".frq\");\n    proxStream = cfsDir.openFile(segment + \".prx\");\n    openNorms(cfsDir);\n\n    if (fieldInfos.hasVectors()) { // open term vector files only as needed\n      termVectorsReader = new TermVectorsReader(cfsDir, segment, fieldInfos);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e971ba25509e21a130fef61f0687be0446ca27a0","date":1095369217,"type":3,"author":"Doug Cutting","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentReader#initialize(SegmentInfo).mjava","pathOld":"src/java/org/apache/lucene/index/SegmentReader#initialize(SegmentInfo).mjava","sourceNew":"   private void initialize(SegmentInfo si) throws IOException\n   {\n    segment = si.name;\n\n    // Use compound file directory for some files, if it exists\n    Directory cfsDir = directory();\n    if (directory().fileExists(segment + \".cfs\")) {\n      cfsReader = new CompoundFileReader(directory(), segment + \".cfs\");\n      cfsDir = cfsReader;\n    }\n\n    // No compound file exists - use the multi-file format\n    fieldInfos = new FieldInfos(cfsDir, segment + \".fnm\");\n    fieldsReader = new FieldsReader(cfsDir, segment, fieldInfos);\n\n    tis = new TermInfosReader(cfsDir, segment, fieldInfos);\n\n    // NOTE: the bitvector is stored using the regular directory, not cfs\n    if (hasDeletions(si))\n      deletedDocs = new BitVector(directory(), segment + \".del\");\n\n    // make sure that all index files have been read or are kept open\n    // so that if an index update removes them we'll still have them\n    freqStream = cfsDir.openInput(segment + \".frq\");\n    proxStream = cfsDir.openInput(segment + \".prx\");\n    openNorms(cfsDir);\n\n    if (fieldInfos.hasVectors()) { // open term vector files only as needed\n      termVectorsReader = new TermVectorsReader(cfsDir, segment, fieldInfos);\n    }\n  }\n\n","sourceOld":"   private void initialize(SegmentInfo si) throws IOException\n   {\n    segment = si.name;\n\n    // Use compound file directory for some files, if it exists\n    Directory cfsDir = directory();\n    if (directory().fileExists(segment + \".cfs\")) {\n      cfsReader = new CompoundFileReader(directory(), segment + \".cfs\");\n      cfsDir = cfsReader;\n    }\n\n    // No compound file exists - use the multi-file format\n    fieldInfos = new FieldInfos(cfsDir, segment + \".fnm\");\n    fieldsReader = new FieldsReader(cfsDir, segment, fieldInfos);\n\n    tis = new TermInfosReader(cfsDir, segment, fieldInfos);\n\n    // NOTE: the bitvector is stored using the regular directory, not cfs\n    if (hasDeletions(si))\n      deletedDocs = new BitVector(directory(), segment + \".del\");\n\n    // make sure that all index files have been read or are kept open\n    // so that if an index update removes them we'll still have them\n    freqStream = cfsDir.openFile(segment + \".frq\");\n    proxStream = cfsDir.openFile(segment + \".prx\");\n    openNorms(cfsDir);\n\n    if (fieldInfos.hasVectors()) { // open term vector files only as needed\n      termVectorsReader = new TermVectorsReader(cfsDir, segment, fieldInfos);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6e5d88b55f1b57feab6da94a5c635a224539bd2a","date":1095877947,"type":3,"author":"Doug Cutting","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentReader#initialize(SegmentInfo).mjava","pathOld":"src/java/org/apache/lucene/index/SegmentReader#initialize(SegmentInfo).mjava","sourceNew":"   private void initialize(SegmentInfo si) throws IOException {\n    segment = si.name;\n\n    // Use compound file directory for some files, if it exists\n    Directory cfsDir = directory();\n    if (directory().fileExists(segment + \".cfs\")) {\n      cfsReader = new CompoundFileReader(directory(), segment + \".cfs\");\n      cfsDir = cfsReader;\n    }\n\n    // No compound file exists - use the multi-file format\n    fieldInfos = new FieldInfos(cfsDir, segment + \".fnm\");\n    fieldsReader = new FieldsReader(cfsDir, segment, fieldInfos);\n\n    tis = new TermInfosReader(cfsDir, segment, fieldInfos);\n\n    // NOTE: the bitvector is stored using the regular directory, not cfs\n    if (hasDeletions(si))\n      deletedDocs = new BitVector(directory(), segment + \".del\");\n\n    // make sure that all index files have been read or are kept open\n    // so that if an index update removes them we'll still have them\n    freqStream = cfsDir.openInput(segment + \".frq\");\n    proxStream = cfsDir.openInput(segment + \".prx\");\n    openNorms(cfsDir);\n\n    if (fieldInfos.hasVectors()) { // open term vector files only as needed\n      termVectorsReader = new TermVectorsReader(cfsDir, segment, fieldInfos);\n    }\n  }\n\n","sourceOld":"   private void initialize(SegmentInfo si) throws IOException\n   {\n    segment = si.name;\n\n    // Use compound file directory for some files, if it exists\n    Directory cfsDir = directory();\n    if (directory().fileExists(segment + \".cfs\")) {\n      cfsReader = new CompoundFileReader(directory(), segment + \".cfs\");\n      cfsDir = cfsReader;\n    }\n\n    // No compound file exists - use the multi-file format\n    fieldInfos = new FieldInfos(cfsDir, segment + \".fnm\");\n    fieldsReader = new FieldsReader(cfsDir, segment, fieldInfos);\n\n    tis = new TermInfosReader(cfsDir, segment, fieldInfos);\n\n    // NOTE: the bitvector is stored using the regular directory, not cfs\n    if (hasDeletions(si))\n      deletedDocs = new BitVector(directory(), segment + \".del\");\n\n    // make sure that all index files have been read or are kept open\n    // so that if an index update removes them we'll still have them\n    freqStream = cfsDir.openInput(segment + \".frq\");\n    proxStream = cfsDir.openInput(segment + \".prx\");\n    openNorms(cfsDir);\n\n    if (fieldInfos.hasVectors()) { // open term vector files only as needed\n      termVectorsReader = new TermVectorsReader(cfsDir, segment, fieldInfos);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9e98927024757b7944e3ab5bf88134d5f7f30600","date":1097059223,"type":3,"author":"Christoph Goller","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentReader#initialize(SegmentInfo).mjava","pathOld":"src/java/org/apache/lucene/index/SegmentReader#initialize(SegmentInfo).mjava","sourceNew":"   private void initialize(SegmentInfo si) throws IOException {\n    segment = si.name;\n\n    // Use compound file directory for some files, if it exists\n    Directory cfsDir = directory();\n    if (directory().fileExists(segment + \".cfs\")) {\n      cfsReader = new CompoundFileReader(directory(), segment + \".cfs\");\n      cfsDir = cfsReader;\n    }\n\n    // No compound file exists - use the multi-file format\n    fieldInfos = new FieldInfos(cfsDir, segment + \".fnm\");\n    fieldsReader = new FieldsReader(cfsDir, segment, fieldInfos);\n\n    tis = new TermInfosReader(cfsDir, segment, fieldInfos);\n\n    // NOTE: the bitvector is stored using the regular directory, not cfs\n    if (hasDeletions(si))\n      deletedDocs = new BitVector(directory(), segment + \".del\");\n\n    // make sure that all index files have been read or are kept open\n    // so that if an index update removes them we'll still have them\n    freqStream = cfsDir.openInput(segment + \".frq\");\n    proxStream = cfsDir.openInput(segment + \".prx\");\n    openNorms(cfsDir);\n\n    if (fieldInfos.hasVectors()) { // open term vector files only as needed\n      termVectorsReaderOrig = new TermVectorsReader(cfsDir, segment, fieldInfos);\n    }\n  }\n\n","sourceOld":"   private void initialize(SegmentInfo si) throws IOException {\n    segment = si.name;\n\n    // Use compound file directory for some files, if it exists\n    Directory cfsDir = directory();\n    if (directory().fileExists(segment + \".cfs\")) {\n      cfsReader = new CompoundFileReader(directory(), segment + \".cfs\");\n      cfsDir = cfsReader;\n    }\n\n    // No compound file exists - use the multi-file format\n    fieldInfos = new FieldInfos(cfsDir, segment + \".fnm\");\n    fieldsReader = new FieldsReader(cfsDir, segment, fieldInfos);\n\n    tis = new TermInfosReader(cfsDir, segment, fieldInfos);\n\n    // NOTE: the bitvector is stored using the regular directory, not cfs\n    if (hasDeletions(si))\n      deletedDocs = new BitVector(directory(), segment + \".del\");\n\n    // make sure that all index files have been read or are kept open\n    // so that if an index update removes them we'll still have them\n    freqStream = cfsDir.openInput(segment + \".frq\");\n    proxStream = cfsDir.openInput(segment + \".prx\");\n    openNorms(cfsDir);\n\n    if (fieldInfos.hasVectors()) { // open term vector files only as needed\n      termVectorsReader = new TermVectorsReader(cfsDir, segment, fieldInfos);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"284c1d3c8b19931bf6f312fae7470487f5d9e580","date":1163805527,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentReader#initialize(SegmentInfo).mjava","pathOld":"src/java/org/apache/lucene/index/SegmentReader#initialize(SegmentInfo).mjava","sourceNew":"  private void initialize(SegmentInfo si) throws IOException {\n    segment = si.name;\n    this.si = si;\n\n    boolean success = false;\n\n    try {\n      // Use compound file directory for some files, if it exists\n      Directory cfsDir = directory();\n      if (si.getUseCompoundFile()) {\n        cfsReader = new CompoundFileReader(directory(), segment + \".cfs\");\n        cfsDir = cfsReader;\n      }\n\n      // No compound file exists - use the multi-file format\n      fieldInfos = new FieldInfos(cfsDir, segment + \".fnm\");\n      fieldsReader = new FieldsReader(cfsDir, segment, fieldInfos);\n\n      tis = new TermInfosReader(cfsDir, segment, fieldInfos);\n      \n      // NOTE: the bitvector is stored using the regular directory, not cfs\n      if (hasDeletions(si)) {\n        deletedDocs = new BitVector(directory(), si.getDelFileName());\n      }\n\n      // make sure that all index files have been read or are kept open\n      // so that if an index update removes them we'll still have them\n      freqStream = cfsDir.openInput(segment + \".frq\");\n      proxStream = cfsDir.openInput(segment + \".prx\");\n      openNorms(cfsDir);\n\n      if (fieldInfos.hasVectors()) { // open term vector files only as needed\n        termVectorsReaderOrig = new TermVectorsReader(cfsDir, segment, fieldInfos);\n      }\n      success = true;\n    } finally {\n\n      // With lock-less commits, it's entirely possible (and\n      // fine) to hit a FileNotFound exception above.  In\n      // this case, we want to explicitly close any subset\n      // of things that were opened so that we don't have to\n      // wait for a GC to do so.\n      if (!success) {\n        doClose();\n      }\n    }\n  }\n\n","sourceOld":"   private void initialize(SegmentInfo si) throws IOException {\n    segment = si.name;\n\n    // Use compound file directory for some files, if it exists\n    Directory cfsDir = directory();\n    if (directory().fileExists(segment + \".cfs\")) {\n      cfsReader = new CompoundFileReader(directory(), segment + \".cfs\");\n      cfsDir = cfsReader;\n    }\n\n    // No compound file exists - use the multi-file format\n    fieldInfos = new FieldInfos(cfsDir, segment + \".fnm\");\n    fieldsReader = new FieldsReader(cfsDir, segment, fieldInfos);\n\n    tis = new TermInfosReader(cfsDir, segment, fieldInfos);\n\n    // NOTE: the bitvector is stored using the regular directory, not cfs\n    if (hasDeletions(si))\n      deletedDocs = new BitVector(directory(), segment + \".del\");\n\n    // make sure that all index files have been read or are kept open\n    // so that if an index update removes them we'll still have them\n    freqStream = cfsDir.openInput(segment + \".frq\");\n    proxStream = cfsDir.openInput(segment + \".prx\");\n    openNorms(cfsDir);\n\n    if (fieldInfos.hasVectors()) { // open term vector files only as needed\n      termVectorsReaderOrig = new TermVectorsReader(cfsDir, segment, fieldInfos);\n    }\n  }\n\n","bugFix":null,"bugIntro":["1b54a9bc667895a2095a886184bf69a3179e63df"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fcf637fee66c296142fb5989e338efc018320655","date":1168455996,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentReader#initialize(SegmentInfo).mjava","pathOld":"src/java/org/apache/lucene/index/SegmentReader#initialize(SegmentInfo).mjava","sourceNew":"  private void initialize(SegmentInfo si) throws IOException {\n    segment = si.name;\n    this.si = si;\n\n    boolean success = false;\n\n    try {\n      // Use compound file directory for some files, if it exists\n      Directory cfsDir = directory();\n      if (si.getUseCompoundFile()) {\n        cfsReader = new CompoundFileReader(directory(), segment + \".cfs\");\n        cfsDir = cfsReader;\n      }\n\n      // No compound file exists - use the multi-file format\n      fieldInfos = new FieldInfos(cfsDir, segment + \".fnm\");\n      fieldsReader = new FieldsReader(cfsDir, segment, fieldInfos);\n\n      // Verify two sources of \"maxDoc\" agree:\n      if (fieldsReader.size() != si.docCount) {\n        throw new IllegalStateException(\"doc counts differ for segment \" + si.name + \": fieldsReader shows \" + fieldsReader.size() + \" but segmentInfo shows \" + si.docCount);\n      }\n\n      tis = new TermInfosReader(cfsDir, segment, fieldInfos);\n      \n      // NOTE: the bitvector is stored using the regular directory, not cfs\n      if (hasDeletions(si)) {\n        deletedDocs = new BitVector(directory(), si.getDelFileName());\n\n        // Verify # deletes does not exceed maxDoc for this segment:\n        if (deletedDocs.count() > maxDoc()) {\n          throw new IllegalStateException(\"number of deletes (\" + deletedDocs.count() + \") exceeds max doc (\" + maxDoc() + \") for segment \" + si.name);\n        }\n      }\n\n      // make sure that all index files have been read or are kept open\n      // so that if an index update removes them we'll still have them\n      freqStream = cfsDir.openInput(segment + \".frq\");\n      proxStream = cfsDir.openInput(segment + \".prx\");\n      openNorms(cfsDir);\n\n      if (fieldInfos.hasVectors()) { // open term vector files only as needed\n        termVectorsReaderOrig = new TermVectorsReader(cfsDir, segment, fieldInfos);\n      }\n      success = true;\n    } finally {\n\n      // With lock-less commits, it's entirely possible (and\n      // fine) to hit a FileNotFound exception above.  In\n      // this case, we want to explicitly close any subset\n      // of things that were opened so that we don't have to\n      // wait for a GC to do so.\n      if (!success) {\n        doClose();\n      }\n    }\n  }\n\n","sourceOld":"  private void initialize(SegmentInfo si) throws IOException {\n    segment = si.name;\n    this.si = si;\n\n    boolean success = false;\n\n    try {\n      // Use compound file directory for some files, if it exists\n      Directory cfsDir = directory();\n      if (si.getUseCompoundFile()) {\n        cfsReader = new CompoundFileReader(directory(), segment + \".cfs\");\n        cfsDir = cfsReader;\n      }\n\n      // No compound file exists - use the multi-file format\n      fieldInfos = new FieldInfos(cfsDir, segment + \".fnm\");\n      fieldsReader = new FieldsReader(cfsDir, segment, fieldInfos);\n\n      tis = new TermInfosReader(cfsDir, segment, fieldInfos);\n      \n      // NOTE: the bitvector is stored using the regular directory, not cfs\n      if (hasDeletions(si)) {\n        deletedDocs = new BitVector(directory(), si.getDelFileName());\n      }\n\n      // make sure that all index files have been read or are kept open\n      // so that if an index update removes them we'll still have them\n      freqStream = cfsDir.openInput(segment + \".frq\");\n      proxStream = cfsDir.openInput(segment + \".prx\");\n      openNorms(cfsDir);\n\n      if (fieldInfos.hasVectors()) { // open term vector files only as needed\n        termVectorsReaderOrig = new TermVectorsReader(cfsDir, segment, fieldInfos);\n      }\n      success = true;\n    } finally {\n\n      // With lock-less commits, it's entirely possible (and\n      // fine) to hit a FileNotFound exception above.  In\n      // this case, we want to explicitly close any subset\n      // of things that were opened so that we don't have to\n      // wait for a GC to do so.\n      if (!success) {\n        doClose();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["1b54a9bc667895a2095a886184bf69a3179e63df"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1b54a9bc667895a2095a886184bf69a3179e63df","date":1172088096,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentReader#initialize(SegmentInfo).mjava","pathOld":"src/java/org/apache/lucene/index/SegmentReader#initialize(SegmentInfo).mjava","sourceNew":"  private void initialize(SegmentInfo si) throws CorruptIndexException, IOException {\n    segment = si.name;\n    this.si = si;\n\n    boolean success = false;\n\n    try {\n      // Use compound file directory for some files, if it exists\n      Directory cfsDir = directory();\n      if (si.getUseCompoundFile()) {\n        cfsReader = new CompoundFileReader(directory(), segment + \".cfs\");\n        cfsDir = cfsReader;\n      }\n\n      // No compound file exists - use the multi-file format\n      fieldInfos = new FieldInfos(cfsDir, segment + \".fnm\");\n      fieldsReader = new FieldsReader(cfsDir, segment, fieldInfos);\n\n      // Verify two sources of \"maxDoc\" agree:\n      if (fieldsReader.size() != si.docCount) {\n        throw new CorruptIndexException(\"doc counts differ for segment \" + si.name + \": fieldsReader shows \" + fieldsReader.size() + \" but segmentInfo shows \" + si.docCount);\n      }\n\n      tis = new TermInfosReader(cfsDir, segment, fieldInfos);\n      \n      // NOTE: the bitvector is stored using the regular directory, not cfs\n      if (hasDeletions(si)) {\n        deletedDocs = new BitVector(directory(), si.getDelFileName());\n\n        // Verify # deletes does not exceed maxDoc for this segment:\n        if (deletedDocs.count() > maxDoc()) {\n          throw new CorruptIndexException(\"number of deletes (\" + deletedDocs.count() + \") exceeds max doc (\" + maxDoc() + \") for segment \" + si.name);\n        }\n      }\n\n      // make sure that all index files have been read or are kept open\n      // so that if an index update removes them we'll still have them\n      freqStream = cfsDir.openInput(segment + \".frq\");\n      proxStream = cfsDir.openInput(segment + \".prx\");\n      openNorms(cfsDir);\n\n      if (fieldInfos.hasVectors()) { // open term vector files only as needed\n        termVectorsReaderOrig = new TermVectorsReader(cfsDir, segment, fieldInfos);\n      }\n      success = true;\n    } finally {\n\n      // With lock-less commits, it's entirely possible (and\n      // fine) to hit a FileNotFound exception above.  In\n      // this case, we want to explicitly close any subset\n      // of things that were opened so that we don't have to\n      // wait for a GC to do so.\n      if (!success) {\n        doClose();\n      }\n    }\n  }\n\n","sourceOld":"  private void initialize(SegmentInfo si) throws IOException {\n    segment = si.name;\n    this.si = si;\n\n    boolean success = false;\n\n    try {\n      // Use compound file directory for some files, if it exists\n      Directory cfsDir = directory();\n      if (si.getUseCompoundFile()) {\n        cfsReader = new CompoundFileReader(directory(), segment + \".cfs\");\n        cfsDir = cfsReader;\n      }\n\n      // No compound file exists - use the multi-file format\n      fieldInfos = new FieldInfos(cfsDir, segment + \".fnm\");\n      fieldsReader = new FieldsReader(cfsDir, segment, fieldInfos);\n\n      // Verify two sources of \"maxDoc\" agree:\n      if (fieldsReader.size() != si.docCount) {\n        throw new IllegalStateException(\"doc counts differ for segment \" + si.name + \": fieldsReader shows \" + fieldsReader.size() + \" but segmentInfo shows \" + si.docCount);\n      }\n\n      tis = new TermInfosReader(cfsDir, segment, fieldInfos);\n      \n      // NOTE: the bitvector is stored using the regular directory, not cfs\n      if (hasDeletions(si)) {\n        deletedDocs = new BitVector(directory(), si.getDelFileName());\n\n        // Verify # deletes does not exceed maxDoc for this segment:\n        if (deletedDocs.count() > maxDoc()) {\n          throw new IllegalStateException(\"number of deletes (\" + deletedDocs.count() + \") exceeds max doc (\" + maxDoc() + \") for segment \" + si.name);\n        }\n      }\n\n      // make sure that all index files have been read or are kept open\n      // so that if an index update removes them we'll still have them\n      freqStream = cfsDir.openInput(segment + \".frq\");\n      proxStream = cfsDir.openInput(segment + \".prx\");\n      openNorms(cfsDir);\n\n      if (fieldInfos.hasVectors()) { // open term vector files only as needed\n        termVectorsReaderOrig = new TermVectorsReader(cfsDir, segment, fieldInfos);\n      }\n      success = true;\n    } finally {\n\n      // With lock-less commits, it's entirely possible (and\n      // fine) to hit a FileNotFound exception above.  In\n      // this case, we want to explicitly close any subset\n      // of things that were opened so that we don't have to\n      // wait for a GC to do so.\n      if (!success) {\n        doClose();\n      }\n    }\n  }\n\n","bugFix":["284c1d3c8b19931bf6f312fae7470487f5d9e580","fcf637fee66c296142fb5989e338efc018320655"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f6dba7919de4ff4ed6ff17f90619203772722f08","date":1180451647,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentReader#initialize(SegmentInfo,int).mjava","pathOld":"src/java/org/apache/lucene/index/SegmentReader#initialize(SegmentInfo).mjava","sourceNew":"  private void initialize(SegmentInfo si, int readBufferSize) throws CorruptIndexException, IOException {\n    segment = si.name;\n    this.si = si;\n\n    boolean success = false;\n\n    try {\n      // Use compound file directory for some files, if it exists\n      Directory cfsDir = directory();\n      if (si.getUseCompoundFile()) {\n        cfsReader = new CompoundFileReader(directory(), segment + \".cfs\", readBufferSize);\n        cfsDir = cfsReader;\n      }\n\n      // No compound file exists - use the multi-file format\n      fieldInfos = new FieldInfos(cfsDir, segment + \".fnm\");\n      fieldsReader = new FieldsReader(cfsDir, segment, fieldInfos, readBufferSize);\n\n      // Verify two sources of \"maxDoc\" agree:\n      if (fieldsReader.size() != si.docCount) {\n        throw new CorruptIndexException(\"doc counts differ for segment \" + si.name + \": fieldsReader shows \" + fieldsReader.size() + \" but segmentInfo shows \" + si.docCount);\n      }\n\n      tis = new TermInfosReader(cfsDir, segment, fieldInfos, readBufferSize);\n      \n      // NOTE: the bitvector is stored using the regular directory, not cfs\n      if (hasDeletions(si)) {\n        deletedDocs = new BitVector(directory(), si.getDelFileName());\n\n        // Verify # deletes does not exceed maxDoc for this segment:\n        if (deletedDocs.count() > maxDoc()) {\n          throw new CorruptIndexException(\"number of deletes (\" + deletedDocs.count() + \") exceeds max doc (\" + maxDoc() + \") for segment \" + si.name);\n        }\n      }\n\n      // make sure that all index files have been read or are kept open\n      // so that if an index update removes them we'll still have them\n      freqStream = cfsDir.openInput(segment + \".frq\", readBufferSize);\n      proxStream = cfsDir.openInput(segment + \".prx\", readBufferSize);\n      openNorms(cfsDir, readBufferSize);\n\n      if (fieldInfos.hasVectors()) { // open term vector files only as needed\n        termVectorsReaderOrig = new TermVectorsReader(cfsDir, segment, fieldInfos, readBufferSize);\n      }\n      success = true;\n    } finally {\n\n      // With lock-less commits, it's entirely possible (and\n      // fine) to hit a FileNotFound exception above.  In\n      // this case, we want to explicitly close any subset\n      // of things that were opened so that we don't have to\n      // wait for a GC to do so.\n      if (!success) {\n        doClose();\n      }\n    }\n  }\n\n","sourceOld":"  private void initialize(SegmentInfo si) throws CorruptIndexException, IOException {\n    segment = si.name;\n    this.si = si;\n\n    boolean success = false;\n\n    try {\n      // Use compound file directory for some files, if it exists\n      Directory cfsDir = directory();\n      if (si.getUseCompoundFile()) {\n        cfsReader = new CompoundFileReader(directory(), segment + \".cfs\");\n        cfsDir = cfsReader;\n      }\n\n      // No compound file exists - use the multi-file format\n      fieldInfos = new FieldInfos(cfsDir, segment + \".fnm\");\n      fieldsReader = new FieldsReader(cfsDir, segment, fieldInfos);\n\n      // Verify two sources of \"maxDoc\" agree:\n      if (fieldsReader.size() != si.docCount) {\n        throw new CorruptIndexException(\"doc counts differ for segment \" + si.name + \": fieldsReader shows \" + fieldsReader.size() + \" but segmentInfo shows \" + si.docCount);\n      }\n\n      tis = new TermInfosReader(cfsDir, segment, fieldInfos);\n      \n      // NOTE: the bitvector is stored using the regular directory, not cfs\n      if (hasDeletions(si)) {\n        deletedDocs = new BitVector(directory(), si.getDelFileName());\n\n        // Verify # deletes does not exceed maxDoc for this segment:\n        if (deletedDocs.count() > maxDoc()) {\n          throw new CorruptIndexException(\"number of deletes (\" + deletedDocs.count() + \") exceeds max doc (\" + maxDoc() + \") for segment \" + si.name);\n        }\n      }\n\n      // make sure that all index files have been read or are kept open\n      // so that if an index update removes them we'll still have them\n      freqStream = cfsDir.openInput(segment + \".frq\");\n      proxStream = cfsDir.openInput(segment + \".prx\");\n      openNorms(cfsDir);\n\n      if (fieldInfos.hasVectors()) { // open term vector files only as needed\n        termVectorsReaderOrig = new TermVectorsReader(cfsDir, segment, fieldInfos);\n      }\n      success = true;\n    } finally {\n\n      // With lock-less commits, it's entirely possible (and\n      // fine) to hit a FileNotFound exception above.  In\n      // this case, we want to explicitly close any subset\n      // of things that were opened so that we don't have to\n      // wait for a GC to do so.\n      if (!success) {\n        doClose();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"fcf637fee66c296142fb5989e338efc018320655":["284c1d3c8b19931bf6f312fae7470487f5d9e580"],"e971ba25509e21a130fef61f0687be0446ca27a0":["f653aecb322b74d99e6ecdb93765e453a3d7aa71"],"284c1d3c8b19931bf6f312fae7470487f5d9e580":["9e98927024757b7944e3ab5bf88134d5f7f30600"],"9e98927024757b7944e3ab5bf88134d5f7f30600":["6e5d88b55f1b57feab6da94a5c635a224539bd2a"],"1b54a9bc667895a2095a886184bf69a3179e63df":["fcf637fee66c296142fb5989e338efc018320655"],"6e5d88b55f1b57feab6da94a5c635a224539bd2a":["e971ba25509e21a130fef61f0687be0446ca27a0"],"f6dba7919de4ff4ed6ff17f90619203772722f08":["1b54a9bc667895a2095a886184bf69a3179e63df"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"f653aecb322b74d99e6ecdb93765e453a3d7aa71":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["f6dba7919de4ff4ed6ff17f90619203772722f08"]},"commit2Childs":{"fcf637fee66c296142fb5989e338efc018320655":["1b54a9bc667895a2095a886184bf69a3179e63df"],"e971ba25509e21a130fef61f0687be0446ca27a0":["6e5d88b55f1b57feab6da94a5c635a224539bd2a"],"284c1d3c8b19931bf6f312fae7470487f5d9e580":["fcf637fee66c296142fb5989e338efc018320655"],"9e98927024757b7944e3ab5bf88134d5f7f30600":["284c1d3c8b19931bf6f312fae7470487f5d9e580"],"6e5d88b55f1b57feab6da94a5c635a224539bd2a":["9e98927024757b7944e3ab5bf88134d5f7f30600"],"1b54a9bc667895a2095a886184bf69a3179e63df":["f6dba7919de4ff4ed6ff17f90619203772722f08"],"f6dba7919de4ff4ed6ff17f90619203772722f08":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["f653aecb322b74d99e6ecdb93765e453a3d7aa71"],"f653aecb322b74d99e6ecdb93765e453a3d7aa71":["e971ba25509e21a130fef61f0687be0446ca27a0"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}