{"path":"lucene/core/src/java/org/apache/lucene/store/SimpleFSDirectory.SimpleFSIndexInput#readInternal(byte[],int,int).mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/store/SimpleFSDirectory.SimpleFSIndexInput#readInternal(byte[],int,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/store/SimpleFSDirectory.SimpleFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":"    /** IndexInput methods */\n    @Override\n    protected void readInternal(byte[] b, int offset, int len)\n         throws IOException {\n      synchronized (file) {\n        long position = off + getFilePointer();\n        if (position != file.position) {\n          file.seek(position);\n          file.position = position;\n        }\n        int total = 0;\n\n        if (position + len > end) {\n          throw new EOFException(\"read past EOF: \" + this);\n        }\n\n        try {\n          do {\n            final int readLength;\n            if (total + chunkSize > len) {\n              readLength = len - total;\n            } else {\n              // LUCENE-1566 - work around JVM Bug by breaking very large reads into chunks\n              readLength = chunkSize;\n            }\n            final int i = file.read(b, offset + total, readLength);\n            file.position += i;\n            total += i;\n          } while (total < len);\n        } catch (OutOfMemoryError e) {\n          // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n          // with a large chunk size in the fast path.\n          final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n          outOfMemoryError.initCause(e);\n          throw outOfMemoryError;\n        } catch (IOException ioe) {\n          throw new IOException(ioe.getMessage() + \": \" + this, ioe);\n        }\n      }\n    }\n\n","sourceOld":"    /** IndexInput methods */\n    @Override\n    protected void readInternal(byte[] b, int offset, int len)\n         throws IOException {\n      synchronized (file) {\n        long position = off + getFilePointer();\n        if (position != file.position) {\n          file.seek(position);\n          file.position = position;\n        }\n        int total = 0;\n\n        if (position + len > end) {\n          throw new EOFException(\"read past EOF: \" + this);\n        }\n\n        try {\n          do {\n            final int readLength;\n            if (total + chunkSize > len) {\n              readLength = len - total;\n            } else {\n              // LUCENE-1566 - work around JVM Bug by breaking very large reads into chunks\n              readLength = chunkSize;\n            }\n            final int i = file.read(b, offset + total, readLength);\n            file.position += i;\n            total += i;\n          } while (total < len);\n        } catch (OutOfMemoryError e) {\n          // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n          // with a large chunk size in the fast path.\n          final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n          outOfMemoryError.initCause(e);\n          throw outOfMemoryError;\n        } catch (IOException ioe) {\n          throw new IOException(ioe.getMessage() + \": \" + this, ioe);\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c50a88b580d08fc67d9a94b8b32b9d07a5972f8c","date":1347635292,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/store/SimpleFSDirectory.SimpleFSIndexInput#readInternal(byte[],int,int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/store/SimpleFSDirectory.SimpleFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":"    /** IndexInput methods */\n    @Override\n    protected void readInternal(byte[] b, int offset, int len)\n         throws IOException {\n      synchronized (file) {\n        long position = off + getFilePointer();\n        file.seek(position);\n        int total = 0;\n\n        if (position + len > end) {\n          throw new EOFException(\"read past EOF: \" + this);\n        }\n\n        try {\n          do {\n            final int readLength;\n            if (total + chunkSize > len) {\n              readLength = len - total;\n            } else {\n              // LUCENE-1566 - work around JVM Bug by breaking very large reads into chunks\n              readLength = chunkSize;\n            }\n            final int i = file.read(b, offset + total, readLength);\n            total += i;\n          } while (total < len);\n        } catch (OutOfMemoryError e) {\n          // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n          // with a large chunk size in the fast path.\n          final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n          outOfMemoryError.initCause(e);\n          throw outOfMemoryError;\n        } catch (IOException ioe) {\n          throw new IOException(ioe.getMessage() + \": \" + this, ioe);\n        }\n      }\n    }\n\n","sourceOld":"    /** IndexInput methods */\n    @Override\n    protected void readInternal(byte[] b, int offset, int len)\n         throws IOException {\n      synchronized (file) {\n        long position = off + getFilePointer();\n        if (position != file.position) {\n          file.seek(position);\n          file.position = position;\n        }\n        int total = 0;\n\n        if (position + len > end) {\n          throw new EOFException(\"read past EOF: \" + this);\n        }\n\n        try {\n          do {\n            final int readLength;\n            if (total + chunkSize > len) {\n              readLength = len - total;\n            } else {\n              // LUCENE-1566 - work around JVM Bug by breaking very large reads into chunks\n              readLength = chunkSize;\n            }\n            final int i = file.read(b, offset + total, readLength);\n            file.position += i;\n            total += i;\n          } while (total < len);\n        } catch (OutOfMemoryError e) {\n          // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n          // with a large chunk size in the fast path.\n          final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n          outOfMemoryError.initCause(e);\n          throw outOfMemoryError;\n        } catch (IOException ioe) {\n          throw new IOException(ioe.getMessage() + \": \" + this, ioe);\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ed4c1297d7566d06b37bf0571ead16ec3cb9f8a7","date":1375992438,"type":3,"author":"Grant Ingersoll","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/store/SimpleFSDirectory.SimpleFSIndexInput#readInternal(byte[],int,int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/store/SimpleFSDirectory.SimpleFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":"    /** IndexInput methods */\n    @Override\n    protected void readInternal(byte[] b, int offset, int len)\n         throws IOException {\n      synchronized (file) {\n        long position = off + getFilePointer();\n        file.seek(position);\n        int total = 0;\n\n        if (position + len > end) {\n          throw new EOFException(\"read past EOF: \" + this);\n        }\n\n        try {\n          do {\n            final int readLength;\n            if (total + chunkSize > len) {\n              readLength = len - total;\n            } else {\n              // LUCENE-1566 - work around JVM Bug by breaking very large reads into chunks\n              readLength = chunkSize;\n            }\n            final int i = file.read(b, offset + total, readLength);\n            if (i < 0){//be defensive here, even though we checked before hand, something could have changed\n             throw new EOFException(\"read past EOF: \" + this + \" off: \" + offset + \" len: \" + len + \" total: \" + total + \" readLen: \" + readLength + \" end: \" + end);\n            }\n            total += i;\n          } while (total < len);\n        } catch (OutOfMemoryError e) {\n          // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n          // with a large chunk size in the fast path.\n          final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n          outOfMemoryError.initCause(e);\n          throw outOfMemoryError;\n        } catch (IOException ioe) {\n          throw new IOException(ioe.getMessage() + \": \" + this, ioe);\n        }\n      }\n    }\n\n","sourceOld":"    /** IndexInput methods */\n    @Override\n    protected void readInternal(byte[] b, int offset, int len)\n         throws IOException {\n      synchronized (file) {\n        long position = off + getFilePointer();\n        file.seek(position);\n        int total = 0;\n\n        if (position + len > end) {\n          throw new EOFException(\"read past EOF: \" + this);\n        }\n\n        try {\n          do {\n            final int readLength;\n            if (total + chunkSize > len) {\n              readLength = len - total;\n            } else {\n              // LUCENE-1566 - work around JVM Bug by breaking very large reads into chunks\n              readLength = chunkSize;\n            }\n            final int i = file.read(b, offset + total, readLength);\n            total += i;\n          } while (total < len);\n        } catch (OutOfMemoryError e) {\n          // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n          // with a large chunk size in the fast path.\n          final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n          outOfMemoryError.initCause(e);\n          throw outOfMemoryError;\n        } catch (IOException ioe) {\n          throw new IOException(ioe.getMessage() + \": \" + this, ioe);\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0dcf83b1542b01ad5cddb41b01dc51f751215919","date":1376231222,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/store/SimpleFSDirectory.SimpleFSIndexInput#readInternal(byte[],int,int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/store/SimpleFSDirectory.SimpleFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":"    /** IndexInput methods */\n    @Override\n    protected void readInternal(byte[] b, int offset, int len)\n         throws IOException {\n      synchronized (file) {\n        long position = off + getFilePointer();\n        file.seek(position);\n        int total = 0;\n\n        if (position + len > end) {\n          throw new EOFException(\"read past EOF: \" + this);\n        }\n\n        try {\n          while (total < len) {\n            final int toRead = Math.min(CHUNK_SIZE, len - total);\n            final int i = file.read(b, offset + total, toRead);\n            if (i < 0) { // be defensive here, even though we checked before hand, something could have changed\n             throw new EOFException(\"read past EOF: \" + this + \" off: \" + offset + \" len: \" + len + \" total: \" + total + \" chunkLen: \" + toRead + \" end: \" + end);\n            }\n            assert i > 0 : \"RandomAccessFile.read with non zero-length toRead must always read at least one byte\";\n            total += i;\n          }\n          assert total == len;\n        } catch (IOException ioe) {\n          throw new IOException(ioe.getMessage() + \": \" + this, ioe);\n        }\n      }\n    }\n\n","sourceOld":"    /** IndexInput methods */\n    @Override\n    protected void readInternal(byte[] b, int offset, int len)\n         throws IOException {\n      synchronized (file) {\n        long position = off + getFilePointer();\n        file.seek(position);\n        int total = 0;\n\n        if (position + len > end) {\n          throw new EOFException(\"read past EOF: \" + this);\n        }\n\n        try {\n          do {\n            final int readLength;\n            if (total + chunkSize > len) {\n              readLength = len - total;\n            } else {\n              // LUCENE-1566 - work around JVM Bug by breaking very large reads into chunks\n              readLength = chunkSize;\n            }\n            final int i = file.read(b, offset + total, readLength);\n            if (i < 0){//be defensive here, even though we checked before hand, something could have changed\n             throw new EOFException(\"read past EOF: \" + this + \" off: \" + offset + \" len: \" + len + \" total: \" + total + \" readLen: \" + readLength + \" end: \" + end);\n            }\n            total += i;\n          } while (total < len);\n        } catch (OutOfMemoryError e) {\n          // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n          // with a large chunk size in the fast path.\n          final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n          outOfMemoryError.initCause(e);\n          throw outOfMemoryError;\n        } catch (IOException ioe) {\n          throw new IOException(ioe.getMessage() + \": \" + this, ioe);\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","date":1376366778,"type":3,"author":"Han Jiang","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/store/SimpleFSDirectory.SimpleFSIndexInput#readInternal(byte[],int,int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/store/SimpleFSDirectory.SimpleFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":"    /** IndexInput methods */\n    @Override\n    protected void readInternal(byte[] b, int offset, int len)\n         throws IOException {\n      synchronized (file) {\n        long position = off + getFilePointer();\n        file.seek(position);\n        int total = 0;\n\n        if (position + len > end) {\n          throw new EOFException(\"read past EOF: \" + this);\n        }\n\n        try {\n          while (total < len) {\n            final int toRead = Math.min(CHUNK_SIZE, len - total);\n            final int i = file.read(b, offset + total, toRead);\n            if (i < 0) { // be defensive here, even though we checked before hand, something could have changed\n             throw new EOFException(\"read past EOF: \" + this + \" off: \" + offset + \" len: \" + len + \" total: \" + total + \" chunkLen: \" + toRead + \" end: \" + end);\n            }\n            assert i > 0 : \"RandomAccessFile.read with non zero-length toRead must always read at least one byte\";\n            total += i;\n          }\n          assert total == len;\n        } catch (IOException ioe) {\n          throw new IOException(ioe.getMessage() + \": \" + this, ioe);\n        }\n      }\n    }\n\n","sourceOld":"    /** IndexInput methods */\n    @Override\n    protected void readInternal(byte[] b, int offset, int len)\n         throws IOException {\n      synchronized (file) {\n        long position = off + getFilePointer();\n        file.seek(position);\n        int total = 0;\n\n        if (position + len > end) {\n          throw new EOFException(\"read past EOF: \" + this);\n        }\n\n        try {\n          do {\n            final int readLength;\n            if (total + chunkSize > len) {\n              readLength = len - total;\n            } else {\n              // LUCENE-1566 - work around JVM Bug by breaking very large reads into chunks\n              readLength = chunkSize;\n            }\n            final int i = file.read(b, offset + total, readLength);\n            total += i;\n          } while (total < len);\n        } catch (OutOfMemoryError e) {\n          // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n          // with a large chunk size in the fast path.\n          final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n          outOfMemoryError.initCause(e);\n          throw outOfMemoryError;\n        } catch (IOException ioe) {\n          throw new IOException(ioe.getMessage() + \": \" + this, ioe);\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"22c378dfd7d922a48b48093933404133eb253fe7","date":1410711399,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/store/SimpleFSDirectory.SimpleFSIndexInput#readInternal(byte[],int,int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/store/SimpleFSDirectory.SimpleFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        bb = byteBuf;\n        byteBuf.clear().position(offset);\n      } else {\n        bb = ByteBuffer.wrap(b, offset, len);\n      }\n\n      synchronized(channel) {\n        long pos = getFilePointer() + off;\n        \n        if (pos + len > end) {\n          throw new EOFException(\"read past EOF: \" + this);\n        }\n               \n        try {\n          channel.position(pos);\n\n          int readLength = len;\n          while (readLength > 0) {\n            final int toRead = Math.min(CHUNK_SIZE, readLength);\n            bb.limit(bb.position() + toRead);\n            assert bb.remaining() == toRead;\n            final int i = channel.read(bb);\n            if (i < 0) { // be defensive here, even though we checked before hand, something could have changed\n              throw new EOFException(\"read past EOF: \" + this + \" off: \" + offset + \" len: \" + len + \" pos: \" + pos + \" chunkLen: \" + toRead + \" end: \" + end);\n            }\n            assert i > 0 : \"SeekableByteChannel.read with non zero-length bb.remaining() must always read at least one byte (Channel is in blocking mode, see spec of ReadableByteChannel)\";\n            pos += i;\n            readLength -= i;\n          }\n          assert readLength == 0;\n        } catch (IOException ioe) {\n          throw new IOException(ioe.getMessage() + \": \" + this, ioe);\n        }\n      }\n    }\n\n","sourceOld":"    /** IndexInput methods */\n    @Override\n    protected void readInternal(byte[] b, int offset, int len)\n         throws IOException {\n      synchronized (file) {\n        long position = off + getFilePointer();\n        file.seek(position);\n        int total = 0;\n\n        if (position + len > end) {\n          throw new EOFException(\"read past EOF: \" + this);\n        }\n\n        try {\n          while (total < len) {\n            final int toRead = Math.min(CHUNK_SIZE, len - total);\n            final int i = file.read(b, offset + total, toRead);\n            if (i < 0) { // be defensive here, even though we checked before hand, something could have changed\n             throw new EOFException(\"read past EOF: \" + this + \" off: \" + offset + \" len: \" + len + \" total: \" + total + \" chunkLen: \" + toRead + \" end: \" + end);\n            }\n            assert i > 0 : \"RandomAccessFile.read with non zero-length toRead must always read at least one byte\";\n            total += i;\n          }\n          assert total == len;\n        } catch (IOException ioe) {\n          throw new IOException(ioe.getMessage() + \": \" + this, ioe);\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"09ae13d58e10d8dc818c75db483ed8b53c725f1d","date":1583449465,"type":4,"author":"Yannick Welsch","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/store/SimpleFSDirectory.SimpleFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":null,"sourceOld":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        bb = byteBuf;\n        byteBuf.clear().position(offset);\n      } else {\n        bb = ByteBuffer.wrap(b, offset, len);\n      }\n\n      synchronized(channel) {\n        long pos = getFilePointer() + off;\n        \n        if (pos + len > end) {\n          throw new EOFException(\"read past EOF: \" + this);\n        }\n               \n        try {\n          channel.position(pos);\n\n          int readLength = len;\n          while (readLength > 0) {\n            final int toRead = Math.min(CHUNK_SIZE, readLength);\n            bb.limit(bb.position() + toRead);\n            assert bb.remaining() == toRead;\n            final int i = channel.read(bb);\n            if (i < 0) { // be defensive here, even though we checked before hand, something could have changed\n              throw new EOFException(\"read past EOF: \" + this + \" off: \" + offset + \" len: \" + len + \" pos: \" + pos + \" chunkLen: \" + toRead + \" end: \" + end);\n            }\n            assert i > 0 : \"SeekableByteChannel.read with non zero-length bb.remaining() must always read at least one byte (Channel is in blocking mode, see spec of ReadableByteChannel)\";\n            pos += i;\n            readLength -= i;\n          }\n          assert readLength == 0;\n        } catch (IOException ioe) {\n          throw new IOException(ioe.getMessage() + \": \" + this, ioe);\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9ca2a57dbb0665332572cf1a0dffa67234089615","date":1583491342,"type":4,"author":"Robert Muir","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/store/SimpleFSDirectory.SimpleFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":null,"sourceOld":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        bb = byteBuf;\n        byteBuf.clear().position(offset);\n      } else {\n        bb = ByteBuffer.wrap(b, offset, len);\n      }\n\n      synchronized(channel) {\n        long pos = getFilePointer() + off;\n        \n        if (pos + len > end) {\n          throw new EOFException(\"read past EOF: \" + this);\n        }\n               \n        try {\n          channel.position(pos);\n\n          int readLength = len;\n          while (readLength > 0) {\n            final int toRead = Math.min(CHUNK_SIZE, readLength);\n            bb.limit(bb.position() + toRead);\n            assert bb.remaining() == toRead;\n            final int i = channel.read(bb);\n            if (i < 0) { // be defensive here, even though we checked before hand, something could have changed\n              throw new EOFException(\"read past EOF: \" + this + \" off: \" + offset + \" len: \" + len + \" pos: \" + pos + \" chunkLen: \" + toRead + \" end: \" + end);\n            }\n            assert i > 0 : \"SeekableByteChannel.read with non zero-length bb.remaining() must always read at least one byte (Channel is in blocking mode, see spec of ReadableByteChannel)\";\n            pos += i;\n            readLength -= i;\n          }\n          assert readLength == 0;\n        } catch (IOException ioe) {\n          throw new IOException(ioe.getMessage() + \": \" + this, ioe);\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"9ca2a57dbb0665332572cf1a0dffa67234089615":["22c378dfd7d922a48b48093933404133eb253fe7","09ae13d58e10d8dc818c75db483ed8b53c725f1d"],"09ae13d58e10d8dc818c75db483ed8b53c725f1d":["22c378dfd7d922a48b48093933404133eb253fe7"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"22c378dfd7d922a48b48093933404133eb253fe7":["0dcf83b1542b01ad5cddb41b01dc51f751215919"],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":["c50a88b580d08fc67d9a94b8b32b9d07a5972f8c"],"0dcf83b1542b01ad5cddb41b01dc51f751215919":["ed4c1297d7566d06b37bf0571ead16ec3cb9f8a7"],"ed4c1297d7566d06b37bf0571ead16ec3cb9f8a7":["c50a88b580d08fc67d9a94b8b32b9d07a5972f8c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9ca2a57dbb0665332572cf1a0dffa67234089615"],"c50a88b580d08fc67d9a94b8b32b9d07a5972f8c":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"9ca2a57dbb0665332572cf1a0dffa67234089615":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"09ae13d58e10d8dc818c75db483ed8b53c725f1d":["9ca2a57dbb0665332572cf1a0dffa67234089615"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["c50a88b580d08fc67d9a94b8b32b9d07a5972f8c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"22c378dfd7d922a48b48093933404133eb253fe7":["9ca2a57dbb0665332572cf1a0dffa67234089615","09ae13d58e10d8dc818c75db483ed8b53c725f1d"],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":[],"0dcf83b1542b01ad5cddb41b01dc51f751215919":["22c378dfd7d922a48b48093933404133eb253fe7"],"ed4c1297d7566d06b37bf0571ead16ec3cb9f8a7":["0dcf83b1542b01ad5cddb41b01dc51f751215919"],"c50a88b580d08fc67d9a94b8b32b9d07a5972f8c":["8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","ed4c1297d7566d06b37bf0571ead16ec3cb9f8a7"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}