{"path":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsNoDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsNoDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#copyFieldsNoDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","sourceNew":"  private int copyFieldsNoDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                    final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    final int maxDoc = reader.maxDoc();\n    int docCount = 0;\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      while (docCount < maxDoc) {\n        int len = Math.min(MAX_RAW_MERGE_DOCS, maxDoc - docCount);\n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, docCount, len);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, len);\n        docCount += len;\n        checkAbort.work(300 * len);\n      }\n    } else {\n      for (; docCount < maxDoc; docCount++) {\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(docCount);\n        fieldsWriter.addDocument(doc);\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","sourceOld":"  private int copyFieldsNoDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                    final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    final int maxDoc = reader.maxDoc();\n    int docCount = 0;\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      while (docCount < maxDoc) {\n        int len = Math.min(MAX_RAW_MERGE_DOCS, maxDoc - docCount);\n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, docCount, len);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, len);\n        docCount += len;\n        checkAbort.work(300 * len);\n      }\n    } else {\n      for (; docCount < maxDoc; docCount++) {\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(docCount);\n        fieldsWriter.addDocument(doc);\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b0c7a8f7304b75b1528814c5820fa23a96816c27","date":1298314239,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsNoDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsNoDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","sourceNew":"  private int copyFieldsNoDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                    final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    final int maxDoc = reader.maxDoc();\n    int docCount = 0;\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      while (docCount < maxDoc) {\n        int len = Math.min(MAX_RAW_MERGE_DOCS, maxDoc - docCount);\n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, docCount, len);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, len);\n        docCount += len;\n        checkAbort.work(300 * len);\n      }\n    } else {\n      for (; docCount < maxDoc; docCount++) {\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(docCount);\n        fieldsWriter.addDocument(doc, fieldInfos);\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","sourceOld":"  private int copyFieldsNoDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                    final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    final int maxDoc = reader.maxDoc();\n    int docCount = 0;\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      while (docCount < maxDoc) {\n        int len = Math.min(MAX_RAW_MERGE_DOCS, maxDoc - docCount);\n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, docCount, len);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, len);\n        docCount += len;\n        checkAbort.work(300 * len);\n      }\n    } else {\n      for (; docCount < maxDoc; docCount++) {\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(docCount);\n        fieldsWriter.addDocument(doc);\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bde51b089eb7f86171eb3406e38a274743f9b7ac","date":1298336439,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsNoDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsNoDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","sourceNew":"  private int copyFieldsNoDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                    final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    final int maxDoc = reader.maxDoc();\n    int docCount = 0;\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      while (docCount < maxDoc) {\n        int len = Math.min(MAX_RAW_MERGE_DOCS, maxDoc - docCount);\n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, docCount, len);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, len);\n        docCount += len;\n        checkAbort.work(300 * len);\n      }\n    } else {\n      for (; docCount < maxDoc; docCount++) {\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(docCount);\n        fieldsWriter.addDocument(doc, fieldInfos);\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","sourceOld":"  private int copyFieldsNoDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                    final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    final int maxDoc = reader.maxDoc();\n    int docCount = 0;\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      while (docCount < maxDoc) {\n        int len = Math.min(MAX_RAW_MERGE_DOCS, maxDoc - docCount);\n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, docCount, len);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, len);\n        docCount += len;\n        checkAbort.work(300 * len);\n      }\n    } else {\n      for (; docCount < maxDoc; docCount++) {\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(docCount);\n        fieldsWriter.addDocument(doc);\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"14ec33385f6fbb6ce172882d14605790418a5d31","date":1298910796,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsNoDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsNoDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","sourceNew":"  private int copyFieldsNoDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                    final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    final int maxDoc = reader.maxDoc();\n    int docCount = 0;\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      while (docCount < maxDoc) {\n        int len = Math.min(MAX_RAW_MERGE_DOCS, maxDoc - docCount);\n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, docCount, len);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, len);\n        docCount += len;\n        checkAbort.work(300 * len);\n      }\n    } else {\n      for (; docCount < maxDoc; docCount++) {\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(docCount);\n        fieldsWriter.addDocument(doc);\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","sourceOld":"  private int copyFieldsNoDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                    final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    final int maxDoc = reader.maxDoc();\n    int docCount = 0;\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      while (docCount < maxDoc) {\n        int len = Math.min(MAX_RAW_MERGE_DOCS, maxDoc - docCount);\n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, docCount, len);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, len);\n        docCount += len;\n        checkAbort.work(300 * len);\n      }\n    } else {\n      for (; docCount < maxDoc; docCount++) {\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(docCount);\n        fieldsWriter.addDocument(doc, fieldInfos);\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1224a4027481acce15495b03bce9b48b93b42722","date":1300792329,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsNoDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsNoDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","sourceNew":"  private int copyFieldsNoDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                    final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    final int maxDoc = reader.maxDoc();\n    int docCount = 0;\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      while (docCount < maxDoc) {\n        int len = Math.min(MAX_RAW_MERGE_DOCS, maxDoc - docCount);\n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, docCount, len);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, len);\n        docCount += len;\n        checkAbort.work(300 * len);\n      }\n    } else {\n      for (; docCount < maxDoc; docCount++) {\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(docCount);\n        fieldsWriter.addDocument(doc, fieldInfos);\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","sourceOld":"  private int copyFieldsNoDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                    final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    final int maxDoc = reader.maxDoc();\n    int docCount = 0;\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      while (docCount < maxDoc) {\n        int len = Math.min(MAX_RAW_MERGE_DOCS, maxDoc - docCount);\n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, docCount, len);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, len);\n        docCount += len;\n        checkAbort.work(300 * len);\n      }\n    } else {\n      for (; docCount < maxDoc; docCount++) {\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(docCount);\n        fieldsWriter.addDocument(doc);\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d619839baa8ce5503e496b94a9e42ad6f079293f","date":1301309428,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsNoDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsNoDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","sourceNew":"  private int copyFieldsNoDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                    final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    final int maxDoc = reader.maxDoc();\n    int docCount = 0;\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      while (docCount < maxDoc) {\n        int len = Math.min(MAX_RAW_MERGE_DOCS, maxDoc - docCount);\n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, docCount, len);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, len);\n        docCount += len;\n        checkAbort.work(300 * len);\n      }\n    } else {\n      for (; docCount < maxDoc; docCount++) {\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(docCount);\n        fieldsWriter.addDocument(doc, fieldInfos);\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","sourceOld":"  private int copyFieldsNoDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                    final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    final int maxDoc = reader.maxDoc();\n    int docCount = 0;\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      while (docCount < maxDoc) {\n        int len = Math.min(MAX_RAW_MERGE_DOCS, maxDoc - docCount);\n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, docCount, len);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, len);\n        docCount += len;\n        checkAbort.work(300 * len);\n      }\n    } else {\n      for (; docCount < maxDoc; docCount++) {\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(docCount);\n        fieldsWriter.addDocument(doc);\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0061262413ecc163d6eebba1b5c43ab91a0c2dc5","date":1311195279,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsNoDeletions(FieldsWriter,MergeState.IndexReaderAndLiveDocs,FieldsReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsNoDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","sourceNew":"  private int copyFieldsNoDeletions(final FieldsWriter fieldsWriter, final MergeState.IndexReaderAndLiveDocs reader,\n                                    final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    final int maxDoc = reader.reader.maxDoc();\n    int docCount = 0;\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      while (docCount < maxDoc) {\n        int len = Math.min(MAX_RAW_MERGE_DOCS, maxDoc - docCount);\n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, docCount, len);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, len);\n        docCount += len;\n        checkAbort.work(300 * len);\n      }\n    } else {\n      for (; docCount < maxDoc; docCount++) {\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.reader.document(docCount);\n        fieldsWriter.addDocument(doc, fieldInfos);\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","sourceOld":"  private int copyFieldsNoDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                    final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    final int maxDoc = reader.maxDoc();\n    int docCount = 0;\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      while (docCount < maxDoc) {\n        int len = Math.min(MAX_RAW_MERGE_DOCS, maxDoc - docCount);\n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, docCount, len);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, len);\n        docCount += len;\n        checkAbort.work(300 * len);\n      }\n    } else {\n      for (; docCount < maxDoc; docCount++) {\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(docCount);\n        fieldsWriter.addDocument(doc, fieldInfos);\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"0061262413ecc163d6eebba1b5c43ab91a0c2dc5":["1224a4027481acce15495b03bce9b48b93b42722"],"14ec33385f6fbb6ce172882d14605790418a5d31":["b0c7a8f7304b75b1528814c5820fa23a96816c27"],"d619839baa8ce5503e496b94a9e42ad6f079293f":["9454a6510e2db155fb01faa5c049b06ece95fab9","1224a4027481acce15495b03bce9b48b93b42722"],"b0c7a8f7304b75b1528814c5820fa23a96816c27":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["9454a6510e2db155fb01faa5c049b06ece95fab9","b0c7a8f7304b75b1528814c5820fa23a96816c27"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["0061262413ecc163d6eebba1b5c43ab91a0c2dc5"],"1224a4027481acce15495b03bce9b48b93b42722":["14ec33385f6fbb6ce172882d14605790418a5d31"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"0061262413ecc163d6eebba1b5c43ab91a0c2dc5":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"14ec33385f6fbb6ce172882d14605790418a5d31":["1224a4027481acce15495b03bce9b48b93b42722"],"d619839baa8ce5503e496b94a9e42ad6f079293f":[],"b0c7a8f7304b75b1528814c5820fa23a96816c27":["14ec33385f6fbb6ce172882d14605790418a5d31","bde51b089eb7f86171eb3406e38a274743f9b7ac"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":[],"1224a4027481acce15495b03bce9b48b93b42722":["0061262413ecc163d6eebba1b5c43ab91a0c2dc5","d619839baa8ce5503e496b94a9e42ad6f079293f"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["d619839baa8ce5503e496b94a9e42ad6f079293f","b0c7a8f7304b75b1528814c5820fa23a96816c27","bde51b089eb7f86171eb3406e38a274743f9b7ac"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["d619839baa8ce5503e496b94a9e42ad6f079293f","bde51b089eb7f86171eb3406e38a274743f9b7ac","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}