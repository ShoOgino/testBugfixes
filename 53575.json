{"path":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","commits":[{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","sourceNew":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n    DocValues.Type[] dvTypes = new DocValues.Type[]{\n        DocValues.Type.BYTES_VAR_STRAIGHT,\n        DocValues.Type.BYTES_VAR_SORTED\n    };\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    boolean canUseDV = !\"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n    DocValues.Type dvType = canUseDV ? dvTypes[random.nextInt(dvTypes.length)] : null;\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<String>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<String, Map<String, Set<String>>>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<String, Set<String>>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<String>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new Field(\"id\", String.format(\"%09d\", i), StringField.TYPE_UNSTORED));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue, dvType);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue, dvType);\n      }\n      doc.add(new Field(\"content\", content, TextField.TYPE_UNSTORED));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    w.close();\n    return new IndexContext(dir, reader, dvType, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","sourceOld":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n    DocValues.Type[] dvTypes = new DocValues.Type[]{\n        DocValues.Type.BYTES_VAR_STRAIGHT,\n        DocValues.Type.BYTES_VAR_SORTED\n    };\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    boolean canUseDV = !\"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n    DocValues.Type dvType = canUseDV ? dvTypes[random.nextInt(dvTypes.length)] : null;\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<String>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<String, Map<String, Set<String>>>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<String, Set<String>>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<String>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new Field(\"id\", String.format(\"%09d\", i), StringField.TYPE_UNSTORED));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue, dvType);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue, dvType);\n      }\n      doc.add(new Field(\"content\", content, TextField.TYPE_UNSTORED));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    w.close();\n    return new IndexContext(dir, reader, dvType, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"57ae3024996ccdb3c36c42cb890e1efb37df4ce8","date":1338343651,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","sourceNew":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n    DocValues.Type[] dvTypes = new DocValues.Type[]{\n        DocValues.Type.BYTES_VAR_STRAIGHT,\n        DocValues.Type.BYTES_VAR_SORTED\n    };\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    boolean canUseDV = true;\n    DocValues.Type dvType = canUseDV ? dvTypes[random.nextInt(dvTypes.length)] : null;\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<String>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<String, Map<String, Set<String>>>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<String, Set<String>>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<String>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new Field(\"id\", String.format(\"%09d\", i), StringField.TYPE_UNSTORED));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue, dvType);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue, dvType);\n      }\n      doc.add(new Field(\"content\", content, TextField.TYPE_UNSTORED));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    w.close();\n    return new IndexContext(dir, reader, dvType, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","sourceOld":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n    DocValues.Type[] dvTypes = new DocValues.Type[]{\n        DocValues.Type.BYTES_VAR_STRAIGHT,\n        DocValues.Type.BYTES_VAR_SORTED\n    };\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    boolean canUseDV = !\"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n    DocValues.Type dvType = canUseDV ? dvTypes[random.nextInt(dvTypes.length)] : null;\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<String>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<String, Map<String, Set<String>>>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<String, Set<String>>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<String>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new Field(\"id\", String.format(\"%09d\", i), StringField.TYPE_UNSTORED));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue, dvType);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue, dvType);\n      }\n      doc.add(new Field(\"content\", content, TextField.TYPE_UNSTORED));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    w.close();\n    return new IndexContext(dir, reader, dvType, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"04f07771a2a7dd3a395700665ed839c3dae2def2","date":1339350139,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","sourceNew":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n    DocValues.Type[] dvTypes = new DocValues.Type[]{\n        DocValues.Type.BYTES_VAR_STRAIGHT,\n        DocValues.Type.BYTES_VAR_SORTED\n    };\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    boolean canUseDV = true;\n    DocValues.Type dvType = canUseDV ? dvTypes[random.nextInt(dvTypes.length)] : null;\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<String>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<String, Map<String, Set<String>>>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<String, Set<String>>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<String>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", String.format(\"%09d\", i), Field.Store.NO));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue, dvType);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue, dvType);\n      }\n      doc.add(new TextField(\"content\", content, Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    w.close();\n    return new IndexContext(dir, reader, dvType, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","sourceOld":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n    DocValues.Type[] dvTypes = new DocValues.Type[]{\n        DocValues.Type.BYTES_VAR_STRAIGHT,\n        DocValues.Type.BYTES_VAR_SORTED\n    };\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    boolean canUseDV = true;\n    DocValues.Type dvType = canUseDV ? dvTypes[random.nextInt(dvTypes.length)] : null;\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<String>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<String, Map<String, Set<String>>>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<String, Set<String>>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<String>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new Field(\"id\", String.format(\"%09d\", i), StringField.TYPE_UNSTORED));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue, dvType);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue, dvType);\n      }\n      doc.add(new Field(\"content\", content, TextField.TYPE_UNSTORED));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    w.close();\n    return new IndexContext(dir, reader, dvType, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","bugFix":["166661dd25a09458b128e5c0b86e3b762a6ded68"],"bugIntro":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b4c18e3a5a8908e0fa2ea7c1a3507a214b70153b","date":1341673943,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","sourceNew":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n    DocValues.Type[] dvTypes = new DocValues.Type[]{\n        DocValues.Type.BYTES_VAR_STRAIGHT,\n        DocValues.Type.BYTES_VAR_SORTED\n    };\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    boolean canUseDV = true;\n    DocValues.Type dvType = canUseDV ? dvTypes[random.nextInt(dvTypes.length)] : null;\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<String>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<String, Map<String, Set<String>>>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<String, Set<String>>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<String>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", String.format(Locale.ROOT, \"%09d\", i), Field.Store.NO));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue, dvType);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue, dvType);\n      }\n      doc.add(new TextField(\"content\", content, Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    w.close();\n    return new IndexContext(dir, reader, dvType, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","sourceOld":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n    DocValues.Type[] dvTypes = new DocValues.Type[]{\n        DocValues.Type.BYTES_VAR_STRAIGHT,\n        DocValues.Type.BYTES_VAR_SORTED\n    };\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    boolean canUseDV = true;\n    DocValues.Type dvType = canUseDV ? dvTypes[random.nextInt(dvTypes.length)] : null;\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<String>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<String, Map<String, Set<String>>>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<String, Set<String>>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<String>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", String.format(\"%09d\", i), Field.Store.NO));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue, dvType);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue, dvType);\n      }\n      doc.add(new TextField(\"content\", content, Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    w.close();\n    return new IndexContext(dir, reader, dvType, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","bugFix":null,"bugIntro":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2acf500f78aa12b92e371fd89c719291986b6b90","date":1341846236,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","sourceNew":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n    DocValues.Type[] dvTypes = new DocValues.Type[]{\n        DocValues.Type.BYTES_VAR_STRAIGHT,\n        DocValues.Type.BYTES_VAR_SORTED\n    };\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    boolean canUseDV = true;\n    DocValues.Type dvType = canUseDV ? dvTypes[random.nextInt(dvTypes.length)] : null;\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<String>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<String, Map<String, Set<String>>>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<String, Set<String>>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<String>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", String.format(Locale.ROOT, \"%09d\", i), Field.Store.NO));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue, dvType);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue, dvType);\n      }\n      doc.add(new TextField(\"content\", content, Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    w.close();\n    return new IndexContext(dir, reader, dvType, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","sourceOld":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n    DocValues.Type[] dvTypes = new DocValues.Type[]{\n        DocValues.Type.BYTES_VAR_STRAIGHT,\n        DocValues.Type.BYTES_VAR_SORTED\n    };\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    boolean canUseDV = true;\n    DocValues.Type dvType = canUseDV ? dvTypes[random.nextInt(dvTypes.length)] : null;\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<String>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<String, Map<String, Set<String>>>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<String, Set<String>>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<String>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", String.format(\"%09d\", i), Field.Store.NO));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue, dvType);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue, dvType);\n      }\n      doc.add(new TextField(\"content\", content, Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    w.close();\n    return new IndexContext(dir, reader, dvType, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"46d8ada1fff8d18cb197c38c7983225162599948","date":1341853497,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","sourceNew":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n    DocValues.Type[] dvTypes = new DocValues.Type[]{\n        DocValues.Type.BYTES_VAR_STRAIGHT,\n        DocValues.Type.BYTES_VAR_SORTED\n    };\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    boolean canUseDV = true;\n    DocValues.Type dvType = canUseDV ? dvTypes[random.nextInt(dvTypes.length)] : null;\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<String>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<String, Map<String, Set<String>>>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<String, Set<String>>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<String>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", String.format(Locale.ROOT, \"%09d\", i), Field.Store.NO));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue, dvType);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue, dvType);\n      }\n      doc.add(new TextField(\"content\", content, Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    w.close();\n    return new IndexContext(dir, reader, dvType, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","sourceOld":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n    DocValues.Type[] dvTypes = new DocValues.Type[]{\n        DocValues.Type.BYTES_VAR_STRAIGHT,\n        DocValues.Type.BYTES_VAR_SORTED\n    };\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    boolean canUseDV = true;\n    DocValues.Type dvType = canUseDV ? dvTypes[random.nextInt(dvTypes.length)] : null;\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<String>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<String, Map<String, Set<String>>>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<String, Set<String>>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<String>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", String.format(\"%09d\", i), Field.Store.NO));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue, dvType);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue, dvType);\n      }\n      doc.add(new TextField(\"content\", content, Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    w.close();\n    return new IndexContext(dir, reader, dvType, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe33227f6805edab2036cbb80645cc4e2d1fa424","date":1342713534,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","sourceNew":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n    DocValues.Type[] dvTypes = new DocValues.Type[]{\n        DocValues.Type.BYTES_VAR_STRAIGHT,\n        DocValues.Type.BYTES_VAR_SORTED\n    };\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    boolean canUseDV = true;\n    DocValues.Type dvType = canUseDV ? dvTypes[random.nextInt(dvTypes.length)] : null;\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<String>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<String, Map<String, Set<String>>>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<String, Set<String>>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<String>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", String.format(Locale.ROOT, \"%09d\", i), Field.Store.NO));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue, dvType);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue, dvType);\n      }\n      doc.add(new TextField(\"content\", content, Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    w.close();\n    return new IndexContext(dir, reader, dvType, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","sourceOld":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n    DocValues.Type[] dvTypes = new DocValues.Type[]{\n        DocValues.Type.BYTES_VAR_STRAIGHT,\n        DocValues.Type.BYTES_VAR_SORTED\n    };\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    boolean canUseDV = true;\n    DocValues.Type dvType = canUseDV ? dvTypes[random.nextInt(dvTypes.length)] : null;\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<String>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<String, Map<String, Set<String>>>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<String, Set<String>>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<String>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", String.format(\"%09d\", i), Field.Store.NO));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue, dvType);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue, dvType);\n      }\n      doc.add(new TextField(\"content\", content, Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    w.close();\n    return new IndexContext(dir, reader, dvType, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"854f97cd3613b9579fba83755c80b697e2f3993f","date":1353527621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","sourceNew":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n    DocValues.Type[] dvTypes = new DocValues.Type[]{\n        DocValues.Type.BYTES_VAR_STRAIGHT,\n        DocValues.Type.BYTES_VAR_SORTED\n    };\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    boolean canUseDV = true;\n    DocValues.Type dvType = canUseDV ? dvTypes[random.nextInt(dvTypes.length)] : null;\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<String>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<String, Map<String, Set<String>>>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<String, Set<String>>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<String>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", String.format(Locale.ROOT, \"%09d\", i), Field.Store.YES));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue, dvType);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue, dvType);\n      }\n      doc.add(new TextField(\"content\", content, Field.Store.YES));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    if (VERBOSE) {\n      for(int docID=0;docID<reader.maxDoc();docID++) {\n        StoredDocument doc = reader.document(docID);\n        System.out.println(\"docID=\" + docID + \" id=\" + doc.get(\"id\") + \" content=\" + doc.get(\"content\") + \" author=\" + doc.get(\"author\") + \" publisher=\" + doc.get(\"publisher\"));\n      }\n    }\n\n    w.close();\n    return new IndexContext(dir, reader, dvType, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","sourceOld":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n    DocValues.Type[] dvTypes = new DocValues.Type[]{\n        DocValues.Type.BYTES_VAR_STRAIGHT,\n        DocValues.Type.BYTES_VAR_SORTED\n    };\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    boolean canUseDV = true;\n    DocValues.Type dvType = canUseDV ? dvTypes[random.nextInt(dvTypes.length)] : null;\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<String>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<String, Map<String, Set<String>>>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<String, Set<String>>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<String>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", String.format(Locale.ROOT, \"%09d\", i), Field.Store.NO));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue, dvType);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue, dvType);\n      }\n      doc.add(new TextField(\"content\", content, Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    w.close();\n    return new IndexContext(dir, reader, dvType, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0837ab0472feecb3a54260729d845f839e1cbd72","date":1358283639,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","sourceNew":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n    DocValuesType[] dvTypes = new DocValuesType[]{\n        DocValuesType.BINARY,\n        DocValuesType.SORTED\n    };\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    boolean canUseDV = true;\n    DocValuesType dvType = canUseDV ? dvTypes[random.nextInt(dvTypes.length)] : null;\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<String>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<String, Map<String, Set<String>>>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<String, Set<String>>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<String>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", String.format(Locale.ROOT, \"%09d\", i), Field.Store.YES));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue, dvType);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue, dvType);\n      }\n      doc.add(new TextField(\"content\", content, Field.Store.YES));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    if (VERBOSE) {\n      for(int docID=0;docID<reader.maxDoc();docID++) {\n        StoredDocument doc = reader.document(docID);\n        System.out.println(\"docID=\" + docID + \" id=\" + doc.get(\"id\") + \" content=\" + doc.get(\"content\") + \" author=\" + doc.get(\"author\") + \" publisher=\" + doc.get(\"publisher\"));\n      }\n    }\n\n    w.close();\n    return new IndexContext(dir, reader, dvType, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","sourceOld":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n    DocValues.Type[] dvTypes = new DocValues.Type[]{\n        DocValues.Type.BYTES_VAR_STRAIGHT,\n        DocValues.Type.BYTES_VAR_SORTED\n    };\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    boolean canUseDV = true;\n    DocValues.Type dvType = canUseDV ? dvTypes[random.nextInt(dvTypes.length)] : null;\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<String>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<String, Map<String, Set<String>>>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<String, Set<String>>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<String>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", String.format(Locale.ROOT, \"%09d\", i), Field.Store.YES));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue, dvType);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue, dvType);\n      }\n      doc.add(new TextField(\"content\", content, Field.Store.YES));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    if (VERBOSE) {\n      for(int docID=0;docID<reader.maxDoc();docID++) {\n        StoredDocument doc = reader.document(docID);\n        System.out.println(\"docID=\" + docID + \" id=\" + doc.get(\"id\") + \" content=\" + doc.get(\"content\") + \" author=\" + doc.get(\"author\") + \" publisher=\" + doc.get(\"publisher\"));\n      }\n    }\n\n    w.close();\n    return new IndexContext(dir, reader, dvType, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d4d69c535930b5cce125cff868d40f6373dc27d4","date":1360270101,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","sourceNew":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n    DocValuesType[] dvTypes = new DocValuesType[]{\n        DocValuesType.BINARY,\n        DocValuesType.SORTED\n    };\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    boolean canUseDV = true;\n    DocValuesType dvType = canUseDV ? dvTypes[random.nextInt(dvTypes.length)] : null;\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<String>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<String, Map<String, Set<String>>>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<String, Set<String>>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<String>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", String.format(Locale.ROOT, \"%09d\", i), Field.Store.YES));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue, dvType);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue, dvType);\n      }\n      doc.add(new TextField(\"content\", content, Field.Store.YES));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    if (VERBOSE) {\n      for(int docID=0;docID<reader.maxDoc();docID++) {\n        StoredDocument doc = reader.document(docID);\n        System.out.println(\"docID=\" + docID + \" id=\" + doc.get(\"id\") + \" content=\" + doc.get(\"content\") + \" author=\" + doc.get(\"author\") + \" publisher=\" + doc.get(\"publisher\"));\n      }\n    }\n\n    w.close();\n    return new IndexContext(dir, reader, dvType, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","sourceOld":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n    DocValues.Type[] dvTypes = new DocValues.Type[]{\n        DocValues.Type.BYTES_VAR_STRAIGHT,\n        DocValues.Type.BYTES_VAR_SORTED\n    };\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    boolean canUseDV = true;\n    DocValues.Type dvType = canUseDV ? dvTypes[random.nextInt(dvTypes.length)] : null;\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<String>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<String, Map<String, Set<String>>>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<String, Set<String>>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<String>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", String.format(Locale.ROOT, \"%09d\", i), Field.Store.NO));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue, dvType);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue, dvType);\n      }\n      doc.add(new TextField(\"content\", content, Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    w.close();\n    return new IndexContext(dir, reader, dvType, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","bugFix":["166661dd25a09458b128e5c0b86e3b762a6ded68","b4c18e3a5a8908e0fa2ea7c1a3507a214b70153b","04f07771a2a7dd3a395700665ed839c3dae2def2"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"97d4692d0c601ff773f0a2231967312428a904e4","date":1366026608,"type":3,"author":"Martijn van Groningen","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","sourceNew":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n    DocValuesType[] dvTypes = new DocValuesType[]{\n        DocValuesType.BINARY,\n        DocValuesType.SORTED\n    };\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    boolean canUseDV = true;\n    DocValuesType dvType = canUseDV ? dvTypes[random.nextInt(dvTypes.length)] : null;\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", String.format(Locale.ROOT, \"%09d\", i), Field.Store.YES));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue, dvType);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue, dvType);\n      }\n      doc.add(new TextField(\"content\", content, Field.Store.YES));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    if (VERBOSE) {\n      for(int docID=0;docID<reader.maxDoc();docID++) {\n        StoredDocument doc = reader.document(docID);\n        System.out.println(\"docID=\" + docID + \" id=\" + doc.get(\"id\") + \" content=\" + doc.get(\"content\") + \" author=\" + doc.get(\"author\") + \" publisher=\" + doc.get(\"publisher\"));\n      }\n    }\n\n    w.close();\n    return new IndexContext(dir, reader, dvType, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","sourceOld":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n    DocValuesType[] dvTypes = new DocValuesType[]{\n        DocValuesType.BINARY,\n        DocValuesType.SORTED\n    };\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    boolean canUseDV = true;\n    DocValuesType dvType = canUseDV ? dvTypes[random.nextInt(dvTypes.length)] : null;\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<String>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<String, Map<String, Set<String>>>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<String, Set<String>>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<String>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", String.format(Locale.ROOT, \"%09d\", i), Field.Store.YES));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue, dvType);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue, dvType);\n      }\n      doc.add(new TextField(\"content\", content, Field.Store.YES));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    if (VERBOSE) {\n      for(int docID=0;docID<reader.maxDoc();docID++) {\n        StoredDocument doc = reader.document(docID);\n        System.out.println(\"docID=\" + docID + \" id=\" + doc.get(\"id\") + \" content=\" + doc.get(\"content\") + \" author=\" + doc.get(\"author\") + \" publisher=\" + doc.get(\"publisher\"));\n      }\n    }\n\n    w.close();\n    return new IndexContext(dir, reader, dvType, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","sourceNew":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n    DocValuesType[] dvTypes = new DocValuesType[]{\n        DocValuesType.BINARY,\n        DocValuesType.SORTED\n    };\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    boolean canUseDV = true;\n    DocValuesType dvType = canUseDV ? dvTypes[random.nextInt(dvTypes.length)] : null;\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", String.format(Locale.ROOT, \"%09d\", i), Field.Store.YES));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue, dvType);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue, dvType);\n      }\n      doc.add(new TextField(\"content\", content, Field.Store.YES));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    if (VERBOSE) {\n      for(int docID=0;docID<reader.maxDoc();docID++) {\n        StoredDocument doc = reader.document(docID);\n        System.out.println(\"docID=\" + docID + \" id=\" + doc.get(\"id\") + \" content=\" + doc.get(\"content\") + \" author=\" + doc.get(\"author\") + \" publisher=\" + doc.get(\"publisher\"));\n      }\n    }\n\n    w.shutdown();\n    return new IndexContext(dir, reader, dvType, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","sourceOld":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n    DocValuesType[] dvTypes = new DocValuesType[]{\n        DocValuesType.BINARY,\n        DocValuesType.SORTED\n    };\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    boolean canUseDV = true;\n    DocValuesType dvType = canUseDV ? dvTypes[random.nextInt(dvTypes.length)] : null;\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", String.format(Locale.ROOT, \"%09d\", i), Field.Store.YES));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue, dvType);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue, dvType);\n      }\n      doc.add(new TextField(\"content\", content, Field.Store.YES));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    if (VERBOSE) {\n      for(int docID=0;docID<reader.maxDoc();docID++) {\n        StoredDocument doc = reader.document(docID);\n        System.out.println(\"docID=\" + docID + \" id=\" + doc.get(\"id\") + \" content=\" + doc.get(\"content\") + \" author=\" + doc.get(\"author\") + \" publisher=\" + doc.get(\"publisher\"));\n      }\n    }\n\n    w.close();\n    return new IndexContext(dir, reader, dvType, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6d1c58e9d1ed4b363d0b00ad5209e1b01c418347","date":1399827661,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","sourceNew":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    DocValuesType dvType = DocValuesType.SORTED;\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", String.format(Locale.ROOT, \"%09d\", i), Field.Store.YES));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue, dvType);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue, dvType);\n      }\n      doc.add(new TextField(\"content\", content, Field.Store.YES));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    if (VERBOSE) {\n      for(int docID=0;docID<reader.maxDoc();docID++) {\n        StoredDocument doc = reader.document(docID);\n        System.out.println(\"docID=\" + docID + \" id=\" + doc.get(\"id\") + \" content=\" + doc.get(\"content\") + \" author=\" + doc.get(\"author\") + \" publisher=\" + doc.get(\"publisher\"));\n      }\n    }\n\n    w.shutdown();\n    return new IndexContext(dir, reader, dvType, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","sourceOld":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n    DocValuesType[] dvTypes = new DocValuesType[]{\n        DocValuesType.BINARY,\n        DocValuesType.SORTED\n    };\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    boolean canUseDV = true;\n    DocValuesType dvType = canUseDV ? dvTypes[random.nextInt(dvTypes.length)] : null;\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", String.format(Locale.ROOT, \"%09d\", i), Field.Store.YES));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue, dvType);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue, dvType);\n      }\n      doc.add(new TextField(\"content\", content, Field.Store.YES));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    if (VERBOSE) {\n      for(int docID=0;docID<reader.maxDoc();docID++) {\n        StoredDocument doc = reader.document(docID);\n        System.out.println(\"docID=\" + docID + \" id=\" + doc.get(\"id\") + \" content=\" + doc.get(\"content\") + \" author=\" + doc.get(\"author\") + \" publisher=\" + doc.get(\"publisher\"));\n      }\n    }\n\n    w.shutdown();\n    return new IndexContext(dir, reader, dvType, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d7d9cc55f861840a92a2bda7a9985ec2e2485902","date":1399828429,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","sourceNew":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", String.format(Locale.ROOT, \"%09d\", i), Field.Store.YES));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue);\n      }\n      doc.add(new TextField(\"content\", content, Field.Store.YES));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    if (VERBOSE) {\n      for(int docID=0;docID<reader.maxDoc();docID++) {\n        StoredDocument doc = reader.document(docID);\n        System.out.println(\"docID=\" + docID + \" id=\" + doc.get(\"id\") + \" content=\" + doc.get(\"content\") + \" author=\" + doc.get(\"author\") + \" publisher=\" + doc.get(\"publisher\"));\n      }\n    }\n\n    w.shutdown();\n    return new IndexContext(dir, reader, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","sourceOld":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    DocValuesType dvType = DocValuesType.SORTED;\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", String.format(Locale.ROOT, \"%09d\", i), Field.Store.YES));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue, dvType);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue, dvType);\n      }\n      doc.add(new TextField(\"content\", content, Field.Store.YES));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    if (VERBOSE) {\n      for(int docID=0;docID<reader.maxDoc();docID++) {\n        StoredDocument doc = reader.document(docID);\n        System.out.println(\"docID=\" + docID + \" id=\" + doc.get(\"id\") + \" content=\" + doc.get(\"content\") + \" author=\" + doc.get(\"author\") + \" publisher=\" + doc.get(\"publisher\"));\n      }\n    }\n\n    w.shutdown();\n    return new IndexContext(dir, reader, dvType, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"93dd449115a9247533e44bab47e8429e5dccbc6d","date":1400258396,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","sourceNew":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", String.format(Locale.ROOT, \"%09d\", i), Field.Store.YES));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue);\n      }\n      doc.add(new TextField(\"content\", content, Field.Store.YES));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    if (VERBOSE) {\n      for(int docID=0;docID<reader.maxDoc();docID++) {\n        StoredDocument doc = reader.document(docID);\n        System.out.println(\"docID=\" + docID + \" id=\" + doc.get(\"id\") + \" content=\" + doc.get(\"content\") + \" author=\" + doc.get(\"author\") + \" publisher=\" + doc.get(\"publisher\"));\n      }\n    }\n\n    w.shutdown();\n    return new IndexContext(dir, reader, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","sourceOld":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n    DocValuesType[] dvTypes = new DocValuesType[]{\n        DocValuesType.BINARY,\n        DocValuesType.SORTED\n    };\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    boolean canUseDV = true;\n    DocValuesType dvType = canUseDV ? dvTypes[random.nextInt(dvTypes.length)] : null;\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", String.format(Locale.ROOT, \"%09d\", i), Field.Store.YES));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue, dvType);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue, dvType);\n      }\n      doc.add(new TextField(\"content\", content, Field.Store.YES));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    if (VERBOSE) {\n      for(int docID=0;docID<reader.maxDoc();docID++) {\n        StoredDocument doc = reader.document(docID);\n        System.out.println(\"docID=\" + docID + \" id=\" + doc.get(\"id\") + \" content=\" + doc.get(\"content\") + \" author=\" + doc.get(\"author\") + \" publisher=\" + doc.get(\"publisher\"));\n      }\n    }\n\n    w.shutdown();\n    return new IndexContext(dir, reader, dvType, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"56572ec06f1407c066d6b7399413178b33176cd8","date":1400495675,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","sourceNew":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", String.format(Locale.ROOT, \"%09d\", i), Field.Store.YES));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue);\n      }\n      doc.add(new TextField(\"content\", content, Field.Store.YES));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    if (VERBOSE) {\n      for(int docID=0;docID<reader.maxDoc();docID++) {\n        StoredDocument doc = reader.document(docID);\n        System.out.println(\"docID=\" + docID + \" id=\" + doc.get(\"id\") + \" content=\" + doc.get(\"content\") + \" author=\" + doc.get(\"author\") + \" publisher=\" + doc.get(\"publisher\"));\n      }\n    }\n\n    w.shutdown();\n    return new IndexContext(dir, reader, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","sourceOld":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n    DocValuesType[] dvTypes = new DocValuesType[]{\n        DocValuesType.BINARY,\n        DocValuesType.SORTED\n    };\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    boolean canUseDV = true;\n    DocValuesType dvType = canUseDV ? dvTypes[random.nextInt(dvTypes.length)] : null;\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", String.format(Locale.ROOT, \"%09d\", i), Field.Store.YES));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue, dvType);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue, dvType);\n      }\n      doc.add(new TextField(\"content\", content, Field.Store.YES));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    if (VERBOSE) {\n      for(int docID=0;docID<reader.maxDoc();docID++) {\n        StoredDocument doc = reader.document(docID);\n        System.out.println(\"docID=\" + docID + \" id=\" + doc.get(\"id\") + \" content=\" + doc.get(\"content\") + \" author=\" + doc.get(\"author\") + \" publisher=\" + doc.get(\"publisher\"));\n      }\n    }\n\n    w.shutdown();\n    return new IndexContext(dir, reader, dvType, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","sourceNew":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", String.format(Locale.ROOT, \"%09d\", i), Field.Store.YES));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue);\n      }\n      doc.add(new TextField(\"content\", content, Field.Store.YES));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    if (VERBOSE) {\n      for(int docID=0;docID<reader.maxDoc();docID++) {\n        StoredDocument doc = reader.document(docID);\n        System.out.println(\"docID=\" + docID + \" id=\" + doc.get(\"id\") + \" content=\" + doc.get(\"content\") + \" author=\" + doc.get(\"author\") + \" publisher=\" + doc.get(\"publisher\"));\n      }\n    }\n\n    w.shutdown();\n    return new IndexContext(dir, reader, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","sourceOld":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", String.format(Locale.ROOT, \"%09d\", i), Field.Store.YES));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue);\n      }\n      doc.add(new TextField(\"content\", content, Field.Store.YES));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    if (VERBOSE) {\n      for(int docID=0;docID<reader.maxDoc();docID++) {\n        StoredDocument doc = reader.document(docID);\n        System.out.println(\"docID=\" + docID + \" id=\" + doc.get(\"id\") + \" content=\" + doc.get(\"content\") + \" author=\" + doc.get(\"author\") + \" publisher=\" + doc.get(\"publisher\"));\n      }\n    }\n\n    w.shutdown();\n    return new IndexContext(dir, reader, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","sourceNew":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", String.format(Locale.ROOT, \"%09d\", i), Field.Store.YES));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue);\n      }\n      doc.add(new TextField(\"content\", content, Field.Store.YES));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    if (VERBOSE) {\n      for(int docID=0;docID<reader.maxDoc();docID++) {\n        StoredDocument doc = reader.document(docID);\n        System.out.println(\"docID=\" + docID + \" id=\" + doc.get(\"id\") + \" content=\" + doc.get(\"content\") + \" author=\" + doc.get(\"author\") + \" publisher=\" + doc.get(\"publisher\"));\n      }\n    }\n\n    w.close();\n    return new IndexContext(dir, reader, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","sourceOld":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", String.format(Locale.ROOT, \"%09d\", i), Field.Store.YES));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue);\n      }\n      doc.add(new TextField(\"content\", content, Field.Store.YES));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    if (VERBOSE) {\n      for(int docID=0;docID<reader.maxDoc();docID++) {\n        StoredDocument doc = reader.document(docID);\n        System.out.println(\"docID=\" + docID + \" id=\" + doc.get(\"id\") + \" content=\" + doc.get(\"content\") + \" author=\" + doc.get(\"author\") + \" publisher=\" + doc.get(\"publisher\"));\n      }\n    }\n\n    w.shutdown();\n    return new IndexContext(dir, reader, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0ed6b1234af94a2693d3e6550e7b3ee41fd3f51c","date":1416362965,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","sourceNew":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", String.format(Locale.ROOT, \"%09d\", i), Field.Store.YES));\n      doc.add(new SortedDocValuesField(\"id\", new BytesRef(String.format(Locale.ROOT, \"%09d\", i))));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue);\n      }\n      doc.add(new TextField(\"content\", content, Field.Store.YES));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    if (VERBOSE) {\n      for(int docID=0;docID<reader.maxDoc();docID++) {\n        StoredDocument doc = reader.document(docID);\n        System.out.println(\"docID=\" + docID + \" id=\" + doc.get(\"id\") + \" content=\" + doc.get(\"content\") + \" author=\" + doc.get(\"author\") + \" publisher=\" + doc.get(\"publisher\"));\n      }\n    }\n\n    w.close();\n    return new IndexContext(dir, reader, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","sourceOld":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", String.format(Locale.ROOT, \"%09d\", i), Field.Store.YES));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue);\n      }\n      doc.add(new TextField(\"content\", content, Field.Store.YES));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    if (VERBOSE) {\n      for(int docID=0;docID<reader.maxDoc();docID++) {\n        StoredDocument doc = reader.document(docID);\n        System.out.println(\"docID=\" + docID + \" id=\" + doc.get(\"id\") + \" content=\" + doc.get(\"content\") + \" author=\" + doc.get(\"author\") + \" publisher=\" + doc.get(\"publisher\"));\n      }\n    }\n\n    w.close();\n    return new IndexContext(dir, reader, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c","date":1453060490,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","sourceNew":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", String.format(Locale.ROOT, \"%09d\", i), Field.Store.YES));\n      doc.add(new SortedDocValuesField(\"id\", new BytesRef(String.format(Locale.ROOT, \"%09d\", i))));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue);\n      }\n      doc.add(new TextField(\"content\", content, Field.Store.YES));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    if (VERBOSE) {\n      for(int docID=0;docID<reader.maxDoc();docID++) {\n        Document doc = reader.document(docID);\n        System.out.println(\"docID=\" + docID + \" id=\" + doc.get(\"id\") + \" content=\" + doc.get(\"content\") + \" author=\" + doc.get(\"author\") + \" publisher=\" + doc.get(\"publisher\"));\n      }\n    }\n\n    w.close();\n    return new IndexContext(dir, reader, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","sourceOld":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", String.format(Locale.ROOT, \"%09d\", i), Field.Store.YES));\n      doc.add(new SortedDocValuesField(\"id\", new BytesRef(String.format(Locale.ROOT, \"%09d\", i))));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue);\n      }\n      doc.add(new TextField(\"content\", content, Field.Store.YES));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    if (VERBOSE) {\n      for(int docID=0;docID<reader.maxDoc();docID++) {\n        StoredDocument doc = reader.document(docID);\n        System.out.println(\"docID=\" + docID + \" id=\" + doc.get(\"id\") + \" content=\" + doc.get(\"content\") + \" author=\" + doc.get(\"author\") + \" publisher=\" + doc.get(\"publisher\"));\n      }\n    }\n\n    w.close();\n    return new IndexContext(dir, reader, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5de502b5478255493125e7e801411ba17a6682ec","date":1490974101,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","sourceNew":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", String.format(Locale.ROOT, \"%09d\", i), Field.Store.YES));\n      doc.add(new SortedDocValuesField(\"id\", new BytesRef(String.format(Locale.ROOT, \"%09d\", i))));\n      if (groupValue != null) {\n        addField(doc, GROUP_FIELD, groupValue);\n      }\n      if (countValue != null) {\n        addField(doc, COUNT_FIELD, countValue);\n      }\n      doc.add(new TextField(\"content\", content, Field.Store.YES));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    if (VERBOSE) {\n      for(int docID=0;docID<reader.maxDoc();docID++) {\n        Document doc = reader.document(docID);\n        System.out.println(\"docID=\" + docID + \" id=\" + doc.get(\"id\") + \" content=\" + doc.get(\"content\") + \" author=\" + doc.get(\"author\") + \" publisher=\" + doc.get(\"publisher\"));\n      }\n    }\n\n    w.close();\n    return new IndexContext(dir, reader, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","sourceOld":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", String.format(Locale.ROOT, \"%09d\", i), Field.Store.YES));\n      doc.add(new SortedDocValuesField(\"id\", new BytesRef(String.format(Locale.ROOT, \"%09d\", i))));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue);\n      }\n      doc.add(new TextField(\"content\", content, Field.Store.YES));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    if (VERBOSE) {\n      for(int docID=0;docID<reader.maxDoc();docID++) {\n        Document doc = reader.document(docID);\n        System.out.println(\"docID=\" + docID + \" id=\" + doc.get(\"id\") + \" content=\" + doc.get(\"content\") + \" author=\" + doc.get(\"author\") + \" publisher=\" + doc.get(\"publisher\"));\n      }\n    }\n\n    w.close();\n    return new IndexContext(dir, reader, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6f20fd35e3055a0c5b387df0b986a68d65d86441","date":1491045405,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest#createIndexContext().mjava","sourceNew":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", String.format(Locale.ROOT, \"%09d\", i), Field.Store.YES));\n      doc.add(new SortedDocValuesField(\"id\", new BytesRef(String.format(Locale.ROOT, \"%09d\", i))));\n      if (groupValue != null) {\n        addField(doc, GROUP_FIELD, groupValue);\n      }\n      if (countValue != null) {\n        addField(doc, COUNT_FIELD, countValue);\n      }\n      doc.add(new TextField(\"content\", content, Field.Store.YES));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    if (VERBOSE) {\n      for(int docID=0;docID<reader.maxDoc();docID++) {\n        Document doc = reader.document(docID);\n        System.out.println(\"docID=\" + docID + \" id=\" + doc.get(\"id\") + \" content=\" + doc.get(\"content\") + \" author=\" + doc.get(\"author\") + \" publisher=\" + doc.get(\"publisher\"));\n      }\n    }\n\n    w.close();\n    return new IndexContext(dir, reader, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","sourceOld":"  private IndexContext createIndexContext() throws Exception {\n    Random random = random();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())\n      );\n\n    int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;\n    String[] groupValues = new String[numDocs / 5];\n    String[] countValues = new String[numDocs / 10];\n    for (int i = 0; i < groupValues.length; i++) {\n      groupValues[i] = generateRandomNonEmptyString();\n    }\n    for (int i = 0; i < countValues.length; i++) {\n      countValues[i] = generateRandomNonEmptyString();\n    }\n    \n    List<String> contentStrings = new ArrayList<>();\n    Map<String, Map<String, Set<String>>> searchTermToGroupCounts = new HashMap<>();\n    for (int i = 1; i <= numDocs; i++) {\n      String groupValue = random.nextInt(23) == 14 ? null : groupValues[random.nextInt(groupValues.length)];\n      String countValue = random.nextInt(21) == 13 ? null : countValues[random.nextInt(countValues.length)];\n      String content = \"random\" + random.nextInt(numDocs / 20);\n      Map<String, Set<String>> groupToCounts = searchTermToGroupCounts.get(content);\n      if (groupToCounts == null) {\n        // Groups sort always DOCID asc...\n        searchTermToGroupCounts.put(content, groupToCounts = new LinkedHashMap<>());\n        contentStrings.add(content);\n      }\n\n      Set<String> countsVals = groupToCounts.get(groupValue);\n      if (countsVals == null) {\n        groupToCounts.put(groupValue, countsVals = new HashSet<>());\n      }\n      countsVals.add(countValue);\n\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", String.format(Locale.ROOT, \"%09d\", i), Field.Store.YES));\n      doc.add(new SortedDocValuesField(\"id\", new BytesRef(String.format(Locale.ROOT, \"%09d\", i))));\n      if (groupValue != null) {\n        addField(doc, groupField, groupValue);\n      }\n      if (countValue != null) {\n        addField(doc, countField, countValue);\n      }\n      doc.add(new TextField(\"content\", content, Field.Store.YES));\n      w.addDocument(doc);\n    }\n\n    DirectoryReader reader = w.getReader();\n    if (VERBOSE) {\n      for(int docID=0;docID<reader.maxDoc();docID++) {\n        Document doc = reader.document(docID);\n        System.out.println(\"docID=\" + docID + \" id=\" + doc.get(\"id\") + \" content=\" + doc.get(\"content\") + \" author=\" + doc.get(\"author\") + \" publisher=\" + doc.get(\"publisher\"));\n      }\n    }\n\n    w.close();\n    return new IndexContext(dir, reader, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"5de502b5478255493125e7e801411ba17a6682ec":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["93dd449115a9247533e44bab47e8429e5dccbc6d"],"6f20fd35e3055a0c5b387df0b986a68d65d86441":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c"],"2acf500f78aa12b92e371fd89c719291986b6b90":["04f07771a2a7dd3a395700665ed839c3dae2def2","b4c18e3a5a8908e0fa2ea7c1a3507a214b70153b"],"56572ec06f1407c066d6b7399413178b33176cd8":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a","93dd449115a9247533e44bab47e8429e5dccbc6d"],"46d8ada1fff8d18cb197c38c7983225162599948":["04f07771a2a7dd3a395700665ed839c3dae2def2","2acf500f78aa12b92e371fd89c719291986b6b90"],"0ed6b1234af94a2693d3e6550e7b3ee41fd3f51c":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["57ae3024996ccdb3c36c42cb890e1efb37df4ce8"],"57ae3024996ccdb3c36c42cb890e1efb37df4ce8":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"6d1c58e9d1ed4b363d0b00ad5209e1b01c418347":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["2acf500f78aa12b92e371fd89c719291986b6b90","0837ab0472feecb3a54260729d845f839e1cbd72"],"93dd449115a9247533e44bab47e8429e5dccbc6d":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a","d7d9cc55f861840a92a2bda7a9985ec2e2485902"],"b4c18e3a5a8908e0fa2ea7c1a3507a214b70153b":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":["04f07771a2a7dd3a395700665ed839c3dae2def2","2acf500f78aa12b92e371fd89c719291986b6b90"],"97d4692d0c601ff773f0a2231967312428a904e4":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"0837ab0472feecb3a54260729d845f839e1cbd72":["854f97cd3613b9579fba83755c80b697e2f3993f"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["97d4692d0c601ff773f0a2231967312428a904e4"],"854f97cd3613b9579fba83755c80b697e2f3993f":["2acf500f78aa12b92e371fd89c719291986b6b90"],"d7d9cc55f861840a92a2bda7a9985ec2e2485902":["6d1c58e9d1ed4b363d0b00ad5209e1b01c418347"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["5de502b5478255493125e7e801411ba17a6682ec"],"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c":["0ed6b1234af94a2693d3e6550e7b3ee41fd3f51c"]},"commit2Childs":{"5de502b5478255493125e7e801411ba17a6682ec":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"6f20fd35e3055a0c5b387df0b986a68d65d86441":[],"2acf500f78aa12b92e371fd89c719291986b6b90":["46d8ada1fff8d18cb197c38c7983225162599948","d4d69c535930b5cce125cff868d40f6373dc27d4","fe33227f6805edab2036cbb80645cc4e2d1fa424","854f97cd3613b9579fba83755c80b697e2f3993f"],"56572ec06f1407c066d6b7399413178b33176cd8":[],"46d8ada1fff8d18cb197c38c7983225162599948":[],"04f07771a2a7dd3a395700665ed839c3dae2def2":["2acf500f78aa12b92e371fd89c719291986b6b90","46d8ada1fff8d18cb197c38c7983225162599948","b4c18e3a5a8908e0fa2ea7c1a3507a214b70153b","fe33227f6805edab2036cbb80645cc4e2d1fa424"],"0ed6b1234af94a2693d3e6550e7b3ee41fd3f51c":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c"],"57ae3024996ccdb3c36c42cb890e1efb37df4ce8":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["57ae3024996ccdb3c36c42cb890e1efb37df4ce8"],"6d1c58e9d1ed4b363d0b00ad5209e1b01c418347":["d7d9cc55f861840a92a2bda7a9985ec2e2485902"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["97d4692d0c601ff773f0a2231967312428a904e4"],"93dd449115a9247533e44bab47e8429e5dccbc6d":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","56572ec06f1407c066d6b7399413178b33176cd8"],"b4c18e3a5a8908e0fa2ea7c1a3507a214b70153b":["2acf500f78aa12b92e371fd89c719291986b6b90"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"97d4692d0c601ff773f0a2231967312428a904e4":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["0ed6b1234af94a2693d3e6550e7b3ee41fd3f51c"],"0837ab0472feecb3a54260729d845f839e1cbd72":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["56572ec06f1407c066d6b7399413178b33176cd8","6d1c58e9d1ed4b363d0b00ad5209e1b01c418347","93dd449115a9247533e44bab47e8429e5dccbc6d"],"854f97cd3613b9579fba83755c80b697e2f3993f":["0837ab0472feecb3a54260729d845f839e1cbd72"],"d7d9cc55f861840a92a2bda7a9985ec2e2485902":["93dd449115a9247533e44bab47e8429e5dccbc6d"],"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c":["5de502b5478255493125e7e801411ba17a6682ec","6f20fd35e3055a0c5b387df0b986a68d65d86441"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["6f20fd35e3055a0c5b387df0b986a68d65d86441","56572ec06f1407c066d6b7399413178b33176cd8","46d8ada1fff8d18cb197c38c7983225162599948","fe33227f6805edab2036cbb80645cc4e2d1fa424","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}