{"path":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/TestSimLargeCluster#testFreediskTracking().mjava","commits":[{"id":"bffd02b7c57b27d76ece244beb098f61c974b9d9","date":1568827127,"type":0,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/TestSimLargeCluster#testFreediskTracking().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  public void testFreediskTracking() throws Exception {\n    int NUM_DOCS = 100000;\n    String collectionName = \"testFreeDisk\";\n    SolrClient solrClient = cluster.simGetSolrClient();\n    CollectionAdminRequest.Create create = CollectionAdminRequest.createCollection(collectionName,\n        \"conf\",2, 2);\n    create.process(solrClient);\n\n    CloudUtil.waitForState(cluster, \"Timed out waiting for replicas of new collection to be active\",\n        collectionName, CloudUtil.clusterShape(2, 2, false, true));\n    ClusterState clusterState = cluster.getClusterStateProvider().getClusterState();\n    DocCollection coll = clusterState.getCollection(collectionName);\n    Set<String> nodes = coll.getReplicas().stream()\n        .map(r -> r.getNodeName())\n        .collect(Collectors.toSet());\n    Map<String, Number> initialFreedisk = getFreeDiskPerNode(nodes);\n\n    // test small updates\n    for (int i = 0; i < NUM_DOCS; i++) {\n      SolrInputDocument doc = new SolrInputDocument(\"id\", \"id-\" + i);\n      solrClient.add(collectionName, doc);\n    }\n    Map<String, Number> updatedFreedisk = getFreeDiskPerNode(nodes);\n    double delta = getDeltaFreeDiskBytes(initialFreedisk, updatedFreedisk);\n    // 2 replicas - twice as much delta\n    assertEquals(SimClusterStateProvider.DEFAULT_DOC_SIZE_BYTES * NUM_DOCS * 2, delta, delta * 0.1);\n\n    // test small deletes - delete half of docs\n    for (int i = 0; i < NUM_DOCS / 2; i++) {\n      solrClient.deleteById(collectionName, \"id-\" + i);\n    }\n    Map<String, Number> updatedFreedisk1 = getFreeDiskPerNode(nodes);\n    double delta1 = getDeltaFreeDiskBytes(initialFreedisk, updatedFreedisk1);\n    // 2 replicas but half the docs\n    assertEquals(SimClusterStateProvider.DEFAULT_DOC_SIZE_BYTES * NUM_DOCS * 2 / 2, delta1, delta1 * 0.1);\n\n    // test bulk delete\n    solrClient.deleteByQuery(collectionName, \"*:*\");\n    Map<String, Number> updatedFreedisk2 = getFreeDiskPerNode(nodes);\n    double delta2 = getDeltaFreeDiskBytes(initialFreedisk, updatedFreedisk2);\n    // 0 docs - initial freedisk\n    log.info(cluster.dumpClusterState(true));\n    assertEquals(0.0, delta2, delta2 * 0.1);\n\n    // test bulk update\n    UpdateRequest ureq = new UpdateRequest();\n    ureq.setDocIterator(new FakeDocIterator(0, NUM_DOCS));\n    ureq.process(solrClient, collectionName);\n    Map<String, Number> updatedFreedisk3 = getFreeDiskPerNode(nodes);\n    double delta3 = getDeltaFreeDiskBytes(initialFreedisk, updatedFreedisk3);\n    assertEquals(SimClusterStateProvider.DEFAULT_DOC_SIZE_BYTES * NUM_DOCS * 2, delta3, delta3 * 0.1);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe9f2c4a0d7ac164e4bdd4eee7f87131aec83fd4","date":1588172214,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/TestSimLargeCluster#testFreediskTracking().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/TestSimLargeCluster#testFreediskTracking().mjava","sourceNew":"  @Test\n  public void testFreediskTracking() throws Exception {\n    int NUM_DOCS = 100000;\n    String collectionName = \"testFreeDisk\";\n    SolrClient solrClient = cluster.simGetSolrClient();\n    CollectionAdminRequest.Create create = CollectionAdminRequest.createCollection(collectionName,\n        \"conf\",2, 2);\n    create.process(solrClient);\n\n    CloudUtil.waitForState(cluster, \"Timed out waiting for replicas of new collection to be active\",\n        collectionName, CloudUtil.clusterShape(2, 2, false, true));\n    ClusterState clusterState = cluster.getClusterStateProvider().getClusterState();\n    DocCollection coll = clusterState.getCollection(collectionName);\n    Set<String> nodes = coll.getReplicas().stream()\n        .map(r -> r.getNodeName())\n        .collect(Collectors.toSet());\n    Map<String, Number> initialFreedisk = getFreeDiskPerNode(nodes);\n\n    // test small updates\n    for (int i = 0; i < NUM_DOCS; i++) {\n      SolrInputDocument doc = new SolrInputDocument(\"id\", \"id-\" + i);\n      solrClient.add(collectionName, doc);\n    }\n    Map<String, Number> updatedFreedisk = getFreeDiskPerNode(nodes);\n    double delta = getDeltaFreeDiskBytes(initialFreedisk, updatedFreedisk);\n    // 2 replicas - twice as much delta\n    assertEquals(SimClusterStateProvider.DEFAULT_DOC_SIZE_BYTES * NUM_DOCS * 2, delta, delta * 0.1);\n\n    // test small deletes - delete half of docs\n    for (int i = 0; i < NUM_DOCS / 2; i++) {\n      solrClient.deleteById(collectionName, \"id-\" + i);\n    }\n    Map<String, Number> updatedFreedisk1 = getFreeDiskPerNode(nodes);\n    double delta1 = getDeltaFreeDiskBytes(initialFreedisk, updatedFreedisk1);\n    // 2 replicas but half the docs\n    assertEquals(SimClusterStateProvider.DEFAULT_DOC_SIZE_BYTES * NUM_DOCS * 2 / 2, delta1, delta1 * 0.1);\n\n    // test bulk delete\n    solrClient.deleteByQuery(collectionName, \"*:*\");\n    Map<String, Number> updatedFreedisk2 = getFreeDiskPerNode(nodes);\n    double delta2 = getDeltaFreeDiskBytes(initialFreedisk, updatedFreedisk2);\n    // 0 docs - initial freedisk\n    if (log.isInfoEnabled()) {\n      log.info(cluster.dumpClusterState(true));\n    }\n    assertEquals(0.0, delta2, delta2 * 0.1);\n\n    // test bulk update\n    UpdateRequest ureq = new UpdateRequest();\n    ureq.setDocIterator(new FakeDocIterator(0, NUM_DOCS));\n    ureq.process(solrClient, collectionName);\n    Map<String, Number> updatedFreedisk3 = getFreeDiskPerNode(nodes);\n    double delta3 = getDeltaFreeDiskBytes(initialFreedisk, updatedFreedisk3);\n    assertEquals(SimClusterStateProvider.DEFAULT_DOC_SIZE_BYTES * NUM_DOCS * 2, delta3, delta3 * 0.1);\n  }\n\n","sourceOld":"  @Test\n  public void testFreediskTracking() throws Exception {\n    int NUM_DOCS = 100000;\n    String collectionName = \"testFreeDisk\";\n    SolrClient solrClient = cluster.simGetSolrClient();\n    CollectionAdminRequest.Create create = CollectionAdminRequest.createCollection(collectionName,\n        \"conf\",2, 2);\n    create.process(solrClient);\n\n    CloudUtil.waitForState(cluster, \"Timed out waiting for replicas of new collection to be active\",\n        collectionName, CloudUtil.clusterShape(2, 2, false, true));\n    ClusterState clusterState = cluster.getClusterStateProvider().getClusterState();\n    DocCollection coll = clusterState.getCollection(collectionName);\n    Set<String> nodes = coll.getReplicas().stream()\n        .map(r -> r.getNodeName())\n        .collect(Collectors.toSet());\n    Map<String, Number> initialFreedisk = getFreeDiskPerNode(nodes);\n\n    // test small updates\n    for (int i = 0; i < NUM_DOCS; i++) {\n      SolrInputDocument doc = new SolrInputDocument(\"id\", \"id-\" + i);\n      solrClient.add(collectionName, doc);\n    }\n    Map<String, Number> updatedFreedisk = getFreeDiskPerNode(nodes);\n    double delta = getDeltaFreeDiskBytes(initialFreedisk, updatedFreedisk);\n    // 2 replicas - twice as much delta\n    assertEquals(SimClusterStateProvider.DEFAULT_DOC_SIZE_BYTES * NUM_DOCS * 2, delta, delta * 0.1);\n\n    // test small deletes - delete half of docs\n    for (int i = 0; i < NUM_DOCS / 2; i++) {\n      solrClient.deleteById(collectionName, \"id-\" + i);\n    }\n    Map<String, Number> updatedFreedisk1 = getFreeDiskPerNode(nodes);\n    double delta1 = getDeltaFreeDiskBytes(initialFreedisk, updatedFreedisk1);\n    // 2 replicas but half the docs\n    assertEquals(SimClusterStateProvider.DEFAULT_DOC_SIZE_BYTES * NUM_DOCS * 2 / 2, delta1, delta1 * 0.1);\n\n    // test bulk delete\n    solrClient.deleteByQuery(collectionName, \"*:*\");\n    Map<String, Number> updatedFreedisk2 = getFreeDiskPerNode(nodes);\n    double delta2 = getDeltaFreeDiskBytes(initialFreedisk, updatedFreedisk2);\n    // 0 docs - initial freedisk\n    log.info(cluster.dumpClusterState(true));\n    assertEquals(0.0, delta2, delta2 * 0.1);\n\n    // test bulk update\n    UpdateRequest ureq = new UpdateRequest();\n    ureq.setDocIterator(new FakeDocIterator(0, NUM_DOCS));\n    ureq.process(solrClient, collectionName);\n    Map<String, Number> updatedFreedisk3 = getFreeDiskPerNode(nodes);\n    double delta3 = getDeltaFreeDiskBytes(initialFreedisk, updatedFreedisk3);\n    assertEquals(SimClusterStateProvider.DEFAULT_DOC_SIZE_BYTES * NUM_DOCS * 2, delta3, delta3 * 0.1);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3f504512a03d978990cbff30db0522b354e846db","date":1595247421,"type":4,"author":"Ishan Chattopadhyaya","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/TestSimLargeCluster#testFreediskTracking().mjava","sourceNew":null,"sourceOld":"  @Test\n  public void testFreediskTracking() throws Exception {\n    int NUM_DOCS = 100000;\n    String collectionName = \"testFreeDisk\";\n    SolrClient solrClient = cluster.simGetSolrClient();\n    CollectionAdminRequest.Create create = CollectionAdminRequest.createCollection(collectionName,\n        \"conf\",2, 2);\n    create.process(solrClient);\n\n    CloudUtil.waitForState(cluster, \"Timed out waiting for replicas of new collection to be active\",\n        collectionName, CloudUtil.clusterShape(2, 2, false, true));\n    ClusterState clusterState = cluster.getClusterStateProvider().getClusterState();\n    DocCollection coll = clusterState.getCollection(collectionName);\n    Set<String> nodes = coll.getReplicas().stream()\n        .map(r -> r.getNodeName())\n        .collect(Collectors.toSet());\n    Map<String, Number> initialFreedisk = getFreeDiskPerNode(nodes);\n\n    // test small updates\n    for (int i = 0; i < NUM_DOCS; i++) {\n      SolrInputDocument doc = new SolrInputDocument(\"id\", \"id-\" + i);\n      solrClient.add(collectionName, doc);\n    }\n    Map<String, Number> updatedFreedisk = getFreeDiskPerNode(nodes);\n    double delta = getDeltaFreeDiskBytes(initialFreedisk, updatedFreedisk);\n    // 2 replicas - twice as much delta\n    assertEquals(SimClusterStateProvider.DEFAULT_DOC_SIZE_BYTES * NUM_DOCS * 2, delta, delta * 0.1);\n\n    // test small deletes - delete half of docs\n    for (int i = 0; i < NUM_DOCS / 2; i++) {\n      solrClient.deleteById(collectionName, \"id-\" + i);\n    }\n    Map<String, Number> updatedFreedisk1 = getFreeDiskPerNode(nodes);\n    double delta1 = getDeltaFreeDiskBytes(initialFreedisk, updatedFreedisk1);\n    // 2 replicas but half the docs\n    assertEquals(SimClusterStateProvider.DEFAULT_DOC_SIZE_BYTES * NUM_DOCS * 2 / 2, delta1, delta1 * 0.1);\n\n    // test bulk delete\n    solrClient.deleteByQuery(collectionName, \"*:*\");\n    Map<String, Number> updatedFreedisk2 = getFreeDiskPerNode(nodes);\n    double delta2 = getDeltaFreeDiskBytes(initialFreedisk, updatedFreedisk2);\n    // 0 docs - initial freedisk\n    if (log.isInfoEnabled()) {\n      log.info(cluster.dumpClusterState(true));\n    }\n    assertEquals(0.0, delta2, delta2 * 0.1);\n\n    // test bulk update\n    UpdateRequest ureq = new UpdateRequest();\n    ureq.setDocIterator(new FakeDocIterator(0, NUM_DOCS));\n    ureq.process(solrClient, collectionName);\n    Map<String, Number> updatedFreedisk3 = getFreeDiskPerNode(nodes);\n    double delta3 = getDeltaFreeDiskBytes(initialFreedisk, updatedFreedisk3);\n    assertEquals(SimClusterStateProvider.DEFAULT_DOC_SIZE_BYTES * NUM_DOCS * 2, delta3, delta3 * 0.1);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"3f504512a03d978990cbff30db0522b354e846db":["fe9f2c4a0d7ac164e4bdd4eee7f87131aec83fd4"],"fe9f2c4a0d7ac164e4bdd4eee7f87131aec83fd4":["bffd02b7c57b27d76ece244beb098f61c974b9d9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3f504512a03d978990cbff30db0522b354e846db"],"bffd02b7c57b27d76ece244beb098f61c974b9d9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"3f504512a03d978990cbff30db0522b354e846db":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"fe9f2c4a0d7ac164e4bdd4eee7f87131aec83fd4":["3f504512a03d978990cbff30db0522b354e846db"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["bffd02b7c57b27d76ece244beb098f61c974b9d9"],"bffd02b7c57b27d76ece244beb098f61c974b9d9":["fe9f2c4a0d7ac164e4bdd4eee7f87131aec83fd4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}