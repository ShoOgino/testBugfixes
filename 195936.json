{"path":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/TokenStreamFromTermVector#init().mjava","commits":[{"id":"1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2","date":1475611903,"type":0,"author":"David Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/TokenStreamFromTermVector#init().mjava","pathOld":"/dev/null","sourceNew":"  //We delay initialization because we can see which attributes the consumer wants, particularly payloads\n  private void init() throws IOException {\n    assert !initialized;\n    int dpEnumFlags = 0;\n    if (vector.hasOffsets()) {\n      offsetAttribute = addAttribute(OffsetAttribute.class);\n      dpEnumFlags |= PostingsEnum.OFFSETS;\n    }\n    if (vector.hasPayloads() && hasAttribute(PayloadAttribute.class)) {\n      payloadAttribute = getAttribute(PayloadAttribute.class);\n      payloadsBytesRefArray = new BytesRefArray(Counter.newCounter());\n      spareBytesRefBuilder = new BytesRefBuilder();\n      dpEnumFlags |= PostingsEnum.PAYLOADS;\n    }\n\n    // We put term data here\n    termCharsBuilder = new CharsRefBuilder();\n    termCharsBuilder.grow(initTotalTermCharLen());\n\n    // Step 1: iterate termsEnum and create a token, placing into a bucketed array (given a load factor)\n\n    final TokenLL[] tokenBuckets = initTokenBucketsArray();\n    final double OFFSET_TO_BUCKET_IDX = loadFactor / AVG_CHARS_PER_POSITION;\n    final double POSITION_TO_BUCKET_IDX = loadFactor;\n\n    final TermsEnum termsEnum = vector.iterator();\n    BytesRef termBytesRef;\n    PostingsEnum dpEnum = null;\n    final CharsRefBuilder tempCharsRefBuilder = new CharsRefBuilder();//only for UTF8->UTF16 call\n\n    TERM_LOOP:\n    while ((termBytesRef = termsEnum.next()) != null) {\n      //Grab the term (in same way as BytesRef.utf8ToString() but we don't want a String obj)\n      // note: if term vectors supported seek by ord then we might just keep an int and seek by ord on-demand\n      tempCharsRefBuilder.grow(termBytesRef.length);\n      final int termCharsLen = UnicodeUtil.UTF8toUTF16(termBytesRef, tempCharsRefBuilder.chars());\n      final int termCharsOff = termCharsBuilder.length();\n      termCharsBuilder.append(tempCharsRefBuilder.chars(), 0, termCharsLen);\n      dpEnum = termsEnum.postings(dpEnum, dpEnumFlags);\n      assert dpEnum != null; // presumably checked by TokenSources.hasPositions earlier\n      int currentDocId = dpEnum.advance(filteredDocId);\n      if (currentDocId != filteredDocId) {\n        continue; //Not expected\n      }\n      final int freq = dpEnum.freq();\n      for (int j = 0; j < freq; j++) {\n        TokenLL token = new TokenLL();\n        token.position = dpEnum.nextPosition(); // can be -1 if not in the TV\n        token.termCharsOff = termCharsOff;\n        token.termCharsLen = (short) Math.min(termCharsLen, Short.MAX_VALUE);\n        // copy offset (if it's there) and compute bucketIdx\n        int bucketIdx;\n        if (offsetAttribute != null) {\n          token.startOffset = dpEnum.startOffset();\n          if (offsetLength >= 0 && token.startOffset > offsetLength) {\n            continue TERM_LOOP;//filter this token out; exceeds threshold\n          }\n          token.endOffsetInc = (short) Math.min(dpEnum.endOffset() - token.startOffset, Short.MAX_VALUE);\n          bucketIdx = (int) (token.startOffset * OFFSET_TO_BUCKET_IDX);\n        } else {\n          bucketIdx = (int) (token.position * POSITION_TO_BUCKET_IDX);\n        }\n        if (bucketIdx >= tokenBuckets.length) {\n          bucketIdx = tokenBuckets.length - 1;\n        }\n\n        if (payloadAttribute != null) {\n          final BytesRef payload = dpEnum.getPayload();\n          token.payloadIndex = payload == null ? -1 : payloadsBytesRefArray.append(payload);\n        }\n\n        //Add token to the head of the bucket linked list\n        token.next = tokenBuckets[bucketIdx];\n        tokenBuckets[bucketIdx] = token;\n      }\n    }\n\n    // Step 2:  Link all Tokens into a linked-list and sort all tokens at the same position\n\n    firstToken = initLinkAndSortTokens(tokenBuckets);\n\n    // If the term vector didn't have positions, synthesize them\n    if (!vector.hasPositions() && firstToken != null) {\n      TokenLL prevToken = firstToken;\n      prevToken.position = 0;\n      for (TokenLL token = prevToken.next; token != null; prevToken = token, token = token.next) {\n        if (prevToken.startOffset == token.startOffset) {\n          token.position = prevToken.position;\n        } else {\n          token.position = prevToken.position + 1;\n        }\n      }\n    }\n\n    initialized = true;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/TokenStreamFromTermVector#init().mjava","pathOld":"/dev/null","sourceNew":"  //We delay initialization because we can see which attributes the consumer wants, particularly payloads\n  private void init() throws IOException {\n    assert !initialized;\n    int dpEnumFlags = 0;\n    if (vector.hasOffsets()) {\n      offsetAttribute = addAttribute(OffsetAttribute.class);\n      dpEnumFlags |= PostingsEnum.OFFSETS;\n    }\n    if (vector.hasPayloads() && hasAttribute(PayloadAttribute.class)) {\n      payloadAttribute = getAttribute(PayloadAttribute.class);\n      payloadsBytesRefArray = new BytesRefArray(Counter.newCounter());\n      spareBytesRefBuilder = new BytesRefBuilder();\n      dpEnumFlags |= PostingsEnum.PAYLOADS;\n    }\n\n    // We put term data here\n    termCharsBuilder = new CharsRefBuilder();\n    termCharsBuilder.grow(initTotalTermCharLen());\n\n    // Step 1: iterate termsEnum and create a token, placing into a bucketed array (given a load factor)\n\n    final TokenLL[] tokenBuckets = initTokenBucketsArray();\n    final double OFFSET_TO_BUCKET_IDX = loadFactor / AVG_CHARS_PER_POSITION;\n    final double POSITION_TO_BUCKET_IDX = loadFactor;\n\n    final TermsEnum termsEnum = vector.iterator();\n    BytesRef termBytesRef;\n    PostingsEnum dpEnum = null;\n    final CharsRefBuilder tempCharsRefBuilder = new CharsRefBuilder();//only for UTF8->UTF16 call\n\n    TERM_LOOP:\n    while ((termBytesRef = termsEnum.next()) != null) {\n      //Grab the term (in same way as BytesRef.utf8ToString() but we don't want a String obj)\n      // note: if term vectors supported seek by ord then we might just keep an int and seek by ord on-demand\n      tempCharsRefBuilder.grow(termBytesRef.length);\n      final int termCharsLen = UnicodeUtil.UTF8toUTF16(termBytesRef, tempCharsRefBuilder.chars());\n      final int termCharsOff = termCharsBuilder.length();\n      termCharsBuilder.append(tempCharsRefBuilder.chars(), 0, termCharsLen);\n      dpEnum = termsEnum.postings(dpEnum, dpEnumFlags);\n      assert dpEnum != null; // presumably checked by TokenSources.hasPositions earlier\n      int currentDocId = dpEnum.advance(filteredDocId);\n      if (currentDocId != filteredDocId) {\n        continue; //Not expected\n      }\n      final int freq = dpEnum.freq();\n      for (int j = 0; j < freq; j++) {\n        TokenLL token = new TokenLL();\n        token.position = dpEnum.nextPosition(); // can be -1 if not in the TV\n        token.termCharsOff = termCharsOff;\n        token.termCharsLen = (short) Math.min(termCharsLen, Short.MAX_VALUE);\n        // copy offset (if it's there) and compute bucketIdx\n        int bucketIdx;\n        if (offsetAttribute != null) {\n          token.startOffset = dpEnum.startOffset();\n          if (offsetLength >= 0 && token.startOffset > offsetLength) {\n            continue TERM_LOOP;//filter this token out; exceeds threshold\n          }\n          token.endOffsetInc = (short) Math.min(dpEnum.endOffset() - token.startOffset, Short.MAX_VALUE);\n          bucketIdx = (int) (token.startOffset * OFFSET_TO_BUCKET_IDX);\n        } else {\n          bucketIdx = (int) (token.position * POSITION_TO_BUCKET_IDX);\n        }\n        if (bucketIdx >= tokenBuckets.length) {\n          bucketIdx = tokenBuckets.length - 1;\n        }\n\n        if (payloadAttribute != null) {\n          final BytesRef payload = dpEnum.getPayload();\n          token.payloadIndex = payload == null ? -1 : payloadsBytesRefArray.append(payload);\n        }\n\n        //Add token to the head of the bucket linked list\n        token.next = tokenBuckets[bucketIdx];\n        tokenBuckets[bucketIdx] = token;\n      }\n    }\n\n    // Step 2:  Link all Tokens into a linked-list and sort all tokens at the same position\n\n    firstToken = initLinkAndSortTokens(tokenBuckets);\n\n    // If the term vector didn't have positions, synthesize them\n    if (!vector.hasPositions() && firstToken != null) {\n      TokenLL prevToken = firstToken;\n      prevToken.position = 0;\n      for (TokenLL token = prevToken.next; token != null; prevToken = token, token = token.next) {\n        if (prevToken.startOffset == token.startOffset) {\n          token.position = prevToken.position;\n        } else {\n          token.position = prevToken.position + 1;\n        }\n      }\n    }\n\n    initialized = true;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f2e9861e4a2b724d9fc51b618714c579491b78d7","date":1479244606,"type":4,"author":"David Smiley","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/TokenStreamFromTermVector#init().mjava","sourceNew":null,"sourceOld":"  //We delay initialization because we can see which attributes the consumer wants, particularly payloads\n  private void init() throws IOException {\n    assert !initialized;\n    int dpEnumFlags = 0;\n    if (vector.hasOffsets()) {\n      offsetAttribute = addAttribute(OffsetAttribute.class);\n      dpEnumFlags |= PostingsEnum.OFFSETS;\n    }\n    if (vector.hasPayloads() && hasAttribute(PayloadAttribute.class)) {\n      payloadAttribute = getAttribute(PayloadAttribute.class);\n      payloadsBytesRefArray = new BytesRefArray(Counter.newCounter());\n      spareBytesRefBuilder = new BytesRefBuilder();\n      dpEnumFlags |= PostingsEnum.PAYLOADS;\n    }\n\n    // We put term data here\n    termCharsBuilder = new CharsRefBuilder();\n    termCharsBuilder.grow(initTotalTermCharLen());\n\n    // Step 1: iterate termsEnum and create a token, placing into a bucketed array (given a load factor)\n\n    final TokenLL[] tokenBuckets = initTokenBucketsArray();\n    final double OFFSET_TO_BUCKET_IDX = loadFactor / AVG_CHARS_PER_POSITION;\n    final double POSITION_TO_BUCKET_IDX = loadFactor;\n\n    final TermsEnum termsEnum = vector.iterator();\n    BytesRef termBytesRef;\n    PostingsEnum dpEnum = null;\n    final CharsRefBuilder tempCharsRefBuilder = new CharsRefBuilder();//only for UTF8->UTF16 call\n\n    TERM_LOOP:\n    while ((termBytesRef = termsEnum.next()) != null) {\n      //Grab the term (in same way as BytesRef.utf8ToString() but we don't want a String obj)\n      // note: if term vectors supported seek by ord then we might just keep an int and seek by ord on-demand\n      tempCharsRefBuilder.grow(termBytesRef.length);\n      final int termCharsLen = UnicodeUtil.UTF8toUTF16(termBytesRef, tempCharsRefBuilder.chars());\n      final int termCharsOff = termCharsBuilder.length();\n      termCharsBuilder.append(tempCharsRefBuilder.chars(), 0, termCharsLen);\n      dpEnum = termsEnum.postings(dpEnum, dpEnumFlags);\n      assert dpEnum != null; // presumably checked by TokenSources.hasPositions earlier\n      int currentDocId = dpEnum.advance(filteredDocId);\n      if (currentDocId != filteredDocId) {\n        continue; //Not expected\n      }\n      final int freq = dpEnum.freq();\n      for (int j = 0; j < freq; j++) {\n        TokenLL token = new TokenLL();\n        token.position = dpEnum.nextPosition(); // can be -1 if not in the TV\n        token.termCharsOff = termCharsOff;\n        token.termCharsLen = (short) Math.min(termCharsLen, Short.MAX_VALUE);\n        // copy offset (if it's there) and compute bucketIdx\n        int bucketIdx;\n        if (offsetAttribute != null) {\n          token.startOffset = dpEnum.startOffset();\n          if (offsetLength >= 0 && token.startOffset > offsetLength) {\n            continue TERM_LOOP;//filter this token out; exceeds threshold\n          }\n          token.endOffsetInc = (short) Math.min(dpEnum.endOffset() - token.startOffset, Short.MAX_VALUE);\n          bucketIdx = (int) (token.startOffset * OFFSET_TO_BUCKET_IDX);\n        } else {\n          bucketIdx = (int) (token.position * POSITION_TO_BUCKET_IDX);\n        }\n        if (bucketIdx >= tokenBuckets.length) {\n          bucketIdx = tokenBuckets.length - 1;\n        }\n\n        if (payloadAttribute != null) {\n          final BytesRef payload = dpEnum.getPayload();\n          token.payloadIndex = payload == null ? -1 : payloadsBytesRefArray.append(payload);\n        }\n\n        //Add token to the head of the bucket linked list\n        token.next = tokenBuckets[bucketIdx];\n        tokenBuckets[bucketIdx] = token;\n      }\n    }\n\n    // Step 2:  Link all Tokens into a linked-list and sort all tokens at the same position\n\n    firstToken = initLinkAndSortTokens(tokenBuckets);\n\n    // If the term vector didn't have positions, synthesize them\n    if (!vector.hasPositions() && firstToken != null) {\n      TokenLL prevToken = firstToken;\n      prevToken.position = 0;\n      for (TokenLL token = prevToken.next; token != null; prevToken = token, token = token.next) {\n        if (prevToken.startOffset == token.startOffset) {\n          token.position = prevToken.position;\n        } else {\n          token.position = prevToken.position + 1;\n        }\n      }\n    }\n\n    initialized = true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a1ef55e1fff7ff44354432770ad8bc19be1fcc75","date":1479266056,"type":4,"author":"Kevin Risden","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/TokenStreamFromTermVector#init().mjava","sourceNew":null,"sourceOld":"  //We delay initialization because we can see which attributes the consumer wants, particularly payloads\n  private void init() throws IOException {\n    assert !initialized;\n    int dpEnumFlags = 0;\n    if (vector.hasOffsets()) {\n      offsetAttribute = addAttribute(OffsetAttribute.class);\n      dpEnumFlags |= PostingsEnum.OFFSETS;\n    }\n    if (vector.hasPayloads() && hasAttribute(PayloadAttribute.class)) {\n      payloadAttribute = getAttribute(PayloadAttribute.class);\n      payloadsBytesRefArray = new BytesRefArray(Counter.newCounter());\n      spareBytesRefBuilder = new BytesRefBuilder();\n      dpEnumFlags |= PostingsEnum.PAYLOADS;\n    }\n\n    // We put term data here\n    termCharsBuilder = new CharsRefBuilder();\n    termCharsBuilder.grow(initTotalTermCharLen());\n\n    // Step 1: iterate termsEnum and create a token, placing into a bucketed array (given a load factor)\n\n    final TokenLL[] tokenBuckets = initTokenBucketsArray();\n    final double OFFSET_TO_BUCKET_IDX = loadFactor / AVG_CHARS_PER_POSITION;\n    final double POSITION_TO_BUCKET_IDX = loadFactor;\n\n    final TermsEnum termsEnum = vector.iterator();\n    BytesRef termBytesRef;\n    PostingsEnum dpEnum = null;\n    final CharsRefBuilder tempCharsRefBuilder = new CharsRefBuilder();//only for UTF8->UTF16 call\n\n    TERM_LOOP:\n    while ((termBytesRef = termsEnum.next()) != null) {\n      //Grab the term (in same way as BytesRef.utf8ToString() but we don't want a String obj)\n      // note: if term vectors supported seek by ord then we might just keep an int and seek by ord on-demand\n      tempCharsRefBuilder.grow(termBytesRef.length);\n      final int termCharsLen = UnicodeUtil.UTF8toUTF16(termBytesRef, tempCharsRefBuilder.chars());\n      final int termCharsOff = termCharsBuilder.length();\n      termCharsBuilder.append(tempCharsRefBuilder.chars(), 0, termCharsLen);\n      dpEnum = termsEnum.postings(dpEnum, dpEnumFlags);\n      assert dpEnum != null; // presumably checked by TokenSources.hasPositions earlier\n      int currentDocId = dpEnum.advance(filteredDocId);\n      if (currentDocId != filteredDocId) {\n        continue; //Not expected\n      }\n      final int freq = dpEnum.freq();\n      for (int j = 0; j < freq; j++) {\n        TokenLL token = new TokenLL();\n        token.position = dpEnum.nextPosition(); // can be -1 if not in the TV\n        token.termCharsOff = termCharsOff;\n        token.termCharsLen = (short) Math.min(termCharsLen, Short.MAX_VALUE);\n        // copy offset (if it's there) and compute bucketIdx\n        int bucketIdx;\n        if (offsetAttribute != null) {\n          token.startOffset = dpEnum.startOffset();\n          if (offsetLength >= 0 && token.startOffset > offsetLength) {\n            continue TERM_LOOP;//filter this token out; exceeds threshold\n          }\n          token.endOffsetInc = (short) Math.min(dpEnum.endOffset() - token.startOffset, Short.MAX_VALUE);\n          bucketIdx = (int) (token.startOffset * OFFSET_TO_BUCKET_IDX);\n        } else {\n          bucketIdx = (int) (token.position * POSITION_TO_BUCKET_IDX);\n        }\n        if (bucketIdx >= tokenBuckets.length) {\n          bucketIdx = tokenBuckets.length - 1;\n        }\n\n        if (payloadAttribute != null) {\n          final BytesRef payload = dpEnum.getPayload();\n          token.payloadIndex = payload == null ? -1 : payloadsBytesRefArray.append(payload);\n        }\n\n        //Add token to the head of the bucket linked list\n        token.next = tokenBuckets[bucketIdx];\n        tokenBuckets[bucketIdx] = token;\n      }\n    }\n\n    // Step 2:  Link all Tokens into a linked-list and sort all tokens at the same position\n\n    firstToken = initLinkAndSortTokens(tokenBuckets);\n\n    // If the term vector didn't have positions, synthesize them\n    if (!vector.hasPositions() && firstToken != null) {\n      TokenLL prevToken = firstToken;\n      prevToken.position = 0;\n      for (TokenLL token = prevToken.next; token != null; prevToken = token, token = token.next) {\n        if (prevToken.startOffset == token.startOffset) {\n          token.position = prevToken.position;\n        } else {\n          token.position = prevToken.position + 1;\n        }\n      }\n    }\n\n    initialized = true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"f2e9861e4a2b724d9fc51b618714c579491b78d7":["1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a1ef55e1fff7ff44354432770ad8bc19be1fcc75":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","f2e9861e4a2b724d9fc51b618714c579491b78d7"],"1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["f2e9861e4a2b724d9fc51b618714c579491b78d7"]},"commit2Childs":{"f2e9861e4a2b724d9fc51b618714c579491b78d7":["a1ef55e1fff7ff44354432770ad8bc19be1fcc75","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"a1ef55e1fff7ff44354432770ad8bc19be1fcc75":[],"1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2":["f2e9861e4a2b724d9fc51b618714c579491b78d7","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a1ef55e1fff7ff44354432770ad8bc19be1fcc75"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a1ef55e1fff7ff44354432770ad8bc19be1fcc75","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}