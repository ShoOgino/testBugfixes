{"path":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","commits":[{"id":"75ffaed6edffab734233feb10c3a40decddfeb63","date":1344540694,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","pathOld":"/dev/null","sourceNew":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(new StringReader(\"here we go\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(new StringReader(\"another\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    SegmentReader sr = getOnlySegmentReader(reader);\n    DocsAndPositionsEnum de = sr.termPositionsEnum(null, \"field\", new BytesRef(\"withPayload\"));\n    de.nextDoc();\n    de.nextPosition();\n    assertTrue(de.hasPayload());\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e780da71c6146a490672a8ef0b709f7ede5229b1","date":1344770972,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","sourceNew":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(new StringReader(\"here we go\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(new StringReader(\"another\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    AtomicReader sr = reader.getSequentialSubReaders().get(0);\n    DocsAndPositionsEnum de = sr.termPositionsEnum(null, \"field\", new BytesRef(\"withPayload\"));\n    de.nextDoc();\n    de.nextPosition();\n    assertTrue(de.hasPayload());\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(new StringReader(\"here we go\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(new StringReader(\"another\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    SegmentReader sr = getOnlySegmentReader(reader);\n    DocsAndPositionsEnum de = sr.termPositionsEnum(null, \"field\", new BytesRef(\"withPayload\"));\n    de.nextDoc();\n    de.nextPosition();\n    assertTrue(de.hasPayload());\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["ced66195b26fdb1f77ee00e2a77ec6918dedd766"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552","date":1344797146,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","sourceNew":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(new StringReader(\"here we go\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(new StringReader(\"another\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    AtomicReader sr = reader.getSequentialSubReaders().get(0);\n    DocsAndPositionsEnum de = sr.termPositionsEnum(null, \"field\", new BytesRef(\"withPayload\"));\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(new StringReader(\"here we go\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(new StringReader(\"another\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    AtomicReader sr = reader.getSequentialSubReaders().get(0);\n    DocsAndPositionsEnum de = sr.termPositionsEnum(null, \"field\", new BytesRef(\"withPayload\"));\n    de.nextDoc();\n    de.nextPosition();\n    assertTrue(de.hasPayload());\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c7869f64c874ebf7f317d22c00baf2b6857797a6","date":1344856617,"type":0,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","pathOld":"/dev/null","sourceNew":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(new StringReader(\"here we go\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(new StringReader(\"another\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    AtomicReader sr = reader.getSequentialSubReaders().get(0);\n    DocsAndPositionsEnum de = sr.termPositionsEnum(null, \"field\", new BytesRef(\"withPayload\"));\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9","date":1344867506,"type":0,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","pathOld":"/dev/null","sourceNew":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(new StringReader(\"here we go\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(new StringReader(\"another\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    AtomicReader sr = reader.getSequentialSubReaders().get(0);\n    DocsAndPositionsEnum de = sr.termPositionsEnum(null, \"field\", new BytesRef(\"withPayload\"));\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ced66195b26fdb1f77ee00e2a77ec6918dedd766","date":1344948886,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","sourceNew":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(new StringReader(\"here we go\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(new StringReader(\"another\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    AtomicReader sr = SlowCompositeReaderWrapper.wrap(reader);\n    DocsAndPositionsEnum de = sr.termPositionsEnum(null, \"field\", new BytesRef(\"withPayload\"));\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(new StringReader(\"here we go\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(new StringReader(\"another\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    AtomicReader sr = reader.getSequentialSubReaders().get(0);\n    DocsAndPositionsEnum de = sr.termPositionsEnum(null, \"field\", new BytesRef(\"withPayload\"));\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":["e780da71c6146a490672a8ef0b709f7ede5229b1"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198","date":1345029782,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","sourceNew":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(new StringReader(\"here we go\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(new StringReader(\"another\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    AtomicReader sr = SlowCompositeReaderWrapper.wrap(reader);\n    DocsAndPositionsEnum de = sr.termPositionsEnum(null, \"field\", new BytesRef(\"withPayload\"));\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(new StringReader(\"here we go\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(new StringReader(\"another\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    AtomicReader sr = reader.getSequentialSubReaders().get(0);\n    DocsAndPositionsEnum de = sr.termPositionsEnum(null, \"field\", new BytesRef(\"withPayload\"));\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3c188105a9aae04f56c24996f98f8333fc825d2e","date":1345031914,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","sourceNew":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(new StringReader(\"here we go\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(new StringReader(\"another\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    AtomicReader sr = SlowCompositeReaderWrapper.wrap(reader);\n    DocsAndPositionsEnum de = sr.termPositionsEnum(null, \"field\", new BytesRef(\"withPayload\"));\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(new StringReader(\"here we go\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(new StringReader(\"another\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    AtomicReader sr = reader.getSequentialSubReaders().get(0);\n    DocsAndPositionsEnum de = sr.termPositionsEnum(null, \"field\", new BytesRef(\"withPayload\"));\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1c93396a1df03720cb20e2c2f513a6fa59b21e4c","date":1345032673,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","sourceNew":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(new StringReader(\"here we go\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(new StringReader(\"another\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    AtomicReader sr = reader.getSequentialSubReaders().get(0);\n    DocsAndPositionsEnum de = sr.termPositionsEnum(null, \"field\", new BytesRef(\"withPayload\"));\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(new StringReader(\"here we go\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(new StringReader(\"another\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    AtomicReader sr = SlowCompositeReaderWrapper.wrap(reader);\n    DocsAndPositionsEnum de = sr.termPositionsEnum(null, \"field\", new BytesRef(\"withPayload\"));\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b05c56a41b733e02a189c48895922b5bd8c7f3d1","date":1345033322,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","sourceNew":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(new StringReader(\"here we go\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(new StringReader(\"another\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    AtomicReader sr = SlowCompositeReaderWrapper.wrap(reader);\n    DocsAndPositionsEnum de = sr.termPositionsEnum(null, \"field\", new BytesRef(\"withPayload\"));\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(new StringReader(\"here we go\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(new StringReader(\"another\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    AtomicReader sr = reader.getSequentialSubReaders().get(0);\n    DocsAndPositionsEnum de = sr.termPositionsEnum(null, \"field\", new BytesRef(\"withPayload\"));\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b6a0e3c1c21aac8ecf75706605133012833585c7","date":1347535263,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","sourceNew":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(new StringReader(\"here we go\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(new StringReader(\"another\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    AtomicReader sr = SlowCompositeReaderWrapper.wrap(reader);\n    DocsAndPositionsEnum de = sr.termPositionsEnum(new Term(\"field\", \"withPayload\"));\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(new StringReader(\"here we go\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(new StringReader(\"another\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    AtomicReader sr = SlowCompositeReaderWrapper.wrap(reader);\n    DocsAndPositionsEnum de = sr.termPositionsEnum(null, \"field\", new BytesRef(\"withPayload\"));\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","date":1389274049,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","sourceNew":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    AtomicReader sr = SlowCompositeReaderWrapper.wrap(reader);\n    DocsAndPositionsEnum de = sr.termPositionsEnum(new Term(\"field\", \"withPayload\"));\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(new StringReader(\"here we go\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(new StringReader(\"another\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    AtomicReader sr = SlowCompositeReaderWrapper.wrap(reader);\n    DocsAndPositionsEnum de = sr.termPositionsEnum(new Term(\"field\", \"withPayload\"));\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","sourceNew":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    AtomicReader sr = SlowCompositeReaderWrapper.wrap(reader);\n    DocsAndPositionsEnum de = sr.termPositionsEnum(new Term(\"field\", \"withPayload\"));\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.shutdown();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    AtomicReader sr = SlowCompositeReaderWrapper.wrap(reader);\n    DocsAndPositionsEnum de = sr.termPositionsEnum(new Term(\"field\", \"withPayload\"));\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"923f36bb0db6f793cf62dbb68723ae3bfbaf1d75","date":1399205975,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","sourceNew":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    AtomicReader sr = SlowCompositeReaderWrapper.wrap(reader);\n    DocsAndPositionsEnum de = sr.termPositionsEnum(new Term(\"field\", \"withPayload\"));\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.shutdown();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    AtomicReader sr = SlowCompositeReaderWrapper.wrap(reader);\n    DocsAndPositionsEnum de = sr.termPositionsEnum(new Term(\"field\", \"withPayload\"));\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.shutdown();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","sourceNew":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    AtomicReader sr = SlowCompositeReaderWrapper.wrap(reader);\n    DocsAndPositionsEnum de = sr.termPositionsEnum(new Term(\"field\", \"withPayload\"));\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.shutdown();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    AtomicReader sr = SlowCompositeReaderWrapper.wrap(reader);\n    DocsAndPositionsEnum de = sr.termPositionsEnum(new Term(\"field\", \"withPayload\"));\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.shutdown();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","sourceNew":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    AtomicReader sr = SlowCompositeReaderWrapper.wrap(reader);\n    DocsAndPositionsEnum de = sr.termPositionsEnum(new Term(\"field\", \"withPayload\"));\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    AtomicReader sr = SlowCompositeReaderWrapper.wrap(reader);\n    DocsAndPositionsEnum de = sr.termPositionsEnum(new Term(\"field\", \"withPayload\"));\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.shutdown();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","sourceNew":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    LeafReader sr = SlowCompositeReaderWrapper.wrap(reader);\n    DocsAndPositionsEnum de = sr.termPositionsEnum(new Term(\"field\", \"withPayload\"));\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    AtomicReader sr = SlowCompositeReaderWrapper.wrap(reader);\n    DocsAndPositionsEnum de = sr.termPositionsEnum(new Term(\"field\", \"withPayload\"));\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"51f5280f31484820499077f41fcdfe92d527d9dc","date":1423229122,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","sourceNew":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    LeafReader sr = SlowCompositeReaderWrapper.wrap(reader);\n    PostingsEnum de = sr.termDocsEnum(new Term(\"field\", \"withPayload\"), PostingsEnum.FLAG_POSITIONS);\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    LeafReader sr = SlowCompositeReaderWrapper.wrap(reader);\n    DocsAndPositionsEnum de = sr.termPositionsEnum(new Term(\"field\", \"withPayload\"));\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e73063b92d958076ef4ae8beb5f493e8ccdcecb4","date":1424177215,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","sourceNew":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    LeafReader sr = SlowCompositeReaderWrapper.wrap(reader);\n    PostingsEnum de = sr.postings(new Term(\"field\", \"withPayload\"), PostingsEnum.POSITIONS);\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    LeafReader sr = SlowCompositeReaderWrapper.wrap(reader);\n    PostingsEnum de = sr.termDocsEnum(new Term(\"field\", \"withPayload\"), PostingsEnum.FLAG_POSITIONS);\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a9023c2bf8056230665bace786651c0716d78d31","date":1424736578,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","sourceNew":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    LeafReader sr = SlowCompositeReaderWrapper.wrap(reader);\n    PostingsEnum de = sr.postings(new Term(\"field\", \"withPayload\"), PostingsEnum.PAYLOADS);\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    LeafReader sr = SlowCompositeReaderWrapper.wrap(reader);\n    PostingsEnum de = sr.postings(new Term(\"field\", \"withPayload\"), PostingsEnum.POSITIONS);\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1","date":1457644139,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","sourceNew":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    TermsEnum te = MultiFields.getFields(reader).terms(\"field\").iterator();\n    assertTrue(te.seekExact(new BytesRef(\"withPayload\")));\n    PostingsEnum de = te.postings(null, PostingsEnum.PAYLOADS);\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    LeafReader sr = SlowCompositeReaderWrapper.wrap(reader);\n    PostingsEnum de = sr.postings(new Term(\"field\", \"withPayload\"), PostingsEnum.PAYLOADS);\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":["c9fb5f46e264daf5ba3860defe623a89d202dd87","a9023c2bf8056230665bace786651c0716d78d31"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b","date":1497408244,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","sourceNew":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    TermsEnum te = MultiFields.getTerms(reader, \"field\").iterator();\n    assertTrue(te.seekExact(new BytesRef(\"withPayload\")));\n    PostingsEnum de = te.postings(null, PostingsEnum.PAYLOADS);\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    TermsEnum te = MultiFields.getFields(reader).terms(\"field\").iterator();\n    assertTrue(te.seekExact(new BytesRef(\"withPayload\")));\n    PostingsEnum de = te.postings(null, PostingsEnum.PAYLOADS);\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9","date":1498028748,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","sourceNew":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    TermsEnum te = MultiFields.getTerms(reader, \"field\").iterator();\n    assertTrue(te.seekExact(new BytesRef(\"withPayload\")));\n    PostingsEnum de = te.postings(null, PostingsEnum.PAYLOADS);\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    TermsEnum te = MultiFields.getFields(reader).terms(\"field\").iterator();\n    assertTrue(te.seekExact(new BytesRef(\"withPayload\")));\n    PostingsEnum de = te.postings(null, PostingsEnum.PAYLOADS);\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","sourceNew":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    TermsEnum te = MultiFields.getTerms(reader, \"field\").iterator();\n    assertTrue(te.seekExact(new BytesRef(\"withPayload\")));\n    PostingsEnum de = te.postings(null, PostingsEnum.PAYLOADS);\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    TermsEnum te = MultiFields.getFields(reader).terms(\"field\").iterator();\n    assertTrue(te.seekExact(new BytesRef(\"withPayload\")));\n    PostingsEnum de = te.postings(null, PostingsEnum.PAYLOADS);\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"04e775de416dd2d8067b10db1c8af975a1d5017e","date":1539906554,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#testMixupDocs().mjava","sourceNew":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    TermsEnum te = MultiTerms.getTerms(reader, \"field\").iterator();\n    assertTrue(te.seekExact(new BytesRef(\"withPayload\")));\n    PostingsEnum de = te.postings(null, PostingsEnum.PAYLOADS);\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(null);\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    Field field = new TextField(\"field\", \"\", Field.Store.NO);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    DirectoryReader reader = writer.getReader();\n    TermsEnum te = MultiFields.getTerms(reader, \"field\").iterator();\n    assertTrue(te.seekExact(new BytesRef(\"withPayload\")));\n    PostingsEnum de = te.postings(null, PostingsEnum.PAYLOADS);\n    de.nextDoc();\n    de.nextPosition();\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b":["0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1"],"3c188105a9aae04f56c24996f98f8333fc825d2e":["c7869f64c874ebf7f317d22c00baf2b6857797a6"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["b6a0e3c1c21aac8ecf75706605133012833585c7"],"b05c56a41b733e02a189c48895922b5bd8c7f3d1":["1c93396a1df03720cb20e2c2f513a6fa59b21e4c","ced66195b26fdb1f77ee00e2a77ec6918dedd766"],"c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198":["d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9","ced66195b26fdb1f77ee00e2a77ec6918dedd766"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1":["a9023c2bf8056230665bace786651c0716d78d31"],"75ffaed6edffab734233feb10c3a40decddfeb63":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"c7869f64c874ebf7f317d22c00baf2b6857797a6":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552"],"2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552":["e780da71c6146a490672a8ef0b709f7ede5229b1"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"],"e780da71c6146a490672a8ef0b709f7ede5229b1":["75ffaed6edffab734233feb10c3a40decddfeb63"],"ced66195b26fdb1f77ee00e2a77ec6918dedd766":["2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552"],"d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["923f36bb0db6f793cf62dbb68723ae3bfbaf1d75"],"923f36bb0db6f793cf62dbb68723ae3bfbaf1d75":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"1c93396a1df03720cb20e2c2f513a6fa59b21e4c":["3c188105a9aae04f56c24996f98f8333fc825d2e"],"51f5280f31484820499077f41fcdfe92d527d9dc":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"28288370235ed02234a64753cdbf0c6ec096304a":["0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1","e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b"],"e73063b92d958076ef4ae8beb5f493e8ccdcecb4":["51f5280f31484820499077f41fcdfe92d527d9dc"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"b6a0e3c1c21aac8ecf75706605133012833585c7":["ced66195b26fdb1f77ee00e2a77ec6918dedd766"],"04e775de416dd2d8067b10db1c8af975a1d5017e":["28288370235ed02234a64753cdbf0c6ec096304a"],"a9023c2bf8056230665bace786651c0716d78d31":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4"],"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9":["0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1","e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["04e775de416dd2d8067b10db1c8af975a1d5017e"]},"commit2Childs":{"e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b":["28288370235ed02234a64753cdbf0c6ec096304a","2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9"],"3c188105a9aae04f56c24996f98f8333fc825d2e":["1c93396a1df03720cb20e2c2f513a6fa59b21e4c"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"b05c56a41b733e02a189c48895922b5bd8c7f3d1":[],"c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198":[],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["51f5280f31484820499077f41fcdfe92d527d9dc"],"0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1":["e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b","28288370235ed02234a64753cdbf0c6ec096304a","2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9"],"c7869f64c874ebf7f317d22c00baf2b6857797a6":["3c188105a9aae04f56c24996f98f8333fc825d2e"],"75ffaed6edffab734233feb10c3a40decddfeb63":["e780da71c6146a490672a8ef0b709f7ede5229b1"],"2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552":["c7869f64c874ebf7f317d22c00baf2b6857797a6","ced66195b26fdb1f77ee00e2a77ec6918dedd766","d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["75ffaed6edffab734233feb10c3a40decddfeb63","c7869f64c874ebf7f317d22c00baf2b6857797a6","d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["923f36bb0db6f793cf62dbb68723ae3bfbaf1d75"],"e780da71c6146a490672a8ef0b709f7ede5229b1":["2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552"],"ced66195b26fdb1f77ee00e2a77ec6918dedd766":["b05c56a41b733e02a189c48895922b5bd8c7f3d1","c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198","b6a0e3c1c21aac8ecf75706605133012833585c7"],"d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9":["c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"923f36bb0db6f793cf62dbb68723ae3bfbaf1d75":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"1c93396a1df03720cb20e2c2f513a6fa59b21e4c":["b05c56a41b733e02a189c48895922b5bd8c7f3d1"],"51f5280f31484820499077f41fcdfe92d527d9dc":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4"],"28288370235ed02234a64753cdbf0c6ec096304a":["04e775de416dd2d8067b10db1c8af975a1d5017e"],"e73063b92d958076ef4ae8beb5f493e8ccdcecb4":["a9023c2bf8056230665bace786651c0716d78d31"],"b6a0e3c1c21aac8ecf75706605133012833585c7":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"04e775de416dd2d8067b10db1c8af975a1d5017e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a9023c2bf8056230665bace786651c0716d78d31":["0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1"],"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["b05c56a41b733e02a189c48895922b5bd8c7f3d1","c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198","2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}