{"path":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int).mjava","commits":[{"id":"30fd30bfbfa6b9e036bcd99c8339712e965d4a63","date":1351859294,"type":0,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int).mjava","pathOld":"/dev/null","sourceNew":"  public void addField(String fieldName, TokenStream stream, float boost, int positionIncrementGap) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info = null;\n      long sumTotalTermFreq = 0;\n      if ((info = fields.get(fieldName)) != null) {\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      if (!fieldInfos.containsKey(fieldName)) {\n        fieldInfos.put(fieldName, \n            new FieldInfo(fieldName, true, fieldInfos.size(), false, false, false, this.storeOffsets ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS , null, null, null));\n      }\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        if (ref.length == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        if (!storeOffsets) {\n          postingsWriter.writeInt(pos);\n        } else {\n          postingsWriter.writeInt(pos);\n          postingsWriter.writeInt(offsetAtt.startOffset());\n          postingsWriter.writeInt(offsetAtt.endOffset());\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(terms, sliceArray, numTokens, numOverlapTokens, boost, pos, sumTotalTermFreq));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (Exception e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["864042e09edf94aeba885c1835408c3f88ca2325","2eceb3be6e29e3ad7ee08d5025a431c3812aa0d9"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"51ab863868b3d59bc3ee8e196a92ac2c4847328e","date":1351888296,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int).mjava","pathOld":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   *\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost, int positionIncrementGap) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info = null;\n      long sumTotalTermFreq = 0;\n      if ((info = fields.get(fieldName)) != null) {\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      if (!fieldInfos.containsKey(fieldName)) {\n        fieldInfos.put(fieldName, \n            new FieldInfo(fieldName, true, fieldInfos.size(), false, false, false, this.storeOffsets ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS , null, null, null));\n      }\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        if (ref.length == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        if (!storeOffsets) {\n          postingsWriter.writeInt(pos);\n        } else {\n          postingsWriter.writeInt(pos);\n          postingsWriter.writeInt(offsetAtt.startOffset());\n          postingsWriter.writeInt(offsetAtt.endOffset());\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(terms, sliceArray, numTokens, numOverlapTokens, boost, pos, sumTotalTermFreq));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (Exception e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  public void addField(String fieldName, TokenStream stream, float boost, int positionIncrementGap) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info = null;\n      long sumTotalTermFreq = 0;\n      if ((info = fields.get(fieldName)) != null) {\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      if (!fieldInfos.containsKey(fieldName)) {\n        fieldInfos.put(fieldName, \n            new FieldInfo(fieldName, true, fieldInfos.size(), false, false, false, this.storeOffsets ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS , null, null, null));\n      }\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        if (ref.length == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        if (!storeOffsets) {\n          postingsWriter.writeInt(pos);\n        } else {\n          postingsWriter.writeInt(pos);\n          postingsWriter.writeInt(offsetAtt.startOffset());\n          postingsWriter.writeInt(offsetAtt.endOffset());\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(terms, sliceArray, numTokens, numOverlapTokens, boost, pos, sumTotalTermFreq));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (Exception e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["2eceb3be6e29e3ad7ee08d5025a431c3812aa0d9"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"081b68cb9e8f4b5405b40bfb223fd7c587171aa1","date":1360072766,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int).mjava","pathOld":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   *\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost, int positionIncrementGap) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info = null;\n      long sumTotalTermFreq = 0;\n      if ((info = fields.get(fieldName)) != null) {\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      if (!fieldInfos.containsKey(fieldName)) {\n        fieldInfos.put(fieldName, \n            new FieldInfo(fieldName, true, fieldInfos.size(), false, false, false, this.storeOffsets ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS , null, null, null));\n      }\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        if (ref.length == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        if (!storeOffsets) {\n          postingsWriter.writeInt(pos);\n        } else {\n          postingsWriter.writeInt(pos);\n          postingsWriter.writeInt(offsetAtt.startOffset());\n          postingsWriter.writeInt(offsetAtt.endOffset());\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(terms, sliceArray, numTokens, numOverlapTokens, boost, pos, sumTotalTermFreq));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (Exception e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) {\n          stream.close();\n        }\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   *\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost, int positionIncrementGap) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info = null;\n      long sumTotalTermFreq = 0;\n      if ((info = fields.get(fieldName)) != null) {\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      if (!fieldInfos.containsKey(fieldName)) {\n        fieldInfos.put(fieldName, \n            new FieldInfo(fieldName, true, fieldInfos.size(), false, false, false, this.storeOffsets ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS , null, null, null));\n      }\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        if (ref.length == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        if (!storeOffsets) {\n          postingsWriter.writeInt(pos);\n        } else {\n          postingsWriter.writeInt(pos);\n          postingsWriter.writeInt(offsetAtt.startOffset());\n          postingsWriter.writeInt(offsetAtt.endOffset());\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(terms, sliceArray, numTokens, numOverlapTokens, boost, pos, sumTotalTermFreq));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (Exception e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["2eceb3be6e29e3ad7ee08d5025a431c3812aa0d9"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3e4d4ec39bf5396230748ca859ff05ab024b6fc5","date":1360112310,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int).mjava","pathOld":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   *\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost, int positionIncrementGap) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info = null;\n      long sumTotalTermFreq = 0;\n      if ((info = fields.get(fieldName)) != null) {\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      if (!fieldInfos.containsKey(fieldName)) {\n        fieldInfos.put(fieldName, \n            new FieldInfo(fieldName, true, fieldInfos.size(), false, false, false, this.storeOffsets ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS , null, null, null));\n      }\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        if (ref.length == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        if (!storeOffsets) {\n          postingsWriter.writeInt(pos);\n        } else {\n          postingsWriter.writeInt(pos);\n          postingsWriter.writeInt(offsetAtt.startOffset());\n          postingsWriter.writeInt(offsetAtt.endOffset());\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(terms, sliceArray, numTokens, numOverlapTokens, boost, pos, sumTotalTermFreq));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (Exception e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) {\n          stream.close();\n        }\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   *\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost, int positionIncrementGap) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info = null;\n      long sumTotalTermFreq = 0;\n      if ((info = fields.get(fieldName)) != null) {\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      if (!fieldInfos.containsKey(fieldName)) {\n        fieldInfos.put(fieldName, \n            new FieldInfo(fieldName, true, fieldInfos.size(), false, false, false, this.storeOffsets ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS , null, null, null));\n      }\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        if (ref.length == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        if (!storeOffsets) {\n          postingsWriter.writeInt(pos);\n        } else {\n          postingsWriter.writeInt(pos);\n          postingsWriter.writeInt(offsetAtt.startOffset());\n          postingsWriter.writeInt(offsetAtt.endOffset());\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(terms, sliceArray, numTokens, numOverlapTokens, boost, pos, sumTotalTermFreq));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (Exception e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"864042e09edf94aeba885c1835408c3f88ca2325","date":1365514091,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int).mjava","pathOld":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   *\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost, int positionIncrementGap) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info = null;\n      long sumTotalTermFreq = 0;\n      if ((info = fields.get(fieldName)) != null) {\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      if (!fieldInfos.containsKey(fieldName)) {\n        fieldInfos.put(fieldName, \n            new FieldInfo(fieldName, true, fieldInfos.size(), false, false, false, this.storeOffsets ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS , null, null, null));\n      }\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        if (!storeOffsets) {\n          postingsWriter.writeInt(pos);\n        } else {\n          postingsWriter.writeInt(pos);\n          postingsWriter.writeInt(offsetAtt.startOffset());\n          postingsWriter.writeInt(offsetAtt.endOffset());\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(terms, sliceArray, numTokens, numOverlapTokens, boost, pos, sumTotalTermFreq));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (Exception e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) {\n          stream.close();\n        }\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   *\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost, int positionIncrementGap) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info = null;\n      long sumTotalTermFreq = 0;\n      if ((info = fields.get(fieldName)) != null) {\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      if (!fieldInfos.containsKey(fieldName)) {\n        fieldInfos.put(fieldName, \n            new FieldInfo(fieldName, true, fieldInfos.size(), false, false, false, this.storeOffsets ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS , null, null, null));\n      }\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        if (ref.length == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        if (!storeOffsets) {\n          postingsWriter.writeInt(pos);\n        } else {\n          postingsWriter.writeInt(pos);\n          postingsWriter.writeInt(offsetAtt.startOffset());\n          postingsWriter.writeInt(offsetAtt.endOffset());\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(terms, sliceArray, numTokens, numOverlapTokens, boost, pos, sumTotalTermFreq));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (Exception e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) {\n          stream.close();\n        }\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":["30fd30bfbfa6b9e036bcd99c8339712e965d4a63"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2eceb3be6e29e3ad7ee08d5025a431c3812aa0d9","date":1392385887,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int).mjava","pathOld":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via\n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   *\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   *\n   *\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost, int positionIncrementGap) {\n    addField(fieldName, stream, boost, positionIncrementGap, 1);\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   *\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost, int positionIncrementGap) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info = null;\n      long sumTotalTermFreq = 0;\n      if ((info = fields.get(fieldName)) != null) {\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      if (!fieldInfos.containsKey(fieldName)) {\n        fieldInfos.put(fieldName, \n            new FieldInfo(fieldName, true, fieldInfos.size(), false, false, false, this.storeOffsets ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS , null, null, null));\n      }\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        if (!storeOffsets) {\n          postingsWriter.writeInt(pos);\n        } else {\n          postingsWriter.writeInt(pos);\n          postingsWriter.writeInt(offsetAtt.startOffset());\n          postingsWriter.writeInt(offsetAtt.endOffset());\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(terms, sliceArray, numTokens, numOverlapTokens, boost, pos, sumTotalTermFreq));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (Exception e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) {\n          stream.close();\n        }\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":["30fd30bfbfa6b9e036bcd99c8339712e965d4a63","51ab863868b3d59bc3ee8e196a92ac2c4847328e","081b68cb9e8f4b5405b40bfb223fd7c587171aa1"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"acd9883560fd89e6448b2b447302fe543040cd4f","date":1488478696,"type":5,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,int).mjava","pathOld":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via\n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   *\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   *\n   */\n  public void addField(String fieldName, TokenStream stream, int positionIncrementGap) {\n    addField(fieldName, stream, positionIncrementGap, 1);\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via\n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   *\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   *\n   *\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost, int positionIncrementGap) {\n    addField(fieldName, stream, boost, positionIncrementGap, 1);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"081b68cb9e8f4b5405b40bfb223fd7c587171aa1":["51ab863868b3d59bc3ee8e196a92ac2c4847328e"],"51ab863868b3d59bc3ee8e196a92ac2c4847328e":["30fd30bfbfa6b9e036bcd99c8339712e965d4a63"],"3e4d4ec39bf5396230748ca859ff05ab024b6fc5":["51ab863868b3d59bc3ee8e196a92ac2c4847328e","081b68cb9e8f4b5405b40bfb223fd7c587171aa1"],"30fd30bfbfa6b9e036bcd99c8339712e965d4a63":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"acd9883560fd89e6448b2b447302fe543040cd4f":["2eceb3be6e29e3ad7ee08d5025a431c3812aa0d9"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["acd9883560fd89e6448b2b447302fe543040cd4f"],"2eceb3be6e29e3ad7ee08d5025a431c3812aa0d9":["864042e09edf94aeba885c1835408c3f88ca2325"],"864042e09edf94aeba885c1835408c3f88ca2325":["081b68cb9e8f4b5405b40bfb223fd7c587171aa1"]},"commit2Childs":{"081b68cb9e8f4b5405b40bfb223fd7c587171aa1":["3e4d4ec39bf5396230748ca859ff05ab024b6fc5","864042e09edf94aeba885c1835408c3f88ca2325"],"51ab863868b3d59bc3ee8e196a92ac2c4847328e":["081b68cb9e8f4b5405b40bfb223fd7c587171aa1","3e4d4ec39bf5396230748ca859ff05ab024b6fc5"],"3e4d4ec39bf5396230748ca859ff05ab024b6fc5":[],"30fd30bfbfa6b9e036bcd99c8339712e965d4a63":["51ab863868b3d59bc3ee8e196a92ac2c4847328e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["30fd30bfbfa6b9e036bcd99c8339712e965d4a63"],"acd9883560fd89e6448b2b447302fe543040cd4f":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"2eceb3be6e29e3ad7ee08d5025a431c3812aa0d9":["acd9883560fd89e6448b2b447302fe543040cd4f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"864042e09edf94aeba885c1835408c3f88ca2325":["2eceb3be6e29e3ad7ee08d5025a431c3812aa0d9"]},"heads":["3e4d4ec39bf5396230748ca859ff05ab024b6fc5","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}