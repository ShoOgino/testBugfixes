{"path":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/IndexTimeSynonymTest.TokenArrayAnalyzer#createComponents(String,Reader).mjava","commits":[{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/IndexTimeSynonymTest.TokenArrayAnalyzer#createComponents(String,Reader).mjava","pathOld":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/vectorhighlight/IndexTimeSynonymTest.TokenArrayAnalyzer#createComponents(String,Reader).mjava","sourceNew":"    @Override\n    public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n      Tokenizer ts = new Tokenizer(Token.TOKEN_ATTRIBUTE_FACTORY, reader) {\n        final AttributeImpl reusableToken = (AttributeImpl) addAttribute(CharTermAttribute.class);\n        int p = 0;\n        \n        @Override\n        public boolean incrementToken() throws IOException {\n          if( p >= tokens.length ) return false;\n          clearAttributes();\n          tokens[p++].copyTo(reusableToken);\n          return true;\n        }\n\n        @Override\n        public void reset() throws IOException {\n          super.reset();\n          this.p = 0;\n        }\n      };\n      return new TokenStreamComponents(ts);\n    }\n\n","sourceOld":"    @Override\n    public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n      Tokenizer ts = new Tokenizer(Token.TOKEN_ATTRIBUTE_FACTORY, reader) {\n        final AttributeImpl reusableToken = (AttributeImpl) addAttribute(CharTermAttribute.class);\n        int p = 0;\n        \n        @Override\n        public boolean incrementToken() throws IOException {\n          if( p >= tokens.length ) return false;\n          clearAttributes();\n          tokens[p++].copyTo(reusableToken);\n          return true;\n        }\n\n        @Override\n        public void reset() throws IOException {\n          super.reset();\n          this.p = 0;\n        }\n      };\n      return new TokenStreamComponents(ts);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4d3e8520fd031bab31fd0e4d480e55958bc45efe","date":1340901565,"type":3,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/IndexTimeSynonymTest.TokenArrayAnalyzer#createComponents(String,Reader).mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/IndexTimeSynonymTest.TokenArrayAnalyzer#createComponents(String,Reader).mjava","sourceNew":"    @Override\n    public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n      Tokenizer ts = new Tokenizer(Token.TOKEN_ATTRIBUTE_FACTORY, reader) {\n        final AttributeImpl reusableToken = (AttributeImpl) addAttribute(CharTermAttribute.class);\n        int p = 0;\n        \n        @Override\n        public boolean incrementToken() {\n          if( p >= tokens.length ) return false;\n          clearAttributes();\n          tokens[p++].copyTo(reusableToken);\n          return true;\n        }\n\n        @Override\n        public void reset() throws IOException {\n          super.reset();\n          this.p = 0;\n        }\n      };\n      return new TokenStreamComponents(ts);\n    }\n\n","sourceOld":"    @Override\n    public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n      Tokenizer ts = new Tokenizer(Token.TOKEN_ATTRIBUTE_FACTORY, reader) {\n        final AttributeImpl reusableToken = (AttributeImpl) addAttribute(CharTermAttribute.class);\n        int p = 0;\n        \n        @Override\n        public boolean incrementToken() throws IOException {\n          if( p >= tokens.length ) return false;\n          clearAttributes();\n          tokens[p++].copyTo(reusableToken);\n          return true;\n        }\n\n        @Override\n        public void reset() throws IOException {\n          super.reset();\n          this.p = 0;\n        }\n      };\n      return new TokenStreamComponents(ts);\n    }\n\n","bugFix":["9b5756469957918cac40a831acec9cf01c8c2bb3"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe33227f6805edab2036cbb80645cc4e2d1fa424","date":1342713534,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/IndexTimeSynonymTest.TokenArrayAnalyzer#createComponents(String,Reader).mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/IndexTimeSynonymTest.TokenArrayAnalyzer#createComponents(String,Reader).mjava","sourceNew":"    @Override\n    public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n      Tokenizer ts = new Tokenizer(Token.TOKEN_ATTRIBUTE_FACTORY, reader) {\n        final AttributeImpl reusableToken = (AttributeImpl) addAttribute(CharTermAttribute.class);\n        int p = 0;\n        \n        @Override\n        public boolean incrementToken() {\n          if( p >= tokens.length ) return false;\n          clearAttributes();\n          tokens[p++].copyTo(reusableToken);\n          return true;\n        }\n\n        @Override\n        public void reset() throws IOException {\n          super.reset();\n          this.p = 0;\n        }\n      };\n      return new TokenStreamComponents(ts);\n    }\n\n","sourceOld":"    @Override\n    public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n      Tokenizer ts = new Tokenizer(Token.TOKEN_ATTRIBUTE_FACTORY, reader) {\n        final AttributeImpl reusableToken = (AttributeImpl) addAttribute(CharTermAttribute.class);\n        int p = 0;\n        \n        @Override\n        public boolean incrementToken() throws IOException {\n          if( p >= tokens.length ) return false;\n          clearAttributes();\n          tokens[p++].copyTo(reusableToken);\n          return true;\n        }\n\n        @Override\n        public void reset() throws IOException {\n          super.reset();\n          this.p = 0;\n        }\n      };\n      return new TokenStreamComponents(ts);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","date":1389274049,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/IndexTimeSynonymTest.TokenArrayAnalyzer#createComponents(String).mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/IndexTimeSynonymTest.TokenArrayAnalyzer#createComponents(String,Reader).mjava","sourceNew":"    @Override\n    public TokenStreamComponents createComponents(String fieldName) {\n      Tokenizer ts = new Tokenizer(Token.TOKEN_ATTRIBUTE_FACTORY) {\n        final AttributeImpl reusableToken = (AttributeImpl) addAttribute(CharTermAttribute.class);\n        int p = 0;\n        \n        @Override\n        public boolean incrementToken() {\n          if( p >= tokens.length ) return false;\n          clearAttributes();\n          tokens[p++].copyTo(reusableToken);\n          return true;\n        }\n\n        @Override\n        public void reset() throws IOException {\n          super.reset();\n          this.p = 0;\n        }\n      };\n      return new TokenStreamComponents(ts);\n    }\n\n","sourceOld":"    @Override\n    public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n      Tokenizer ts = new Tokenizer(Token.TOKEN_ATTRIBUTE_FACTORY, reader) {\n        final AttributeImpl reusableToken = (AttributeImpl) addAttribute(CharTermAttribute.class);\n        int p = 0;\n        \n        @Override\n        public boolean incrementToken() {\n          if( p >= tokens.length ) return false;\n          clearAttributes();\n          tokens[p++].copyTo(reusableToken);\n          return true;\n        }\n\n        @Override\n        public void reset() throws IOException {\n          super.reset();\n          this.p = 0;\n        }\n      };\n      return new TokenStreamComponents(ts);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["4d3e8520fd031bab31fd0e4d480e55958bc45efe"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":["b89678825b68eccaf09e6ab71675fc0b0af1e099","4d3e8520fd031bab31fd0e4d480e55958bc45efe"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"4d3e8520fd031bab31fd0e4d480e55958bc45efe":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"]},"commit2Childs":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["fe33227f6805edab2036cbb80645cc4e2d1fa424","4d3e8520fd031bab31fd0e4d480e55958bc45efe"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"4d3e8520fd031bab31fd0e4d480e55958bc45efe":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","fe33227f6805edab2036cbb80645cc4e2d1fa424"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["fe33227f6805edab2036cbb80645cc4e2d1fa424","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}