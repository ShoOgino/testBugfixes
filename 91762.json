{"path":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","commits":[{"id":"c903c3d15906a3da96b8c0c2fb704491005fdbdb","date":1453508227,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","pathOld":"solr/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","sourceNew":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {\n        IndexReader reader = context.reader;\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n        };\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {\n        IndexReader reader = context.reader;\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n        };\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a258fbb26824fd104ed795e5d9033d2d040049ee","date":1453508252,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","pathOld":"solr/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","sourceNew":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {\n        IndexReader reader = context.reader;\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n        };\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {\n        IndexReader reader = context.reader;\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n        };\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c26f00b574427b55127e869b935845554afde1fa","date":1310252513,"type":1,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","pathOld":"solr/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","sourceNew":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {\n        IndexReader reader = context.reader;\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n        };\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {\n        IndexReader reader = context.reader;\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n        };\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6620df8541b174097b1133a4fc370adb2e570524","date":1319544675,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","sourceNew":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(final AtomicReaderContext context, final Bits acceptDocs) throws IOException {\n        IndexReader reader = context.reader;\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return BitsFilteredDocIdSet.wrap(new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n          @Override\n          public Bits bits() throws IOException {\n            // random access is expensive for this set\n            return null;\n          }\n\n        }, acceptDocs);\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {\n        IndexReader reader = context.reader;\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n        };\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":["4d3e8520fd031bab31fd0e4d480e55958bc45efe"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"96d207426bd26fa5c1014e26d21d87603aea68b7","date":1327944562,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","sourceNew":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(final AtomicReaderContext context, final Bits acceptDocs) throws IOException {\n        IndexReader reader = context.reader();\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return BitsFilteredDocIdSet.wrap(new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n          @Override\n          public Bits bits() throws IOException {\n            // random access is expensive for this set\n            return null;\n          }\n\n        }, acceptDocs);\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(final AtomicReaderContext context, final Bits acceptDocs) throws IOException {\n        IndexReader reader = context.reader;\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return BitsFilteredDocIdSet.wrap(new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n          @Override\n          public Bits bits() throws IOException {\n            // random access is expensive for this set\n            return null;\n          }\n\n        }, acceptDocs);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5cab9a86bd67202d20b6adc463008c8e982b070a","date":1327966443,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","sourceNew":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(final AtomicReaderContext context, final Bits acceptDocs) throws IOException {\n        IndexReader reader = context.reader();\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return BitsFilteredDocIdSet.wrap(new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n          @Override\n          public Bits bits() throws IOException {\n            // random access is expensive for this set\n            return null;\n          }\n\n        }, acceptDocs);\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(final AtomicReaderContext context, final Bits acceptDocs) throws IOException {\n        IndexReader reader = context.reader;\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return BitsFilteredDocIdSet.wrap(new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n          @Override\n          public Bits bits() throws IOException {\n            // random access is expensive for this set\n            return null;\n          }\n\n        }, acceptDocs);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8e5663809fccfda938d8d46f0106a5301cdd5cf0","date":1328146670,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","sourceNew":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(final AtomicReaderContext context, final Bits acceptDocs) throws IOException {\n        AtomicReader reader = context.reader();\n        // all Solr DocSets that are used as filters only include live docs\n        final Bits acceptDocs2 = acceptDocs == null ? null : (reader.getLiveDocs() == acceptDocs ? null : acceptDocs);\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return BitsFilteredDocIdSet.wrap(new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n          @Override\n          public Bits bits() throws IOException {\n            // random access is expensive for this set\n            return null;\n          }\n\n        }, acceptDocs2);\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(final AtomicReaderContext context, final Bits acceptDocs) throws IOException {\n        IndexReader reader = context.reader();\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return BitsFilteredDocIdSet.wrap(new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n          @Override\n          public Bits bits() throws IOException {\n            // random access is expensive for this set\n            return null;\n          }\n\n        }, acceptDocs);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4d3e8520fd031bab31fd0e4d480e55958bc45efe","date":1340901565,"type":3,"author":"Steven Rowe","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","sourceNew":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(final AtomicReaderContext context, final Bits acceptDocs) {\n        AtomicReader reader = context.reader();\n        // all Solr DocSets that are used as filters only include live docs\n        final Bits acceptDocs2 = acceptDocs == null ? null : (reader.getLiveDocs() == acceptDocs ? null : acceptDocs);\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return BitsFilteredDocIdSet.wrap(new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n          @Override\n          public Bits bits() {\n            // random access is expensive for this set\n            return null;\n          }\n\n        }, acceptDocs2);\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(final AtomicReaderContext context, final Bits acceptDocs) throws IOException {\n        AtomicReader reader = context.reader();\n        // all Solr DocSets that are used as filters only include live docs\n        final Bits acceptDocs2 = acceptDocs == null ? null : (reader.getLiveDocs() == acceptDocs ? null : acceptDocs);\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return BitsFilteredDocIdSet.wrap(new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n          @Override\n          public Bits bits() throws IOException {\n            // random access is expensive for this set\n            return null;\n          }\n\n        }, acceptDocs2);\n      }\n    };\n  }\n\n","bugFix":["1919b234a992d6438a59ccbb02bd0656162e602e","19a6a2b00bd8d0dbd9684de00bb3018e4664a1e5","6620df8541b174097b1133a4fc370adb2e570524"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe33227f6805edab2036cbb80645cc4e2d1fa424","date":1342713534,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","sourceNew":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(final AtomicReaderContext context, final Bits acceptDocs) {\n        AtomicReader reader = context.reader();\n        // all Solr DocSets that are used as filters only include live docs\n        final Bits acceptDocs2 = acceptDocs == null ? null : (reader.getLiveDocs() == acceptDocs ? null : acceptDocs);\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return BitsFilteredDocIdSet.wrap(new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n          @Override\n          public Bits bits() {\n            // random access is expensive for this set\n            return null;\n          }\n\n        }, acceptDocs2);\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(final AtomicReaderContext context, final Bits acceptDocs) throws IOException {\n        AtomicReader reader = context.reader();\n        // all Solr DocSets that are used as filters only include live docs\n        final Bits acceptDocs2 = acceptDocs == null ? null : (reader.getLiveDocs() == acceptDocs ? null : acceptDocs);\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return BitsFilteredDocIdSet.wrap(new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n          @Override\n          public Bits bits() throws IOException {\n            // random access is expensive for this set\n            return null;\n          }\n\n        }, acceptDocs2);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7b9f056598bc578796f7c2eaa4b2bb8eaab5c23a","date":1363294103,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","sourceNew":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(final AtomicReaderContext context, final Bits acceptDocs) {\n        AtomicReader reader = context.reader();\n        // all Solr DocSets that are used as filters only include live docs\n        final Bits acceptDocs2 = acceptDocs == null ? null : (reader.getLiveDocs() == acceptDocs ? null : acceptDocs);\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return BitsFilteredDocIdSet.wrap(new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n              @Override\n              public long cost() {\n                return docs.length;\n              }\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n          @Override\n          public Bits bits() {\n            // random access is expensive for this set\n            return null;\n          }\n\n        }, acceptDocs2);\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(final AtomicReaderContext context, final Bits acceptDocs) {\n        AtomicReader reader = context.reader();\n        // all Solr DocSets that are used as filters only include live docs\n        final Bits acceptDocs2 = acceptDocs == null ? null : (reader.getLiveDocs() == acceptDocs ? null : acceptDocs);\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return BitsFilteredDocIdSet.wrap(new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n          @Override\n          public Bits bits() {\n            // random access is expensive for this set\n            return null;\n          }\n\n        }, acceptDocs2);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"19275ba31e621f6da1b83bf13af75233876fd3d4","date":1374846698,"type":3,"author":"Han Jiang","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","sourceNew":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(final AtomicReaderContext context, final Bits acceptDocs) {\n        AtomicReader reader = context.reader();\n        // all Solr DocSets that are used as filters only include live docs\n        final Bits acceptDocs2 = acceptDocs == null ? null : (reader.getLiveDocs() == acceptDocs ? null : acceptDocs);\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return BitsFilteredDocIdSet.wrap(new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n              @Override\n              public long cost() {\n                return docs.length;\n              }\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n          @Override\n          public Bits bits() {\n            // random access is expensive for this set\n            return null;\n          }\n\n        }, acceptDocs2);\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(final AtomicReaderContext context, final Bits acceptDocs) {\n        AtomicReader reader = context.reader();\n        // all Solr DocSets that are used as filters only include live docs\n        final Bits acceptDocs2 = acceptDocs == null ? null : (reader.getLiveDocs() == acceptDocs ? null : acceptDocs);\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return BitsFilteredDocIdSet.wrap(new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n              @Override\n              public long cost() {\n                return docs.length;\n              }\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n          @Override\n          public Bits bits() {\n            // random access is expensive for this set\n            return null;\n          }\n\n        }, acceptDocs2);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"716d18f3a9b0993bc679d7fa7abdc9bfb03411ec","date":1376375609,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","sourceNew":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(final AtomicReaderContext context, final Bits acceptDocs) {\n        AtomicReader reader = context.reader();\n        // all Solr DocSets that are used as filters only include live docs\n        final Bits acceptDocs2 = acceptDocs == null ? null : (reader.getLiveDocs() == acceptDocs ? null : acceptDocs);\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return BitsFilteredDocIdSet.wrap(new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n              @Override\n              public long cost() {\n                return docs.length;\n              }\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n          @Override\n          public Bits bits() {\n            // random access is expensive for this set\n            return null;\n          }\n\n        }, acceptDocs2);\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(final AtomicReaderContext context, final Bits acceptDocs) {\n        AtomicReader reader = context.reader();\n        // all Solr DocSets that are used as filters only include live docs\n        final Bits acceptDocs2 = acceptDocs == null ? null : (reader.getLiveDocs() == acceptDocs ? null : acceptDocs);\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return BitsFilteredDocIdSet.wrap(new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n              @Override\n              public long cost() {\n                return docs.length;\n              }\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n          @Override\n          public Bits bits() {\n            // random access is expensive for this set\n            return null;\n          }\n\n        }, acceptDocs2);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54ea8c8c94ae9da9a366175e2abbe1dde3aa0453","date":1402659583,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","sourceNew":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(final AtomicReaderContext context, final Bits acceptDocs) {\n        AtomicReader reader = context.reader();\n        // all Solr DocSets that are used as filters only include live docs\n        final Bits acceptDocs2 = acceptDocs == null ? null : (reader.getLiveDocs() == acceptDocs ? null : acceptDocs);\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return BitsFilteredDocIdSet.wrap(new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n              @Override\n              public long cost() {\n                return docs.length;\n              }\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n          @Override\n          public long ramBytesUsed() {\n            return RamUsageEstimator.sizeOf(docs);\n          }\n          \n          @Override\n          public Bits bits() {\n            // random access is expensive for this set\n            return null;\n          }\n\n        }, acceptDocs2);\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(final AtomicReaderContext context, final Bits acceptDocs) {\n        AtomicReader reader = context.reader();\n        // all Solr DocSets that are used as filters only include live docs\n        final Bits acceptDocs2 = acceptDocs == null ? null : (reader.getLiveDocs() == acceptDocs ? null : acceptDocs);\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return BitsFilteredDocIdSet.wrap(new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n              @Override\n              public long cost() {\n                return docs.length;\n              }\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n          @Override\n          public Bits bits() {\n            // random access is expensive for this set\n            return null;\n          }\n\n        }, acceptDocs2);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c6f080a2ab37c464dd98db173f6cbf10dc74f211","date":1402946779,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","sourceNew":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(final AtomicReaderContext context, final Bits acceptDocs) {\n        AtomicReader reader = context.reader();\n        // all Solr DocSets that are used as filters only include live docs\n        final Bits acceptDocs2 = acceptDocs == null ? null : (reader.getLiveDocs() == acceptDocs ? null : acceptDocs);\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return BitsFilteredDocIdSet.wrap(new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n              @Override\n              public long cost() {\n                return docs.length;\n              }\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n          @Override\n          public long ramBytesUsed() {\n            return RamUsageEstimator.sizeOf(docs);\n          }\n          \n          @Override\n          public Bits bits() {\n            // random access is expensive for this set\n            return null;\n          }\n\n        }, acceptDocs2);\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(final AtomicReaderContext context, final Bits acceptDocs) {\n        AtomicReader reader = context.reader();\n        // all Solr DocSets that are used as filters only include live docs\n        final Bits acceptDocs2 = acceptDocs == null ? null : (reader.getLiveDocs() == acceptDocs ? null : acceptDocs);\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return BitsFilteredDocIdSet.wrap(new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n              @Override\n              public long cost() {\n                return docs.length;\n              }\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n          @Override\n          public Bits bits() {\n            // random access is expensive for this set\n            return null;\n          }\n\n        }, acceptDocs2);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","sourceNew":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(final LeafReaderContext context, final Bits acceptDocs) {\n        LeafReader reader = context.reader();\n        // all Solr DocSets that are used as filters only include live docs\n        final Bits acceptDocs2 = acceptDocs == null ? null : (reader.getLiveDocs() == acceptDocs ? null : acceptDocs);\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return BitsFilteredDocIdSet.wrap(new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n              @Override\n              public long cost() {\n                return docs.length;\n              }\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n          @Override\n          public long ramBytesUsed() {\n            return RamUsageEstimator.sizeOf(docs);\n          }\n          \n          @Override\n          public Bits bits() {\n            // random access is expensive for this set\n            return null;\n          }\n\n        }, acceptDocs2);\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(final AtomicReaderContext context, final Bits acceptDocs) {\n        AtomicReader reader = context.reader();\n        // all Solr DocSets that are used as filters only include live docs\n        final Bits acceptDocs2 = acceptDocs == null ? null : (reader.getLiveDocs() == acceptDocs ? null : acceptDocs);\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return BitsFilteredDocIdSet.wrap(new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n              @Override\n              public long cost() {\n                return docs.length;\n              }\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n          @Override\n          public long ramBytesUsed() {\n            return RamUsageEstimator.sizeOf(docs);\n          }\n          \n          @Override\n          public Bits bits() {\n            // random access is expensive for this set\n            return null;\n          }\n\n        }, acceptDocs2);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fb58c5f77afb63ba911f6d62f4c1d89f15e56dc6","date":1424027250,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","sourceNew":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(final LeafReaderContext context, final Bits acceptDocs) {\n        LeafReader reader = context.reader();\n        // all Solr DocSets that are used as filters only include live docs\n        final Bits acceptDocs2 = acceptDocs == null ? null : (reader.getLiveDocs() == acceptDocs ? null : acceptDocs);\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return BitsFilteredDocIdSet.wrap(new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n              @Override\n              public long cost() {\n                return docs.length;\n              }\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n          @Override\n          public long ramBytesUsed() {\n            return RamUsageEstimator.sizeOf(docs);\n          }\n          \n          @Override\n          public Bits bits() {\n            // random access is expensive for this set\n            return null;\n          }\n\n        }, acceptDocs2);\n      }\n      @Override\n      public String toString(String field) {\n        return \"SortedIntDocSetTopFilter\";\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(final LeafReaderContext context, final Bits acceptDocs) {\n        LeafReader reader = context.reader();\n        // all Solr DocSets that are used as filters only include live docs\n        final Bits acceptDocs2 = acceptDocs == null ? null : (reader.getLiveDocs() == acceptDocs ? null : acceptDocs);\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return BitsFilteredDocIdSet.wrap(new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n              @Override\n              public long cost() {\n                return docs.length;\n              }\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n          @Override\n          public long ramBytesUsed() {\n            return RamUsageEstimator.sizeOf(docs);\n          }\n          \n          @Override\n          public Bits bits() {\n            // random access is expensive for this set\n            return null;\n          }\n\n        }, acceptDocs2);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"60596f28be69b10c37a56a303c2dbea07b2ca4ba","date":1425060541,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","sourceNew":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(final LeafReaderContext context, final Bits acceptDocs) {\n        LeafReader reader = context.reader();\n        // all Solr DocSets that are used as filters only include live docs\n        final Bits acceptDocs2 = acceptDocs == null ? null : (reader.getLiveDocs() == acceptDocs ? null : acceptDocs);\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return BitsFilteredDocIdSet.wrap(new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n              @Override\n              public long cost() {\n                return docs.length;\n              }\n            };\n          }\n\n          @Override\n          public long ramBytesUsed() {\n            return RamUsageEstimator.sizeOf(docs);\n          }\n          \n          @Override\n          public Bits bits() {\n            // random access is expensive for this set\n            return null;\n          }\n\n        }, acceptDocs2);\n      }\n      @Override\n      public String toString(String field) {\n        return \"SortedIntDocSetTopFilter\";\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(final LeafReaderContext context, final Bits acceptDocs) {\n        LeafReader reader = context.reader();\n        // all Solr DocSets that are used as filters only include live docs\n        final Bits acceptDocs2 = acceptDocs == null ? null : (reader.getLiveDocs() == acceptDocs ? null : acceptDocs);\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return BitsFilteredDocIdSet.wrap(new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n              @Override\n              public long cost() {\n                return docs.length;\n              }\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n          @Override\n          public long ramBytesUsed() {\n            return RamUsageEstimator.sizeOf(docs);\n          }\n          \n          @Override\n          public Bits bits() {\n            // random access is expensive for this set\n            return null;\n          }\n\n        }, acceptDocs2);\n      }\n      @Override\n      public String toString(String field) {\n        return \"SortedIntDocSetTopFilter\";\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":3,"author":"Ryan Ernst","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","sourceNew":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(final LeafReaderContext context, final Bits acceptDocs) {\n        LeafReader reader = context.reader();\n        // all Solr DocSets that are used as filters only include live docs\n        final Bits acceptDocs2 = acceptDocs == null ? null : (reader.getLiveDocs() == acceptDocs ? null : acceptDocs);\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return BitsFilteredDocIdSet.wrap(new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n              @Override\n              public long cost() {\n                return docs.length;\n              }\n            };\n          }\n\n          @Override\n          public long ramBytesUsed() {\n            return RamUsageEstimator.sizeOf(docs);\n          }\n          \n          @Override\n          public Bits bits() {\n            // random access is expensive for this set\n            return null;\n          }\n\n        }, acceptDocs2);\n      }\n      @Override\n      public String toString(String field) {\n        return \"SortedIntDocSetTopFilter\";\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(final LeafReaderContext context, final Bits acceptDocs) {\n        LeafReader reader = context.reader();\n        // all Solr DocSets that are used as filters only include live docs\n        final Bits acceptDocs2 = acceptDocs == null ? null : (reader.getLiveDocs() == acceptDocs ? null : acceptDocs);\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return BitsFilteredDocIdSet.wrap(new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n              @Override\n              public long cost() {\n                return docs.length;\n              }\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n          @Override\n          public long ramBytesUsed() {\n            return RamUsageEstimator.sizeOf(docs);\n          }\n          \n          @Override\n          public Bits bits() {\n            // random access is expensive for this set\n            return null;\n          }\n\n        }, acceptDocs2);\n      }\n      @Override\n      public String toString(String field) {\n        return \"SortedIntDocSetTopFilter\";\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0e7bc21595222ae4f75509300fbb7726691f387f","date":1464078795,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","sourceNew":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(final LeafReaderContext context, final Bits acceptDocs) {\n        LeafReader reader = context.reader();\n        // all Solr DocSets that are used as filters only include live docs\n        final Bits acceptDocs2 = acceptDocs == null ? null : (reader.getLiveDocs() == acceptDocs ? null : acceptDocs);\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return BitsFilteredDocIdSet.wrap(new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n              @Override\n              public long cost() {\n                return docs.length;\n              }\n            };\n          }\n\n          @Override\n          public long ramBytesUsed() {\n            return RamUsageEstimator.sizeOf(docs);\n          }\n          \n          @Override\n          public Bits bits() {\n            // random access is expensive for this set\n            return null;\n          }\n\n        }, acceptDocs2);\n      }\n      @Override\n      public String toString(String field) {\n        return \"SortedIntDocSetTopFilter\";\n      }\n\n      // Equivalence should/could be based on docs here? How did it work previously?\n\n      @Override\n      public boolean equals(Object other) {\n        return other == this;\n      }\n\n      @Override\n      public int hashCode() {\n        return System.identityHashCode(this);\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(final LeafReaderContext context, final Bits acceptDocs) {\n        LeafReader reader = context.reader();\n        // all Solr DocSets that are used as filters only include live docs\n        final Bits acceptDocs2 = acceptDocs == null ? null : (reader.getLiveDocs() == acceptDocs ? null : acceptDocs);\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return BitsFilteredDocIdSet.wrap(new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n              @Override\n              public long cost() {\n                return docs.length;\n              }\n            };\n          }\n\n          @Override\n          public long ramBytesUsed() {\n            return RamUsageEstimator.sizeOf(docs);\n          }\n          \n          @Override\n          public Bits bits() {\n            // random access is expensive for this set\n            return null;\n          }\n\n        }, acceptDocs2);\n      }\n      @Override\n      public String toString(String field) {\n        return \"SortedIntDocSetTopFilter\";\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","sourceNew":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(final LeafReaderContext context, final Bits acceptDocs) {\n        LeafReader reader = context.reader();\n        // all Solr DocSets that are used as filters only include live docs\n        final Bits acceptDocs2 = acceptDocs == null ? null : (reader.getLiveDocs() == acceptDocs ? null : acceptDocs);\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return BitsFilteredDocIdSet.wrap(new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n              @Override\n              public long cost() {\n                return docs.length;\n              }\n            };\n          }\n\n          @Override\n          public long ramBytesUsed() {\n            return RamUsageEstimator.sizeOf(docs);\n          }\n          \n          @Override\n          public Bits bits() {\n            // random access is expensive for this set\n            return null;\n          }\n\n        }, acceptDocs2);\n      }\n      @Override\n      public String toString(String field) {\n        return \"SortedIntDocSetTopFilter\";\n      }\n\n      // Equivalence should/could be based on docs here? How did it work previously?\n\n      @Override\n      public boolean equals(Object other) {\n        return other == this;\n      }\n\n      @Override\n      public int hashCode() {\n        return System.identityHashCode(this);\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(final LeafReaderContext context, final Bits acceptDocs) {\n        LeafReader reader = context.reader();\n        // all Solr DocSets that are used as filters only include live docs\n        final Bits acceptDocs2 = acceptDocs == null ? null : (reader.getLiveDocs() == acceptDocs ? null : acceptDocs);\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return BitsFilteredDocIdSet.wrap(new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n              @Override\n              public long cost() {\n                return docs.length;\n              }\n            };\n          }\n\n          @Override\n          public long ramBytesUsed() {\n            return RamUsageEstimator.sizeOf(docs);\n          }\n          \n          @Override\n          public Bits bits() {\n            // random access is expensive for this set\n            return null;\n          }\n\n        }, acceptDocs2);\n      }\n      @Override\n      public String toString(String field) {\n        return \"SortedIntDocSetTopFilter\";\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"716d18f3a9b0993bc679d7fa7abdc9bfb03411ec":["7b9f056598bc578796f7c2eaa4b2bb8eaab5c23a","19275ba31e621f6da1b83bf13af75233876fd3d4"],"6620df8541b174097b1133a4fc370adb2e570524":["c26f00b574427b55127e869b935845554afde1fa"],"54ea8c8c94ae9da9a366175e2abbe1dde3aa0453":["19275ba31e621f6da1b83bf13af75233876fd3d4"],"60596f28be69b10c37a56a303c2dbea07b2ca4ba":["fb58c5f77afb63ba911f6d62f4c1d89f15e56dc6"],"c26f00b574427b55127e869b935845554afde1fa":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"8e5663809fccfda938d8d46f0106a5301cdd5cf0":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"96d207426bd26fa5c1014e26d21d87603aea68b7":["6620df8541b174097b1133a4fc370adb2e570524"],"7b9f056598bc578796f7c2eaa4b2bb8eaab5c23a":["4d3e8520fd031bab31fd0e4d480e55958bc45efe"],"19275ba31e621f6da1b83bf13af75233876fd3d4":["7b9f056598bc578796f7c2eaa4b2bb8eaab5c23a"],"c6f080a2ab37c464dd98db173f6cbf10dc74f211":["19275ba31e621f6da1b83bf13af75233876fd3d4","54ea8c8c94ae9da9a366175e2abbe1dde3aa0453"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["54ea8c8c94ae9da9a366175e2abbe1dde3aa0453"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["6620df8541b174097b1133a4fc370adb2e570524","96d207426bd26fa5c1014e26d21d87603aea68b7"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["60596f28be69b10c37a56a303c2dbea07b2ca4ba","0e7bc21595222ae4f75509300fbb7726691f387f"],"fb58c5f77afb63ba911f6d62f4c1d89f15e56dc6":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["fb58c5f77afb63ba911f6d62f4c1d89f15e56dc6","60596f28be69b10c37a56a303c2dbea07b2ca4ba"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":["8e5663809fccfda938d8d46f0106a5301cdd5cf0","4d3e8520fd031bab31fd0e4d480e55958bc45efe"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"0e7bc21595222ae4f75509300fbb7726691f387f":["60596f28be69b10c37a56a303c2dbea07b2ca4ba"],"a258fbb26824fd104ed795e5d9033d2d040049ee":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"4d3e8520fd031bab31fd0e4d480e55958bc45efe":["8e5663809fccfda938d8d46f0106a5301cdd5cf0"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["0e7bc21595222ae4f75509300fbb7726691f387f"]},"commit2Childs":{"716d18f3a9b0993bc679d7fa7abdc9bfb03411ec":[],"6620df8541b174097b1133a4fc370adb2e570524":["96d207426bd26fa5c1014e26d21d87603aea68b7","5cab9a86bd67202d20b6adc463008c8e982b070a"],"54ea8c8c94ae9da9a366175e2abbe1dde3aa0453":["c6f080a2ab37c464dd98db173f6cbf10dc74f211","c9fb5f46e264daf5ba3860defe623a89d202dd87"],"c26f00b574427b55127e869b935845554afde1fa":["6620df8541b174097b1133a4fc370adb2e570524"],"60596f28be69b10c37a56a303c2dbea07b2ca4ba":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","0e7bc21595222ae4f75509300fbb7726691f387f"],"8e5663809fccfda938d8d46f0106a5301cdd5cf0":["fe33227f6805edab2036cbb80645cc4e2d1fa424","4d3e8520fd031bab31fd0e4d480e55958bc45efe"],"96d207426bd26fa5c1014e26d21d87603aea68b7":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"7b9f056598bc578796f7c2eaa4b2bb8eaab5c23a":["716d18f3a9b0993bc679d7fa7abdc9bfb03411ec","19275ba31e621f6da1b83bf13af75233876fd3d4"],"19275ba31e621f6da1b83bf13af75233876fd3d4":["716d18f3a9b0993bc679d7fa7abdc9bfb03411ec","54ea8c8c94ae9da9a366175e2abbe1dde3aa0453","c6f080a2ab37c464dd98db173f6cbf10dc74f211"],"c6f080a2ab37c464dd98db173f6cbf10dc74f211":[],"5cab9a86bd67202d20b6adc463008c8e982b070a":["8e5663809fccfda938d8d46f0106a5301cdd5cf0"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["fb58c5f77afb63ba911f6d62f4c1d89f15e56dc6"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"fb58c5f77afb63ba911f6d62f4c1d89f15e56dc6":["60596f28be69b10c37a56a303c2dbea07b2ca4ba","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["c26f00b574427b55127e869b935845554afde1fa"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c26f00b574427b55127e869b935845554afde1fa","c903c3d15906a3da96b8c0c2fb704491005fdbdb","a258fbb26824fd104ed795e5d9033d2d040049ee"],"0e7bc21595222ae4f75509300fbb7726691f387f":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a258fbb26824fd104ed795e5d9033d2d040049ee":[],"4d3e8520fd031bab31fd0e4d480e55958bc45efe":["7b9f056598bc578796f7c2eaa4b2bb8eaab5c23a","fe33227f6805edab2036cbb80645cc4e2d1fa424"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["716d18f3a9b0993bc679d7fa7abdc9bfb03411ec","c6f080a2ab37c464dd98db173f6cbf10dc74f211","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","fe33227f6805edab2036cbb80645cc4e2d1fa424","a258fbb26824fd104ed795e5d9033d2d040049ee","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}