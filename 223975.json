{"path":"solr/core/src/java/org/apache/solr/store/hdfs/HdfsLocalityReporter#initializeMetrics(SolrMetricsContext,String).mjava","commits":[{"id":"c959a64c5b52cc12eb8daa17f4f0ed9cf2dfcaaa","date":1571411704,"type":1,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/store/hdfs/HdfsLocalityReporter#initializeMetrics(SolrMetricsContext,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/store/hdfs/HdfsLocalityReporter#initializeMetrics(SolrMetricManager,String,String,String).mjava","sourceNew":"  /**\n   * Provide statistics on HDFS block locality, both in terms of bytes and block counts.\n   */\n  @Override\n  public void initializeMetrics(SolrMetricsContext parentContext, String scope) {\n    solrMetricsContext = parentContext.getChildContext(this);\n    MetricsMap metricsMap = new MetricsMap((detailed, map) -> {\n      long totalBytes = 0;\n      long localBytes = 0;\n      int totalCount = 0;\n      int localCount = 0;\n\n      for (Iterator<HdfsDirectory> iterator = cache.keySet().iterator(); iterator.hasNext();) {\n        HdfsDirectory hdfsDirectory = iterator.next();\n\n        if (hdfsDirectory.isClosed()) {\n          iterator.remove();\n        } else {\n          try {\n            refreshDirectory(hdfsDirectory);\n            Map<FileStatus,BlockLocation[]> blockMap = cache.get(hdfsDirectory);\n\n            // For every block in every file in this directory, count it\n            for (BlockLocation[] locations : blockMap.values()) {\n              for (BlockLocation bl : locations) {\n                totalBytes += bl.getLength();\n                totalCount++;\n\n                if (Arrays.asList(bl.getHosts()).contains(hostname)) {\n                  localBytes += bl.getLength();\n                  localCount++;\n                }\n              }\n            }\n          } catch (IOException e) {\n            log.warn(\"Could not retrieve locality information for {} due to exception: {}\",\n                hdfsDirectory.getHdfsDirPath(), e);\n          }\n        }\n      }\n      map.put(LOCALITY_BYTES_TOTAL, totalBytes);\n      map.put(LOCALITY_BYTES_LOCAL, localBytes);\n      if (localBytes == 0) {\n        map.put(LOCALITY_BYTES_RATIO, 0);\n      } else {\n        map.put(LOCALITY_BYTES_RATIO, localBytes / (double) totalBytes);\n      }\n      map.put(LOCALITY_BLOCKS_TOTAL, totalCount);\n      map.put(LOCALITY_BLOCKS_LOCAL, localCount);\n      if (localCount == 0) {\n        map.put(LOCALITY_BLOCKS_RATIO, 0);\n      } else {\n        map.put(LOCALITY_BLOCKS_RATIO, localCount / (double) totalCount);\n      }\n    });\n    solrMetricsContext.gauge(this, metricsMap, true, \"hdfsLocality\", getCategory().toString(), scope);\n  }\n\n","sourceOld":"  /**\n   * Provide statistics on HDFS block locality, both in terms of bytes and block counts.\n   */\n  @Override\n  public void initializeMetrics(SolrMetricManager manager, String registryName, String tag, String scope) {\n    this.metricManager = manager;\n    this.registryName = registryName;\n    registry = manager.registry(registryName);\n    MetricsMap metricsMap = new MetricsMap((detailed, map) -> {\n      long totalBytes = 0;\n      long localBytes = 0;\n      int totalCount = 0;\n      int localCount = 0;\n\n      for (Iterator<HdfsDirectory> iterator = cache.keySet().iterator(); iterator.hasNext();) {\n        HdfsDirectory hdfsDirectory = iterator.next();\n\n        if (hdfsDirectory.isClosed()) {\n          iterator.remove();\n        } else {\n          try {\n            refreshDirectory(hdfsDirectory);\n            Map<FileStatus,BlockLocation[]> blockMap = cache.get(hdfsDirectory);\n\n            // For every block in every file in this directory, count it\n            for (BlockLocation[] locations : blockMap.values()) {\n              for (BlockLocation bl : locations) {\n                totalBytes += bl.getLength();\n                totalCount++;\n\n                if (Arrays.asList(bl.getHosts()).contains(hostname)) {\n                  localBytes += bl.getLength();\n                  localCount++;\n                }\n              }\n            }\n          } catch (IOException e) {\n            log.warn(\"Could not retrieve locality information for {} due to exception: {}\",\n                hdfsDirectory.getHdfsDirPath(), e);\n          }\n        }\n      }\n      map.put(LOCALITY_BYTES_TOTAL, totalBytes);\n      map.put(LOCALITY_BYTES_LOCAL, localBytes);\n      if (localBytes == 0) {\n        map.put(LOCALITY_BYTES_RATIO, 0);\n      } else {\n        map.put(LOCALITY_BYTES_RATIO, localBytes / (double) totalBytes);\n      }\n      map.put(LOCALITY_BLOCKS_TOTAL, totalCount);\n      map.put(LOCALITY_BLOCKS_LOCAL, localCount);\n      if (localCount == 0) {\n        map.put(LOCALITY_BLOCKS_RATIO, 0);\n      } else {\n        map.put(LOCALITY_BLOCKS_RATIO, localCount / (double) totalCount);\n      }\n    });\n    manager.registerGauge(this, registryName, metricsMap, tag, true, \"hdfsLocality\", getCategory().toString(), scope);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c2af5a711bd6e2d33e0221ced0f47ac596ed275","date":1572877903,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/store/hdfs/HdfsLocalityReporter#initializeMetrics(SolrMetricsContext,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/store/hdfs/HdfsLocalityReporter#initializeMetrics(SolrMetricsContext,String).mjava","sourceNew":"  /**\n   * Provide statistics on HDFS block locality, both in terms of bytes and block counts.\n   */\n  @Override\n  public void initializeMetrics(SolrMetricsContext parentContext, String scope) {\n    solrMetricsContext = parentContext.getChildContext(this);\n    MetricsMap metricsMap = new MetricsMap((detailed, map) -> {\n      long totalBytes = 0;\n      long localBytes = 0;\n      int totalCount = 0;\n      int localCount = 0;\n\n      for (Iterator<HdfsDirectory> iterator = cache.keySet().iterator(); iterator.hasNext();) {\n        HdfsDirectory hdfsDirectory = iterator.next();\n\n        if (hdfsDirectory.isClosed()) {\n          iterator.remove();\n        } else {\n          try {\n            refreshDirectory(hdfsDirectory);\n            Map<FileStatus,BlockLocation[]> blockMap = cache.get(hdfsDirectory);\n\n            // For every block in every file in this directory, count it\n            for (BlockLocation[] locations : blockMap.values()) {\n              for (BlockLocation bl : locations) {\n                totalBytes += bl.getLength();\n                totalCount++;\n\n                if (Arrays.asList(bl.getHosts()).contains(hostname)) {\n                  localBytes += bl.getLength();\n                  localCount++;\n                }\n              }\n            }\n          } catch (IOException e) {\n            log.warn(\"Could not retrieve locality information for {} due to exception: {}\",\n                hdfsDirectory.getHdfsDirPath(), e);\n          }\n        }\n      }\n      map.put(LOCALITY_BYTES_TOTAL, totalBytes);\n      map.put(LOCALITY_BYTES_LOCAL, localBytes);\n      if (localBytes == 0) {\n        map.put(LOCALITY_BYTES_RATIO, 0);\n      } else {\n        map.put(LOCALITY_BYTES_RATIO, localBytes / (double) totalBytes);\n      }\n      map.put(LOCALITY_BLOCKS_TOTAL, totalCount);\n      map.put(LOCALITY_BLOCKS_LOCAL, localCount);\n      if (localCount == 0) {\n        map.put(LOCALITY_BLOCKS_RATIO, 0);\n      } else {\n        map.put(LOCALITY_BLOCKS_RATIO, localCount / (double) totalCount);\n      }\n    });\n    solrMetricsContext.gauge(metricsMap, true, \"hdfsLocality\", getCategory().toString(), scope);\n  }\n\n","sourceOld":"  /**\n   * Provide statistics on HDFS block locality, both in terms of bytes and block counts.\n   */\n  @Override\n  public void initializeMetrics(SolrMetricsContext parentContext, String scope) {\n    solrMetricsContext = parentContext.getChildContext(this);\n    MetricsMap metricsMap = new MetricsMap((detailed, map) -> {\n      long totalBytes = 0;\n      long localBytes = 0;\n      int totalCount = 0;\n      int localCount = 0;\n\n      for (Iterator<HdfsDirectory> iterator = cache.keySet().iterator(); iterator.hasNext();) {\n        HdfsDirectory hdfsDirectory = iterator.next();\n\n        if (hdfsDirectory.isClosed()) {\n          iterator.remove();\n        } else {\n          try {\n            refreshDirectory(hdfsDirectory);\n            Map<FileStatus,BlockLocation[]> blockMap = cache.get(hdfsDirectory);\n\n            // For every block in every file in this directory, count it\n            for (BlockLocation[] locations : blockMap.values()) {\n              for (BlockLocation bl : locations) {\n                totalBytes += bl.getLength();\n                totalCount++;\n\n                if (Arrays.asList(bl.getHosts()).contains(hostname)) {\n                  localBytes += bl.getLength();\n                  localCount++;\n                }\n              }\n            }\n          } catch (IOException e) {\n            log.warn(\"Could not retrieve locality information for {} due to exception: {}\",\n                hdfsDirectory.getHdfsDirPath(), e);\n          }\n        }\n      }\n      map.put(LOCALITY_BYTES_TOTAL, totalBytes);\n      map.put(LOCALITY_BYTES_LOCAL, localBytes);\n      if (localBytes == 0) {\n        map.put(LOCALITY_BYTES_RATIO, 0);\n      } else {\n        map.put(LOCALITY_BYTES_RATIO, localBytes / (double) totalBytes);\n      }\n      map.put(LOCALITY_BLOCKS_TOTAL, totalCount);\n      map.put(LOCALITY_BLOCKS_LOCAL, localCount);\n      if (localCount == 0) {\n        map.put(LOCALITY_BLOCKS_RATIO, 0);\n      } else {\n        map.put(LOCALITY_BLOCKS_RATIO, localCount / (double) totalCount);\n      }\n    });\n    solrMetricsContext.gauge(this, metricsMap, true, \"hdfsLocality\", getCategory().toString(), scope);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54faedfb0e03479a38f5ee82f2dfaeea536e9404","date":1587251295,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/store/hdfs/HdfsLocalityReporter#initializeMetrics(SolrMetricsContext,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/store/hdfs/HdfsLocalityReporter#initializeMetrics(SolrMetricsContext,String).mjava","sourceNew":"  /**\n   * Provide statistics on HDFS block locality, both in terms of bytes and block counts.\n   */\n  @Override\n  public void initializeMetrics(SolrMetricsContext parentContext, String scope) {\n    solrMetricsContext = parentContext.getChildContext(this);\n    MetricsMap metricsMap = new MetricsMap((detailed, map) -> {\n      long totalBytes = 0;\n      long localBytes = 0;\n      int totalCount = 0;\n      int localCount = 0;\n\n      for (Iterator<HdfsDirectory> iterator = cache.keySet().iterator(); iterator.hasNext();) {\n        HdfsDirectory hdfsDirectory = iterator.next();\n\n        if (hdfsDirectory.isClosed()) {\n          iterator.remove();\n        } else {\n          try {\n            refreshDirectory(hdfsDirectory);\n            Map<FileStatus,BlockLocation[]> blockMap = cache.get(hdfsDirectory);\n\n            // For every block in every file in this directory, count it\n            for (BlockLocation[] locations : blockMap.values()) {\n              for (BlockLocation bl : locations) {\n                totalBytes += bl.getLength();\n                totalCount++;\n\n                if (Arrays.asList(bl.getHosts()).contains(hostname)) {\n                  localBytes += bl.getLength();\n                  localCount++;\n                }\n              }\n            }\n          } catch (IOException e) {\n            if (log.isWarnEnabled()) {\n              log.warn(\"Could not retrieve locality information for {} due to exception: {}\",\n                  hdfsDirectory.getHdfsDirPath(), e);\n            }\n          }\n        }\n      }\n      map.put(LOCALITY_BYTES_TOTAL, totalBytes);\n      map.put(LOCALITY_BYTES_LOCAL, localBytes);\n      if (localBytes == 0) {\n        map.put(LOCALITY_BYTES_RATIO, 0);\n      } else {\n        map.put(LOCALITY_BYTES_RATIO, localBytes / (double) totalBytes);\n      }\n      map.put(LOCALITY_BLOCKS_TOTAL, totalCount);\n      map.put(LOCALITY_BLOCKS_LOCAL, localCount);\n      if (localCount == 0) {\n        map.put(LOCALITY_BLOCKS_RATIO, 0);\n      } else {\n        map.put(LOCALITY_BLOCKS_RATIO, localCount / (double) totalCount);\n      }\n    });\n    solrMetricsContext.gauge(metricsMap, true, \"hdfsLocality\", getCategory().toString(), scope);\n  }\n\n","sourceOld":"  /**\n   * Provide statistics on HDFS block locality, both in terms of bytes and block counts.\n   */\n  @Override\n  public void initializeMetrics(SolrMetricsContext parentContext, String scope) {\n    solrMetricsContext = parentContext.getChildContext(this);\n    MetricsMap metricsMap = new MetricsMap((detailed, map) -> {\n      long totalBytes = 0;\n      long localBytes = 0;\n      int totalCount = 0;\n      int localCount = 0;\n\n      for (Iterator<HdfsDirectory> iterator = cache.keySet().iterator(); iterator.hasNext();) {\n        HdfsDirectory hdfsDirectory = iterator.next();\n\n        if (hdfsDirectory.isClosed()) {\n          iterator.remove();\n        } else {\n          try {\n            refreshDirectory(hdfsDirectory);\n            Map<FileStatus,BlockLocation[]> blockMap = cache.get(hdfsDirectory);\n\n            // For every block in every file in this directory, count it\n            for (BlockLocation[] locations : blockMap.values()) {\n              for (BlockLocation bl : locations) {\n                totalBytes += bl.getLength();\n                totalCount++;\n\n                if (Arrays.asList(bl.getHosts()).contains(hostname)) {\n                  localBytes += bl.getLength();\n                  localCount++;\n                }\n              }\n            }\n          } catch (IOException e) {\n            log.warn(\"Could not retrieve locality information for {} due to exception: {}\",\n                hdfsDirectory.getHdfsDirPath(), e);\n          }\n        }\n      }\n      map.put(LOCALITY_BYTES_TOTAL, totalBytes);\n      map.put(LOCALITY_BYTES_LOCAL, localBytes);\n      if (localBytes == 0) {\n        map.put(LOCALITY_BYTES_RATIO, 0);\n      } else {\n        map.put(LOCALITY_BYTES_RATIO, localBytes / (double) totalBytes);\n      }\n      map.put(LOCALITY_BLOCKS_TOTAL, totalCount);\n      map.put(LOCALITY_BLOCKS_LOCAL, localCount);\n      if (localCount == 0) {\n        map.put(LOCALITY_BLOCKS_RATIO, 0);\n      } else {\n        map.put(LOCALITY_BLOCKS_RATIO, localCount / (double) totalCount);\n      }\n    });\n    solrMetricsContext.gauge(metricsMap, true, \"hdfsLocality\", getCategory().toString(), scope);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"575e66bd4b2349209027f6801184da7fc3cba13f","date":1587609169,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/store/hdfs/HdfsLocalityReporter#initializeMetrics(SolrMetricsContext,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/store/hdfs/HdfsLocalityReporter#initializeMetrics(SolrMetricsContext,String).mjava","sourceNew":"  /**\n   * Provide statistics on HDFS block locality, both in terms of bytes and block counts.\n   */\n  @Override\n  public void initializeMetrics(SolrMetricsContext parentContext, String scope) {\n    solrMetricsContext = parentContext.getChildContext(this);\n    MetricsMap metricsMap = new MetricsMap((detailed, map) -> {\n      long totalBytes = 0;\n      long localBytes = 0;\n      int totalCount = 0;\n      int localCount = 0;\n\n      for (Iterator<HdfsDirectory> iterator = cache.keySet().iterator(); iterator.hasNext();) {\n        HdfsDirectory hdfsDirectory = iterator.next();\n\n        if (hdfsDirectory.isClosed()) {\n          iterator.remove();\n        } else {\n          try {\n            refreshDirectory(hdfsDirectory);\n            Map<FileStatus,BlockLocation[]> blockMap = cache.get(hdfsDirectory);\n\n            // For every block in every file in this directory, count it\n            for (BlockLocation[] locations : blockMap.values()) {\n              for (BlockLocation bl : locations) {\n                totalBytes += bl.getLength();\n                totalCount++;\n\n                if (Arrays.asList(bl.getHosts()).contains(hostname)) {\n                  localBytes += bl.getLength();\n                  localCount++;\n                }\n              }\n            }\n          } catch (IOException e) {\n            log.warn(\"Could not retrieve locality information for {} due to exception: {}\",\n                hdfsDirectory.getHdfsDirPath(), e);\n          }\n        }\n      }\n      map.put(LOCALITY_BYTES_TOTAL, totalBytes);\n      map.put(LOCALITY_BYTES_LOCAL, localBytes);\n      if (localBytes == 0) {\n        map.put(LOCALITY_BYTES_RATIO, 0);\n      } else {\n        map.put(LOCALITY_BYTES_RATIO, localBytes / (double) totalBytes);\n      }\n      map.put(LOCALITY_BLOCKS_TOTAL, totalCount);\n      map.put(LOCALITY_BLOCKS_LOCAL, localCount);\n      if (localCount == 0) {\n        map.put(LOCALITY_BLOCKS_RATIO, 0);\n      } else {\n        map.put(LOCALITY_BLOCKS_RATIO, localCount / (double) totalCount);\n      }\n    });\n    solrMetricsContext.gauge(metricsMap, true, \"hdfsLocality\", getCategory().toString(), scope);\n  }\n\n","sourceOld":"  /**\n   * Provide statistics on HDFS block locality, both in terms of bytes and block counts.\n   */\n  @Override\n  public void initializeMetrics(SolrMetricsContext parentContext, String scope) {\n    solrMetricsContext = parentContext.getChildContext(this);\n    MetricsMap metricsMap = new MetricsMap((detailed, map) -> {\n      long totalBytes = 0;\n      long localBytes = 0;\n      int totalCount = 0;\n      int localCount = 0;\n\n      for (Iterator<HdfsDirectory> iterator = cache.keySet().iterator(); iterator.hasNext();) {\n        HdfsDirectory hdfsDirectory = iterator.next();\n\n        if (hdfsDirectory.isClosed()) {\n          iterator.remove();\n        } else {\n          try {\n            refreshDirectory(hdfsDirectory);\n            Map<FileStatus,BlockLocation[]> blockMap = cache.get(hdfsDirectory);\n\n            // For every block in every file in this directory, count it\n            for (BlockLocation[] locations : blockMap.values()) {\n              for (BlockLocation bl : locations) {\n                totalBytes += bl.getLength();\n                totalCount++;\n\n                if (Arrays.asList(bl.getHosts()).contains(hostname)) {\n                  localBytes += bl.getLength();\n                  localCount++;\n                }\n              }\n            }\n          } catch (IOException e) {\n            if (log.isWarnEnabled()) {\n              log.warn(\"Could not retrieve locality information for {} due to exception: {}\",\n                  hdfsDirectory.getHdfsDirPath(), e);\n            }\n          }\n        }\n      }\n      map.put(LOCALITY_BYTES_TOTAL, totalBytes);\n      map.put(LOCALITY_BYTES_LOCAL, localBytes);\n      if (localBytes == 0) {\n        map.put(LOCALITY_BYTES_RATIO, 0);\n      } else {\n        map.put(LOCALITY_BYTES_RATIO, localBytes / (double) totalBytes);\n      }\n      map.put(LOCALITY_BLOCKS_TOTAL, totalCount);\n      map.put(LOCALITY_BLOCKS_LOCAL, localCount);\n      if (localCount == 0) {\n        map.put(LOCALITY_BLOCKS_RATIO, 0);\n      } else {\n        map.put(LOCALITY_BLOCKS_RATIO, localCount / (double) totalCount);\n      }\n    });\n    solrMetricsContext.gauge(metricsMap, true, \"hdfsLocality\", getCategory().toString(), scope);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"233211c3dbff6f367588be5bbb4ac77f72eae193","date":1602085975,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/store/hdfs/HdfsLocalityReporter#initializeMetrics(SolrMetricsContext,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/store/hdfs/HdfsLocalityReporter#initializeMetrics(SolrMetricsContext,String).mjava","sourceNew":"  /**\n   * Provide statistics on HDFS block locality, both in terms of bytes and block counts.\n   */\n  @Override\n  public void initializeMetrics(SolrMetricsContext parentContext, String scope) {\n    solrMetricsContext = parentContext.getChildContext(this);\n    MetricsMap metricsMap = new MetricsMap(map -> {\n      long totalBytes = 0;\n      long localBytes = 0;\n      int totalCount = 0;\n      int localCount = 0;\n\n      for (Iterator<HdfsDirectory> iterator = cache.keySet().iterator(); iterator.hasNext();) {\n        HdfsDirectory hdfsDirectory = iterator.next();\n\n        if (hdfsDirectory.isClosed()) {\n          iterator.remove();\n        } else {\n          try {\n            refreshDirectory(hdfsDirectory);\n            Map<FileStatus,BlockLocation[]> blockMap = cache.get(hdfsDirectory);\n\n            // For every block in every file in this directory, count it\n            for (BlockLocation[] locations : blockMap.values()) {\n              for (BlockLocation bl : locations) {\n                totalBytes += bl.getLength();\n                totalCount++;\n\n                if (Arrays.asList(bl.getHosts()).contains(hostname)) {\n                  localBytes += bl.getLength();\n                  localCount++;\n                }\n              }\n            }\n          } catch (IOException e) {\n            log.warn(\"Could not retrieve locality information for {} due to exception: {}\",\n                hdfsDirectory.getHdfsDirPath(), e);\n          }\n        }\n      }\n      map.put(LOCALITY_BYTES_TOTAL, totalBytes);\n      map.put(LOCALITY_BYTES_LOCAL, localBytes);\n      if (localBytes == 0) {\n        map.put(LOCALITY_BYTES_RATIO, 0);\n      } else {\n        map.put(LOCALITY_BYTES_RATIO, localBytes / (double) totalBytes);\n      }\n      map.put(LOCALITY_BLOCKS_TOTAL, totalCount);\n      map.put(LOCALITY_BLOCKS_LOCAL, localCount);\n      if (localCount == 0) {\n        map.put(LOCALITY_BLOCKS_RATIO, 0);\n      } else {\n        map.put(LOCALITY_BLOCKS_RATIO, localCount / (double) totalCount);\n      }\n    });\n    solrMetricsContext.gauge(metricsMap, true, \"hdfsLocality\", getCategory().toString(), scope);\n  }\n\n","sourceOld":"  /**\n   * Provide statistics on HDFS block locality, both in terms of bytes and block counts.\n   */\n  @Override\n  public void initializeMetrics(SolrMetricsContext parentContext, String scope) {\n    solrMetricsContext = parentContext.getChildContext(this);\n    MetricsMap metricsMap = new MetricsMap((detailed, map) -> {\n      long totalBytes = 0;\n      long localBytes = 0;\n      int totalCount = 0;\n      int localCount = 0;\n\n      for (Iterator<HdfsDirectory> iterator = cache.keySet().iterator(); iterator.hasNext();) {\n        HdfsDirectory hdfsDirectory = iterator.next();\n\n        if (hdfsDirectory.isClosed()) {\n          iterator.remove();\n        } else {\n          try {\n            refreshDirectory(hdfsDirectory);\n            Map<FileStatus,BlockLocation[]> blockMap = cache.get(hdfsDirectory);\n\n            // For every block in every file in this directory, count it\n            for (BlockLocation[] locations : blockMap.values()) {\n              for (BlockLocation bl : locations) {\n                totalBytes += bl.getLength();\n                totalCount++;\n\n                if (Arrays.asList(bl.getHosts()).contains(hostname)) {\n                  localBytes += bl.getLength();\n                  localCount++;\n                }\n              }\n            }\n          } catch (IOException e) {\n            log.warn(\"Could not retrieve locality information for {} due to exception: {}\",\n                hdfsDirectory.getHdfsDirPath(), e);\n          }\n        }\n      }\n      map.put(LOCALITY_BYTES_TOTAL, totalBytes);\n      map.put(LOCALITY_BYTES_LOCAL, localBytes);\n      if (localBytes == 0) {\n        map.put(LOCALITY_BYTES_RATIO, 0);\n      } else {\n        map.put(LOCALITY_BYTES_RATIO, localBytes / (double) totalBytes);\n      }\n      map.put(LOCALITY_BLOCKS_TOTAL, totalCount);\n      map.put(LOCALITY_BLOCKS_LOCAL, localCount);\n      if (localCount == 0) {\n        map.put(LOCALITY_BLOCKS_RATIO, 0);\n      } else {\n        map.put(LOCALITY_BLOCKS_RATIO, localCount / (double) totalCount);\n      }\n    });\n    solrMetricsContext.gauge(metricsMap, true, \"hdfsLocality\", getCategory().toString(), scope);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"54faedfb0e03479a38f5ee82f2dfaeea536e9404":["7c2af5a711bd6e2d33e0221ced0f47ac596ed275"],"7c2af5a711bd6e2d33e0221ced0f47ac596ed275":["c959a64c5b52cc12eb8daa17f4f0ed9cf2dfcaaa"],"c959a64c5b52cc12eb8daa17f4f0ed9cf2dfcaaa":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"575e66bd4b2349209027f6801184da7fc3cba13f":["54faedfb0e03479a38f5ee82f2dfaeea536e9404"],"233211c3dbff6f367588be5bbb4ac77f72eae193":["575e66bd4b2349209027f6801184da7fc3cba13f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["233211c3dbff6f367588be5bbb4ac77f72eae193"]},"commit2Childs":{"54faedfb0e03479a38f5ee82f2dfaeea536e9404":["575e66bd4b2349209027f6801184da7fc3cba13f"],"7c2af5a711bd6e2d33e0221ced0f47ac596ed275":["54faedfb0e03479a38f5ee82f2dfaeea536e9404"],"c959a64c5b52cc12eb8daa17f4f0ed9cf2dfcaaa":["7c2af5a711bd6e2d33e0221ced0f47ac596ed275"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c959a64c5b52cc12eb8daa17f4f0ed9cf2dfcaaa"],"575e66bd4b2349209027f6801184da7fc3cba13f":["233211c3dbff6f367588be5bbb4ac77f72eae193"],"233211c3dbff6f367588be5bbb4ac77f72eae193":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}