{"path":"lucene/src/test/org/apache/lucene/index/TestDocTermOrds#testRandom().mjava","commits":[{"id":"95ae76773bf2b95987d5f9c8f566ab3738953fb4","date":1301758351,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocTermOrds#testRandom().mjava","pathOld":"/dev/null","sourceNew":"  public void testRandom() throws Exception {\n    MockDirectoryWrapper dir = newDirectory();\n\n    final int NUM_TERMS = 100 * RANDOM_MULTIPLIER;\n    final Set<BytesRef> terms = new HashSet<BytesRef>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = _TestUtil.randomRealisticUnicodeString(random);\n      //final String s = _TestUtil.randomSimpleString(random);\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = 1000 * RANDOM_MULTIPLIER;\n\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer());\n\n    // Sometimes swap in codec that impls ord():\n    if (random.nextInt(10) == 7) {\n      // Make sure terms index has ords:\n      CoreCodecProvider cp = new CoreCodecProvider();\n      cp.register(new StandardCodecWithOrds());\n      cp.setDefaultFieldCodec(\"StandardOrds\");\n\n      // So checkIndex on close works\n      dir.setCodecProvider(cp);\n      conf.setCodecProvider(cp);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random, dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<Integer>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      NumericField idField = new NumericField(\"id\");\n      doc.add(idField.setIntValue(id));\n      \n      final int termCount = _TestUtil.nextInt(random, 0, 20*RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random.nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newField(\"field\", termsArray[ord].utf8ToString(), Field.Index.NOT_ANALYZED);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final IndexReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n\n    for(IndexReader subR : r.getSequentialSubReaders()) {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: sub=\" + subR);\n      }\n      verify(subR, idToOrds, termsArray, null);\n    }\n\n    // Also test top-level reader: its enum does not support\n    // ord, so this forces the OrdWrapper to run:\n    if (VERBOSE) {\n      System.out.println(\"TEST: top reader\");\n    }\n    verify(r, idToOrds, termsArray, null);\n\n    FieldCache.DEFAULT.purge(r);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["d19974432be9aed28ee7dca73bdf01d139e763a9","ced66195b26fdb1f77ee00e2a77ec6918dedd766","fa0f44f887719e97183771e977cfc4bfb485b766"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"45669a651c970812a680841b97a77cce06af559f","date":1301922222,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocTermOrds#testRandom().mjava","pathOld":"/dev/null","sourceNew":"  public void testRandom() throws Exception {\n    MockDirectoryWrapper dir = newDirectory();\n\n    final int NUM_TERMS = 100 * RANDOM_MULTIPLIER;\n    final Set<BytesRef> terms = new HashSet<BytesRef>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = _TestUtil.randomRealisticUnicodeString(random);\n      //final String s = _TestUtil.randomSimpleString(random);\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = 1000 * RANDOM_MULTIPLIER;\n\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer());\n\n    // Sometimes swap in codec that impls ord():\n    if (random.nextInt(10) == 7) {\n      // Make sure terms index has ords:\n      CoreCodecProvider cp = new CoreCodecProvider();\n      cp.register(new StandardCodecWithOrds());\n      cp.setDefaultFieldCodec(\"StandardOrds\");\n\n      // So checkIndex on close works\n      dir.setCodecProvider(cp);\n      conf.setCodecProvider(cp);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random, dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<Integer>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      NumericField idField = new NumericField(\"id\");\n      doc.add(idField.setIntValue(id));\n      \n      final int termCount = _TestUtil.nextInt(random, 0, 20*RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random.nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newField(\"field\", termsArray[ord].utf8ToString(), Field.Index.NOT_ANALYZED);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final IndexReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n\n    for(IndexReader subR : r.getSequentialSubReaders()) {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: sub=\" + subR);\n      }\n      verify(subR, idToOrds, termsArray, null);\n    }\n\n    // Also test top-level reader: its enum does not support\n    // ord, so this forces the OrdWrapper to run:\n    if (VERBOSE) {\n      System.out.println(\"TEST: top reader\");\n    }\n    verify(r, idToOrds, termsArray, null);\n\n    FieldCache.DEFAULT.purge(r);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f2c5f0cb44df114db4228c8f77861714b5cabaea","date":1302542431,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocTermOrds#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocTermOrds#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    MockDirectoryWrapper dir = newDirectory();\n\n    final int NUM_TERMS = 100 * RANDOM_MULTIPLIER;\n    final Set<BytesRef> terms = new HashSet<BytesRef>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = _TestUtil.randomRealisticUnicodeString(random);\n      //final String s = _TestUtil.randomSimpleString(random);\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = 1000 * RANDOM_MULTIPLIER;\n\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n\n    // Sometimes swap in codec that impls ord():\n    if (random.nextInt(10) == 7) {\n      // Make sure terms index has ords:\n      CoreCodecProvider cp = new CoreCodecProvider();\n      cp.register(new StandardCodecWithOrds());\n      cp.setDefaultFieldCodec(\"StandardOrds\");\n\n      // So checkIndex on close works\n      dir.setCodecProvider(cp);\n      conf.setCodecProvider(cp);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random, dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<Integer>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      NumericField idField = new NumericField(\"id\");\n      doc.add(idField.setIntValue(id));\n      \n      final int termCount = _TestUtil.nextInt(random, 0, 20*RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random.nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newField(\"field\", termsArray[ord].utf8ToString(), Field.Index.NOT_ANALYZED);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final IndexReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n\n    for(IndexReader subR : r.getSequentialSubReaders()) {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: sub=\" + subR);\n      }\n      verify(subR, idToOrds, termsArray, null);\n    }\n\n    // Also test top-level reader: its enum does not support\n    // ord, so this forces the OrdWrapper to run:\n    if (VERBOSE) {\n      System.out.println(\"TEST: top reader\");\n    }\n    verify(r, idToOrds, termsArray, null);\n\n    FieldCache.DEFAULT.purge(r);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    MockDirectoryWrapper dir = newDirectory();\n\n    final int NUM_TERMS = 100 * RANDOM_MULTIPLIER;\n    final Set<BytesRef> terms = new HashSet<BytesRef>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = _TestUtil.randomRealisticUnicodeString(random);\n      //final String s = _TestUtil.randomSimpleString(random);\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = 1000 * RANDOM_MULTIPLIER;\n\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer());\n\n    // Sometimes swap in codec that impls ord():\n    if (random.nextInt(10) == 7) {\n      // Make sure terms index has ords:\n      CoreCodecProvider cp = new CoreCodecProvider();\n      cp.register(new StandardCodecWithOrds());\n      cp.setDefaultFieldCodec(\"StandardOrds\");\n\n      // So checkIndex on close works\n      dir.setCodecProvider(cp);\n      conf.setCodecProvider(cp);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random, dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<Integer>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      NumericField idField = new NumericField(\"id\");\n      doc.add(idField.setIntValue(id));\n      \n      final int termCount = _TestUtil.nextInt(random, 0, 20*RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random.nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newField(\"field\", termsArray[ord].utf8ToString(), Field.Index.NOT_ANALYZED);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final IndexReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n\n    for(IndexReader subR : r.getSequentialSubReaders()) {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: sub=\" + subR);\n      }\n      verify(subR, idToOrds, termsArray, null);\n    }\n\n    // Also test top-level reader: its enum does not support\n    // ord, so this forces the OrdWrapper to run:\n    if (VERBOSE) {\n      System.out.println(\"TEST: top reader\");\n    }\n    verify(r, idToOrds, termsArray, null);\n\n    FieldCache.DEFAULT.purge(r);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"962d04139994fce5193143ef35615499a9a96d78","date":1302693744,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocTermOrds#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocTermOrds#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    MockDirectoryWrapper dir = newDirectory();\n\n    final int NUM_TERMS = 100 * RANDOM_MULTIPLIER;\n    final Set<BytesRef> terms = new HashSet<BytesRef>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = _TestUtil.randomRealisticUnicodeString(random);\n      //final String s = _TestUtil.randomSimpleString(random);\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = 1000 * RANDOM_MULTIPLIER;\n\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n\n    // Sometimes swap in codec that impls ord():\n    if (random.nextInt(10) == 7) {\n      // Make sure terms index has ords:\n      CoreCodecProvider cp = new CoreCodecProvider();\n      cp.register(new StandardCodecWithOrds());\n      cp.setDefaultFieldCodec(\"StandardOrds\");\n\n      // So checkIndex on close works\n      dir.setCodecProvider(cp);\n      conf.setCodecProvider(cp);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random, dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<Integer>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      NumericField idField = new NumericField(\"id\");\n      doc.add(idField.setIntValue(id));\n      \n      final int termCount = _TestUtil.nextInt(random, 0, 20*RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random.nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newField(\"field\", termsArray[ord].utf8ToString(), Field.Index.NOT_ANALYZED);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final IndexReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n\n    for(IndexReader subR : r.getSequentialSubReaders()) {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: sub=\" + subR);\n      }\n      verify(subR, idToOrds, termsArray, null);\n    }\n\n    // Also test top-level reader: its enum does not support\n    // ord, so this forces the OrdWrapper to run:\n    if (VERBOSE) {\n      System.out.println(\"TEST: top reader\");\n    }\n    verify(r, idToOrds, termsArray, null);\n\n    FieldCache.DEFAULT.purge(r);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    MockDirectoryWrapper dir = newDirectory();\n\n    final int NUM_TERMS = 100 * RANDOM_MULTIPLIER;\n    final Set<BytesRef> terms = new HashSet<BytesRef>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = _TestUtil.randomRealisticUnicodeString(random);\n      //final String s = _TestUtil.randomSimpleString(random);\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = 1000 * RANDOM_MULTIPLIER;\n\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer());\n\n    // Sometimes swap in codec that impls ord():\n    if (random.nextInt(10) == 7) {\n      // Make sure terms index has ords:\n      CoreCodecProvider cp = new CoreCodecProvider();\n      cp.register(new StandardCodecWithOrds());\n      cp.setDefaultFieldCodec(\"StandardOrds\");\n\n      // So checkIndex on close works\n      dir.setCodecProvider(cp);\n      conf.setCodecProvider(cp);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random, dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<Integer>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      NumericField idField = new NumericField(\"id\");\n      doc.add(idField.setIntValue(id));\n      \n      final int termCount = _TestUtil.nextInt(random, 0, 20*RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random.nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newField(\"field\", termsArray[ord].utf8ToString(), Field.Index.NOT_ANALYZED);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final IndexReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n\n    for(IndexReader subR : r.getSequentialSubReaders()) {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: sub=\" + subR);\n      }\n      verify(subR, idToOrds, termsArray, null);\n    }\n\n    // Also test top-level reader: its enum does not support\n    // ord, so this forces the OrdWrapper to run:\n    if (VERBOSE) {\n      System.out.println(\"TEST: top reader\");\n    }\n    verify(r, idToOrds, termsArray, null);\n\n    FieldCache.DEFAULT.purge(r);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocTermOrds#testRandom().mjava","pathOld":"/dev/null","sourceNew":"  public void testRandom() throws Exception {\n    MockDirectoryWrapper dir = newDirectory();\n\n    final int NUM_TERMS = 100 * RANDOM_MULTIPLIER;\n    final Set<BytesRef> terms = new HashSet<BytesRef>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = _TestUtil.randomRealisticUnicodeString(random);\n      //final String s = _TestUtil.randomSimpleString(random);\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = 1000 * RANDOM_MULTIPLIER;\n\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n\n    // Sometimes swap in codec that impls ord():\n    if (random.nextInt(10) == 7) {\n      // Make sure terms index has ords:\n      CoreCodecProvider cp = new CoreCodecProvider();\n      cp.register(new StandardCodecWithOrds());\n      cp.setDefaultFieldCodec(\"StandardOrds\");\n\n      // So checkIndex on close works\n      dir.setCodecProvider(cp);\n      conf.setCodecProvider(cp);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random, dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<Integer>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      NumericField idField = new NumericField(\"id\");\n      doc.add(idField.setIntValue(id));\n      \n      final int termCount = _TestUtil.nextInt(random, 0, 20*RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random.nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newField(\"field\", termsArray[ord].utf8ToString(), Field.Index.NOT_ANALYZED);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final IndexReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n\n    for(IndexReader subR : r.getSequentialSubReaders()) {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: sub=\" + subR);\n      }\n      verify(subR, idToOrds, termsArray, null);\n    }\n\n    // Also test top-level reader: its enum does not support\n    // ord, so this forces the OrdWrapper to run:\n    if (VERBOSE) {\n      System.out.println(\"TEST: top reader\");\n    }\n    verify(r, idToOrds, termsArray, null);\n\n    FieldCache.DEFAULT.purge(r);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":0,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocTermOrds#testRandom().mjava","pathOld":"/dev/null","sourceNew":"  public void testRandom() throws Exception {\n    MockDirectoryWrapper dir = newDirectory();\n\n    final int NUM_TERMS = 100 * RANDOM_MULTIPLIER;\n    final Set<BytesRef> terms = new HashSet<BytesRef>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = _TestUtil.randomRealisticUnicodeString(random);\n      //final String s = _TestUtil.randomSimpleString(random);\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = 1000 * RANDOM_MULTIPLIER;\n\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n\n    // Sometimes swap in codec that impls ord():\n    if (random.nextInt(10) == 7) {\n      // Make sure terms index has ords:\n      CoreCodecProvider cp = new CoreCodecProvider();\n      cp.register(new StandardCodecWithOrds());\n      cp.setDefaultFieldCodec(\"StandardOrds\");\n\n      // So checkIndex on close works\n      dir.setCodecProvider(cp);\n      conf.setCodecProvider(cp);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random, dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<Integer>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      NumericField idField = new NumericField(\"id\");\n      doc.add(idField.setIntValue(id));\n      \n      final int termCount = _TestUtil.nextInt(random, 0, 20*RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random.nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newField(\"field\", termsArray[ord].utf8ToString(), Field.Index.NOT_ANALYZED);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final IndexReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n\n    for(IndexReader subR : r.getSequentialSubReaders()) {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: sub=\" + subR);\n      }\n      verify(subR, idToOrds, termsArray, null);\n    }\n\n    // Also test top-level reader: its enum does not support\n    // ord, so this forces the OrdWrapper to run:\n    if (VERBOSE) {\n      System.out.println(\"TEST: top reader\");\n    }\n    verify(r, idToOrds, termsArray, null);\n\n    FieldCache.DEFAULT.purge(r);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ff6fd241dc6610f7f81b62e3ba4cedf105939623","date":1307331653,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocTermOrds#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocTermOrds#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    MockDirectoryWrapper dir = newDirectory();\n\n    final int NUM_TERMS = (TEST_NIGHTLY ? 100 : 20) * RANDOM_MULTIPLIER;\n    final Set<BytesRef> terms = new HashSet<BytesRef>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = _TestUtil.randomRealisticUnicodeString(random);\n      //final String s = _TestUtil.randomSimpleString(random);\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = (TEST_NIGHTLY ? 1000 : 100) * RANDOM_MULTIPLIER;\n\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n\n    // Sometimes swap in codec that impls ord():\n    if (random.nextInt(10) == 7) {\n      // Make sure terms index has ords:\n      CoreCodecProvider cp = new CoreCodecProvider();\n      cp.register(new StandardCodecWithOrds());\n      cp.setDefaultFieldCodec(\"StandardOrds\");\n\n      // So checkIndex on close works\n      dir.setCodecProvider(cp);\n      conf.setCodecProvider(cp);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random, dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<Integer>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      NumericField idField = new NumericField(\"id\");\n      doc.add(idField.setIntValue(id));\n      \n      final int termCount = _TestUtil.nextInt(random, 0, 20*RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random.nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newField(\"field\", termsArray[ord].utf8ToString(), Field.Index.NOT_ANALYZED_NO_NORMS);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final IndexReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n\n    for(IndexReader subR : r.getSequentialSubReaders()) {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: sub=\" + subR);\n      }\n      verify(subR, idToOrds, termsArray, null);\n    }\n\n    // Also test top-level reader: its enum does not support\n    // ord, so this forces the OrdWrapper to run:\n    if (VERBOSE) {\n      System.out.println(\"TEST: top reader\");\n    }\n    verify(r, idToOrds, termsArray, null);\n\n    FieldCache.DEFAULT.purge(r);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    MockDirectoryWrapper dir = newDirectory();\n\n    final int NUM_TERMS = 100 * RANDOM_MULTIPLIER;\n    final Set<BytesRef> terms = new HashSet<BytesRef>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = _TestUtil.randomRealisticUnicodeString(random);\n      //final String s = _TestUtil.randomSimpleString(random);\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = 1000 * RANDOM_MULTIPLIER;\n\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n\n    // Sometimes swap in codec that impls ord():\n    if (random.nextInt(10) == 7) {\n      // Make sure terms index has ords:\n      CoreCodecProvider cp = new CoreCodecProvider();\n      cp.register(new StandardCodecWithOrds());\n      cp.setDefaultFieldCodec(\"StandardOrds\");\n\n      // So checkIndex on close works\n      dir.setCodecProvider(cp);\n      conf.setCodecProvider(cp);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random, dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<Integer>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      NumericField idField = new NumericField(\"id\");\n      doc.add(idField.setIntValue(id));\n      \n      final int termCount = _TestUtil.nextInt(random, 0, 20*RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random.nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newField(\"field\", termsArray[ord].utf8ToString(), Field.Index.NOT_ANALYZED);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final IndexReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n\n    for(IndexReader subR : r.getSequentialSubReaders()) {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: sub=\" + subR);\n      }\n      verify(subR, idToOrds, termsArray, null);\n    }\n\n    // Also test top-level reader: its enum does not support\n    // ord, so this forces the OrdWrapper to run:\n    if (VERBOSE) {\n      System.out.println(\"TEST: top reader\");\n    }\n    verify(r, idToOrds, termsArray, null);\n\n    FieldCache.DEFAULT.purge(r);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"79c2cb24929f2649a8875fb629086171f914d5ce","date":1307332717,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocTermOrds#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocTermOrds#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    MockDirectoryWrapper dir = newDirectory();\n\n    final int NUM_TERMS = (TEST_NIGHTLY ? 100 : 20) * RANDOM_MULTIPLIER;\n    final Set<BytesRef> terms = new HashSet<BytesRef>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = _TestUtil.randomRealisticUnicodeString(random);\n      //final String s = _TestUtil.randomSimpleString(random);\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = (TEST_NIGHTLY ? 1000 : 100) * RANDOM_MULTIPLIER;\n\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n\n    // Sometimes swap in codec that impls ord():\n    if (random.nextInt(10) == 7) {\n      // Make sure terms index has ords:\n      CoreCodecProvider cp = new CoreCodecProvider();\n      cp.register(new StandardCodecWithOrds());\n      cp.setDefaultFieldCodec(\"StandardOrds\");\n\n      // So checkIndex on close works\n      dir.setCodecProvider(cp);\n      conf.setCodecProvider(cp);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random, dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<Integer>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      NumericField idField = new NumericField(\"id\");\n      doc.add(idField.setIntValue(id));\n      \n      final int termCount = _TestUtil.nextInt(random, 0, 20*RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random.nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newField(\"field\", termsArray[ord].utf8ToString(), Field.Index.NOT_ANALYZED_NO_NORMS);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final IndexReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n\n    for(IndexReader subR : r.getSequentialSubReaders()) {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: sub=\" + subR);\n      }\n      verify(subR, idToOrds, termsArray, null);\n    }\n\n    // Also test top-level reader: its enum does not support\n    // ord, so this forces the OrdWrapper to run:\n    if (VERBOSE) {\n      System.out.println(\"TEST: top reader\");\n    }\n    verify(r, idToOrds, termsArray, null);\n\n    FieldCache.DEFAULT.purge(r);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    MockDirectoryWrapper dir = newDirectory();\n\n    final int NUM_TERMS = 100 * RANDOM_MULTIPLIER;\n    final Set<BytesRef> terms = new HashSet<BytesRef>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = _TestUtil.randomRealisticUnicodeString(random);\n      //final String s = _TestUtil.randomSimpleString(random);\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = 1000 * RANDOM_MULTIPLIER;\n\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n\n    // Sometimes swap in codec that impls ord():\n    if (random.nextInt(10) == 7) {\n      // Make sure terms index has ords:\n      CoreCodecProvider cp = new CoreCodecProvider();\n      cp.register(new StandardCodecWithOrds());\n      cp.setDefaultFieldCodec(\"StandardOrds\");\n\n      // So checkIndex on close works\n      dir.setCodecProvider(cp);\n      conf.setCodecProvider(cp);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random, dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<Integer>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      NumericField idField = new NumericField(\"id\");\n      doc.add(idField.setIntValue(id));\n      \n      final int termCount = _TestUtil.nextInt(random, 0, 20*RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random.nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newField(\"field\", termsArray[ord].utf8ToString(), Field.Index.NOT_ANALYZED);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final IndexReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n\n    for(IndexReader subR : r.getSequentialSubReaders()) {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: sub=\" + subR);\n      }\n      verify(subR, idToOrds, termsArray, null);\n    }\n\n    // Also test top-level reader: its enum does not support\n    // ord, so this forces the OrdWrapper to run:\n    if (VERBOSE) {\n      System.out.println(\"TEST: top reader\");\n    }\n    verify(r, idToOrds, termsArray, null);\n\n    FieldCache.DEFAULT.purge(r);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0762b640e0d0d12b6edb96db68986e13145c3484","date":1307575932,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocTermOrds#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocTermOrds#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    MockDirectoryWrapper dir = newDirectory();\n\n    final int NUM_TERMS = atLeast(20);\n    final Set<BytesRef> terms = new HashSet<BytesRef>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = _TestUtil.randomRealisticUnicodeString(random);\n      //final String s = _TestUtil.randomSimpleString(random);\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = atLeast(100);\n\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n\n    // Sometimes swap in codec that impls ord():\n    if (random.nextInt(10) == 7) {\n      // Make sure terms index has ords:\n      CoreCodecProvider cp = new CoreCodecProvider();\n      cp.register(new StandardCodecWithOrds());\n      cp.setDefaultFieldCodec(\"StandardOrds\");\n\n      // So checkIndex on close works\n      dir.setCodecProvider(cp);\n      conf.setCodecProvider(cp);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random, dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<Integer>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      NumericField idField = new NumericField(\"id\");\n      doc.add(idField.setIntValue(id));\n      \n      final int termCount = _TestUtil.nextInt(random, 0, 20*RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random.nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newField(\"field\", termsArray[ord].utf8ToString(), Field.Index.NOT_ANALYZED_NO_NORMS);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final IndexReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n\n    for(IndexReader subR : r.getSequentialSubReaders()) {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: sub=\" + subR);\n      }\n      verify(subR, idToOrds, termsArray, null);\n    }\n\n    // Also test top-level reader: its enum does not support\n    // ord, so this forces the OrdWrapper to run:\n    if (VERBOSE) {\n      System.out.println(\"TEST: top reader\");\n    }\n    verify(r, idToOrds, termsArray, null);\n\n    FieldCache.DEFAULT.purge(r);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    MockDirectoryWrapper dir = newDirectory();\n\n    final int NUM_TERMS = (TEST_NIGHTLY ? 100 : 20) * RANDOM_MULTIPLIER;\n    final Set<BytesRef> terms = new HashSet<BytesRef>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = _TestUtil.randomRealisticUnicodeString(random);\n      //final String s = _TestUtil.randomSimpleString(random);\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = (TEST_NIGHTLY ? 1000 : 100) * RANDOM_MULTIPLIER;\n\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n\n    // Sometimes swap in codec that impls ord():\n    if (random.nextInt(10) == 7) {\n      // Make sure terms index has ords:\n      CoreCodecProvider cp = new CoreCodecProvider();\n      cp.register(new StandardCodecWithOrds());\n      cp.setDefaultFieldCodec(\"StandardOrds\");\n\n      // So checkIndex on close works\n      dir.setCodecProvider(cp);\n      conf.setCodecProvider(cp);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random, dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<Integer>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      NumericField idField = new NumericField(\"id\");\n      doc.add(idField.setIntValue(id));\n      \n      final int termCount = _TestUtil.nextInt(random, 0, 20*RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random.nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newField(\"field\", termsArray[ord].utf8ToString(), Field.Index.NOT_ANALYZED_NO_NORMS);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final IndexReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n\n    for(IndexReader subR : r.getSequentialSubReaders()) {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: sub=\" + subR);\n      }\n      verify(subR, idToOrds, termsArray, null);\n    }\n\n    // Also test top-level reader: its enum does not support\n    // ord, so this forces the OrdWrapper to run:\n    if (VERBOSE) {\n      System.out.println(\"TEST: top reader\");\n    }\n    verify(r, idToOrds, termsArray, null);\n\n    FieldCache.DEFAULT.purge(r);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"77cf4379b2824f6ea34b091c495d6e95c38ff9e2","date":1307610475,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocTermOrds#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocTermOrds#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    MockDirectoryWrapper dir = newDirectory();\n\n    final int NUM_TERMS = atLeast(20);\n    final Set<BytesRef> terms = new HashSet<BytesRef>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = _TestUtil.randomRealisticUnicodeString(random);\n      //final String s = _TestUtil.randomSimpleString(random);\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = atLeast(100);\n\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n\n    // Sometimes swap in codec that impls ord():\n    if (random.nextInt(10) == 7) {\n      // Make sure terms index has ords:\n      CoreCodecProvider cp = new CoreCodecProvider();\n      cp.register(new StandardCodecWithOrds());\n      cp.setDefaultFieldCodec(\"StandardOrds\");\n\n      // So checkIndex on close works\n      dir.setCodecProvider(cp);\n      conf.setCodecProvider(cp);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random, dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<Integer>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      NumericField idField = new NumericField(\"id\");\n      doc.add(idField.setIntValue(id));\n      \n      final int termCount = _TestUtil.nextInt(random, 0, 20*RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random.nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newField(\"field\", termsArray[ord].utf8ToString(), Field.Index.NOT_ANALYZED_NO_NORMS);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final IndexReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n\n    for(IndexReader subR : r.getSequentialSubReaders()) {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: sub=\" + subR);\n      }\n      verify(subR, idToOrds, termsArray, null);\n    }\n\n    // Also test top-level reader: its enum does not support\n    // ord, so this forces the OrdWrapper to run:\n    if (VERBOSE) {\n      System.out.println(\"TEST: top reader\");\n    }\n    verify(r, idToOrds, termsArray, null);\n\n    FieldCache.DEFAULT.purge(r);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    MockDirectoryWrapper dir = newDirectory();\n\n    final int NUM_TERMS = 100 * RANDOM_MULTIPLIER;\n    final Set<BytesRef> terms = new HashSet<BytesRef>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = _TestUtil.randomRealisticUnicodeString(random);\n      //final String s = _TestUtil.randomSimpleString(random);\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = 1000 * RANDOM_MULTIPLIER;\n\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n\n    // Sometimes swap in codec that impls ord():\n    if (random.nextInt(10) == 7) {\n      // Make sure terms index has ords:\n      CoreCodecProvider cp = new CoreCodecProvider();\n      cp.register(new StandardCodecWithOrds());\n      cp.setDefaultFieldCodec(\"StandardOrds\");\n\n      // So checkIndex on close works\n      dir.setCodecProvider(cp);\n      conf.setCodecProvider(cp);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random, dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<Integer>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      NumericField idField = new NumericField(\"id\");\n      doc.add(idField.setIntValue(id));\n      \n      final int termCount = _TestUtil.nextInt(random, 0, 20*RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random.nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newField(\"field\", termsArray[ord].utf8ToString(), Field.Index.NOT_ANALYZED);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final IndexReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n\n    for(IndexReader subR : r.getSequentialSubReaders()) {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: sub=\" + subR);\n      }\n      verify(subR, idToOrds, termsArray, null);\n    }\n\n    // Also test top-level reader: its enum does not support\n    // ord, so this forces the OrdWrapper to run:\n    if (VERBOSE) {\n      System.out.println(\"TEST: top reader\");\n    }\n    verify(r, idToOrds, termsArray, null);\n\n    FieldCache.DEFAULT.purge(r);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","date":1307729864,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocTermOrds#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocTermOrds#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    MockDirectoryWrapper dir = newDirectory();\n\n    final int NUM_TERMS = atLeast(20);\n    final Set<BytesRef> terms = new HashSet<BytesRef>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = _TestUtil.randomRealisticUnicodeString(random);\n      //final String s = _TestUtil.randomSimpleString(random);\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = atLeast(100);\n\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n\n    // Sometimes swap in codec that impls ord():\n    if (random.nextInt(10) == 7) {\n      // Make sure terms index has ords:\n      CoreCodecProvider cp = new CoreCodecProvider();\n      cp.register(new StandardCodecWithOrds());\n      cp.setDefaultFieldCodec(\"StandardOrds\");\n\n      // So checkIndex on close works\n      dir.setCodecProvider(cp);\n      conf.setCodecProvider(cp);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random, dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<Integer>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      NumericField idField = new NumericField(\"id\");\n      doc.add(idField.setIntValue(id));\n      \n      final int termCount = _TestUtil.nextInt(random, 0, 20*RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random.nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newField(\"field\", termsArray[ord].utf8ToString(), Field.Index.NOT_ANALYZED_NO_NORMS);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final IndexReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n\n    for(IndexReader subR : r.getSequentialSubReaders()) {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: sub=\" + subR);\n      }\n      verify(subR, idToOrds, termsArray, null);\n    }\n\n    // Also test top-level reader: its enum does not support\n    // ord, so this forces the OrdWrapper to run:\n    if (VERBOSE) {\n      System.out.println(\"TEST: top reader\");\n    }\n    verify(r, idToOrds, termsArray, null);\n\n    FieldCache.DEFAULT.purge(r);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    MockDirectoryWrapper dir = newDirectory();\n\n    final int NUM_TERMS = (TEST_NIGHTLY ? 100 : 20) * RANDOM_MULTIPLIER;\n    final Set<BytesRef> terms = new HashSet<BytesRef>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = _TestUtil.randomRealisticUnicodeString(random);\n      //final String s = _TestUtil.randomSimpleString(random);\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = (TEST_NIGHTLY ? 1000 : 100) * RANDOM_MULTIPLIER;\n\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n\n    // Sometimes swap in codec that impls ord():\n    if (random.nextInt(10) == 7) {\n      // Make sure terms index has ords:\n      CoreCodecProvider cp = new CoreCodecProvider();\n      cp.register(new StandardCodecWithOrds());\n      cp.setDefaultFieldCodec(\"StandardOrds\");\n\n      // So checkIndex on close works\n      dir.setCodecProvider(cp);\n      conf.setCodecProvider(cp);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random, dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<Integer>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      NumericField idField = new NumericField(\"id\");\n      doc.add(idField.setIntValue(id));\n      \n      final int termCount = _TestUtil.nextInt(random, 0, 20*RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random.nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newField(\"field\", termsArray[ord].utf8ToString(), Field.Index.NOT_ANALYZED_NO_NORMS);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final IndexReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n\n    for(IndexReader subR : r.getSequentialSubReaders()) {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: sub=\" + subR);\n      }\n      verify(subR, idToOrds, termsArray, null);\n    }\n\n    // Also test top-level reader: its enum does not support\n    // ord, so this forces the OrdWrapper to run:\n    if (VERBOSE) {\n      System.out.println(\"TEST: top reader\");\n    }\n    verify(r, idToOrds, termsArray, null);\n\n    FieldCache.DEFAULT.purge(r);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1509f151d7692d84fae414b2b799ac06ba60fcb4","date":1314451621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocTermOrds#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocTermOrds#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    MockDirectoryWrapper dir = newDirectory();\n\n    final int NUM_TERMS = atLeast(20);\n    final Set<BytesRef> terms = new HashSet<BytesRef>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = _TestUtil.randomRealisticUnicodeString(random);\n      //final String s = _TestUtil.randomSimpleString(random);\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = atLeast(100);\n\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n\n    // Sometimes swap in codec that impls ord():\n    if (random.nextInt(10) == 7) {\n      // Make sure terms index has ords:\n      CoreCodecProvider cp = new CoreCodecProvider();\n      cp.register(new StandardCodecWithOrds());\n      cp.setDefaultFieldCodec(\"StandardOrds\");\n\n      // So checkIndex on close works\n      dir.setCodecProvider(cp);\n      conf.setCodecProvider(cp);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random, dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<Integer>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      NumericField idField = new NumericField(\"id\");\n      doc.add(idField.setIntValue(id));\n      \n      final int termCount = _TestUtil.nextInt(random, 0, 20*RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random.nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newField(\"field\", termsArray[ord].utf8ToString(), StringField.TYPE_UNSTORED);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final IndexReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n\n    for(IndexReader subR : r.getSequentialSubReaders()) {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: sub=\" + subR);\n      }\n      verify(subR, idToOrds, termsArray, null);\n    }\n\n    // Also test top-level reader: its enum does not support\n    // ord, so this forces the OrdWrapper to run:\n    if (VERBOSE) {\n      System.out.println(\"TEST: top reader\");\n    }\n    verify(r, idToOrds, termsArray, null);\n\n    FieldCache.DEFAULT.purge(r);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    MockDirectoryWrapper dir = newDirectory();\n\n    final int NUM_TERMS = atLeast(20);\n    final Set<BytesRef> terms = new HashSet<BytesRef>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = _TestUtil.randomRealisticUnicodeString(random);\n      //final String s = _TestUtil.randomSimpleString(random);\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = atLeast(100);\n\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n\n    // Sometimes swap in codec that impls ord():\n    if (random.nextInt(10) == 7) {\n      // Make sure terms index has ords:\n      CoreCodecProvider cp = new CoreCodecProvider();\n      cp.register(new StandardCodecWithOrds());\n      cp.setDefaultFieldCodec(\"StandardOrds\");\n\n      // So checkIndex on close works\n      dir.setCodecProvider(cp);\n      conf.setCodecProvider(cp);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random, dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<Integer>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      NumericField idField = new NumericField(\"id\");\n      doc.add(idField.setIntValue(id));\n      \n      final int termCount = _TestUtil.nextInt(random, 0, 20*RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random.nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newField(\"field\", termsArray[ord].utf8ToString(), Field.Index.NOT_ANALYZED_NO_NORMS);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final IndexReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n\n    for(IndexReader subR : r.getSequentialSubReaders()) {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: sub=\" + subR);\n      }\n      verify(subR, idToOrds, termsArray, null);\n    }\n\n    // Also test top-level reader: its enum does not support\n    // ord, so this forces the OrdWrapper to run:\n    if (VERBOSE) {\n      System.out.println(\"TEST: top reader\");\n    }\n    verify(r, idToOrds, termsArray, null);\n\n    FieldCache.DEFAULT.purge(r);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7b91922b55d15444d554721b352861d028eb8278","date":1320421415,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocTermOrds#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocTermOrds#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    MockDirectoryWrapper dir = newDirectory();\n\n    final int NUM_TERMS = atLeast(20);\n    final Set<BytesRef> terms = new HashSet<BytesRef>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = _TestUtil.randomRealisticUnicodeString(random);\n      //final String s = _TestUtil.randomSimpleString(random);\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = atLeast(100);\n\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n\n    // Sometimes swap in codec that impls ord():\n    if (random.nextInt(10) == 7) {\n      // Make sure terms index has ords:\n      Codec codec = _TestUtil.alwaysPostingsFormat(PostingsFormat.forName(\"Lucene40WithOrds\"));\n      conf.setCodec(codec);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random, dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<Integer>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      NumericField idField = new NumericField(\"id\");\n      doc.add(idField.setIntValue(id));\n      \n      final int termCount = _TestUtil.nextInt(random, 0, 20*RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random.nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newField(\"field\", termsArray[ord].utf8ToString(), StringField.TYPE_UNSTORED);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final IndexReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n\n    for(IndexReader subR : r.getSequentialSubReaders()) {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: sub=\" + subR);\n      }\n      verify(subR, idToOrds, termsArray, null);\n    }\n\n    // Also test top-level reader: its enum does not support\n    // ord, so this forces the OrdWrapper to run:\n    if (VERBOSE) {\n      System.out.println(\"TEST: top reader\");\n    }\n    verify(r, idToOrds, termsArray, null);\n\n    FieldCache.DEFAULT.purge(r);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    MockDirectoryWrapper dir = newDirectory();\n\n    final int NUM_TERMS = atLeast(20);\n    final Set<BytesRef> terms = new HashSet<BytesRef>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = _TestUtil.randomRealisticUnicodeString(random);\n      //final String s = _TestUtil.randomSimpleString(random);\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = atLeast(100);\n\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n\n    // Sometimes swap in codec that impls ord():\n    if (random.nextInt(10) == 7) {\n      // Make sure terms index has ords:\n      CoreCodecProvider cp = new CoreCodecProvider();\n      cp.register(new StandardCodecWithOrds());\n      cp.setDefaultFieldCodec(\"StandardOrds\");\n\n      // So checkIndex on close works\n      dir.setCodecProvider(cp);\n      conf.setCodecProvider(cp);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random, dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<Integer>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      NumericField idField = new NumericField(\"id\");\n      doc.add(idField.setIntValue(id));\n      \n      final int termCount = _TestUtil.nextInt(random, 0, 20*RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random.nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newField(\"field\", termsArray[ord].utf8ToString(), StringField.TYPE_UNSTORED);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final IndexReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n\n    for(IndexReader subR : r.getSequentialSubReaders()) {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: sub=\" + subR);\n      }\n      verify(subR, idToOrds, termsArray, null);\n    }\n\n    // Also test top-level reader: its enum does not support\n    // ord, so this forces the OrdWrapper to run:\n    if (VERBOSE) {\n      System.out.println(\"TEST: top reader\");\n    }\n    verify(r, idToOrds, termsArray, null);\n\n    FieldCache.DEFAULT.purge(r);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fa0f44f887719e97183771e977cfc4bfb485b766","date":1326668713,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocTermOrds#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocTermOrds#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    MockDirectoryWrapper dir = newDirectory();\n\n    final int NUM_TERMS = atLeast(20);\n    final Set<BytesRef> terms = new HashSet<BytesRef>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = _TestUtil.randomRealisticUnicodeString(random);\n      //final String s = _TestUtil.randomSimpleString(random);\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = atLeast(100);\n\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n\n    // Sometimes swap in codec that impls ord():\n    if (random.nextInt(10) == 7) {\n      // Make sure terms index has ords:\n      Codec codec = _TestUtil.alwaysPostingsFormat(PostingsFormat.forName(\"Lucene40WithOrds\"));\n      conf.setCodec(codec);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random, dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<Integer>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      doc.add(new NumericField(\"id\", id));\n      \n      final int termCount = _TestUtil.nextInt(random, 0, 20*RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random.nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newField(\"field\", termsArray[ord].utf8ToString(), StringField.TYPE_UNSTORED);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final IndexReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n\n    for(IndexReader subR : r.getSequentialSubReaders()) {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: sub=\" + subR);\n      }\n      verify(subR, idToOrds, termsArray, null);\n    }\n\n    // Also test top-level reader: its enum does not support\n    // ord, so this forces the OrdWrapper to run:\n    if (VERBOSE) {\n      System.out.println(\"TEST: top reader\");\n    }\n    verify(r, idToOrds, termsArray, null);\n\n    FieldCache.DEFAULT.purge(r);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    MockDirectoryWrapper dir = newDirectory();\n\n    final int NUM_TERMS = atLeast(20);\n    final Set<BytesRef> terms = new HashSet<BytesRef>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = _TestUtil.randomRealisticUnicodeString(random);\n      //final String s = _TestUtil.randomSimpleString(random);\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = atLeast(100);\n\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n\n    // Sometimes swap in codec that impls ord():\n    if (random.nextInt(10) == 7) {\n      // Make sure terms index has ords:\n      Codec codec = _TestUtil.alwaysPostingsFormat(PostingsFormat.forName(\"Lucene40WithOrds\"));\n      conf.setCodec(codec);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random, dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<Integer>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      NumericField idField = new NumericField(\"id\");\n      doc.add(idField.setIntValue(id));\n      \n      final int termCount = _TestUtil.nextInt(random, 0, 20*RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random.nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newField(\"field\", termsArray[ord].utf8ToString(), StringField.TYPE_UNSTORED);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final IndexReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n\n    for(IndexReader subR : r.getSequentialSubReaders()) {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: sub=\" + subR);\n      }\n      verify(subR, idToOrds, termsArray, null);\n    }\n\n    // Also test top-level reader: its enum does not support\n    // ord, so this forces the OrdWrapper to run:\n    if (VERBOSE) {\n      System.out.println(\"TEST: top reader\");\n    }\n    verify(r, idToOrds, termsArray, null);\n\n    FieldCache.DEFAULT.purge(r);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":["95ae76773bf2b95987d5f9c8f566ab3738953fb4"],"bugIntro":["a78a90fc9701e511308346ea29f4f5e548bb39fe"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2dd6ecb8250c497ed227653279d6a4f470bfbb31","date":1326814483,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocTermOrds#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocTermOrds#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    MockDirectoryWrapper dir = newDirectory();\n\n    final int NUM_TERMS = atLeast(20);\n    final Set<BytesRef> terms = new HashSet<BytesRef>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = _TestUtil.randomRealisticUnicodeString(random);\n      //final String s = _TestUtil.randomSimpleString(random);\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = atLeast(100);\n\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n\n    // Sometimes swap in codec that impls ord():\n    if (random.nextInt(10) == 7) {\n      // Make sure terms index has ords:\n      Codec codec = _TestUtil.alwaysPostingsFormat(PostingsFormat.forName(\"Lucene40WithOrds\"));\n      conf.setCodec(codec);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random, dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<Integer>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      doc.add(new NumericField(\"id\", id));\n      \n      final int termCount = _TestUtil.nextInt(random, 0, 20*RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random.nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newField(\"field\", termsArray[ord].utf8ToString(), StringField.TYPE_UNSTORED);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final IndexReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n\n    for(IndexReader subR : r.getSequentialSubReaders()) {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: sub=\" + subR);\n      }\n      verify(subR, idToOrds, termsArray, null);\n    }\n\n    // Also test top-level reader: its enum does not support\n    // ord, so this forces the OrdWrapper to run:\n    if (VERBOSE) {\n      System.out.println(\"TEST: top reader\");\n    }\n    verify(new SlowMultiReaderWrapper(r), idToOrds, termsArray, null);\n\n    FieldCache.DEFAULT.purge(r);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    MockDirectoryWrapper dir = newDirectory();\n\n    final int NUM_TERMS = atLeast(20);\n    final Set<BytesRef> terms = new HashSet<BytesRef>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = _TestUtil.randomRealisticUnicodeString(random);\n      //final String s = _TestUtil.randomSimpleString(random);\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = atLeast(100);\n\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n\n    // Sometimes swap in codec that impls ord():\n    if (random.nextInt(10) == 7) {\n      // Make sure terms index has ords:\n      Codec codec = _TestUtil.alwaysPostingsFormat(PostingsFormat.forName(\"Lucene40WithOrds\"));\n      conf.setCodec(codec);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random, dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<Integer>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      doc.add(new NumericField(\"id\", id));\n      \n      final int termCount = _TestUtil.nextInt(random, 0, 20*RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random.nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newField(\"field\", termsArray[ord].utf8ToString(), StringField.TYPE_UNSTORED);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final IndexReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n\n    for(IndexReader subR : r.getSequentialSubReaders()) {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: sub=\" + subR);\n      }\n      verify(subR, idToOrds, termsArray, null);\n    }\n\n    // Also test top-level reader: its enum does not support\n    // ord, so this forces the OrdWrapper to run:\n    if (VERBOSE) {\n      System.out.println(\"TEST: top reader\");\n    }\n    verify(r, idToOrds, termsArray, null);\n\n    FieldCache.DEFAULT.purge(r);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"386d1b0dcb065f1bfc494b1407cb41c536b95485","date":1327848512,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocTermOrds#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocTermOrds#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    MockDirectoryWrapper dir = newDirectory();\n\n    final int NUM_TERMS = atLeast(20);\n    final Set<BytesRef> terms = new HashSet<BytesRef>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = _TestUtil.randomRealisticUnicodeString(random);\n      //final String s = _TestUtil.randomSimpleString(random);\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = atLeast(100);\n\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n\n    // Sometimes swap in codec that impls ord():\n    if (random.nextInt(10) == 7) {\n      // Make sure terms index has ords:\n      Codec codec = _TestUtil.alwaysPostingsFormat(PostingsFormat.forName(\"Lucene40WithOrds\"));\n      conf.setCodec(codec);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random, dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<Integer>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      doc.add(new NumericField(\"id\", id));\n      \n      final int termCount = _TestUtil.nextInt(random, 0, 20*RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random.nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newField(\"field\", termsArray[ord].utf8ToString(), StringField.TYPE_UNSTORED);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final IndexReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n\n    for(IndexReader subR : r.getSequentialSubReaders()) {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: sub=\" + subR);\n      }\n      verify(subR, idToOrds, termsArray, null);\n    }\n\n    // Also test top-level reader: its enum does not support\n    // ord, so this forces the OrdWrapper to run:\n    if (VERBOSE) {\n      System.out.println(\"TEST: top reader\");\n    }\n    verify(SlowCompositeReaderWrapper.wrap(r), idToOrds, termsArray, null);\n\n    FieldCache.DEFAULT.purge(r);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    MockDirectoryWrapper dir = newDirectory();\n\n    final int NUM_TERMS = atLeast(20);\n    final Set<BytesRef> terms = new HashSet<BytesRef>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = _TestUtil.randomRealisticUnicodeString(random);\n      //final String s = _TestUtil.randomSimpleString(random);\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = atLeast(100);\n\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n\n    // Sometimes swap in codec that impls ord():\n    if (random.nextInt(10) == 7) {\n      // Make sure terms index has ords:\n      Codec codec = _TestUtil.alwaysPostingsFormat(PostingsFormat.forName(\"Lucene40WithOrds\"));\n      conf.setCodec(codec);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random, dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<Integer>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      doc.add(new NumericField(\"id\", id));\n      \n      final int termCount = _TestUtil.nextInt(random, 0, 20*RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random.nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newField(\"field\", termsArray[ord].utf8ToString(), StringField.TYPE_UNSTORED);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final IndexReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n\n    for(IndexReader subR : r.getSequentialSubReaders()) {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: sub=\" + subR);\n      }\n      verify(subR, idToOrds, termsArray, null);\n    }\n\n    // Also test top-level reader: its enum does not support\n    // ord, so this forces the OrdWrapper to run:\n    if (VERBOSE) {\n      System.out.println(\"TEST: top reader\");\n    }\n    verify(new SlowMultiReaderWrapper(r), idToOrds, termsArray, null);\n\n    FieldCache.DEFAULT.purge(r);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"868186558eb3a854ce7e720a52bb445795d54910","date":1327853682,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocTermOrds#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocTermOrds#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    MockDirectoryWrapper dir = newDirectory();\n\n    final int NUM_TERMS = atLeast(20);\n    final Set<BytesRef> terms = new HashSet<BytesRef>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = _TestUtil.randomRealisticUnicodeString(random);\n      //final String s = _TestUtil.randomSimpleString(random);\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = atLeast(100);\n\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n\n    // Sometimes swap in codec that impls ord():\n    if (random.nextInt(10) == 7) {\n      // Make sure terms index has ords:\n      Codec codec = _TestUtil.alwaysPostingsFormat(PostingsFormat.forName(\"Lucene40WithOrds\"));\n      conf.setCodec(codec);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random, dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<Integer>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      doc.add(new NumericField(\"id\", id));\n      \n      final int termCount = _TestUtil.nextInt(random, 0, 20*RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random.nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newField(\"field\", termsArray[ord].utf8ToString(), StringField.TYPE_UNSTORED);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n\n    for(IndexReader subR : r.getSequentialSubReaders()) {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: sub=\" + subR);\n      }\n      verify((AtomicIndexReader) subR, idToOrds, termsArray, null);\n    }\n\n    // Also test top-level reader: its enum does not support\n    // ord, so this forces the OrdWrapper to run:\n    if (VERBOSE) {\n      System.out.println(\"TEST: top reader\");\n    }\n    AtomicIndexReader slowR = SlowCompositeReaderWrapper.wrap(r);\n    verify(slowR, idToOrds, termsArray, null);\n\n    FieldCache.DEFAULT.purge(slowR);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    MockDirectoryWrapper dir = newDirectory();\n\n    final int NUM_TERMS = atLeast(20);\n    final Set<BytesRef> terms = new HashSet<BytesRef>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = _TestUtil.randomRealisticUnicodeString(random);\n      //final String s = _TestUtil.randomSimpleString(random);\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = atLeast(100);\n\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n\n    // Sometimes swap in codec that impls ord():\n    if (random.nextInt(10) == 7) {\n      // Make sure terms index has ords:\n      Codec codec = _TestUtil.alwaysPostingsFormat(PostingsFormat.forName(\"Lucene40WithOrds\"));\n      conf.setCodec(codec);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random, dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<Integer>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      doc.add(new NumericField(\"id\", id));\n      \n      final int termCount = _TestUtil.nextInt(random, 0, 20*RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random.nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newField(\"field\", termsArray[ord].utf8ToString(), StringField.TYPE_UNSTORED);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final IndexReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n\n    for(IndexReader subR : r.getSequentialSubReaders()) {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: sub=\" + subR);\n      }\n      verify(subR, idToOrds, termsArray, null);\n    }\n\n    // Also test top-level reader: its enum does not support\n    // ord, so this forces the OrdWrapper to run:\n    if (VERBOSE) {\n      System.out.println(\"TEST: top reader\");\n    }\n    verify(SlowCompositeReaderWrapper.wrap(r), idToOrds, termsArray, null);\n\n    FieldCache.DEFAULT.purge(r);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"da6d5ac19a80d65b1e864251f155d30960353b7e","date":1327881054,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocTermOrds#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocTermOrds#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    MockDirectoryWrapper dir = newDirectory();\n\n    final int NUM_TERMS = atLeast(20);\n    final Set<BytesRef> terms = new HashSet<BytesRef>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = _TestUtil.randomRealisticUnicodeString(random);\n      //final String s = _TestUtil.randomSimpleString(random);\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = atLeast(100);\n\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n\n    // Sometimes swap in codec that impls ord():\n    if (random.nextInt(10) == 7) {\n      // Make sure terms index has ords:\n      Codec codec = _TestUtil.alwaysPostingsFormat(PostingsFormat.forName(\"Lucene40WithOrds\"));\n      conf.setCodec(codec);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random, dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<Integer>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      doc.add(new NumericField(\"id\", id));\n      \n      final int termCount = _TestUtil.nextInt(random, 0, 20*RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random.nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newField(\"field\", termsArray[ord].utf8ToString(), StringField.TYPE_UNSTORED);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n\n    for(IndexReader subR : r.getSequentialSubReaders()) {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: sub=\" + subR);\n      }\n      verify((AtomicReader) subR, idToOrds, termsArray, null);\n    }\n\n    // Also test top-level reader: its enum does not support\n    // ord, so this forces the OrdWrapper to run:\n    if (VERBOSE) {\n      System.out.println(\"TEST: top reader\");\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(r);\n    verify(slowR, idToOrds, termsArray, null);\n\n    FieldCache.DEFAULT.purge(slowR);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    MockDirectoryWrapper dir = newDirectory();\n\n    final int NUM_TERMS = atLeast(20);\n    final Set<BytesRef> terms = new HashSet<BytesRef>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = _TestUtil.randomRealisticUnicodeString(random);\n      //final String s = _TestUtil.randomSimpleString(random);\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = atLeast(100);\n\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n\n    // Sometimes swap in codec that impls ord():\n    if (random.nextInt(10) == 7) {\n      // Make sure terms index has ords:\n      Codec codec = _TestUtil.alwaysPostingsFormat(PostingsFormat.forName(\"Lucene40WithOrds\"));\n      conf.setCodec(codec);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random, dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<Integer>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      doc.add(new NumericField(\"id\", id));\n      \n      final int termCount = _TestUtil.nextInt(random, 0, 20*RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random.nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newField(\"field\", termsArray[ord].utf8ToString(), StringField.TYPE_UNSTORED);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n\n    for(IndexReader subR : r.getSequentialSubReaders()) {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: sub=\" + subR);\n      }\n      verify((AtomicIndexReader) subR, idToOrds, termsArray, null);\n    }\n\n    // Also test top-level reader: its enum does not support\n    // ord, so this forces the OrdWrapper to run:\n    if (VERBOSE) {\n      System.out.println(\"TEST: top reader\");\n    }\n    AtomicIndexReader slowR = SlowCompositeReaderWrapper.wrap(r);\n    verify(slowR, idToOrds, termsArray, null);\n\n    FieldCache.DEFAULT.purge(slowR);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["ced66195b26fdb1f77ee00e2a77ec6918dedd766"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5cab9a86bd67202d20b6adc463008c8e982b070a","date":1327966443,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocTermOrds#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocTermOrds#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    MockDirectoryWrapper dir = newDirectory();\n\n    final int NUM_TERMS = atLeast(20);\n    final Set<BytesRef> terms = new HashSet<BytesRef>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = _TestUtil.randomRealisticUnicodeString(random);\n      //final String s = _TestUtil.randomSimpleString(random);\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = atLeast(100);\n\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n\n    // Sometimes swap in codec that impls ord():\n    if (random.nextInt(10) == 7) {\n      // Make sure terms index has ords:\n      Codec codec = _TestUtil.alwaysPostingsFormat(PostingsFormat.forName(\"Lucene40WithOrds\"));\n      conf.setCodec(codec);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random, dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<Integer>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      doc.add(new NumericField(\"id\", id));\n      \n      final int termCount = _TestUtil.nextInt(random, 0, 20*RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random.nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newField(\"field\", termsArray[ord].utf8ToString(), StringField.TYPE_UNSTORED);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n\n    for(IndexReader subR : r.getSequentialSubReaders()) {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: sub=\" + subR);\n      }\n      verify((AtomicReader) subR, idToOrds, termsArray, null);\n    }\n\n    // Also test top-level reader: its enum does not support\n    // ord, so this forces the OrdWrapper to run:\n    if (VERBOSE) {\n      System.out.println(\"TEST: top reader\");\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(r);\n    verify(slowR, idToOrds, termsArray, null);\n\n    FieldCache.DEFAULT.purge(slowR);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    MockDirectoryWrapper dir = newDirectory();\n\n    final int NUM_TERMS = atLeast(20);\n    final Set<BytesRef> terms = new HashSet<BytesRef>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = _TestUtil.randomRealisticUnicodeString(random);\n      //final String s = _TestUtil.randomSimpleString(random);\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = atLeast(100);\n\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n\n    // Sometimes swap in codec that impls ord():\n    if (random.nextInt(10) == 7) {\n      // Make sure terms index has ords:\n      Codec codec = _TestUtil.alwaysPostingsFormat(PostingsFormat.forName(\"Lucene40WithOrds\"));\n      conf.setCodec(codec);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random, dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<Integer>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      doc.add(new NumericField(\"id\", id));\n      \n      final int termCount = _TestUtil.nextInt(random, 0, 20*RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random.nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newField(\"field\", termsArray[ord].utf8ToString(), StringField.TYPE_UNSTORED);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final IndexReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n\n    for(IndexReader subR : r.getSequentialSubReaders()) {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: sub=\" + subR);\n      }\n      verify(subR, idToOrds, termsArray, null);\n    }\n\n    // Also test top-level reader: its enum does not support\n    // ord, so this forces the OrdWrapper to run:\n    if (VERBOSE) {\n      System.out.println(\"TEST: top reader\");\n    }\n    verify(new SlowMultiReaderWrapper(r), idToOrds, termsArray, null);\n\n    FieldCache.DEFAULT.purge(r);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDocTermOrds#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocTermOrds#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    MockDirectoryWrapper dir = newDirectory();\n\n    final int NUM_TERMS = atLeast(20);\n    final Set<BytesRef> terms = new HashSet<BytesRef>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = _TestUtil.randomRealisticUnicodeString(random);\n      //final String s = _TestUtil.randomSimpleString(random);\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = atLeast(100);\n\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n\n    // Sometimes swap in codec that impls ord():\n    if (random.nextInt(10) == 7) {\n      // Make sure terms index has ords:\n      Codec codec = _TestUtil.alwaysPostingsFormat(PostingsFormat.forName(\"Lucene40WithOrds\"));\n      conf.setCodec(codec);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random, dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<Integer>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      doc.add(new NumericField(\"id\", id));\n      \n      final int termCount = _TestUtil.nextInt(random, 0, 20*RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random.nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newField(\"field\", termsArray[ord].utf8ToString(), StringField.TYPE_UNSTORED);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n\n    for(IndexReader subR : r.getSequentialSubReaders()) {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: sub=\" + subR);\n      }\n      verify((AtomicReader) subR, idToOrds, termsArray, null);\n    }\n\n    // Also test top-level reader: its enum does not support\n    // ord, so this forces the OrdWrapper to run:\n    if (VERBOSE) {\n      System.out.println(\"TEST: top reader\");\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(r);\n    verify(slowR, idToOrds, termsArray, null);\n\n    FieldCache.DEFAULT.purge(slowR);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    MockDirectoryWrapper dir = newDirectory();\n\n    final int NUM_TERMS = atLeast(20);\n    final Set<BytesRef> terms = new HashSet<BytesRef>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = _TestUtil.randomRealisticUnicodeString(random);\n      //final String s = _TestUtil.randomSimpleString(random);\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = atLeast(100);\n\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n\n    // Sometimes swap in codec that impls ord():\n    if (random.nextInt(10) == 7) {\n      // Make sure terms index has ords:\n      Codec codec = _TestUtil.alwaysPostingsFormat(PostingsFormat.forName(\"Lucene40WithOrds\"));\n      conf.setCodec(codec);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random, dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<Integer>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      doc.add(new NumericField(\"id\", id));\n      \n      final int termCount = _TestUtil.nextInt(random, 0, 20*RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random.nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newField(\"field\", termsArray[ord].utf8ToString(), StringField.TYPE_UNSTORED);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n\n    for(IndexReader subR : r.getSequentialSubReaders()) {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: sub=\" + subR);\n      }\n      verify((AtomicReader) subR, idToOrds, termsArray, null);\n    }\n\n    // Also test top-level reader: its enum does not support\n    // ord, so this forces the OrdWrapper to run:\n    if (VERBOSE) {\n      System.out.println(\"TEST: top reader\");\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(r);\n    verify(slowR, idToOrds, termsArray, null);\n\n    FieldCache.DEFAULT.purge(slowR);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655":["79c2cb24929f2649a8875fb629086171f914d5ce","0762b640e0d0d12b6edb96db68986e13145c3484"],"ff6fd241dc6610f7f81b62e3ba4cedf105939623":["f2c5f0cb44df114db4228c8f77861714b5cabaea"],"0762b640e0d0d12b6edb96db68986e13145c3484":["ff6fd241dc6610f7f81b62e3ba4cedf105939623"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"962d04139994fce5193143ef35615499a9a96d78":["45669a651c970812a680841b97a77cce06af559f","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"2dd6ecb8250c497ed227653279d6a4f470bfbb31":["fa0f44f887719e97183771e977cfc4bfb485b766"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["2dd6ecb8250c497ed227653279d6a4f470bfbb31","da6d5ac19a80d65b1e864251f155d30960353b7e"],"7b91922b55d15444d554721b352861d028eb8278":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["95ae76773bf2b95987d5f9c8f566ab3738953fb4"],"da6d5ac19a80d65b1e864251f155d30960353b7e":["868186558eb3a854ce7e720a52bb445795d54910"],"79c2cb24929f2649a8875fb629086171f914d5ce":["a3776dccca01c11e7046323cfad46a3b4a471233","ff6fd241dc6610f7f81b62e3ba4cedf105939623"],"a3776dccca01c11e7046323cfad46a3b4a471233":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"77cf4379b2824f6ea34b091c495d6e95c38ff9e2":["135621f3a0670a9394eb563224a3b76cc4dddc0f","0762b640e0d0d12b6edb96db68986e13145c3484"],"fa0f44f887719e97183771e977cfc4bfb485b766":["7b91922b55d15444d554721b352861d028eb8278"],"386d1b0dcb065f1bfc494b1407cb41c536b95485":["2dd6ecb8250c497ed227653279d6a4f470bfbb31"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["0762b640e0d0d12b6edb96db68986e13145c3484"],"95ae76773bf2b95987d5f9c8f566ab3738953fb4":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"45669a651c970812a680841b97a77cce06af559f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","95ae76773bf2b95987d5f9c8f566ab3738953fb4"],"868186558eb3a854ce7e720a52bb445795d54910":["386d1b0dcb065f1bfc494b1407cb41c536b95485"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655":[],"ff6fd241dc6610f7f81b62e3ba4cedf105939623":["0762b640e0d0d12b6edb96db68986e13145c3484","79c2cb24929f2649a8875fb629086171f914d5ce"],"0762b640e0d0d12b6edb96db68986e13145c3484":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","77cf4379b2824f6ea34b091c495d6e95c38ff9e2","1509f151d7692d84fae414b2b799ac06ba60fcb4"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["77cf4379b2824f6ea34b091c495d6e95c38ff9e2"],"962d04139994fce5193143ef35615499a9a96d78":[],"5cab9a86bd67202d20b6adc463008c8e982b070a":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"2dd6ecb8250c497ed227653279d6a4f470bfbb31":["5cab9a86bd67202d20b6adc463008c8e982b070a","386d1b0dcb065f1bfc494b1407cb41c536b95485"],"7b91922b55d15444d554721b352861d028eb8278":["fa0f44f887719e97183771e977cfc4bfb485b766"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["ff6fd241dc6610f7f81b62e3ba4cedf105939623","135621f3a0670a9394eb563224a3b76cc4dddc0f","962d04139994fce5193143ef35615499a9a96d78","a3776dccca01c11e7046323cfad46a3b4a471233"],"79c2cb24929f2649a8875fb629086171f914d5ce":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655"],"da6d5ac19a80d65b1e864251f155d30960353b7e":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"a3776dccca01c11e7046323cfad46a3b4a471233":["79c2cb24929f2649a8875fb629086171f914d5ce"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["135621f3a0670a9394eb563224a3b76cc4dddc0f","a3776dccca01c11e7046323cfad46a3b4a471233","95ae76773bf2b95987d5f9c8f566ab3738953fb4","45669a651c970812a680841b97a77cce06af559f"],"77cf4379b2824f6ea34b091c495d6e95c38ff9e2":[],"fa0f44f887719e97183771e977cfc4bfb485b766":["2dd6ecb8250c497ed227653279d6a4f470bfbb31"],"386d1b0dcb065f1bfc494b1407cb41c536b95485":["868186558eb3a854ce7e720a52bb445795d54910"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["7b91922b55d15444d554721b352861d028eb8278"],"95ae76773bf2b95987d5f9c8f566ab3738953fb4":["f2c5f0cb44df114db4228c8f77861714b5cabaea","45669a651c970812a680841b97a77cce06af559f"],"45669a651c970812a680841b97a77cce06af559f":["962d04139994fce5193143ef35615499a9a96d78"],"868186558eb3a854ce7e720a52bb445795d54910":["da6d5ac19a80d65b1e864251f155d30960353b7e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","962d04139994fce5193143ef35615499a9a96d78","77cf4379b2824f6ea34b091c495d6e95c38ff9e2","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}