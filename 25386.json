{"path":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter#copyVectorsNoDeletions(MergeState,Lucene40TermVectorsReader,AtomicReader,int[],int[]).mjava","commits":[{"id":"c95a819869502635864dac0a788f874787e3395b","date":1341394787,"type":1,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter#copyVectorsNoDeletions(MergeState,Lucene40TermVectorsReader,AtomicReader,int[],int[]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter#copyVectorsNoDeletions(MergeState,Lucene40TermVectorsReader,MergeState.IndexReaderAndLiveDocs,int[],int[]).mjava","sourceNew":"  private int copyVectorsNoDeletions(MergeState mergeState,\n                                      final Lucene40TermVectorsReader matchingVectorsReader,\n                                      final AtomicReader reader,\n                                      int rawDocLengths[],\n                                      int rawDocLengths2[])\n          throws IOException {\n    final int maxDoc = reader.maxDoc();\n    if (matchingVectorsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      int docCount = 0;\n      while (docCount < maxDoc) {\n        int len = Math.min(MAX_RAW_MERGE_DOCS, maxDoc - docCount);\n        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, docCount, len);\n        addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, len);\n        docCount += len;\n        mergeState.checkAbort.work(300 * len);\n      }\n    } else {\n      for (int docNum = 0; docNum < maxDoc; docNum++) {\n        // NOTE: it's very important to first assign to vectors then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Fields vectors = reader.getTermVectors(docNum);\n        addAllDocVectors(vectors, mergeState.fieldInfos);\n        mergeState.checkAbort.work(300);\n      }\n    }\n    return maxDoc;\n  }\n\n","sourceOld":"  private int copyVectorsNoDeletions(MergeState mergeState,\n                                      final Lucene40TermVectorsReader matchingVectorsReader,\n                                      final MergeState.IndexReaderAndLiveDocs reader,\n                                      int rawDocLengths[],\n                                      int rawDocLengths2[])\n          throws IOException {\n    final int maxDoc = reader.reader.maxDoc();\n    if (matchingVectorsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      int docCount = 0;\n      while (docCount < maxDoc) {\n        int len = Math.min(MAX_RAW_MERGE_DOCS, maxDoc - docCount);\n        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, docCount, len);\n        addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, len);\n        docCount += len;\n        mergeState.checkAbort.work(300 * len);\n      }\n    } else {\n      for (int docNum = 0; docNum < maxDoc; docNum++) {\n        // NOTE: it's very important to first assign to vectors then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Fields vectors = reader.reader.getTermVectors(docNum);\n        addAllDocVectors(vectors, mergeState.fieldInfos);\n        mergeState.checkAbort.work(300);\n      }\n    }\n    return maxDoc;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe33227f6805edab2036cbb80645cc4e2d1fa424","date":1342713534,"type":0,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter#copyVectorsNoDeletions(MergeState,Lucene40TermVectorsReader,AtomicReader,int[],int[]).mjava","pathOld":"/dev/null","sourceNew":"  private int copyVectorsNoDeletions(MergeState mergeState,\n                                      final Lucene40TermVectorsReader matchingVectorsReader,\n                                      final AtomicReader reader,\n                                      int rawDocLengths[],\n                                      int rawDocLengths2[])\n          throws IOException {\n    final int maxDoc = reader.maxDoc();\n    if (matchingVectorsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      int docCount = 0;\n      while (docCount < maxDoc) {\n        int len = Math.min(MAX_RAW_MERGE_DOCS, maxDoc - docCount);\n        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, docCount, len);\n        addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, len);\n        docCount += len;\n        mergeState.checkAbort.work(300 * len);\n      }\n    } else {\n      for (int docNum = 0; docNum < maxDoc; docNum++) {\n        // NOTE: it's very important to first assign to vectors then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Fields vectors = reader.getTermVectors(docNum);\n        addAllDocVectors(vectors, mergeState.fieldInfos);\n        mergeState.checkAbort.work(300);\n      }\n    }\n    return maxDoc;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0935c850ea562932997b72c69d93e345f21d7f45","date":1344711506,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter#copyVectorsNoDeletions(MergeState,Lucene40TermVectorsReader,AtomicReader,int[],int[]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter#copyVectorsNoDeletions(MergeState,Lucene40TermVectorsReader,AtomicReader,int[],int[]).mjava","sourceNew":"  private int copyVectorsNoDeletions(MergeState mergeState,\n                                      final Lucene40TermVectorsReader matchingVectorsReader,\n                                      final AtomicReader reader,\n                                      int rawDocLengths[],\n                                      int rawDocLengths2[])\n          throws IOException {\n    final int maxDoc = reader.maxDoc();\n    if (matchingVectorsReader != null && mergeState.currentReaderPayloadProcessor == null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\" and there is no payload processor\n      int docCount = 0;\n      while (docCount < maxDoc) {\n        int len = Math.min(MAX_RAW_MERGE_DOCS, maxDoc - docCount);\n        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, docCount, len);\n        addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, len);\n        docCount += len;\n        mergeState.checkAbort.work(300 * len);\n      }\n    } else {\n      for (int docNum = 0; docNum < maxDoc; docNum++) {\n        // NOTE: it's very important to first assign to vectors then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Fields vectors = reader.getTermVectors(docNum);\n        addAllDocVectors(vectors, mergeState);\n        mergeState.checkAbort.work(300);\n      }\n    }\n    return maxDoc;\n  }\n\n","sourceOld":"  private int copyVectorsNoDeletions(MergeState mergeState,\n                                      final Lucene40TermVectorsReader matchingVectorsReader,\n                                      final AtomicReader reader,\n                                      int rawDocLengths[],\n                                      int rawDocLengths2[])\n          throws IOException {\n    final int maxDoc = reader.maxDoc();\n    if (matchingVectorsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      int docCount = 0;\n      while (docCount < maxDoc) {\n        int len = Math.min(MAX_RAW_MERGE_DOCS, maxDoc - docCount);\n        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, docCount, len);\n        addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, len);\n        docCount += len;\n        mergeState.checkAbort.work(300 * len);\n      }\n    } else {\n      for (int docNum = 0; docNum < maxDoc; docNum++) {\n        // NOTE: it's very important to first assign to vectors then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Fields vectors = reader.getTermVectors(docNum);\n        addAllDocVectors(vectors, mergeState.fieldInfos);\n        mergeState.checkAbort.work(300);\n      }\n    }\n    return maxDoc;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c7869f64c874ebf7f317d22c00baf2b6857797a6","date":1344856617,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter#copyVectorsNoDeletions(MergeState,Lucene40TermVectorsReader,AtomicReader,int[],int[]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter#copyVectorsNoDeletions(MergeState,Lucene40TermVectorsReader,AtomicReader,int[],int[]).mjava","sourceNew":"  private int copyVectorsNoDeletions(MergeState mergeState,\n                                      final Lucene40TermVectorsReader matchingVectorsReader,\n                                      final AtomicReader reader,\n                                      int rawDocLengths[],\n                                      int rawDocLengths2[])\n          throws IOException {\n    final int maxDoc = reader.maxDoc();\n    if (matchingVectorsReader != null && mergeState.currentReaderPayloadProcessor == null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\" and there is no payload processor\n      int docCount = 0;\n      while (docCount < maxDoc) {\n        int len = Math.min(MAX_RAW_MERGE_DOCS, maxDoc - docCount);\n        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, docCount, len);\n        addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, len);\n        docCount += len;\n        mergeState.checkAbort.work(300 * len);\n      }\n    } else {\n      for (int docNum = 0; docNum < maxDoc; docNum++) {\n        // NOTE: it's very important to first assign to vectors then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Fields vectors = reader.getTermVectors(docNum);\n        addAllDocVectors(vectors, mergeState);\n        mergeState.checkAbort.work(300);\n      }\n    }\n    return maxDoc;\n  }\n\n","sourceOld":"  private int copyVectorsNoDeletions(MergeState mergeState,\n                                      final Lucene40TermVectorsReader matchingVectorsReader,\n                                      final AtomicReader reader,\n                                      int rawDocLengths[],\n                                      int rawDocLengths2[])\n          throws IOException {\n    final int maxDoc = reader.maxDoc();\n    if (matchingVectorsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      int docCount = 0;\n      while (docCount < maxDoc) {\n        int len = Math.min(MAX_RAW_MERGE_DOCS, maxDoc - docCount);\n        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, docCount, len);\n        addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, len);\n        docCount += len;\n        mergeState.checkAbort.work(300 * len);\n      }\n    } else {\n      for (int docNum = 0; docNum < maxDoc; docNum++) {\n        // NOTE: it's very important to first assign to vectors then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Fields vectors = reader.getTermVectors(docNum);\n        addAllDocVectors(vectors, mergeState.fieldInfos);\n        mergeState.checkAbort.work(300);\n      }\n    }\n    return maxDoc;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9","date":1344867506,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter#copyVectorsNoDeletions(MergeState,Lucene40TermVectorsReader,AtomicReader,int[],int[]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter#copyVectorsNoDeletions(MergeState,Lucene40TermVectorsReader,AtomicReader,int[],int[]).mjava","sourceNew":"  private int copyVectorsNoDeletions(MergeState mergeState,\n                                      final Lucene40TermVectorsReader matchingVectorsReader,\n                                      final AtomicReader reader,\n                                      int rawDocLengths[],\n                                      int rawDocLengths2[])\n          throws IOException {\n    final int maxDoc = reader.maxDoc();\n    if (matchingVectorsReader != null && mergeState.currentReaderPayloadProcessor == null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\" and there is no payload processor\n      int docCount = 0;\n      while (docCount < maxDoc) {\n        int len = Math.min(MAX_RAW_MERGE_DOCS, maxDoc - docCount);\n        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, docCount, len);\n        addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, len);\n        docCount += len;\n        mergeState.checkAbort.work(300 * len);\n      }\n    } else {\n      for (int docNum = 0; docNum < maxDoc; docNum++) {\n        // NOTE: it's very important to first assign to vectors then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Fields vectors = reader.getTermVectors(docNum);\n        addAllDocVectors(vectors, mergeState);\n        mergeState.checkAbort.work(300);\n      }\n    }\n    return maxDoc;\n  }\n\n","sourceOld":"  private int copyVectorsNoDeletions(MergeState mergeState,\n                                      final Lucene40TermVectorsReader matchingVectorsReader,\n                                      final AtomicReader reader,\n                                      int rawDocLengths[],\n                                      int rawDocLengths2[])\n          throws IOException {\n    final int maxDoc = reader.maxDoc();\n    if (matchingVectorsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      int docCount = 0;\n      while (docCount < maxDoc) {\n        int len = Math.min(MAX_RAW_MERGE_DOCS, maxDoc - docCount);\n        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, docCount, len);\n        addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, len);\n        docCount += len;\n        mergeState.checkAbort.work(300 * len);\n      }\n    } else {\n      for (int docNum = 0; docNum < maxDoc; docNum++) {\n        // NOTE: it's very important to first assign to vectors then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Fields vectors = reader.getTermVectors(docNum);\n        addAllDocVectors(vectors, mergeState.fieldInfos);\n        mergeState.checkAbort.work(300);\n      }\n    }\n    return maxDoc;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bc124b3b129ef11a255212f3af482b771c5b3a6c","date":1344947616,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter#copyVectorsNoDeletions(MergeState,Lucene40TermVectorsReader,AtomicReader,int[],int[]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter#copyVectorsNoDeletions(MergeState,Lucene40TermVectorsReader,AtomicReader,int[],int[]).mjava","sourceNew":"  private int copyVectorsNoDeletions(MergeState mergeState,\n                                      final Lucene40TermVectorsReader matchingVectorsReader,\n                                      final AtomicReader reader,\n                                      int rawDocLengths[],\n                                      int rawDocLengths2[])\n          throws IOException {\n    final int maxDoc = reader.maxDoc();\n    if (matchingVectorsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      int docCount = 0;\n      while (docCount < maxDoc) {\n        int len = Math.min(MAX_RAW_MERGE_DOCS, maxDoc - docCount);\n        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, docCount, len);\n        addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, len);\n        docCount += len;\n        mergeState.checkAbort.work(300 * len);\n      }\n    } else {\n      for (int docNum = 0; docNum < maxDoc; docNum++) {\n        // NOTE: it's very important to first assign to vectors then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Fields vectors = reader.getTermVectors(docNum);\n        addAllDocVectors(vectors, mergeState);\n        mergeState.checkAbort.work(300);\n      }\n    }\n    return maxDoc;\n  }\n\n","sourceOld":"  private int copyVectorsNoDeletions(MergeState mergeState,\n                                      final Lucene40TermVectorsReader matchingVectorsReader,\n                                      final AtomicReader reader,\n                                      int rawDocLengths[],\n                                      int rawDocLengths2[])\n          throws IOException {\n    final int maxDoc = reader.maxDoc();\n    if (matchingVectorsReader != null && mergeState.currentReaderPayloadProcessor == null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\" and there is no payload processor\n      int docCount = 0;\n      while (docCount < maxDoc) {\n        int len = Math.min(MAX_RAW_MERGE_DOCS, maxDoc - docCount);\n        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, docCount, len);\n        addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, len);\n        docCount += len;\n        mergeState.checkAbort.work(300 * len);\n      }\n    } else {\n      for (int docNum = 0; docNum < maxDoc; docNum++) {\n        // NOTE: it's very important to first assign to vectors then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Fields vectors = reader.getTermVectors(docNum);\n        addAllDocVectors(vectors, mergeState);\n        mergeState.checkAbort.work(300);\n      }\n    }\n    return maxDoc;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198","date":1345029782,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter#copyVectorsNoDeletions(MergeState,Lucene40TermVectorsReader,AtomicReader,int[],int[]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter#copyVectorsNoDeletions(MergeState,Lucene40TermVectorsReader,AtomicReader,int[],int[]).mjava","sourceNew":"  private int copyVectorsNoDeletions(MergeState mergeState,\n                                      final Lucene40TermVectorsReader matchingVectorsReader,\n                                      final AtomicReader reader,\n                                      int rawDocLengths[],\n                                      int rawDocLengths2[])\n          throws IOException {\n    final int maxDoc = reader.maxDoc();\n    if (matchingVectorsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      int docCount = 0;\n      while (docCount < maxDoc) {\n        int len = Math.min(MAX_RAW_MERGE_DOCS, maxDoc - docCount);\n        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, docCount, len);\n        addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, len);\n        docCount += len;\n        mergeState.checkAbort.work(300 * len);\n      }\n    } else {\n      for (int docNum = 0; docNum < maxDoc; docNum++) {\n        // NOTE: it's very important to first assign to vectors then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Fields vectors = reader.getTermVectors(docNum);\n        addAllDocVectors(vectors, mergeState);\n        mergeState.checkAbort.work(300);\n      }\n    }\n    return maxDoc;\n  }\n\n","sourceOld":"  private int copyVectorsNoDeletions(MergeState mergeState,\n                                      final Lucene40TermVectorsReader matchingVectorsReader,\n                                      final AtomicReader reader,\n                                      int rawDocLengths[],\n                                      int rawDocLengths2[])\n          throws IOException {\n    final int maxDoc = reader.maxDoc();\n    if (matchingVectorsReader != null && mergeState.currentReaderPayloadProcessor == null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\" and there is no payload processor\n      int docCount = 0;\n      while (docCount < maxDoc) {\n        int len = Math.min(MAX_RAW_MERGE_DOCS, maxDoc - docCount);\n        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, docCount, len);\n        addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, len);\n        docCount += len;\n        mergeState.checkAbort.work(300 * len);\n      }\n    } else {\n      for (int docNum = 0; docNum < maxDoc; docNum++) {\n        // NOTE: it's very important to first assign to vectors then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Fields vectors = reader.getTermVectors(docNum);\n        addAllDocVectors(vectors, mergeState);\n        mergeState.checkAbort.work(300);\n      }\n    }\n    return maxDoc;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3c188105a9aae04f56c24996f98f8333fc825d2e","date":1345031914,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter#copyVectorsNoDeletions(MergeState,Lucene40TermVectorsReader,AtomicReader,int[],int[]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter#copyVectorsNoDeletions(MergeState,Lucene40TermVectorsReader,AtomicReader,int[],int[]).mjava","sourceNew":"  private int copyVectorsNoDeletions(MergeState mergeState,\n                                      final Lucene40TermVectorsReader matchingVectorsReader,\n                                      final AtomicReader reader,\n                                      int rawDocLengths[],\n                                      int rawDocLengths2[])\n          throws IOException {\n    final int maxDoc = reader.maxDoc();\n    if (matchingVectorsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      int docCount = 0;\n      while (docCount < maxDoc) {\n        int len = Math.min(MAX_RAW_MERGE_DOCS, maxDoc - docCount);\n        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, docCount, len);\n        addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, len);\n        docCount += len;\n        mergeState.checkAbort.work(300 * len);\n      }\n    } else {\n      for (int docNum = 0; docNum < maxDoc; docNum++) {\n        // NOTE: it's very important to first assign to vectors then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Fields vectors = reader.getTermVectors(docNum);\n        addAllDocVectors(vectors, mergeState);\n        mergeState.checkAbort.work(300);\n      }\n    }\n    return maxDoc;\n  }\n\n","sourceOld":"  private int copyVectorsNoDeletions(MergeState mergeState,\n                                      final Lucene40TermVectorsReader matchingVectorsReader,\n                                      final AtomicReader reader,\n                                      int rawDocLengths[],\n                                      int rawDocLengths2[])\n          throws IOException {\n    final int maxDoc = reader.maxDoc();\n    if (matchingVectorsReader != null && mergeState.currentReaderPayloadProcessor == null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\" and there is no payload processor\n      int docCount = 0;\n      while (docCount < maxDoc) {\n        int len = Math.min(MAX_RAW_MERGE_DOCS, maxDoc - docCount);\n        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, docCount, len);\n        addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, len);\n        docCount += len;\n        mergeState.checkAbort.work(300 * len);\n      }\n    } else {\n      for (int docNum = 0; docNum < maxDoc; docNum++) {\n        // NOTE: it's very important to first assign to vectors then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Fields vectors = reader.getTermVectors(docNum);\n        addAllDocVectors(vectors, mergeState);\n        mergeState.checkAbort.work(300);\n      }\n    }\n    return maxDoc;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1c93396a1df03720cb20e2c2f513a6fa59b21e4c","date":1345032673,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter#copyVectorsNoDeletions(MergeState,Lucene40TermVectorsReader,AtomicReader,int[],int[]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter#copyVectorsNoDeletions(MergeState,Lucene40TermVectorsReader,AtomicReader,int[],int[]).mjava","sourceNew":"  private int copyVectorsNoDeletions(MergeState mergeState,\n                                      final Lucene40TermVectorsReader matchingVectorsReader,\n                                      final AtomicReader reader,\n                                      int rawDocLengths[],\n                                      int rawDocLengths2[])\n          throws IOException {\n    final int maxDoc = reader.maxDoc();\n    if (matchingVectorsReader != null && mergeState.currentReaderPayloadProcessor == null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\" and there is no payload processor\n      int docCount = 0;\n      while (docCount < maxDoc) {\n        int len = Math.min(MAX_RAW_MERGE_DOCS, maxDoc - docCount);\n        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, docCount, len);\n        addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, len);\n        docCount += len;\n        mergeState.checkAbort.work(300 * len);\n      }\n    } else {\n      for (int docNum = 0; docNum < maxDoc; docNum++) {\n        // NOTE: it's very important to first assign to vectors then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Fields vectors = reader.getTermVectors(docNum);\n        addAllDocVectors(vectors, mergeState);\n        mergeState.checkAbort.work(300);\n      }\n    }\n    return maxDoc;\n  }\n\n","sourceOld":"  private int copyVectorsNoDeletions(MergeState mergeState,\n                                      final Lucene40TermVectorsReader matchingVectorsReader,\n                                      final AtomicReader reader,\n                                      int rawDocLengths[],\n                                      int rawDocLengths2[])\n          throws IOException {\n    final int maxDoc = reader.maxDoc();\n    if (matchingVectorsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      int docCount = 0;\n      while (docCount < maxDoc) {\n        int len = Math.min(MAX_RAW_MERGE_DOCS, maxDoc - docCount);\n        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, docCount, len);\n        addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, len);\n        docCount += len;\n        mergeState.checkAbort.work(300 * len);\n      }\n    } else {\n      for (int docNum = 0; docNum < maxDoc; docNum++) {\n        // NOTE: it's very important to first assign to vectors then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Fields vectors = reader.getTermVectors(docNum);\n        addAllDocVectors(vectors, mergeState);\n        mergeState.checkAbort.work(300);\n      }\n    }\n    return maxDoc;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b05c56a41b733e02a189c48895922b5bd8c7f3d1","date":1345033322,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter#copyVectorsNoDeletions(MergeState,Lucene40TermVectorsReader,AtomicReader,int[],int[]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter#copyVectorsNoDeletions(MergeState,Lucene40TermVectorsReader,AtomicReader,int[],int[]).mjava","sourceNew":"  private int copyVectorsNoDeletions(MergeState mergeState,\n                                      final Lucene40TermVectorsReader matchingVectorsReader,\n                                      final AtomicReader reader,\n                                      int rawDocLengths[],\n                                      int rawDocLengths2[])\n          throws IOException {\n    final int maxDoc = reader.maxDoc();\n    if (matchingVectorsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      int docCount = 0;\n      while (docCount < maxDoc) {\n        int len = Math.min(MAX_RAW_MERGE_DOCS, maxDoc - docCount);\n        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, docCount, len);\n        addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, len);\n        docCount += len;\n        mergeState.checkAbort.work(300 * len);\n      }\n    } else {\n      for (int docNum = 0; docNum < maxDoc; docNum++) {\n        // NOTE: it's very important to first assign to vectors then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Fields vectors = reader.getTermVectors(docNum);\n        addAllDocVectors(vectors, mergeState);\n        mergeState.checkAbort.work(300);\n      }\n    }\n    return maxDoc;\n  }\n\n","sourceOld":"  private int copyVectorsNoDeletions(MergeState mergeState,\n                                      final Lucene40TermVectorsReader matchingVectorsReader,\n                                      final AtomicReader reader,\n                                      int rawDocLengths[],\n                                      int rawDocLengths2[])\n          throws IOException {\n    final int maxDoc = reader.maxDoc();\n    if (matchingVectorsReader != null && mergeState.currentReaderPayloadProcessor == null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\" and there is no payload processor\n      int docCount = 0;\n      while (docCount < maxDoc) {\n        int len = Math.min(MAX_RAW_MERGE_DOCS, maxDoc - docCount);\n        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, docCount, len);\n        addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, len);\n        docCount += len;\n        mergeState.checkAbort.work(300 * len);\n      }\n    } else {\n      for (int docNum = 0; docNum < maxDoc; docNum++) {\n        // NOTE: it's very important to first assign to vectors then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Fields vectors = reader.getTermVectors(docNum);\n        addAllDocVectors(vectors, mergeState);\n        mergeState.checkAbort.work(300);\n      }\n    }\n    return maxDoc;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"22a2e66dfda83847e80095b8693c660742ab3e9c","date":1408628796,"type":4,"author":"Robert Muir","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter#copyVectorsNoDeletions(MergeState,Lucene40TermVectorsReader,AtomicReader,int[],int[]).mjava","sourceNew":null,"sourceOld":"  private int copyVectorsNoDeletions(MergeState mergeState,\n                                      final Lucene40TermVectorsReader matchingVectorsReader,\n                                      final AtomicReader reader,\n                                      int rawDocLengths[],\n                                      int rawDocLengths2[])\n          throws IOException {\n    final int maxDoc = reader.maxDoc();\n    if (matchingVectorsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      int docCount = 0;\n      while (docCount < maxDoc) {\n        int len = Math.min(MAX_RAW_MERGE_DOCS, maxDoc - docCount);\n        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, docCount, len);\n        addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, len);\n        docCount += len;\n        mergeState.checkAbort.work(300 * len);\n      }\n    } else {\n      for (int docNum = 0; docNum < maxDoc; docNum++) {\n        // NOTE: it's very important to first assign to vectors then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Fields vectors = reader.getTermVectors(docNum);\n        addAllDocVectors(vectors, mergeState);\n        mergeState.checkAbort.work(300);\n      }\n    }\n    return maxDoc;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"c95a819869502635864dac0a788f874787e3395b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"3c188105a9aae04f56c24996f98f8333fc825d2e":["c7869f64c874ebf7f317d22c00baf2b6857797a6"],"b05c56a41b733e02a189c48895922b5bd8c7f3d1":["1c93396a1df03720cb20e2c2f513a6fa59b21e4c","bc124b3b129ef11a255212f3af482b771c5b3a6c"],"1c93396a1df03720cb20e2c2f513a6fa59b21e4c":["3c188105a9aae04f56c24996f98f8333fc825d2e"],"c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198":["d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9","bc124b3b129ef11a255212f3af482b771c5b3a6c"],"c7869f64c874ebf7f317d22c00baf2b6857797a6":["fe33227f6805edab2036cbb80645cc4e2d1fa424","0935c850ea562932997b72c69d93e345f21d7f45"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","c95a819869502635864dac0a788f874787e3395b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"22a2e66dfda83847e80095b8693c660742ab3e9c":["bc124b3b129ef11a255212f3af482b771c5b3a6c"],"bc124b3b129ef11a255212f3af482b771c5b3a6c":["0935c850ea562932997b72c69d93e345f21d7f45"],"0935c850ea562932997b72c69d93e345f21d7f45":["c95a819869502635864dac0a788f874787e3395b"],"d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9":["c95a819869502635864dac0a788f874787e3395b","0935c850ea562932997b72c69d93e345f21d7f45"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["22a2e66dfda83847e80095b8693c660742ab3e9c"]},"commit2Childs":{"c95a819869502635864dac0a788f874787e3395b":["fe33227f6805edab2036cbb80645cc4e2d1fa424","0935c850ea562932997b72c69d93e345f21d7f45","d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9"],"3c188105a9aae04f56c24996f98f8333fc825d2e":["1c93396a1df03720cb20e2c2f513a6fa59b21e4c"],"b05c56a41b733e02a189c48895922b5bd8c7f3d1":[],"1c93396a1df03720cb20e2c2f513a6fa59b21e4c":["b05c56a41b733e02a189c48895922b5bd8c7f3d1"],"c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198":[],"c7869f64c874ebf7f317d22c00baf2b6857797a6":["3c188105a9aae04f56c24996f98f8333fc825d2e"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":["c7869f64c874ebf7f317d22c00baf2b6857797a6"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c95a819869502635864dac0a788f874787e3395b","fe33227f6805edab2036cbb80645cc4e2d1fa424"],"22a2e66dfda83847e80095b8693c660742ab3e9c":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"bc124b3b129ef11a255212f3af482b771c5b3a6c":["b05c56a41b733e02a189c48895922b5bd8c7f3d1","c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198","22a2e66dfda83847e80095b8693c660742ab3e9c"],"0935c850ea562932997b72c69d93e345f21d7f45":["c7869f64c874ebf7f317d22c00baf2b6857797a6","bc124b3b129ef11a255212f3af482b771c5b3a6c","d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9"],"d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9":["c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["b05c56a41b733e02a189c48895922b5bd8c7f3d1","c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}