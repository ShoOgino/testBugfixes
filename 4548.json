{"path":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState,Lucene50PostingsFormat.FSTLoadMode).mjava","commits":[{"id":"938935e3efe6aaecb925448d7f992783247366de","date":1554389977,"type":1,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState,Lucene50PostingsFormat.FSTLoadMode).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state, Lucene50PostingsFormat.FSTLoadMode fstLoadMode) throws IOException {\n    boolean success = false;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // pre-6.2 index, records whether auto-prefix terms are enabled in the header\n        byte b = termsIn.readByte();\n        if (b != 0) {\n          throw new CorruptIndexException(\"Index header pretends the index has auto-prefix terms: \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn);\n      seekDir(indexIn);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final BytesRef rootCode = readBytesRef(termsIn);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = termsIn.readVLong();\n        // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm, state.openedFromWriter, fstLoadMode));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // pre-6.2 index, records whether auto-prefix terms are enabled in the header\n        byte b = termsIn.readByte();\n        if (b != 0) {\n          throw new CorruptIndexException(\"Index header pretends the index has auto-prefix terms: \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn);\n      seekDir(indexIn);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final BytesRef rootCode = readBytesRef(termsIn);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = termsIn.readVLong();\n        // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm, state.openedFromWriter));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"763da4a9605e47013078edc323b9d4b608f0f9e0","date":1555353576,"type":5,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState,FSTLoadMode).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState,Lucene50PostingsFormat.FSTLoadMode).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state, FSTLoadMode defaultLoadMode) throws IOException {\n    boolean success = false;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // pre-6.2 index, records whether auto-prefix terms are enabled in the header\n        byte b = termsIn.readByte();\n        if (b != 0) {\n          throw new CorruptIndexException(\"Index header pretends the index has auto-prefix terms: \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn);\n      seekDir(indexIn);\n\n      final FSTLoadMode fstLoadMode = getLoadMode(state.readerAttributes, FST_MODE_KEY, defaultLoadMode);\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final BytesRef rootCode = readBytesRef(termsIn);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = termsIn.readVLong();\n        // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final FSTLoadMode perFieldLoadMode = getLoadMode(state.readerAttributes, FST_MODE_KEY + \".\" + fieldInfo.name, fstLoadMode);\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,\n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm, state.openedFromWriter, perFieldLoadMode));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state, Lucene50PostingsFormat.FSTLoadMode fstLoadMode) throws IOException {\n    boolean success = false;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // pre-6.2 index, records whether auto-prefix terms are enabled in the header\n        byte b = termsIn.readByte();\n        if (b != 0) {\n          throw new CorruptIndexException(\"Index header pretends the index has auto-prefix terms: \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn);\n      seekDir(indexIn);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final BytesRef rootCode = readBytesRef(termsIn);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = termsIn.readVLong();\n        // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm, state.openedFromWriter, fstLoadMode));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"763da4a9605e47013078edc323b9d4b608f0f9e0":["938935e3efe6aaecb925448d7f992783247366de"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"938935e3efe6aaecb925448d7f992783247366de":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["763da4a9605e47013078edc323b9d4b608f0f9e0"]},"commit2Childs":{"763da4a9605e47013078edc323b9d4b608f0f9e0":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["938935e3efe6aaecb925448d7f992783247366de"],"938935e3efe6aaecb925448d7f992783247366de":["763da4a9605e47013078edc323b9d4b608f0f9e0"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}