{"path":"solr/core/src/java/org/apache/solr/filestore/DistribPackageStore#distribute(FileInfo).mjava","commits":[{"id":"d218decf811b7a0a4d86218c54c79c74a962374b","date":1578632144,"type":0,"author":"Ishan Chattopadhyaya","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/filestore/DistribPackageStore#distribute(FileInfo).mjava","pathOld":"/dev/null","sourceNew":"  private void distribute(FileInfo info) {\n    try {\n      String dirName = info.path.substring(0, info.path.lastIndexOf('/'));\n      coreContainer.getZkController().getZkClient().makePath(ZK_PACKAGESTORE + dirName, false, true);\n      coreContainer.getZkController().getZkClient().create(ZK_PACKAGESTORE + info.path, info.getDetails().getMetaData().sha512.getBytes(UTF_8),\n          CreateMode.PERSISTENT, true);\n    } catch (Exception e) {\n      throw new SolrException(SERVER_ERROR, \"Unable to create an entry in ZK\", e);\n    }\n    tmpFiles.put(info.path, info);\n\n    List<String> nodes = coreContainer.getPackageStoreAPI().shuffledNodes();\n    int i = 0;\n    int FETCHFROM_SRC = 50;\n    String myNodeName = coreContainer.getZkController().getNodeName();\n    try {\n      for (String node : nodes) {\n        String baseUrl = coreContainer.getZkController().getZkStateReader().getBaseUrlForNodeName(node);\n        String url = baseUrl.replace(\"/solr\", \"/api\") + \"/node/files\" + info.path + \"?getFrom=\";\n        if (i < FETCHFROM_SRC) {\n          // this is to protect very large clusters from overwhelming a single node\n          // the first FETCHFROM_SRC nodes will be asked to fetch from this node.\n          // it's there in  the memory now. So , it must be served fast\n          url += myNodeName;\n        } else {\n          if (i == FETCHFROM_SRC) {\n            // This is just an optimization\n            // at this point a bunch of nodes are already downloading from me\n            // I'll wait for them to finish before asking other nodes to download from each other\n            try {\n              Thread.sleep(2 * 1000);\n            } catch (Exception e) {\n            }\n          }\n          // trying to avoid the thundering herd problem when there are a very large no:of nodes\n          // others should try to fetch it from any node where it is available. By now,\n          // almost FETCHFROM_SRC other nodes may have it\n          url += \"*\";\n        }\n        try {\n          //fire and forget\n          Utils.executeGET(coreContainer.getUpdateShardHandler().getDefaultHttpClient(), url, null);\n        } catch (Exception e) {\n          log.info(\"Node: \" + node +\n              \" failed to respond for file fetch notification\", e);\n          //ignore the exception\n          // some nodes may be down or not responding\n        }\n        i++;\n      }\n    } finally {\n      coreContainer.getUpdateShardHandler().getUpdateExecutor().submit(() -> {\n        try {\n          Thread.sleep(10 * 1000);\n        } finally {\n          tmpFiles.remove(info.path);\n        }\n        return null;\n      });\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b8f0a7504661c8e51be5c63e87f9d79a36d9116c","date":1578657638,"type":0,"author":"Dawid Weiss","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/filestore/DistribPackageStore#distribute(FileInfo).mjava","pathOld":"/dev/null","sourceNew":"  private void distribute(FileInfo info) {\n    try {\n      String dirName = info.path.substring(0, info.path.lastIndexOf('/'));\n      coreContainer.getZkController().getZkClient().makePath(ZK_PACKAGESTORE + dirName, false, true);\n      coreContainer.getZkController().getZkClient().create(ZK_PACKAGESTORE + info.path, info.getDetails().getMetaData().sha512.getBytes(UTF_8),\n          CreateMode.PERSISTENT, true);\n    } catch (Exception e) {\n      throw new SolrException(SERVER_ERROR, \"Unable to create an entry in ZK\", e);\n    }\n    tmpFiles.put(info.path, info);\n\n    List<String> nodes = coreContainer.getPackageStoreAPI().shuffledNodes();\n    int i = 0;\n    int FETCHFROM_SRC = 50;\n    String myNodeName = coreContainer.getZkController().getNodeName();\n    try {\n      for (String node : nodes) {\n        String baseUrl = coreContainer.getZkController().getZkStateReader().getBaseUrlForNodeName(node);\n        String url = baseUrl.replace(\"/solr\", \"/api\") + \"/node/files\" + info.path + \"?getFrom=\";\n        if (i < FETCHFROM_SRC) {\n          // this is to protect very large clusters from overwhelming a single node\n          // the first FETCHFROM_SRC nodes will be asked to fetch from this node.\n          // it's there in  the memory now. So , it must be served fast\n          url += myNodeName;\n        } else {\n          if (i == FETCHFROM_SRC) {\n            // This is just an optimization\n            // at this point a bunch of nodes are already downloading from me\n            // I'll wait for them to finish before asking other nodes to download from each other\n            try {\n              Thread.sleep(2 * 1000);\n            } catch (Exception e) {\n            }\n          }\n          // trying to avoid the thundering herd problem when there are a very large no:of nodes\n          // others should try to fetch it from any node where it is available. By now,\n          // almost FETCHFROM_SRC other nodes may have it\n          url += \"*\";\n        }\n        try {\n          //fire and forget\n          Utils.executeGET(coreContainer.getUpdateShardHandler().getDefaultHttpClient(), url, null);\n        } catch (Exception e) {\n          log.info(\"Node: \" + node +\n              \" failed to respond for file fetch notification\", e);\n          //ignore the exception\n          // some nodes may be down or not responding\n        }\n        i++;\n      }\n    } finally {\n      coreContainer.getUpdateShardHandler().getUpdateExecutor().submit(() -> {\n        try {\n          Thread.sleep(10 * 1000);\n        } finally {\n          tmpFiles.remove(info.path);\n        }\n        return null;\n      });\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"140be51d03394488536f4aacedace29f9b318347","date":1587170432,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/filestore/DistribPackageStore#distribute(FileInfo).mjava","pathOld":"solr/core/src/java/org/apache/solr/filestore/DistribPackageStore#distribute(FileInfo).mjava","sourceNew":"  private void distribute(FileInfo info) {\n    try {\n      String dirName = info.path.substring(0, info.path.lastIndexOf('/'));\n      coreContainer.getZkController().getZkClient().makePath(ZK_PACKAGESTORE + dirName, false, true);\n      coreContainer.getZkController().getZkClient().create(ZK_PACKAGESTORE + info.path, info.getDetails().getMetaData().sha512.getBytes(UTF_8),\n          CreateMode.PERSISTENT, true);\n    } catch (Exception e) {\n      throw new SolrException(SERVER_ERROR, \"Unable to create an entry in ZK\", e);\n    }\n    tmpFiles.put(info.path, info);\n\n    List<String> nodes = coreContainer.getPackageStoreAPI().shuffledNodes();\n    int i = 0;\n    int FETCHFROM_SRC = 50;\n    String myNodeName = coreContainer.getZkController().getNodeName();\n    try {\n      for (String node : nodes) {\n        String baseUrl = coreContainer.getZkController().getZkStateReader().getBaseUrlForNodeName(node);\n        String url = baseUrl.replace(\"/solr\", \"/api\") + \"/node/files\" + info.path + \"?getFrom=\";\n        if (i < FETCHFROM_SRC) {\n          // this is to protect very large clusters from overwhelming a single node\n          // the first FETCHFROM_SRC nodes will be asked to fetch from this node.\n          // it's there in  the memory now. So , it must be served fast\n          url += myNodeName;\n        } else {\n          if (i == FETCHFROM_SRC) {\n            // This is just an optimization\n            // at this point a bunch of nodes are already downloading from me\n            // I'll wait for them to finish before asking other nodes to download from each other\n            try {\n              Thread.sleep(2 * 1000);\n            } catch (Exception e) {\n            }\n          }\n          // trying to avoid the thundering herd problem when there are a very large no:of nodes\n          // others should try to fetch it from any node where it is available. By now,\n          // almost FETCHFROM_SRC other nodes may have it\n          url += \"*\";\n        }\n        try {\n          //fire and forget\n          Utils.executeGET(coreContainer.getUpdateShardHandler().getDefaultHttpClient(), url, null);\n        } catch (Exception e) {\n          log.info(\"Node: {} failed to respond for file fetch notification\",  node, e);\n          //ignore the exception\n          // some nodes may be down or not responding\n        }\n        i++;\n      }\n    } finally {\n      coreContainer.getUpdateShardHandler().getUpdateExecutor().submit(() -> {\n        try {\n          Thread.sleep(10 * 1000);\n        } finally {\n          tmpFiles.remove(info.path);\n        }\n        return null;\n      });\n    }\n  }\n\n","sourceOld":"  private void distribute(FileInfo info) {\n    try {\n      String dirName = info.path.substring(0, info.path.lastIndexOf('/'));\n      coreContainer.getZkController().getZkClient().makePath(ZK_PACKAGESTORE + dirName, false, true);\n      coreContainer.getZkController().getZkClient().create(ZK_PACKAGESTORE + info.path, info.getDetails().getMetaData().sha512.getBytes(UTF_8),\n          CreateMode.PERSISTENT, true);\n    } catch (Exception e) {\n      throw new SolrException(SERVER_ERROR, \"Unable to create an entry in ZK\", e);\n    }\n    tmpFiles.put(info.path, info);\n\n    List<String> nodes = coreContainer.getPackageStoreAPI().shuffledNodes();\n    int i = 0;\n    int FETCHFROM_SRC = 50;\n    String myNodeName = coreContainer.getZkController().getNodeName();\n    try {\n      for (String node : nodes) {\n        String baseUrl = coreContainer.getZkController().getZkStateReader().getBaseUrlForNodeName(node);\n        String url = baseUrl.replace(\"/solr\", \"/api\") + \"/node/files\" + info.path + \"?getFrom=\";\n        if (i < FETCHFROM_SRC) {\n          // this is to protect very large clusters from overwhelming a single node\n          // the first FETCHFROM_SRC nodes will be asked to fetch from this node.\n          // it's there in  the memory now. So , it must be served fast\n          url += myNodeName;\n        } else {\n          if (i == FETCHFROM_SRC) {\n            // This is just an optimization\n            // at this point a bunch of nodes are already downloading from me\n            // I'll wait for them to finish before asking other nodes to download from each other\n            try {\n              Thread.sleep(2 * 1000);\n            } catch (Exception e) {\n            }\n          }\n          // trying to avoid the thundering herd problem when there are a very large no:of nodes\n          // others should try to fetch it from any node where it is available. By now,\n          // almost FETCHFROM_SRC other nodes may have it\n          url += \"*\";\n        }\n        try {\n          //fire and forget\n          Utils.executeGET(coreContainer.getUpdateShardHandler().getDefaultHttpClient(), url, null);\n        } catch (Exception e) {\n          log.info(\"Node: \" + node +\n              \" failed to respond for file fetch notification\", e);\n          //ignore the exception\n          // some nodes may be down or not responding\n        }\n        i++;\n      }\n    } finally {\n      coreContainer.getUpdateShardHandler().getUpdateExecutor().submit(() -> {\n        try {\n          Thread.sleep(10 * 1000);\n        } finally {\n          tmpFiles.remove(info.path);\n        }\n        return null;\n      });\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"64ec73f19361ec6354e55c878a349735fa8bc52e","date":1596183798,"type":3,"author":"Marcus","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/filestore/DistribPackageStore#distribute(FileInfo).mjava","pathOld":"solr/core/src/java/org/apache/solr/filestore/DistribPackageStore#distribute(FileInfo).mjava","sourceNew":"  private void distribute(FileInfo info) {\n    try {\n      String dirName = info.path.substring(0, info.path.lastIndexOf('/'));\n      coreContainer.getZkController().getZkClient().makePath(ZK_PACKAGESTORE + dirName, false, true);\n      coreContainer.getZkController().getZkClient().create(ZK_PACKAGESTORE + info.path, info.getDetails().getMetaData().sha512.getBytes(UTF_8),\n              CreateMode.PERSISTENT, true);\n    } catch (Exception e) {\n      throw new SolrException(SERVER_ERROR, \"Unable to create an entry in ZK\", e);\n    }\n    tmpFiles.put(info.path, info);\n\n    List<String> nodes = coreContainer.getPackageStoreAPI().shuffledNodes();\n    int i = 0;\n    int FETCHFROM_SRC = 50;\n    String myNodeName = coreContainer.getZkController().getNodeName();\n    try {\n      for (String node : nodes) {\n        String baseUrl = coreContainer.getZkController().getZkStateReader().getBaseUrlForNodeName(node);\n        String url = baseUrl.replace(\"/solr\", \"/api\") + \"/node/files\" + info.path + \"?getFrom=\";\n        if (i < FETCHFROM_SRC) {\n          // this is to protect very large clusters from overwhelming a single node\n          // the first FETCHFROM_SRC nodes will be asked to fetch from this node.\n          // it's there in  the memory now. So , it must be served fast\n          url += myNodeName;\n        } else {\n          if (i == FETCHFROM_SRC) {\n            // This is just an optimization\n            // at this point a bunch of nodes are already downloading from me\n            // I'll wait for them to finish before asking other nodes to download from each other\n            try {\n              Thread.sleep(2 * 1000);\n            } catch (Exception e) {\n            }\n          }\n          // trying to avoid the thundering herd problem when there are a very large no:of nodes\n          // others should try to fetch it from any node where it is available. By now,\n          // almost FETCHFROM_SRC other nodes may have it\n          url += \"*\";\n        }\n        try {\n          //fire and forget\n          Utils.executeGET(coreContainer.getUpdateShardHandler().getDefaultHttpClient(), url, null);\n        } catch (Exception e) {\n          log.info(\"Node: {} failed to respond for file fetch notification\", node, e);\n          //ignore the exception\n          // some nodes may be down or not responding\n        }\n        i++;\n      }\n    } finally {\n      coreContainer.getUpdateShardHandler().getUpdateExecutor().submit(() -> {\n        try {\n          Thread.sleep(10 * 1000);\n        } finally {\n          tmpFiles.remove(info.path);\n        }\n        return null;\n      });\n    }\n  }\n\n","sourceOld":"  private void distribute(FileInfo info) {\n    try {\n      String dirName = info.path.substring(0, info.path.lastIndexOf('/'));\n      coreContainer.getZkController().getZkClient().makePath(ZK_PACKAGESTORE + dirName, false, true);\n      coreContainer.getZkController().getZkClient().create(ZK_PACKAGESTORE + info.path, info.getDetails().getMetaData().sha512.getBytes(UTF_8),\n          CreateMode.PERSISTENT, true);\n    } catch (Exception e) {\n      throw new SolrException(SERVER_ERROR, \"Unable to create an entry in ZK\", e);\n    }\n    tmpFiles.put(info.path, info);\n\n    List<String> nodes = coreContainer.getPackageStoreAPI().shuffledNodes();\n    int i = 0;\n    int FETCHFROM_SRC = 50;\n    String myNodeName = coreContainer.getZkController().getNodeName();\n    try {\n      for (String node : nodes) {\n        String baseUrl = coreContainer.getZkController().getZkStateReader().getBaseUrlForNodeName(node);\n        String url = baseUrl.replace(\"/solr\", \"/api\") + \"/node/files\" + info.path + \"?getFrom=\";\n        if (i < FETCHFROM_SRC) {\n          // this is to protect very large clusters from overwhelming a single node\n          // the first FETCHFROM_SRC nodes will be asked to fetch from this node.\n          // it's there in  the memory now. So , it must be served fast\n          url += myNodeName;\n        } else {\n          if (i == FETCHFROM_SRC) {\n            // This is just an optimization\n            // at this point a bunch of nodes are already downloading from me\n            // I'll wait for them to finish before asking other nodes to download from each other\n            try {\n              Thread.sleep(2 * 1000);\n            } catch (Exception e) {\n            }\n          }\n          // trying to avoid the thundering herd problem when there are a very large no:of nodes\n          // others should try to fetch it from any node where it is available. By now,\n          // almost FETCHFROM_SRC other nodes may have it\n          url += \"*\";\n        }\n        try {\n          //fire and forget\n          Utils.executeGET(coreContainer.getUpdateShardHandler().getDefaultHttpClient(), url, null);\n        } catch (Exception e) {\n          log.info(\"Node: {} failed to respond for file fetch notification\",  node, e);\n          //ignore the exception\n          // some nodes may be down or not responding\n        }\n        i++;\n      }\n    } finally {\n      coreContainer.getUpdateShardHandler().getUpdateExecutor().submit(() -> {\n        try {\n          Thread.sleep(10 * 1000);\n        } finally {\n          tmpFiles.remove(info.path);\n        }\n        return null;\n      });\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b8f0a7504661c8e51be5c63e87f9d79a36d9116c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","d218decf811b7a0a4d86218c54c79c74a962374b"],"64ec73f19361ec6354e55c878a349735fa8bc52e":["140be51d03394488536f4aacedace29f9b318347"],"140be51d03394488536f4aacedace29f9b318347":["d218decf811b7a0a4d86218c54c79c74a962374b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d218decf811b7a0a4d86218c54c79c74a962374b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["64ec73f19361ec6354e55c878a349735fa8bc52e"]},"commit2Childs":{"b8f0a7504661c8e51be5c63e87f9d79a36d9116c":[],"64ec73f19361ec6354e55c878a349735fa8bc52e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b8f0a7504661c8e51be5c63e87f9d79a36d9116c","d218decf811b7a0a4d86218c54c79c74a962374b"],"140be51d03394488536f4aacedace29f9b318347":["64ec73f19361ec6354e55c878a349735fa8bc52e"],"d218decf811b7a0a4d86218c54c79c74a962374b":["b8f0a7504661c8e51be5c63e87f9d79a36d9116c","140be51d03394488536f4aacedace29f9b318347"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["b8f0a7504661c8e51be5c63e87f9d79a36d9116c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}