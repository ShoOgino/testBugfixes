{"path":"solr/core/src/test/org/apache/solr/cloud/api/collections/SplitByPrefixTest#doTest().mjava","commits":[{"id":"a05f3f5161c62339ec5560b8f6958f3df8483618","date":1563550501,"type":0,"author":"yonik","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/api/collections/SplitByPrefixTest#doTest().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  public void doTest() throws IOException, SolrServerException {\n    CollectionAdminRequest\n        .createCollection(COLLECTION_NAME, \"conf\", 1, 1)\n        .setMaxShardsPerNode(100)\n        .process(cluster.getSolrClient());\n\n    cluster.waitForActiveCollection(COLLECTION_NAME, 1, 1);\n\n\n    CloudSolrClient client = cluster.getSolrClient();\n    client.setDefaultCollection(COLLECTION_NAME);\n\n    // splitting an empty collection by prefix should still work (i.e. fall back to old method of just dividing the hash range\n\n    CollectionAdminRequest.SplitShard splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setNumSubShards(2)\n        .setSplitByPrefix(true)\n        .setShardName(\"shard1\");\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(2, 3));  // expectedReplicas==3 because original replica still exists (just inactive)\n\n\n    List<Prefix> prefixes = findPrefixes(20, 0, 0x00ffffff);\n    List<Prefix> uniquePrefixes = removeDups(prefixes);\n    if (uniquePrefixes.size() % 2 == 1) {  // make it an even sized list so we can split it exactly in two\n      uniquePrefixes.remove(uniquePrefixes.size()-1);\n    }\n    log.info(\"Unique prefixes: \" + uniquePrefixes);\n\n    for (Prefix prefix : uniquePrefixes) {\n      client.add( getDoc(prefix.key, \"doc1\") );\n      client.add( getDoc(prefix.key, \"doc2\") );\n    }\n    client.commit();\n\n\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(\"shard1_1\");  // should start out with the range of 0-7fffffff\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(3, 5));\n\n    // OK, now let's check that the correct split point was chosen\n    // We can use the router to find the shards for the middle prefixes and they should be different.\n\n    DocCollection collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    Collection<Slice> slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(uniquePrefixes.size()/2 - 1).key, null, collection);\n    Collection<Slice> slices2 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(uniquePrefixes.size()/2    ).key, null, collection);\n\n    Slice slice1 = slices1.iterator().next();\n    Slice slice2 = slices2.iterator().next();\n\n    assertTrue(slices1.size() == 1 && slices2.size() == 1);\n    assertTrue(slice1 != slice2);\n\n\n    //\n    // now lets add enough documents to the first prefix to get it split out on it's own\n    //\n    for (int i=0; i<uniquePrefixes.size(); i++) {\n      client.add(  getDoc(uniquePrefixes.get(0).key, \"doc\"+(i+100)));\n    }\n    client.commit();\n\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(slice1.getName());\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(4, 7));\n\n    collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(0).key, null, collection);\n    slices2 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(1).key, null, collection);\n\n    slice1 = slices1.iterator().next();\n    slice2 = slices2.iterator().next();\n\n    assertTrue(slices1.size() == 1 && slices2.size() == 1);\n    assertTrue(slice1 != slice2);\n\n\n    // Now if we call split (with splitByPrefix) on a shard that has a single prefix, it should split it in half\n\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(slice1.getName());\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(5, 9));\n\n    collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(0).key, null, collection);\n    slice1 = slices1.iterator().next();\n\n    assertTrue(slices1.size() == 2);\n\n    //\n    // split one more time, this time on a partial prefix/bucket\n    //\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(slice1.getName());\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(6, 11));\n\n    collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(0).key, null, collection);\n\n    assertTrue(slices1.size() == 3);\n\n    // System.err.println(\"### STATE=\" + cluster.getSolrClient().getZkStateReader().getClusterState().getCollection(COLLECTION_NAME));\n    // System.err.println(\"### getActiveSlices()=\" + cluster.getSolrClient().getZkStateReader().getClusterState().getCollection(COLLECTION_NAME).getActiveSlices());\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"36bdabc04743acfe0e82c9cf8208b1111b2b193a","date":1565115020,"type":3,"author":"yonik","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/api/collections/SplitByPrefixTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/api/collections/SplitByPrefixTest#doTest().mjava","sourceNew":"  @Test\n  public void doTest() throws IOException, SolrServerException {\n    CollectionAdminRequest\n        .createCollection(COLLECTION_NAME, \"conf\", 1, 1)\n        .setMaxShardsPerNode(100)\n        .process(cluster.getSolrClient());\n\n    cluster.waitForActiveCollection(COLLECTION_NAME, 1, 1);\n\n\n    CloudSolrClient client = cluster.getSolrClient();\n    client.setDefaultCollection(COLLECTION_NAME);\n\n    // splitting an empty collection by prefix should still work (i.e. fall back to old method of just dividing the hash range\n\n    CollectionAdminRequest.SplitShard splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setNumSubShards(2)\n        .setSplitByPrefix(true)\n        .setShardName(\"shard1\");\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(2, 3));  // expectedReplicas==3 because original replica still exists (just inactive)\n\n\n    List<Prefix> prefixes = findPrefixes(20, 0, 0x00ffffff);\n    List<Prefix> uniquePrefixes = removeDups(prefixes);\n    if (uniquePrefixes.size() % 2 == 1) {  // make it an even sized list so we can split it exactly in two\n      uniquePrefixes.remove(uniquePrefixes.size()-1);\n    }\n    log.info(\"Unique prefixes: \" + uniquePrefixes);\n\n    for (Prefix prefix : uniquePrefixes) {\n      client.add( getDoc(prefix.key, \"doc1\") );\n      client.add( getDoc(prefix.key, \"doc2\") );\n    }\n    client.commit();\n\n\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(\"shard1_1\");  // should start out with the range of 0-7fffffff\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(3, 5));\n\n    // OK, now let's check that the correct split point was chosen\n    // We can use the router to find the shards for the middle prefixes and they should be different.\n\n    DocCollection collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    Collection<Slice> slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(uniquePrefixes.size()/2 - 1).key, null, collection);\n    Collection<Slice> slices2 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(uniquePrefixes.size()/2    ).key, null, collection);\n\n    Slice slice1 = slices1.iterator().next();\n    Slice slice2 = slices2.iterator().next();\n\n    assertTrue(slices1.size() == 1 && slices2.size() == 1);\n    assertTrue(slice1 != slice2);\n\n\n    //\n    // now lets add enough documents to the first prefix to get it split out on its own\n    //\n    for (int i=0; i<uniquePrefixes.size(); i++) {\n      client.add(  getDoc(uniquePrefixes.get(0).key, \"doc\"+(i+100)));\n    }\n    client.commit();\n\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(slice1.getName());\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(4, 7));\n\n    collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(0).key, null, collection);\n    slices2 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(1).key, null, collection);\n\n    slice1 = slices1.iterator().next();\n    slice2 = slices2.iterator().next();\n\n    assertTrue(slices1.size() == 1 && slices2.size() == 1);\n    assertTrue(slice1 != slice2);\n\n\n    // Now if we call split (with splitByPrefix) on a shard that has a single prefix, it should split it in half\n\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(slice1.getName());\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(5, 9));\n\n    collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(0).key, null, collection);\n    slice1 = slices1.iterator().next();\n\n    assertTrue(slices1.size() == 2);\n\n    //\n    // split one more time, this time on a partial prefix/bucket\n    //\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(slice1.getName());\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(6, 11));\n\n    collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(0).key, null, collection);\n\n    assertTrue(slices1.size() == 3);\n\n    // System.err.println(\"### STATE=\" + cluster.getSolrClient().getZkStateReader().getClusterState().getCollection(COLLECTION_NAME));\n    // System.err.println(\"### getActiveSlices()=\" + cluster.getSolrClient().getZkStateReader().getClusterState().getCollection(COLLECTION_NAME).getActiveSlices());\n  }\n\n","sourceOld":"  @Test\n  public void doTest() throws IOException, SolrServerException {\n    CollectionAdminRequest\n        .createCollection(COLLECTION_NAME, \"conf\", 1, 1)\n        .setMaxShardsPerNode(100)\n        .process(cluster.getSolrClient());\n\n    cluster.waitForActiveCollection(COLLECTION_NAME, 1, 1);\n\n\n    CloudSolrClient client = cluster.getSolrClient();\n    client.setDefaultCollection(COLLECTION_NAME);\n\n    // splitting an empty collection by prefix should still work (i.e. fall back to old method of just dividing the hash range\n\n    CollectionAdminRequest.SplitShard splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setNumSubShards(2)\n        .setSplitByPrefix(true)\n        .setShardName(\"shard1\");\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(2, 3));  // expectedReplicas==3 because original replica still exists (just inactive)\n\n\n    List<Prefix> prefixes = findPrefixes(20, 0, 0x00ffffff);\n    List<Prefix> uniquePrefixes = removeDups(prefixes);\n    if (uniquePrefixes.size() % 2 == 1) {  // make it an even sized list so we can split it exactly in two\n      uniquePrefixes.remove(uniquePrefixes.size()-1);\n    }\n    log.info(\"Unique prefixes: \" + uniquePrefixes);\n\n    for (Prefix prefix : uniquePrefixes) {\n      client.add( getDoc(prefix.key, \"doc1\") );\n      client.add( getDoc(prefix.key, \"doc2\") );\n    }\n    client.commit();\n\n\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(\"shard1_1\");  // should start out with the range of 0-7fffffff\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(3, 5));\n\n    // OK, now let's check that the correct split point was chosen\n    // We can use the router to find the shards for the middle prefixes and they should be different.\n\n    DocCollection collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    Collection<Slice> slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(uniquePrefixes.size()/2 - 1).key, null, collection);\n    Collection<Slice> slices2 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(uniquePrefixes.size()/2    ).key, null, collection);\n\n    Slice slice1 = slices1.iterator().next();\n    Slice slice2 = slices2.iterator().next();\n\n    assertTrue(slices1.size() == 1 && slices2.size() == 1);\n    assertTrue(slice1 != slice2);\n\n\n    //\n    // now lets add enough documents to the first prefix to get it split out on it's own\n    //\n    for (int i=0; i<uniquePrefixes.size(); i++) {\n      client.add(  getDoc(uniquePrefixes.get(0).key, \"doc\"+(i+100)));\n    }\n    client.commit();\n\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(slice1.getName());\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(4, 7));\n\n    collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(0).key, null, collection);\n    slices2 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(1).key, null, collection);\n\n    slice1 = slices1.iterator().next();\n    slice2 = slices2.iterator().next();\n\n    assertTrue(slices1.size() == 1 && slices2.size() == 1);\n    assertTrue(slice1 != slice2);\n\n\n    // Now if we call split (with splitByPrefix) on a shard that has a single prefix, it should split it in half\n\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(slice1.getName());\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(5, 9));\n\n    collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(0).key, null, collection);\n    slice1 = slices1.iterator().next();\n\n    assertTrue(slices1.size() == 2);\n\n    //\n    // split one more time, this time on a partial prefix/bucket\n    //\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(slice1.getName());\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(6, 11));\n\n    collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(0).key, null, collection);\n\n    assertTrue(slices1.size() == 3);\n\n    // System.err.println(\"### STATE=\" + cluster.getSolrClient().getZkStateReader().getClusterState().getCollection(COLLECTION_NAME));\n    // System.err.println(\"### getActiveSlices()=\" + cluster.getSolrClient().getZkStateReader().getClusterState().getCollection(COLLECTION_NAME).getActiveSlices());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ebccdcfbac56fd8a05e781b2b8cf7a8f1d447e87","date":1576277705,"type":3,"author":"yonik","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/api/collections/SplitByPrefixTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/api/collections/SplitByPrefixTest#doTest().mjava","sourceNew":"  @Test\n  public void doTest() throws IOException, SolrServerException {\n    // SPLITSHARD is recommended to be run in async mode, so we default to that.\n    // Also, autoscale triggers use async with splits as well.\n    boolean doAsync = true;\n\n    CollectionAdminRequest\n        .createCollection(COLLECTION_NAME, \"conf\", 1, 1)\n        .setMaxShardsPerNode(100)\n        .process(cluster.getSolrClient());\n\n    cluster.waitForActiveCollection(COLLECTION_NAME, 1, 1);\n\n\n    CloudSolrClient client = cluster.getSolrClient();\n    client.setDefaultCollection(COLLECTION_NAME);\n\n    // splitting an empty collection by prefix should still work (i.e. fall back to old method of just dividing the hash range\n\n    CollectionAdminRequest.SplitShard splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setNumSubShards(2)\n        .setSplitByPrefix(true)\n        .setShardName(\"shard1\");\n    if (doAsync) {\n      splitShard.setAsyncId(\"SPLIT1\");\n    }\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(2, 3));  // expectedReplicas==3 because original replica still exists (just inactive)\n\n\n    List<Prefix> prefixes = findPrefixes(20, 0, 0x00ffffff);\n    List<Prefix> uniquePrefixes = removeDups(prefixes);\n    if (uniquePrefixes.size() % 2 == 1) {  // make it an even sized list so we can split it exactly in two\n      uniquePrefixes.remove(uniquePrefixes.size()-1);\n    }\n    log.info(\"Unique prefixes: \" + uniquePrefixes);\n\n    for (Prefix prefix : uniquePrefixes) {\n      client.add( getDoc(prefix.key, \"doc1\") );\n      client.add( getDoc(prefix.key, \"doc2\") );\n    }\n    client.commit();\n\n\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(\"shard1_1\");  // should start out with the range of 0-7fffffff\n    if (doAsync) {\n      splitShard.setAsyncId(\"SPLIT2\");\n    }\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(3, 5));\n\n    // OK, now let's check that the correct split point was chosen\n    // We can use the router to find the shards for the middle prefixes and they should be different.\n\n    DocCollection collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    Collection<Slice> slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(uniquePrefixes.size()/2 - 1).key, null, collection);\n    Collection<Slice> slices2 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(uniquePrefixes.size()/2    ).key, null, collection);\n\n    Slice slice1 = slices1.iterator().next();\n    Slice slice2 = slices2.iterator().next();\n\n    assertTrue(slices1.size() == 1 && slices2.size() == 1);\n    assertTrue(slice1 != slice2);\n\n\n    //\n    // now lets add enough documents to the first prefix to get it split out on its own\n    //\n    for (int i=0; i<uniquePrefixes.size(); i++) {\n      client.add(  getDoc(uniquePrefixes.get(0).key, \"doc\"+(i+100)));\n    }\n    client.commit();\n\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(slice1.getName());\n    if (doAsync) {\n      splitShard.setAsyncId(\"SPLIT3\");\n    }\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(4, 7));\n\n    collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(0).key, null, collection);\n    slices2 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(1).key, null, collection);\n\n    slice1 = slices1.iterator().next();\n    slice2 = slices2.iterator().next();\n\n    assertTrue(slices1.size() == 1 && slices2.size() == 1);\n    assertTrue(slice1 != slice2);\n\n\n    // Now if we call split (with splitByPrefix) on a shard that has a single prefix, it should split it in half\n\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(slice1.getName());\n    if (doAsync) {\n      splitShard.setAsyncId(\"SPLIT4\");\n    }\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(5, 9));\n\n    collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(0).key, null, collection);\n    slice1 = slices1.iterator().next();\n\n    assertTrue(slices1.size() == 2);\n\n    //\n    // split one more time, this time on a partial prefix/bucket\n    //\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(slice1.getName());\n    if (doAsync) {\n      splitShard.setAsyncId(\"SPLIT5\");\n    }\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(6, 11));\n\n    collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(0).key, null, collection);\n\n    assertTrue(slices1.size() == 3);\n\n    // System.err.println(\"### STATE=\" + cluster.getSolrClient().getZkStateReader().getClusterState().getCollection(COLLECTION_NAME));\n    // System.err.println(\"### getActiveSlices()=\" + cluster.getSolrClient().getZkStateReader().getClusterState().getCollection(COLLECTION_NAME).getActiveSlices());\n  }\n\n","sourceOld":"  @Test\n  public void doTest() throws IOException, SolrServerException {\n    CollectionAdminRequest\n        .createCollection(COLLECTION_NAME, \"conf\", 1, 1)\n        .setMaxShardsPerNode(100)\n        .process(cluster.getSolrClient());\n\n    cluster.waitForActiveCollection(COLLECTION_NAME, 1, 1);\n\n\n    CloudSolrClient client = cluster.getSolrClient();\n    client.setDefaultCollection(COLLECTION_NAME);\n\n    // splitting an empty collection by prefix should still work (i.e. fall back to old method of just dividing the hash range\n\n    CollectionAdminRequest.SplitShard splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setNumSubShards(2)\n        .setSplitByPrefix(true)\n        .setShardName(\"shard1\");\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(2, 3));  // expectedReplicas==3 because original replica still exists (just inactive)\n\n\n    List<Prefix> prefixes = findPrefixes(20, 0, 0x00ffffff);\n    List<Prefix> uniquePrefixes = removeDups(prefixes);\n    if (uniquePrefixes.size() % 2 == 1) {  // make it an even sized list so we can split it exactly in two\n      uniquePrefixes.remove(uniquePrefixes.size()-1);\n    }\n    log.info(\"Unique prefixes: \" + uniquePrefixes);\n\n    for (Prefix prefix : uniquePrefixes) {\n      client.add( getDoc(prefix.key, \"doc1\") );\n      client.add( getDoc(prefix.key, \"doc2\") );\n    }\n    client.commit();\n\n\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(\"shard1_1\");  // should start out with the range of 0-7fffffff\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(3, 5));\n\n    // OK, now let's check that the correct split point was chosen\n    // We can use the router to find the shards for the middle prefixes and they should be different.\n\n    DocCollection collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    Collection<Slice> slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(uniquePrefixes.size()/2 - 1).key, null, collection);\n    Collection<Slice> slices2 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(uniquePrefixes.size()/2    ).key, null, collection);\n\n    Slice slice1 = slices1.iterator().next();\n    Slice slice2 = slices2.iterator().next();\n\n    assertTrue(slices1.size() == 1 && slices2.size() == 1);\n    assertTrue(slice1 != slice2);\n\n\n    //\n    // now lets add enough documents to the first prefix to get it split out on its own\n    //\n    for (int i=0; i<uniquePrefixes.size(); i++) {\n      client.add(  getDoc(uniquePrefixes.get(0).key, \"doc\"+(i+100)));\n    }\n    client.commit();\n\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(slice1.getName());\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(4, 7));\n\n    collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(0).key, null, collection);\n    slices2 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(1).key, null, collection);\n\n    slice1 = slices1.iterator().next();\n    slice2 = slices2.iterator().next();\n\n    assertTrue(slices1.size() == 1 && slices2.size() == 1);\n    assertTrue(slice1 != slice2);\n\n\n    // Now if we call split (with splitByPrefix) on a shard that has a single prefix, it should split it in half\n\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(slice1.getName());\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(5, 9));\n\n    collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(0).key, null, collection);\n    slice1 = slices1.iterator().next();\n\n    assertTrue(slices1.size() == 2);\n\n    //\n    // split one more time, this time on a partial prefix/bucket\n    //\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(slice1.getName());\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(6, 11));\n\n    collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(0).key, null, collection);\n\n    assertTrue(slices1.size() == 3);\n\n    // System.err.println(\"### STATE=\" + cluster.getSolrClient().getZkStateReader().getClusterState().getCollection(COLLECTION_NAME));\n    // System.err.println(\"### getActiveSlices()=\" + cluster.getSolrClient().getZkStateReader().getClusterState().getCollection(COLLECTION_NAME).getActiveSlices());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"06595b0c22c7d3075c4104d3820cccf95d9d8a43","date":1576491645,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/api/collections/SplitByPrefixTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/api/collections/SplitByPrefixTest#doTest().mjava","sourceNew":"  @Test\n  public void doTest() throws IOException, SolrServerException {\n    // SPLITSHARD is recommended to be run in async mode, so we default to that.\n    // Also, autoscale triggers use async with splits as well.\n    boolean doAsync = true;\n\n    CollectionAdminRequest\n        .createCollection(COLLECTION_NAME, \"conf\", 1, 1)\n        .setMaxShardsPerNode(100)\n        .process(cluster.getSolrClient());\n\n    cluster.waitForActiveCollection(COLLECTION_NAME, 1, 1);\n\n\n    CloudSolrClient client = cluster.getSolrClient();\n    client.setDefaultCollection(COLLECTION_NAME);\n\n    // splitting an empty collection by prefix should still work (i.e. fall back to old method of just dividing the hash range\n\n    CollectionAdminRequest.SplitShard splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setNumSubShards(2)\n        .setSplitByPrefix(true)\n        .setShardName(\"shard1\");\n    if (doAsync) {\n      splitShard.setAsyncId(\"SPLIT1\");\n    }\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(2, 3));  // expectedReplicas==3 because original replica still exists (just inactive)\n\n\n    List<Prefix> prefixes = findPrefixes(20, 0, 0x00ffffff);\n    List<Prefix> uniquePrefixes = removeDups(prefixes);\n    if (uniquePrefixes.size() % 2 == 1) {  // make it an even sized list so we can split it exactly in two\n      uniquePrefixes.remove(uniquePrefixes.size()-1);\n    }\n    log.info(\"Unique prefixes: \" + uniquePrefixes);\n\n    for (Prefix prefix : uniquePrefixes) {\n      client.add( getDoc(prefix.key, \"doc1\") );\n      client.add( getDoc(prefix.key, \"doc2\") );\n    }\n    client.commit();\n\n\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(\"shard1_1\");  // should start out with the range of 0-7fffffff\n    if (doAsync) {\n      splitShard.setAsyncId(\"SPLIT2\");\n    }\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(3, 5));\n\n    // OK, now let's check that the correct split point was chosen\n    // We can use the router to find the shards for the middle prefixes and they should be different.\n\n    DocCollection collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    Collection<Slice> slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(uniquePrefixes.size()/2 - 1).key, null, collection);\n    Collection<Slice> slices2 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(uniquePrefixes.size()/2    ).key, null, collection);\n\n    Slice slice1 = slices1.iterator().next();\n    Slice slice2 = slices2.iterator().next();\n\n    assertTrue(slices1.size() == 1 && slices2.size() == 1);\n    assertTrue(slice1 != slice2);\n\n\n    //\n    // now lets add enough documents to the first prefix to get it split out on its own\n    //\n    for (int i=0; i<uniquePrefixes.size(); i++) {\n      client.add(  getDoc(uniquePrefixes.get(0).key, \"doc\"+(i+100)));\n    }\n    client.commit();\n\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(slice1.getName());\n    if (doAsync) {\n      splitShard.setAsyncId(\"SPLIT3\");\n    }\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(4, 7));\n\n    collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(0).key, null, collection);\n    slices2 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(1).key, null, collection);\n\n    slice1 = slices1.iterator().next();\n    slice2 = slices2.iterator().next();\n\n    assertTrue(slices1.size() == 1 && slices2.size() == 1);\n    assertTrue(slice1 != slice2);\n\n\n    // Now if we call split (with splitByPrefix) on a shard that has a single prefix, it should split it in half\n\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(slice1.getName());\n    if (doAsync) {\n      splitShard.setAsyncId(\"SPLIT4\");\n    }\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(5, 9));\n\n    collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(0).key, null, collection);\n    slice1 = slices1.iterator().next();\n\n    assertTrue(slices1.size() == 2);\n\n    //\n    // split one more time, this time on a partial prefix/bucket\n    //\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(slice1.getName());\n    if (doAsync) {\n      splitShard.setAsyncId(\"SPLIT5\");\n    }\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(6, 11));\n\n    collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(0).key, null, collection);\n\n    assertTrue(slices1.size() == 3);\n\n    // System.err.println(\"### STATE=\" + cluster.getSolrClient().getZkStateReader().getClusterState().getCollection(COLLECTION_NAME));\n    // System.err.println(\"### getActiveSlices()=\" + cluster.getSolrClient().getZkStateReader().getClusterState().getCollection(COLLECTION_NAME).getActiveSlices());\n  }\n\n","sourceOld":"  @Test\n  public void doTest() throws IOException, SolrServerException {\n    CollectionAdminRequest\n        .createCollection(COLLECTION_NAME, \"conf\", 1, 1)\n        .setMaxShardsPerNode(100)\n        .process(cluster.getSolrClient());\n\n    cluster.waitForActiveCollection(COLLECTION_NAME, 1, 1);\n\n\n    CloudSolrClient client = cluster.getSolrClient();\n    client.setDefaultCollection(COLLECTION_NAME);\n\n    // splitting an empty collection by prefix should still work (i.e. fall back to old method of just dividing the hash range\n\n    CollectionAdminRequest.SplitShard splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setNumSubShards(2)\n        .setSplitByPrefix(true)\n        .setShardName(\"shard1\");\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(2, 3));  // expectedReplicas==3 because original replica still exists (just inactive)\n\n\n    List<Prefix> prefixes = findPrefixes(20, 0, 0x00ffffff);\n    List<Prefix> uniquePrefixes = removeDups(prefixes);\n    if (uniquePrefixes.size() % 2 == 1) {  // make it an even sized list so we can split it exactly in two\n      uniquePrefixes.remove(uniquePrefixes.size()-1);\n    }\n    log.info(\"Unique prefixes: \" + uniquePrefixes);\n\n    for (Prefix prefix : uniquePrefixes) {\n      client.add( getDoc(prefix.key, \"doc1\") );\n      client.add( getDoc(prefix.key, \"doc2\") );\n    }\n    client.commit();\n\n\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(\"shard1_1\");  // should start out with the range of 0-7fffffff\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(3, 5));\n\n    // OK, now let's check that the correct split point was chosen\n    // We can use the router to find the shards for the middle prefixes and they should be different.\n\n    DocCollection collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    Collection<Slice> slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(uniquePrefixes.size()/2 - 1).key, null, collection);\n    Collection<Slice> slices2 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(uniquePrefixes.size()/2    ).key, null, collection);\n\n    Slice slice1 = slices1.iterator().next();\n    Slice slice2 = slices2.iterator().next();\n\n    assertTrue(slices1.size() == 1 && slices2.size() == 1);\n    assertTrue(slice1 != slice2);\n\n\n    //\n    // now lets add enough documents to the first prefix to get it split out on its own\n    //\n    for (int i=0; i<uniquePrefixes.size(); i++) {\n      client.add(  getDoc(uniquePrefixes.get(0).key, \"doc\"+(i+100)));\n    }\n    client.commit();\n\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(slice1.getName());\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(4, 7));\n\n    collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(0).key, null, collection);\n    slices2 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(1).key, null, collection);\n\n    slice1 = slices1.iterator().next();\n    slice2 = slices2.iterator().next();\n\n    assertTrue(slices1.size() == 1 && slices2.size() == 1);\n    assertTrue(slice1 != slice2);\n\n\n    // Now if we call split (with splitByPrefix) on a shard that has a single prefix, it should split it in half\n\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(slice1.getName());\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(5, 9));\n\n    collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(0).key, null, collection);\n    slice1 = slices1.iterator().next();\n\n    assertTrue(slices1.size() == 2);\n\n    //\n    // split one more time, this time on a partial prefix/bucket\n    //\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(slice1.getName());\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(6, 11));\n\n    collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(0).key, null, collection);\n\n    assertTrue(slices1.size() == 3);\n\n    // System.err.println(\"### STATE=\" + cluster.getSolrClient().getZkStateReader().getClusterState().getCollection(COLLECTION_NAME));\n    // System.err.println(\"### getActiveSlices()=\" + cluster.getSolrClient().getZkStateReader().getClusterState().getCollection(COLLECTION_NAME).getActiveSlices());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe9f2c4a0d7ac164e4bdd4eee7f87131aec83fd4","date":1588172214,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/api/collections/SplitByPrefixTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/api/collections/SplitByPrefixTest#doTest().mjava","sourceNew":"  @Test\n  public void doTest() throws IOException, SolrServerException {\n    // SPLITSHARD is recommended to be run in async mode, so we default to that.\n    // Also, autoscale triggers use async with splits as well.\n    boolean doAsync = true;\n\n    CollectionAdminRequest\n        .createCollection(COLLECTION_NAME, \"conf\", 1, 1)\n        .setMaxShardsPerNode(100)\n        .process(cluster.getSolrClient());\n\n    cluster.waitForActiveCollection(COLLECTION_NAME, 1, 1);\n\n\n    CloudSolrClient client = cluster.getSolrClient();\n    client.setDefaultCollection(COLLECTION_NAME);\n\n    // splitting an empty collection by prefix should still work (i.e. fall back to old method of just dividing the hash range\n\n    CollectionAdminRequest.SplitShard splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setNumSubShards(2)\n        .setSplitByPrefix(true)\n        .setShardName(\"shard1\");\n    if (doAsync) {\n      splitShard.setAsyncId(\"SPLIT1\");\n    }\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(2, 3));  // expectedReplicas==3 because original replica still exists (just inactive)\n\n\n    List<Prefix> prefixes = findPrefixes(20, 0, 0x00ffffff);\n    List<Prefix> uniquePrefixes = removeDups(prefixes);\n    if (uniquePrefixes.size() % 2 == 1) {  // make it an even sized list so we can split it exactly in two\n      uniquePrefixes.remove(uniquePrefixes.size()-1);\n    }\n    log.info(\"Unique prefixes: {}\", uniquePrefixes);\n\n    for (Prefix prefix : uniquePrefixes) {\n      client.add( getDoc(prefix.key, \"doc1\") );\n      client.add( getDoc(prefix.key, \"doc2\") );\n    }\n    client.commit();\n\n\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(\"shard1_1\");  // should start out with the range of 0-7fffffff\n    if (doAsync) {\n      splitShard.setAsyncId(\"SPLIT2\");\n    }\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(3, 5));\n\n    // OK, now let's check that the correct split point was chosen\n    // We can use the router to find the shards for the middle prefixes and they should be different.\n\n    DocCollection collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    Collection<Slice> slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(uniquePrefixes.size()/2 - 1).key, null, collection);\n    Collection<Slice> slices2 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(uniquePrefixes.size()/2    ).key, null, collection);\n\n    Slice slice1 = slices1.iterator().next();\n    Slice slice2 = slices2.iterator().next();\n\n    assertTrue(slices1.size() == 1 && slices2.size() == 1);\n    assertTrue(slice1 != slice2);\n\n\n    //\n    // now lets add enough documents to the first prefix to get it split out on its own\n    //\n    for (int i=0; i<uniquePrefixes.size(); i++) {\n      client.add(  getDoc(uniquePrefixes.get(0).key, \"doc\"+(i+100)));\n    }\n    client.commit();\n\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(slice1.getName());\n    if (doAsync) {\n      splitShard.setAsyncId(\"SPLIT3\");\n    }\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(4, 7));\n\n    collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(0).key, null, collection);\n    slices2 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(1).key, null, collection);\n\n    slice1 = slices1.iterator().next();\n    slice2 = slices2.iterator().next();\n\n    assertTrue(slices1.size() == 1 && slices2.size() == 1);\n    assertTrue(slice1 != slice2);\n\n\n    // Now if we call split (with splitByPrefix) on a shard that has a single prefix, it should split it in half\n\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(slice1.getName());\n    if (doAsync) {\n      splitShard.setAsyncId(\"SPLIT4\");\n    }\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(5, 9));\n\n    collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(0).key, null, collection);\n    slice1 = slices1.iterator().next();\n\n    assertTrue(slices1.size() == 2);\n\n    //\n    // split one more time, this time on a partial prefix/bucket\n    //\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(slice1.getName());\n    if (doAsync) {\n      splitShard.setAsyncId(\"SPLIT5\");\n    }\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(6, 11));\n\n    collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(0).key, null, collection);\n\n    assertTrue(slices1.size() == 3);\n\n    // System.err.println(\"### STATE=\" + cluster.getSolrClient().getZkStateReader().getClusterState().getCollection(COLLECTION_NAME));\n    // System.err.println(\"### getActiveSlices()=\" + cluster.getSolrClient().getZkStateReader().getClusterState().getCollection(COLLECTION_NAME).getActiveSlices());\n  }\n\n","sourceOld":"  @Test\n  public void doTest() throws IOException, SolrServerException {\n    // SPLITSHARD is recommended to be run in async mode, so we default to that.\n    // Also, autoscale triggers use async with splits as well.\n    boolean doAsync = true;\n\n    CollectionAdminRequest\n        .createCollection(COLLECTION_NAME, \"conf\", 1, 1)\n        .setMaxShardsPerNode(100)\n        .process(cluster.getSolrClient());\n\n    cluster.waitForActiveCollection(COLLECTION_NAME, 1, 1);\n\n\n    CloudSolrClient client = cluster.getSolrClient();\n    client.setDefaultCollection(COLLECTION_NAME);\n\n    // splitting an empty collection by prefix should still work (i.e. fall back to old method of just dividing the hash range\n\n    CollectionAdminRequest.SplitShard splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setNumSubShards(2)\n        .setSplitByPrefix(true)\n        .setShardName(\"shard1\");\n    if (doAsync) {\n      splitShard.setAsyncId(\"SPLIT1\");\n    }\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(2, 3));  // expectedReplicas==3 because original replica still exists (just inactive)\n\n\n    List<Prefix> prefixes = findPrefixes(20, 0, 0x00ffffff);\n    List<Prefix> uniquePrefixes = removeDups(prefixes);\n    if (uniquePrefixes.size() % 2 == 1) {  // make it an even sized list so we can split it exactly in two\n      uniquePrefixes.remove(uniquePrefixes.size()-1);\n    }\n    log.info(\"Unique prefixes: \" + uniquePrefixes);\n\n    for (Prefix prefix : uniquePrefixes) {\n      client.add( getDoc(prefix.key, \"doc1\") );\n      client.add( getDoc(prefix.key, \"doc2\") );\n    }\n    client.commit();\n\n\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(\"shard1_1\");  // should start out with the range of 0-7fffffff\n    if (doAsync) {\n      splitShard.setAsyncId(\"SPLIT2\");\n    }\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(3, 5));\n\n    // OK, now let's check that the correct split point was chosen\n    // We can use the router to find the shards for the middle prefixes and they should be different.\n\n    DocCollection collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    Collection<Slice> slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(uniquePrefixes.size()/2 - 1).key, null, collection);\n    Collection<Slice> slices2 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(uniquePrefixes.size()/2    ).key, null, collection);\n\n    Slice slice1 = slices1.iterator().next();\n    Slice slice2 = slices2.iterator().next();\n\n    assertTrue(slices1.size() == 1 && slices2.size() == 1);\n    assertTrue(slice1 != slice2);\n\n\n    //\n    // now lets add enough documents to the first prefix to get it split out on its own\n    //\n    for (int i=0; i<uniquePrefixes.size(); i++) {\n      client.add(  getDoc(uniquePrefixes.get(0).key, \"doc\"+(i+100)));\n    }\n    client.commit();\n\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(slice1.getName());\n    if (doAsync) {\n      splitShard.setAsyncId(\"SPLIT3\");\n    }\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(4, 7));\n\n    collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(0).key, null, collection);\n    slices2 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(1).key, null, collection);\n\n    slice1 = slices1.iterator().next();\n    slice2 = slices2.iterator().next();\n\n    assertTrue(slices1.size() == 1 && slices2.size() == 1);\n    assertTrue(slice1 != slice2);\n\n\n    // Now if we call split (with splitByPrefix) on a shard that has a single prefix, it should split it in half\n\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(slice1.getName());\n    if (doAsync) {\n      splitShard.setAsyncId(\"SPLIT4\");\n    }\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(5, 9));\n\n    collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(0).key, null, collection);\n    slice1 = slices1.iterator().next();\n\n    assertTrue(slices1.size() == 2);\n\n    //\n    // split one more time, this time on a partial prefix/bucket\n    //\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(slice1.getName());\n    if (doAsync) {\n      splitShard.setAsyncId(\"SPLIT5\");\n    }\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(6, 11));\n\n    collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(0).key, null, collection);\n\n    assertTrue(slices1.size() == 3);\n\n    // System.err.println(\"### STATE=\" + cluster.getSolrClient().getZkStateReader().getClusterState().getCollection(COLLECTION_NAME));\n    // System.err.println(\"### getActiveSlices()=\" + cluster.getSolrClient().getZkStateReader().getClusterState().getCollection(COLLECTION_NAME).getActiveSlices());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e","date":1594223844,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/api/collections/SplitByPrefixTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/api/collections/SplitByPrefixTest#doTest().mjava","sourceNew":"  @Test\n  public void doTest() throws IOException, SolrServerException {\n    // SPLITSHARD is recommended to be run in async mode, so we default to that.\n    // Also, autoscale triggers use async with splits as well.\n    boolean doAsync = true;\n\n    CollectionAdminRequest\n        .createCollection(COLLECTION_NAME, \"conf\", 1, 1)\n        .process(cluster.getSolrClient());\n\n    cluster.waitForActiveCollection(COLLECTION_NAME, 1, 1);\n\n\n    CloudSolrClient client = cluster.getSolrClient();\n    client.setDefaultCollection(COLLECTION_NAME);\n\n    // splitting an empty collection by prefix should still work (i.e. fall back to old method of just dividing the hash range\n\n    CollectionAdminRequest.SplitShard splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setNumSubShards(2)\n        .setSplitByPrefix(true)\n        .setShardName(\"shard1\");\n    if (doAsync) {\n      splitShard.setAsyncId(\"SPLIT1\");\n    }\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(2, 3));  // expectedReplicas==3 because original replica still exists (just inactive)\n\n\n    List<Prefix> prefixes = findPrefixes(20, 0, 0x00ffffff);\n    List<Prefix> uniquePrefixes = removeDups(prefixes);\n    if (uniquePrefixes.size() % 2 == 1) {  // make it an even sized list so we can split it exactly in two\n      uniquePrefixes.remove(uniquePrefixes.size()-1);\n    }\n    log.info(\"Unique prefixes: {}\", uniquePrefixes);\n\n    for (Prefix prefix : uniquePrefixes) {\n      client.add( getDoc(prefix.key, \"doc1\") );\n      client.add( getDoc(prefix.key, \"doc2\") );\n    }\n    client.commit();\n\n\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(\"shard1_1\");  // should start out with the range of 0-7fffffff\n    if (doAsync) {\n      splitShard.setAsyncId(\"SPLIT2\");\n    }\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(3, 5));\n\n    // OK, now let's check that the correct split point was chosen\n    // We can use the router to find the shards for the middle prefixes and they should be different.\n\n    DocCollection collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    Collection<Slice> slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(uniquePrefixes.size()/2 - 1).key, null, collection);\n    Collection<Slice> slices2 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(uniquePrefixes.size()/2    ).key, null, collection);\n\n    Slice slice1 = slices1.iterator().next();\n    Slice slice2 = slices2.iterator().next();\n\n    assertTrue(slices1.size() == 1 && slices2.size() == 1);\n    assertTrue(slice1 != slice2);\n\n\n    //\n    // now lets add enough documents to the first prefix to get it split out on its own\n    //\n    for (int i=0; i<uniquePrefixes.size(); i++) {\n      client.add(  getDoc(uniquePrefixes.get(0).key, \"doc\"+(i+100)));\n    }\n    client.commit();\n\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(slice1.getName());\n    if (doAsync) {\n      splitShard.setAsyncId(\"SPLIT3\");\n    }\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(4, 7));\n\n    collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(0).key, null, collection);\n    slices2 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(1).key, null, collection);\n\n    slice1 = slices1.iterator().next();\n    slice2 = slices2.iterator().next();\n\n    assertTrue(slices1.size() == 1 && slices2.size() == 1);\n    assertTrue(slice1 != slice2);\n\n\n    // Now if we call split (with splitByPrefix) on a shard that has a single prefix, it should split it in half\n\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(slice1.getName());\n    if (doAsync) {\n      splitShard.setAsyncId(\"SPLIT4\");\n    }\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(5, 9));\n\n    collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(0).key, null, collection);\n    slice1 = slices1.iterator().next();\n\n    assertTrue(slices1.size() == 2);\n\n    //\n    // split one more time, this time on a partial prefix/bucket\n    //\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(slice1.getName());\n    if (doAsync) {\n      splitShard.setAsyncId(\"SPLIT5\");\n    }\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(6, 11));\n\n    collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(0).key, null, collection);\n\n    assertTrue(slices1.size() == 3);\n\n    // System.err.println(\"### STATE=\" + cluster.getSolrClient().getZkStateReader().getClusterState().getCollection(COLLECTION_NAME));\n    // System.err.println(\"### getActiveSlices()=\" + cluster.getSolrClient().getZkStateReader().getClusterState().getCollection(COLLECTION_NAME).getActiveSlices());\n  }\n\n","sourceOld":"  @Test\n  public void doTest() throws IOException, SolrServerException {\n    // SPLITSHARD is recommended to be run in async mode, so we default to that.\n    // Also, autoscale triggers use async with splits as well.\n    boolean doAsync = true;\n\n    CollectionAdminRequest\n        .createCollection(COLLECTION_NAME, \"conf\", 1, 1)\n        .setMaxShardsPerNode(100)\n        .process(cluster.getSolrClient());\n\n    cluster.waitForActiveCollection(COLLECTION_NAME, 1, 1);\n\n\n    CloudSolrClient client = cluster.getSolrClient();\n    client.setDefaultCollection(COLLECTION_NAME);\n\n    // splitting an empty collection by prefix should still work (i.e. fall back to old method of just dividing the hash range\n\n    CollectionAdminRequest.SplitShard splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setNumSubShards(2)\n        .setSplitByPrefix(true)\n        .setShardName(\"shard1\");\n    if (doAsync) {\n      splitShard.setAsyncId(\"SPLIT1\");\n    }\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(2, 3));  // expectedReplicas==3 because original replica still exists (just inactive)\n\n\n    List<Prefix> prefixes = findPrefixes(20, 0, 0x00ffffff);\n    List<Prefix> uniquePrefixes = removeDups(prefixes);\n    if (uniquePrefixes.size() % 2 == 1) {  // make it an even sized list so we can split it exactly in two\n      uniquePrefixes.remove(uniquePrefixes.size()-1);\n    }\n    log.info(\"Unique prefixes: {}\", uniquePrefixes);\n\n    for (Prefix prefix : uniquePrefixes) {\n      client.add( getDoc(prefix.key, \"doc1\") );\n      client.add( getDoc(prefix.key, \"doc2\") );\n    }\n    client.commit();\n\n\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(\"shard1_1\");  // should start out with the range of 0-7fffffff\n    if (doAsync) {\n      splitShard.setAsyncId(\"SPLIT2\");\n    }\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(3, 5));\n\n    // OK, now let's check that the correct split point was chosen\n    // We can use the router to find the shards for the middle prefixes and they should be different.\n\n    DocCollection collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    Collection<Slice> slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(uniquePrefixes.size()/2 - 1).key, null, collection);\n    Collection<Slice> slices2 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(uniquePrefixes.size()/2    ).key, null, collection);\n\n    Slice slice1 = slices1.iterator().next();\n    Slice slice2 = slices2.iterator().next();\n\n    assertTrue(slices1.size() == 1 && slices2.size() == 1);\n    assertTrue(slice1 != slice2);\n\n\n    //\n    // now lets add enough documents to the first prefix to get it split out on its own\n    //\n    for (int i=0; i<uniquePrefixes.size(); i++) {\n      client.add(  getDoc(uniquePrefixes.get(0).key, \"doc\"+(i+100)));\n    }\n    client.commit();\n\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(slice1.getName());\n    if (doAsync) {\n      splitShard.setAsyncId(\"SPLIT3\");\n    }\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(4, 7));\n\n    collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(0).key, null, collection);\n    slices2 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(1).key, null, collection);\n\n    slice1 = slices1.iterator().next();\n    slice2 = slices2.iterator().next();\n\n    assertTrue(slices1.size() == 1 && slices2.size() == 1);\n    assertTrue(slice1 != slice2);\n\n\n    // Now if we call split (with splitByPrefix) on a shard that has a single prefix, it should split it in half\n\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(slice1.getName());\n    if (doAsync) {\n      splitShard.setAsyncId(\"SPLIT4\");\n    }\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(5, 9));\n\n    collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(0).key, null, collection);\n    slice1 = slices1.iterator().next();\n\n    assertTrue(slices1.size() == 2);\n\n    //\n    // split one more time, this time on a partial prefix/bucket\n    //\n    splitShard = CollectionAdminRequest.splitShard(COLLECTION_NAME)\n        .setSplitByPrefix(true)\n        .setShardName(slice1.getName());\n    if (doAsync) {\n      splitShard.setAsyncId(\"SPLIT5\");\n    }\n    splitShard.process(client);\n    waitForState(\"Timed out waiting for sub shards to be active.\",\n        COLLECTION_NAME, activeClusterShape(6, 11));\n\n    collection = client.getZkStateReader().getClusterState().getCollection(COLLECTION_NAME);\n    slices1 = collection.getRouter().getSearchSlicesSingle(uniquePrefixes.get(0).key, null, collection);\n\n    assertTrue(slices1.size() == 3);\n\n    // System.err.println(\"### STATE=\" + cluster.getSolrClient().getZkStateReader().getClusterState().getCollection(COLLECTION_NAME));\n    // System.err.println(\"### getActiveSlices()=\" + cluster.getSolrClient().getZkStateReader().getClusterState().getCollection(COLLECTION_NAME).getActiveSlices());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"ebccdcfbac56fd8a05e781b2b8cf7a8f1d447e87":["36bdabc04743acfe0e82c9cf8208b1111b2b193a"],"fe9f2c4a0d7ac164e4bdd4eee7f87131aec83fd4":["ebccdcfbac56fd8a05e781b2b8cf7a8f1d447e87"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"06595b0c22c7d3075c4104d3820cccf95d9d8a43":["36bdabc04743acfe0e82c9cf8208b1111b2b193a","ebccdcfbac56fd8a05e781b2b8cf7a8f1d447e87"],"a05f3f5161c62339ec5560b8f6958f3df8483618":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"36bdabc04743acfe0e82c9cf8208b1111b2b193a":["a05f3f5161c62339ec5560b8f6958f3df8483618"],"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e":["fe9f2c4a0d7ac164e4bdd4eee7f87131aec83fd4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["e46a76bb135597b8bf35930cfdb3702bdd1cbe6e"]},"commit2Childs":{"ebccdcfbac56fd8a05e781b2b8cf7a8f1d447e87":["fe9f2c4a0d7ac164e4bdd4eee7f87131aec83fd4","06595b0c22c7d3075c4104d3820cccf95d9d8a43"],"fe9f2c4a0d7ac164e4bdd4eee7f87131aec83fd4":["e46a76bb135597b8bf35930cfdb3702bdd1cbe6e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a05f3f5161c62339ec5560b8f6958f3df8483618"],"06595b0c22c7d3075c4104d3820cccf95d9d8a43":[],"a05f3f5161c62339ec5560b8f6958f3df8483618":["36bdabc04743acfe0e82c9cf8208b1111b2b193a"],"36bdabc04743acfe0e82c9cf8208b1111b2b193a":["ebccdcfbac56fd8a05e781b2b8cf7a8f1d447e87","06595b0c22c7d3075c4104d3820cccf95d9d8a43"],"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["06595b0c22c7d3075c4104d3820cccf95d9d8a43","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}