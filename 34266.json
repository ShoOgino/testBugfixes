{"path":"src/test/org/apache/solr/analysis/TestPatternTokenizerFactory#testOffsetCorrection().mjava","commits":[{"id":"98b77b6a0a7c8d189695f0da38c58b5008a9863c","date":1252208742,"type":0,"author":"Koji Sekiguchi","isMerge":false,"pathNew":"src/test/org/apache/solr/analysis/TestPatternTokenizerFactory#testOffsetCorrection().mjava","pathOld":"/dev/null","sourceNew":"  public void testOffsetCorrection() throws Exception {\n    final String INPUT = \"G&uuml;nther G&uuml;nther is here\";\n\n    // create MappingCharFilter\n    MappingCharFilterFactory cfFactory = new MappingCharFilterFactory();\n    List<String> mappingRules = new ArrayList<String>();\n    mappingRules.add( \"\\\"&uuml;\\\" => \\\"ü\\\"\" );\n    NormalizeCharMap normMap = new NormalizeCharMap();\n    cfFactory.parseRules( mappingRules, normMap );\n    CharStream charStream = new MappingCharFilter( normMap, CharReader.get( new StringReader( INPUT ) ) );\n\n    // create PatternTokenizer\n    Map<String,String> args = new HashMap<String, String>();\n    args.put( PatternTokenizerFactory.PATTERN, \"[,;/\\\\s]+\" );\n    PatternTokenizerFactory tokFactory = new PatternTokenizerFactory();\n    tokFactory.init( args );\n    TokenStream stream = tokFactory.create( charStream );\n\n    List<Token> result = getTokens( stream );\n    List<Token> expect = tokens( \"Günther,1,0,12 Günther,1,13,25 is,1,26,28 here,1,29,33\" );\n    assertTokEqualOff( expect, result );\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28e349236232860728fc91596fa4a1ec2c64bde6","date":1253259442,"type":3,"author":"Koji Sekiguchi","isMerge":false,"pathNew":"src/test/org/apache/solr/analysis/TestPatternTokenizerFactory#testOffsetCorrection().mjava","pathOld":"src/test/org/apache/solr/analysis/TestPatternTokenizerFactory#testOffsetCorrection().mjava","sourceNew":"  public void testOffsetCorrection() throws Exception {\n    final String INPUT = \"G&uuml;nther G&uuml;nther is here\";\n\n    // create MappingCharFilter\n    MappingCharFilterFactory cfFactory = new MappingCharFilterFactory();\n    List<String> mappingRules = new ArrayList<String>();\n    mappingRules.add( \"\\\"&uuml;\\\" => \\\"ü\\\"\" );\n    NormalizeCharMap normMap = new NormalizeCharMap();\n    cfFactory.parseRules( mappingRules, normMap );\n    CharStream charStream = new MappingCharFilter( normMap, CharReader.get( new StringReader( INPUT ) ) );\n\n    // create PatternTokenizer\n    Map<String,String> args = new HashMap<String, String>();\n    args.put( PatternTokenizerFactory.PATTERN, \"[,;/\\\\s]+\" );\n    PatternTokenizerFactory tokFactory = new PatternTokenizerFactory();\n    tokFactory.init( args );\n    TokenStream stream = tokFactory.create( charStream );\n\n    List<Token> result = getTokens( stream );\n    List<Token> expect = tokens( \"Günther,1,0,12 Günther,1,13,25 is,1,26,28 here,1,29,33\" );\n    assertTokEqualOff( expect, result );\n    \n    charStream.reset();\n    args.put( PatternTokenizerFactory.PATTERN, \"Günther\" );\n    args.put( PatternTokenizerFactory.GROUP, \"0\" );\n    tokFactory = new PatternTokenizerFactory();\n    tokFactory.init( args );\n    stream = tokFactory.create( charStream );\n\n    result = getTokens( stream );\n    expect = tokens( \"Günther,1,0,12 Günther,1,13,25\" );\n    assertTokEqualOff( expect, result );\n  }\n\n","sourceOld":"  public void testOffsetCorrection() throws Exception {\n    final String INPUT = \"G&uuml;nther G&uuml;nther is here\";\n\n    // create MappingCharFilter\n    MappingCharFilterFactory cfFactory = new MappingCharFilterFactory();\n    List<String> mappingRules = new ArrayList<String>();\n    mappingRules.add( \"\\\"&uuml;\\\" => \\\"ü\\\"\" );\n    NormalizeCharMap normMap = new NormalizeCharMap();\n    cfFactory.parseRules( mappingRules, normMap );\n    CharStream charStream = new MappingCharFilter( normMap, CharReader.get( new StringReader( INPUT ) ) );\n\n    // create PatternTokenizer\n    Map<String,String> args = new HashMap<String, String>();\n    args.put( PatternTokenizerFactory.PATTERN, \"[,;/\\\\s]+\" );\n    PatternTokenizerFactory tokFactory = new PatternTokenizerFactory();\n    tokFactory.init( args );\n    TokenStream stream = tokFactory.create( charStream );\n\n    List<Token> result = getTokens( stream );\n    List<Token> expect = tokens( \"Günther,1,0,12 Günther,1,13,25 is,1,26,28 here,1,29,33\" );\n    assertTokEqualOff( expect, result );\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2fd023a662cc25ae7e0ad0f33d71c476a16d0579","date":1261403630,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"src/test/org/apache/solr/analysis/TestPatternTokenizerFactory#testOffsetCorrection().mjava","pathOld":"src/test/org/apache/solr/analysis/TestPatternTokenizerFactory#testOffsetCorrection().mjava","sourceNew":"  public void testOffsetCorrection() throws Exception {\n    final String INPUT = \"G&uuml;nther G&uuml;nther is here\";\n\n    // create MappingCharFilter\n    MappingCharFilterFactory cfFactory = new MappingCharFilterFactory();\n    List<String> mappingRules = new ArrayList<String>();\n    mappingRules.add( \"\\\"&uuml;\\\" => \\\"ü\\\"\" );\n    NormalizeCharMap normMap = new NormalizeCharMap();\n    cfFactory.parseRules( mappingRules, normMap );\n    CharStream charStream = new MappingCharFilter( normMap, CharReader.get( new StringReader( INPUT ) ) );\n\n    // create PatternTokenizer\n    Map<String,String> args = new HashMap<String, String>();\n    args.put( PatternTokenizerFactory.PATTERN, \"[,;/\\\\s]+\" );\n    PatternTokenizerFactory tokFactory = new PatternTokenizerFactory();\n    tokFactory.init( args );\n    TokenStream stream = tokFactory.create( charStream );\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\", \"is\", \"here\" },\n        new int[] { 0, 13, 26, 29 },\n        new int[] { 12, 25, 28, 33 },\n        new int[] { 1, 1, 1, 1 });\n    \n    charStream = new MappingCharFilter( normMap, CharReader.get( new StringReader( INPUT ) ) );\n    args.put( PatternTokenizerFactory.PATTERN, \"Günther\" );\n    args.put( PatternTokenizerFactory.GROUP, \"0\" );\n    tokFactory = new PatternTokenizerFactory();\n    tokFactory.init( args );\n    stream = tokFactory.create( charStream );\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\" },\n        new int[] { 0, 13 },\n        new int[] { 12, 25 },\n        new int[] { 1, 1 });\n  }\n\n","sourceOld":"  public void testOffsetCorrection() throws Exception {\n    final String INPUT = \"G&uuml;nther G&uuml;nther is here\";\n\n    // create MappingCharFilter\n    MappingCharFilterFactory cfFactory = new MappingCharFilterFactory();\n    List<String> mappingRules = new ArrayList<String>();\n    mappingRules.add( \"\\\"&uuml;\\\" => \\\"ü\\\"\" );\n    NormalizeCharMap normMap = new NormalizeCharMap();\n    cfFactory.parseRules( mappingRules, normMap );\n    CharStream charStream = new MappingCharFilter( normMap, CharReader.get( new StringReader( INPUT ) ) );\n\n    // create PatternTokenizer\n    Map<String,String> args = new HashMap<String, String>();\n    args.put( PatternTokenizerFactory.PATTERN, \"[,;/\\\\s]+\" );\n    PatternTokenizerFactory tokFactory = new PatternTokenizerFactory();\n    tokFactory.init( args );\n    TokenStream stream = tokFactory.create( charStream );\n\n    List<Token> result = getTokens( stream );\n    List<Token> expect = tokens( \"Günther,1,0,12 Günther,1,13,25 is,1,26,28 here,1,29,33\" );\n    assertTokEqualOff( expect, result );\n    \n    charStream.reset();\n    args.put( PatternTokenizerFactory.PATTERN, \"Günther\" );\n    args.put( PatternTokenizerFactory.GROUP, \"0\" );\n    tokFactory = new PatternTokenizerFactory();\n    tokFactory.init( args );\n    stream = tokFactory.create( charStream );\n\n    result = getTokens( stream );\n    expect = tokens( \"Günther,1,0,12 Günther,1,13,25\" );\n    assertTokEqualOff( expect, result );\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"434d3dd2a84fe27b3101b21528019c74c8534e03","date":1268610420,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"src/test/org/apache/solr/analysis/TestPatternTokenizerFactory#testOffsetCorrection().mjava","pathOld":"src/test/org/apache/solr/analysis/TestPatternTokenizerFactory#testOffsetCorrection().mjava","sourceNew":"  public void testOffsetCorrection() throws Exception {\n    final String INPUT = \"G&uuml;nther G&uuml;nther is here\";\n\n    // create MappingCharFilter\n    MappingCharFilterFactory cfFactory = new MappingCharFilterFactory();\n    List<String> mappingRules = new ArrayList<String>();\n    mappingRules.add( \"\\\"&uuml;\\\" => \\\"ü\\\"\" );\n    NormalizeCharMap normMap = new NormalizeCharMap();\n    cfFactory.parseRules( mappingRules, normMap );\n    CharStream charStream = new MappingCharFilter( normMap, CharReader.get( new StringReader( INPUT ) ) );\n\n    // create PatternTokenizer\n    Map<String,String> args = new HashMap<String, String>();\n    args.put( PatternTokenizerFactory.PATTERN, \"[,;/\\\\s]+\" );\n    PatternTokenizerFactory tokFactory = new PatternTokenizerFactory();\n    tokFactory.init( args );\n    TokenStream stream = tokFactory.create( charStream );\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\", \"is\", \"here\" },\n        new int[] { 0, 13, 26, 29 },\n        new int[] { 12, 25, 28, 33 });\n    \n    charStream = new MappingCharFilter( normMap, CharReader.get( new StringReader( INPUT ) ) );\n    args.put( PatternTokenizerFactory.PATTERN, \"Günther\" );\n    args.put( PatternTokenizerFactory.GROUP, \"0\" );\n    tokFactory = new PatternTokenizerFactory();\n    tokFactory.init( args );\n    stream = tokFactory.create( charStream );\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\" },\n        new int[] { 0, 13 },\n        new int[] { 12, 25 });\n  }\n\n","sourceOld":"  public void testOffsetCorrection() throws Exception {\n    final String INPUT = \"G&uuml;nther G&uuml;nther is here\";\n\n    // create MappingCharFilter\n    MappingCharFilterFactory cfFactory = new MappingCharFilterFactory();\n    List<String> mappingRules = new ArrayList<String>();\n    mappingRules.add( \"\\\"&uuml;\\\" => \\\"ü\\\"\" );\n    NormalizeCharMap normMap = new NormalizeCharMap();\n    cfFactory.parseRules( mappingRules, normMap );\n    CharStream charStream = new MappingCharFilter( normMap, CharReader.get( new StringReader( INPUT ) ) );\n\n    // create PatternTokenizer\n    Map<String,String> args = new HashMap<String, String>();\n    args.put( PatternTokenizerFactory.PATTERN, \"[,;/\\\\s]+\" );\n    PatternTokenizerFactory tokFactory = new PatternTokenizerFactory();\n    tokFactory.init( args );\n    TokenStream stream = tokFactory.create( charStream );\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\", \"is\", \"here\" },\n        new int[] { 0, 13, 26, 29 },\n        new int[] { 12, 25, 28, 33 },\n        new int[] { 1, 1, 1, 1 });\n    \n    charStream = new MappingCharFilter( normMap, CharReader.get( new StringReader( INPUT ) ) );\n    args.put( PatternTokenizerFactory.PATTERN, \"Günther\" );\n    args.put( PatternTokenizerFactory.GROUP, \"0\" );\n    tokFactory = new PatternTokenizerFactory();\n    tokFactory.init( args );\n    stream = tokFactory.create( charStream );\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\" },\n        new int[] { 0, 13 },\n        new int[] { 12, 25 },\n        new int[] { 1, 1 });\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ad94625fb8d088209f46650c8097196fec67f00c","date":1453508319,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/src/test/org/apache/solr/analysis/TestPatternTokenizerFactory#testOffsetCorrection().mjava","pathOld":"src/test/org/apache/solr/analysis/TestPatternTokenizerFactory#testOffsetCorrection().mjava","sourceNew":"  public void testOffsetCorrection() throws Exception {\n    final String INPUT = \"G&uuml;nther G&uuml;nther is here\";\n\n    // create MappingCharFilter\n    MappingCharFilterFactory cfFactory = new MappingCharFilterFactory();\n    List<String> mappingRules = new ArrayList<String>();\n    mappingRules.add( \"\\\"&uuml;\\\" => \\\"ü\\\"\" );\n    NormalizeCharMap normMap = new NormalizeCharMap();\n    cfFactory.parseRules( mappingRules, normMap );\n    CharStream charStream = new MappingCharFilter( normMap, CharReader.get( new StringReader( INPUT ) ) );\n\n    // create PatternTokenizer\n    Map<String,String> args = new HashMap<String, String>();\n    args.put( PatternTokenizerFactory.PATTERN, \"[,;/\\\\s]+\" );\n    PatternTokenizerFactory tokFactory = new PatternTokenizerFactory();\n    tokFactory.init( args );\n    TokenStream stream = tokFactory.create( charStream );\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\", \"is\", \"here\" },\n        new int[] { 0, 13, 26, 29 },\n        new int[] { 12, 25, 28, 33 });\n    \n    charStream = new MappingCharFilter( normMap, CharReader.get( new StringReader( INPUT ) ) );\n    args.put( PatternTokenizerFactory.PATTERN, \"Günther\" );\n    args.put( PatternTokenizerFactory.GROUP, \"0\" );\n    tokFactory = new PatternTokenizerFactory();\n    tokFactory.init( args );\n    stream = tokFactory.create( charStream );\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\" },\n        new int[] { 0, 13 },\n        new int[] { 12, 25 });\n  }\n\n","sourceOld":"  public void testOffsetCorrection() throws Exception {\n    final String INPUT = \"G&uuml;nther G&uuml;nther is here\";\n\n    // create MappingCharFilter\n    MappingCharFilterFactory cfFactory = new MappingCharFilterFactory();\n    List<String> mappingRules = new ArrayList<String>();\n    mappingRules.add( \"\\\"&uuml;\\\" => \\\"ü\\\"\" );\n    NormalizeCharMap normMap = new NormalizeCharMap();\n    cfFactory.parseRules( mappingRules, normMap );\n    CharStream charStream = new MappingCharFilter( normMap, CharReader.get( new StringReader( INPUT ) ) );\n\n    // create PatternTokenizer\n    Map<String,String> args = new HashMap<String, String>();\n    args.put( PatternTokenizerFactory.PATTERN, \"[,;/\\\\s]+\" );\n    PatternTokenizerFactory tokFactory = new PatternTokenizerFactory();\n    tokFactory.init( args );\n    TokenStream stream = tokFactory.create( charStream );\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\", \"is\", \"here\" },\n        new int[] { 0, 13, 26, 29 },\n        new int[] { 12, 25, 28, 33 });\n    \n    charStream = new MappingCharFilter( normMap, CharReader.get( new StringReader( INPUT ) ) );\n    args.put( PatternTokenizerFactory.PATTERN, \"Günther\" );\n    args.put( PatternTokenizerFactory.GROUP, \"0\" );\n    tokFactory = new PatternTokenizerFactory();\n    tokFactory.init( args );\n    stream = tokFactory.create( charStream );\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\" },\n        new int[] { 0, 13 },\n        new int[] { 12, 25 });\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"28e349236232860728fc91596fa4a1ec2c64bde6":["98b77b6a0a7c8d189695f0da38c58b5008a9863c"],"98b77b6a0a7c8d189695f0da38c58b5008a9863c":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":[],"ad94625fb8d088209f46650c8097196fec67f00c":["434d3dd2a84fe27b3101b21528019c74c8534e03"],"434d3dd2a84fe27b3101b21528019c74c8534e03":["2fd023a662cc25ae7e0ad0f33d71c476a16d0579"],"2fd023a662cc25ae7e0ad0f33d71c476a16d0579":["28e349236232860728fc91596fa4a1ec2c64bde6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"28e349236232860728fc91596fa4a1ec2c64bde6":["2fd023a662cc25ae7e0ad0f33d71c476a16d0579"],"98b77b6a0a7c8d189695f0da38c58b5008a9863c":["28e349236232860728fc91596fa4a1ec2c64bde6"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":["98b77b6a0a7c8d189695f0da38c58b5008a9863c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"ad94625fb8d088209f46650c8097196fec67f00c":[],"434d3dd2a84fe27b3101b21528019c74c8534e03":["ad94625fb8d088209f46650c8097196fec67f00c"],"2fd023a662cc25ae7e0ad0f33d71c476a16d0579":["434d3dd2a84fe27b3101b21528019c74c8534e03"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["ad94625fb8d088209f46650c8097196fec67f00c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}