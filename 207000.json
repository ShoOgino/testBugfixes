{"path":"contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/WordTokenizer#WordTokenizer(TokenStream,WordSegmenter).mjava","commits":[{"id":"05ff0cc6e864c7d71a48579f2acfca4f58943568","date":1242295762,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/WordTokenizer#WordTokenizer(TokenStream,WordSegmenter).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * 设计上是SentenceTokenizer的下一处理层。将SentenceTokenizer的句子读出，\n   * 利用HHMMSegment主程序将句子分词，然后将分词结果返回。\n   * \n   * @param in 句子的Token\n   * @param smooth 平滑函数\n   * @param dataPath 装载核心字典与二叉字典的目录\n   * @see init()\n   */\n  public WordTokenizer(TokenStream in, WordSegmenter wordSegmenter) {\n    this.in = in;\n    this.wordSegmenter = wordSegmenter;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"be5ef2f970a6c1141562b06dd26ed04c3dc29d70","date":1246444343,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/WordTokenizer#WordTokenizer(TokenStream,WordSegmenter).mjava","pathOld":"contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/WordTokenizer#WordTokenizer(TokenStream,WordSegmenter).mjava","sourceNew":"  /**\n   * Construct a new WordTokenizer.\n   * \n   * @param in {@link TokenStream} of sentences\n   * @param wordSegmenter {@link WordSegmenter} to break sentences into words \n   */\n  public WordTokenizer(TokenStream in, WordSegmenter wordSegmenter) {\n    this.in = in;\n    this.wordSegmenter = wordSegmenter;\n  }\n\n","sourceOld":"  /**\n   * 设计上是SentenceTokenizer的下一处理层。将SentenceTokenizer的句子读出，\n   * 利用HHMMSegment主程序将句子分词，然后将分词结果返回。\n   * \n   * @param in 句子的Token\n   * @param smooth 平滑函数\n   * @param dataPath 装载核心字典与二叉字典的目录\n   * @see init()\n   */\n  public WordTokenizer(TokenStream in, WordSegmenter wordSegmenter) {\n    this.in = in;\n    this.wordSegmenter = wordSegmenter;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"dd745d580729e528151b58aeda87ef82f1b95c9b","date":1248369082,"type":4,"author":"Simon Willnauer","isMerge":false,"pathNew":"/dev/null","pathOld":"contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/WordTokenizer#WordTokenizer(TokenStream,WordSegmenter).mjava","sourceNew":null,"sourceOld":"  /**\n   * Construct a new WordTokenizer.\n   * \n   * @param in {@link TokenStream} of sentences\n   * @param wordSegmenter {@link WordSegmenter} to break sentences into words \n   */\n  public WordTokenizer(TokenStream in, WordSegmenter wordSegmenter) {\n    this.in = in;\n    this.wordSegmenter = wordSegmenter;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"be5ef2f970a6c1141562b06dd26ed04c3dc29d70":["05ff0cc6e864c7d71a48579f2acfca4f58943568"],"dd745d580729e528151b58aeda87ef82f1b95c9b":["be5ef2f970a6c1141562b06dd26ed04c3dc29d70"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"05ff0cc6e864c7d71a48579f2acfca4f58943568":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["dd745d580729e528151b58aeda87ef82f1b95c9b"]},"commit2Childs":{"be5ef2f970a6c1141562b06dd26ed04c3dc29d70":["dd745d580729e528151b58aeda87ef82f1b95c9b"],"dd745d580729e528151b58aeda87ef82f1b95c9b":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["05ff0cc6e864c7d71a48579f2acfca4f58943568"],"05ff0cc6e864c7d71a48579f2acfca4f58943568":["be5ef2f970a6c1141562b06dd26ed04c3dc29d70"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}