{"path":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","commits":[{"id":"2c007e7c4cf8c55bc2a5884e315123afaaeec87f","date":1327520966,"type":0,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n    \n\n    log.info(\"Attempting to update \" + ZkStateReader.CLUSTER_STATE + \" version \"\n        + null);\n    CloudState state = CloudState.load(zkClient, zkStateReader.getCloudState().getLiveNodes());\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n    props.put(ZkStateReader.ROLES_PROP, cloudDesc.getRoles());\n    props.put(ZkStateReader.STATE_PROP, ZkStateReader.DOWN);\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    // we only put a subset of props into the leader node\n    ZkNodeProps leaderProps = new ZkNodeProps(ZkStateReader.BASE_URL_PROP,\n        props.get(ZkStateReader.BASE_URL_PROP), ZkStateReader.CORE_NAME_PROP,\n        props.get(ZkStateReader.CORE_NAME_PROP), ZkStateReader.NODE_NAME_PROP,\n        props.get(ZkStateReader.NODE_NAME_PROP));\n    \n\n    joinElection(collection, coreZkNodeName, shardId, leaderProps);\n    \n    String leaderUrl = zkStateReader.getLeaderUrl(collection,\n        cloudDesc.getShardId(), 30000);\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    if (cc != null) { // CoreContainer only null in tests\n      try {\n        core = cc.getCore(desc.getName());\n\n        boolean startRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n            collection, coreZkNodeName, shardId, leaderProps, core, cc);\n        if (!startRecovery) {\n          publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n        }\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n    } else {\n      publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateCloudState(true);\n\n    return shardId;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["c516f1fb8a8bbd4b308968d3f638304888ce86d1"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d","date":1327523564,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n    \n\n    log.info(\"Attempting to update \" + ZkStateReader.CLUSTER_STATE + \" version \"\n        + null);\n    CloudState state = CloudState.load(zkClient, zkStateReader.getCloudState().getLiveNodes());\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n    props.put(ZkStateReader.ROLES_PROP, cloudDesc.getRoles());\n    props.put(ZkStateReader.STATE_PROP, ZkStateReader.DOWN);\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    // we only put a subset of props into the leader node\n    ZkNodeProps leaderProps = new ZkNodeProps(ZkStateReader.BASE_URL_PROP,\n        props.get(ZkStateReader.BASE_URL_PROP), ZkStateReader.CORE_NAME_PROP,\n        props.get(ZkStateReader.CORE_NAME_PROP), ZkStateReader.NODE_NAME_PROP,\n        props.get(ZkStateReader.NODE_NAME_PROP));\n    \n\n    joinElection(collection, coreZkNodeName, shardId, leaderProps);\n    \n    String leaderUrl = zkStateReader.getLeaderUrl(collection,\n        cloudDesc.getShardId(), 30000);\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    if (cc != null) { // CoreContainer only null in tests\n      try {\n        core = cc.getCore(desc.getName());\n\n        boolean startRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n            collection, coreZkNodeName, shardId, leaderProps, core, cc);\n        if (!startRecovery) {\n          publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n        }\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n    } else {\n      publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateCloudState(true);\n\n    return shardId;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0d22ac6a4146774c1bc8400160fc0b6150294e92","date":1327528604,"type":0,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n    \n\n    log.info(\"Attempting to update \" + ZkStateReader.CLUSTER_STATE + \" version \"\n        + null);\n    CloudState state = CloudState.load(zkClient, zkStateReader.getCloudState().getLiveNodes());\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n    props.put(ZkStateReader.ROLES_PROP, cloudDesc.getRoles());\n    props.put(ZkStateReader.STATE_PROP, ZkStateReader.DOWN);\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    // we only put a subset of props into the leader node\n    ZkNodeProps leaderProps = new ZkNodeProps(ZkStateReader.BASE_URL_PROP,\n        props.get(ZkStateReader.BASE_URL_PROP), ZkStateReader.CORE_NAME_PROP,\n        props.get(ZkStateReader.CORE_NAME_PROP), ZkStateReader.NODE_NAME_PROP,\n        props.get(ZkStateReader.NODE_NAME_PROP));\n    \n\n    joinElection(collection, coreZkNodeName, shardId, leaderProps);\n    \n    String leaderUrl = zkStateReader.getLeaderUrl(collection,\n        cloudDesc.getShardId(), 30000);\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    if (cc != null) { // CoreContainer only null in tests\n      try {\n        core = cc.getCore(desc.getName());\n\n        boolean startRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n            collection, coreZkNodeName, shardId, leaderProps, core, cc);\n        if (!startRecovery) {\n          publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n        }\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n    } else {\n      publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateCloudState(true);\n\n    return shardId;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9981f0cb0e2c6b6512a483e09fa27c840ad08e36","date":1327591744,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","sourceNew":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n    \n\n    log.info(\"Attempting to update \" + ZkStateReader.CLUSTER_STATE + \" version \"\n        + null);\n    CloudState state = CloudState.load(zkClient, zkStateReader.getCloudState().getLiveNodes());\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n    props.put(ZkStateReader.ROLES_PROP, cloudDesc.getRoles());\n    props.put(ZkStateReader.STATE_PROP, ZkStateReader.DOWN);\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    // we only put a subset of props into the leader node\n    ZkNodeProps leaderProps = new ZkNodeProps(ZkStateReader.BASE_URL_PROP,\n        props.get(ZkStateReader.BASE_URL_PROP), ZkStateReader.CORE_NAME_PROP,\n        props.get(ZkStateReader.CORE_NAME_PROP), ZkStateReader.NODE_NAME_PROP,\n        props.get(ZkStateReader.NODE_NAME_PROP));\n    \n\n    joinElection(collection, coreZkNodeName, shardId, leaderProps);\n    \n    String leaderUrl = zkStateReader.getLeaderUrl(collection,\n        cloudDesc.getShardId(), 30000);\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    if (cc != null) { // CoreContainer only null in tests\n      try {\n        core = cc.getCore(desc.getName());\n\n        if (isLeader) {\n          // recover from local transaction log and wait for it to complete before\n          // going active\n          // TODO: should this be moved to another thread? To recoveryStrat?\n          // TODO: should this actually be done earlier, before (or as part of)\n          // leader election perhaps?\n          // TODO: ensure that a replica that is trying to recover waits until I'm\n          // active (or don't make me the\n          // leader until my local replay is done. But this replay is only needed\n          // on the leader - replicas\n          // will do recovery anyway\n          \n          UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n          if (!core.isReloaded() && ulog != null) {\n            Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()\n                .getUpdateLog().recoverFromLog();\n            if (recoveryFuture != null) {\n              recoveryFuture.get(); // NOTE: this could potentially block for\n                                    // minutes or more!\n              // TODO: public as recovering in the mean time?\n            }\n          }\n        }\n        \n        boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n            collection, coreZkNodeName, shardId, leaderProps, core, cc);\n        if (didRecovery) {\n          publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n        }\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n    } else {\n      publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateCloudState(true);\n\n    return shardId;\n  }\n\n","sourceOld":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n    \n\n    log.info(\"Attempting to update \" + ZkStateReader.CLUSTER_STATE + \" version \"\n        + null);\n    CloudState state = CloudState.load(zkClient, zkStateReader.getCloudState().getLiveNodes());\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n    props.put(ZkStateReader.ROLES_PROP, cloudDesc.getRoles());\n    props.put(ZkStateReader.STATE_PROP, ZkStateReader.DOWN);\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    // we only put a subset of props into the leader node\n    ZkNodeProps leaderProps = new ZkNodeProps(ZkStateReader.BASE_URL_PROP,\n        props.get(ZkStateReader.BASE_URL_PROP), ZkStateReader.CORE_NAME_PROP,\n        props.get(ZkStateReader.CORE_NAME_PROP), ZkStateReader.NODE_NAME_PROP,\n        props.get(ZkStateReader.NODE_NAME_PROP));\n    \n\n    joinElection(collection, coreZkNodeName, shardId, leaderProps);\n    \n    String leaderUrl = zkStateReader.getLeaderUrl(collection,\n        cloudDesc.getShardId(), 30000);\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    if (cc != null) { // CoreContainer only null in tests\n      try {\n        core = cc.getCore(desc.getName());\n\n        boolean startRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n            collection, coreZkNodeName, shardId, leaderProps, core, cc);\n        if (!startRecovery) {\n          publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n        }\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n    } else {\n      publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateCloudState(true);\n\n    return shardId;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f97bf7e60e8b1531f7e992482be43dd563dde04e","date":1327594643,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","sourceNew":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return the shardId for the SolrCore\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n    \n\n    log.info(\"Attempting to update \" + ZkStateReader.CLUSTER_STATE + \" version \"\n        + null);\n    CloudState state = CloudState.load(zkClient, zkStateReader.getCloudState().getLiveNodes());\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n    props.put(ZkStateReader.ROLES_PROP, cloudDesc.getRoles());\n    props.put(ZkStateReader.STATE_PROP, ZkStateReader.DOWN);\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    // we only put a subset of props into the leader node\n    ZkNodeProps leaderProps = new ZkNodeProps(ZkStateReader.BASE_URL_PROP,\n        props.get(ZkStateReader.BASE_URL_PROP), ZkStateReader.CORE_NAME_PROP,\n        props.get(ZkStateReader.CORE_NAME_PROP), ZkStateReader.NODE_NAME_PROP,\n        props.get(ZkStateReader.NODE_NAME_PROP));\n    \n\n    joinElection(collection, coreZkNodeName, shardId, leaderProps);\n    \n    String leaderUrl = zkStateReader.getLeaderUrl(collection,\n        cloudDesc.getShardId(), 30000);\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    if (cc != null) { // CoreContainer only null in tests\n      try {\n        core = cc.getCore(desc.getName());\n\n        if (isLeader) {\n          // recover from local transaction log and wait for it to complete before\n          // going active\n          // TODO: should this be moved to another thread? To recoveryStrat?\n          // TODO: should this actually be done earlier, before (or as part of)\n          // leader election perhaps?\n          // TODO: ensure that a replica that is trying to recover waits until I'm\n          // active (or don't make me the\n          // leader until my local replay is done. But this replay is only needed\n          // on the leader - replicas\n          // will do recovery anyway\n          \n          UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n          if (!core.isReloaded() && ulog != null) {\n            Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()\n                .getUpdateLog().recoverFromLog();\n            if (recoveryFuture != null) {\n              recoveryFuture.get(); // NOTE: this could potentially block for\n                                    // minutes or more!\n              // TODO: public as recovering in the mean time?\n            }\n          }\n        }\n        \n        boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n            collection, coreZkNodeName, shardId, leaderProps, core, cc);\n        if (didRecovery) {\n          publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n        }\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n    } else {\n      publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateCloudState(true);\n\n    return shardId;\n  }\n\n","sourceOld":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n    \n\n    log.info(\"Attempting to update \" + ZkStateReader.CLUSTER_STATE + \" version \"\n        + null);\n    CloudState state = CloudState.load(zkClient, zkStateReader.getCloudState().getLiveNodes());\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n    props.put(ZkStateReader.ROLES_PROP, cloudDesc.getRoles());\n    props.put(ZkStateReader.STATE_PROP, ZkStateReader.DOWN);\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    // we only put a subset of props into the leader node\n    ZkNodeProps leaderProps = new ZkNodeProps(ZkStateReader.BASE_URL_PROP,\n        props.get(ZkStateReader.BASE_URL_PROP), ZkStateReader.CORE_NAME_PROP,\n        props.get(ZkStateReader.CORE_NAME_PROP), ZkStateReader.NODE_NAME_PROP,\n        props.get(ZkStateReader.NODE_NAME_PROP));\n    \n\n    joinElection(collection, coreZkNodeName, shardId, leaderProps);\n    \n    String leaderUrl = zkStateReader.getLeaderUrl(collection,\n        cloudDesc.getShardId(), 30000);\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    if (cc != null) { // CoreContainer only null in tests\n      try {\n        core = cc.getCore(desc.getName());\n\n        if (isLeader) {\n          // recover from local transaction log and wait for it to complete before\n          // going active\n          // TODO: should this be moved to another thread? To recoveryStrat?\n          // TODO: should this actually be done earlier, before (or as part of)\n          // leader election perhaps?\n          // TODO: ensure that a replica that is trying to recover waits until I'm\n          // active (or don't make me the\n          // leader until my local replay is done. But this replay is only needed\n          // on the leader - replicas\n          // will do recovery anyway\n          \n          UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n          if (!core.isReloaded() && ulog != null) {\n            Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()\n                .getUpdateLog().recoverFromLog();\n            if (recoveryFuture != null) {\n              recoveryFuture.get(); // NOTE: this could potentially block for\n                                    // minutes or more!\n              // TODO: public as recovering in the mean time?\n            }\n          }\n        }\n        \n        boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n            collection, coreZkNodeName, shardId, leaderProps, core, cc);\n        if (didRecovery) {\n          publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n        }\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n    } else {\n      publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateCloudState(true);\n\n    return shardId;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5dd811388ff9e790b1bf628567bdf6da4ca24cb8","date":1327610284,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","sourceNew":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return the shardId for the SolrCore\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n    \n\n    log.info(\"Attempting to update \" + ZkStateReader.CLUSTER_STATE + \" version \"\n        + null);\n    CloudState state = CloudState.load(zkClient, zkStateReader.getCloudState().getLiveNodes());\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n    props.put(ZkStateReader.ROLES_PROP, cloudDesc.getRoles());\n    props.put(ZkStateReader.STATE_PROP, ZkStateReader.DOWN);\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    // we only put a subset of props into the leader node\n    ZkNodeProps leaderProps = new ZkNodeProps(ZkStateReader.BASE_URL_PROP,\n        props.get(ZkStateReader.BASE_URL_PROP), ZkStateReader.CORE_NAME_PROP,\n        props.get(ZkStateReader.CORE_NAME_PROP), ZkStateReader.NODE_NAME_PROP,\n        props.get(ZkStateReader.NODE_NAME_PROP));\n    \n\n    joinElection(collection, coreZkNodeName, shardId, leaderProps);\n    \n    String leaderUrl = zkStateReader.getLeaderUrl(collection,\n        cloudDesc.getShardId(), 30000);\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    if (cc != null) { // CoreContainer only null in tests\n      try {\n        core = cc.getCore(desc.getName());\n\n        if (isLeader) {\n          // recover from local transaction log and wait for it to complete before\n          // going active\n          // TODO: should this be moved to another thread? To recoveryStrat?\n          // TODO: should this actually be done earlier, before (or as part of)\n          // leader election perhaps?\n          // TODO: ensure that a replica that is trying to recover waits until I'm\n          // active (or don't make me the\n          // leader until my local replay is done. But this replay is only needed\n          // on the leader - replicas\n          // will do recovery anyway\n          \n          UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n          if (!core.isReloaded() && ulog != null) {\n            Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()\n                .getUpdateLog().recoverFromLog();\n            if (recoveryFuture != null) {\n              recoveryFuture.get(); // NOTE: this could potentially block for\n                                    // minutes or more!\n              // TODO: public as recovering in the mean time?\n            }\n          }\n        }\n        \n        boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n            collection, coreZkNodeName, shardId, leaderProps, core, cc);\n        if (!didRecovery) {\n          publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n        }\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n    } else {\n      publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateCloudState(true);\n\n    return shardId;\n  }\n\n","sourceOld":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return the shardId for the SolrCore\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n    \n\n    log.info(\"Attempting to update \" + ZkStateReader.CLUSTER_STATE + \" version \"\n        + null);\n    CloudState state = CloudState.load(zkClient, zkStateReader.getCloudState().getLiveNodes());\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n    props.put(ZkStateReader.ROLES_PROP, cloudDesc.getRoles());\n    props.put(ZkStateReader.STATE_PROP, ZkStateReader.DOWN);\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    // we only put a subset of props into the leader node\n    ZkNodeProps leaderProps = new ZkNodeProps(ZkStateReader.BASE_URL_PROP,\n        props.get(ZkStateReader.BASE_URL_PROP), ZkStateReader.CORE_NAME_PROP,\n        props.get(ZkStateReader.CORE_NAME_PROP), ZkStateReader.NODE_NAME_PROP,\n        props.get(ZkStateReader.NODE_NAME_PROP));\n    \n\n    joinElection(collection, coreZkNodeName, shardId, leaderProps);\n    \n    String leaderUrl = zkStateReader.getLeaderUrl(collection,\n        cloudDesc.getShardId(), 30000);\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    if (cc != null) { // CoreContainer only null in tests\n      try {\n        core = cc.getCore(desc.getName());\n\n        if (isLeader) {\n          // recover from local transaction log and wait for it to complete before\n          // going active\n          // TODO: should this be moved to another thread? To recoveryStrat?\n          // TODO: should this actually be done earlier, before (or as part of)\n          // leader election perhaps?\n          // TODO: ensure that a replica that is trying to recover waits until I'm\n          // active (or don't make me the\n          // leader until my local replay is done. But this replay is only needed\n          // on the leader - replicas\n          // will do recovery anyway\n          \n          UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n          if (!core.isReloaded() && ulog != null) {\n            Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()\n                .getUpdateLog().recoverFromLog();\n            if (recoveryFuture != null) {\n              recoveryFuture.get(); // NOTE: this could potentially block for\n                                    // minutes or more!\n              // TODO: public as recovering in the mean time?\n            }\n          }\n        }\n        \n        boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n            collection, coreZkNodeName, shardId, leaderProps, core, cc);\n        if (didRecovery) {\n          publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n        }\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n    } else {\n      publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateCloudState(true);\n\n    return shardId;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"78a55f24d9b493c2a1cecf79f1d78279062b545b","date":1327688152,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","sourceNew":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return the shardId for the SolrCore\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n    \n\n    log.info(\"Attempting to update \" + ZkStateReader.CLUSTER_STATE + \" version \"\n        + null);\n    CloudState state = CloudState.load(zkClient, zkStateReader.getCloudState().getLiveNodes());\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n    props.put(ZkStateReader.ROLES_PROP, cloudDesc.getRoles());\n    props.put(ZkStateReader.STATE_PROP, ZkStateReader.DOWN);\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    // we only put a subset of props into the leader node\n    ZkNodeProps leaderProps = new ZkNodeProps(ZkStateReader.BASE_URL_PROP,\n        props.get(ZkStateReader.BASE_URL_PROP), ZkStateReader.CORE_NAME_PROP,\n        props.get(ZkStateReader.CORE_NAME_PROP), ZkStateReader.NODE_NAME_PROP,\n        props.get(ZkStateReader.NODE_NAME_PROP));\n    \n\n    joinElection(collection, coreZkNodeName, shardId, leaderProps);\n    \n    String leaderUrl = zkStateReader.getLeaderUrl(collection,\n        cloudDesc.getShardId(), 30000);\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    if (cc != null) { // CoreContainer only null in tests\n      try {\n        core = cc.getCore(desc.getName());\n\n        if (isLeader) {\n          // recover from local transaction log and wait for it to complete before\n          // going active\n          // TODO: should this be moved to another thread? To recoveryStrat?\n          // TODO: should this actually be done earlier, before (or as part of)\n          // leader election perhaps?\n          // TODO: ensure that a replica that is trying to recover waits until I'm\n          // active (or don't make me the\n          // leader until my local replay is done. But this replay is only needed\n          // on the leader - replicas\n          // will do recovery anyway\n          \n          UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n          if (!core.isReloaded() && ulog != null) {\n            Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()\n                .getUpdateLog().recoverFromLog();\n            if (recoveryFuture != null) {\n              recoveryFuture.get(); // NOTE: this could potentially block for\n                                    // minutes or more!\n              // TODO: public as recovering in the mean time?\n            }\n          }\n        }\n        \n        boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n            collection, coreZkNodeName, shardId, leaderProps, core, cc);\n        if (!didRecovery) {\n          publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n        }\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n    } else {\n      publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateCloudState(true);\n\n    return shardId;\n  }\n\n","sourceOld":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n    \n\n    log.info(\"Attempting to update \" + ZkStateReader.CLUSTER_STATE + \" version \"\n        + null);\n    CloudState state = CloudState.load(zkClient, zkStateReader.getCloudState().getLiveNodes());\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n    props.put(ZkStateReader.ROLES_PROP, cloudDesc.getRoles());\n    props.put(ZkStateReader.STATE_PROP, ZkStateReader.DOWN);\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    // we only put a subset of props into the leader node\n    ZkNodeProps leaderProps = new ZkNodeProps(ZkStateReader.BASE_URL_PROP,\n        props.get(ZkStateReader.BASE_URL_PROP), ZkStateReader.CORE_NAME_PROP,\n        props.get(ZkStateReader.CORE_NAME_PROP), ZkStateReader.NODE_NAME_PROP,\n        props.get(ZkStateReader.NODE_NAME_PROP));\n    \n\n    joinElection(collection, coreZkNodeName, shardId, leaderProps);\n    \n    String leaderUrl = zkStateReader.getLeaderUrl(collection,\n        cloudDesc.getShardId(), 30000);\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    if (cc != null) { // CoreContainer only null in tests\n      try {\n        core = cc.getCore(desc.getName());\n\n        boolean startRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n            collection, coreZkNodeName, shardId, leaderProps, core, cc);\n        if (!startRecovery) {\n          publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n        }\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n    } else {\n      publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateCloudState(true);\n\n    return shardId;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fd92b8bcc88e969302510acf77bd6970da3994c4","date":1327839530,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","sourceNew":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return the shardId for the SolrCore\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n    \n\n    log.info(\"Attempting to update \" + ZkStateReader.CLUSTER_STATE + \" version \"\n        + null);\n    CloudState state = CloudState.load(zkClient, zkStateReader.getCloudState().getLiveNodes());\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n    props.put(ZkStateReader.ROLES_PROP, cloudDesc.getRoles());\n    props.put(ZkStateReader.STATE_PROP, ZkStateReader.DOWN);\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    // we only put a subset of props into the leader node\n    ZkNodeProps leaderProps = new ZkNodeProps(ZkStateReader.BASE_URL_PROP,\n        props.get(ZkStateReader.BASE_URL_PROP), ZkStateReader.CORE_NAME_PROP,\n        props.get(ZkStateReader.CORE_NAME_PROP), ZkStateReader.NODE_NAME_PROP,\n        props.get(ZkStateReader.NODE_NAME_PROP));\n    \n\n    joinElection(collection, coreZkNodeName, shardId, leaderProps);\n    \n    String leaderUrl = zkStateReader.getLeaderUrl(collection,\n        cloudDesc.getShardId(), 30000);\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    if (cc != null) { // CoreContainer only null in tests\n      try {\n        core = cc.getCore(desc.getName());\n\n        if (isLeader) {\n          // recover from local transaction log and wait for it to complete before\n          // going active\n          // TODO: should this be moved to another thread? To recoveryStrat?\n          // TODO: should this actually be done earlier, before (or as part of)\n          // leader election perhaps?\n          // TODO: ensure that a replica that is trying to recover waits until I'm\n          // active (or don't make me the\n          // leader until my local replay is done. But this replay is only needed\n          // on the leader - replicas\n          // will do recovery anyway\n          \n          UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n          if (!core.isReloaded() && ulog != null) {\n            Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()\n                .getUpdateLog().recoverFromLog();\n            if (recoveryFuture != null) {\n              recoveryFuture.get(); // NOTE: this could potentially block for\n                                    // minutes or more!\n              // TODO: public as recovering in the mean time?\n            }\n          }\n        }\n        \n        boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n            collection, coreZkNodeName, shardId, leaderProps, core, cc);\n        if (!didRecovery) {\n          publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n        }\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n    } else {\n      publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateCloudState(true);\n\n    return shardId;\n  }\n\n","sourceOld":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n    \n\n    log.info(\"Attempting to update \" + ZkStateReader.CLUSTER_STATE + \" version \"\n        + null);\n    CloudState state = CloudState.load(zkClient, zkStateReader.getCloudState().getLiveNodes());\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n    props.put(ZkStateReader.ROLES_PROP, cloudDesc.getRoles());\n    props.put(ZkStateReader.STATE_PROP, ZkStateReader.DOWN);\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    // we only put a subset of props into the leader node\n    ZkNodeProps leaderProps = new ZkNodeProps(ZkStateReader.BASE_URL_PROP,\n        props.get(ZkStateReader.BASE_URL_PROP), ZkStateReader.CORE_NAME_PROP,\n        props.get(ZkStateReader.CORE_NAME_PROP), ZkStateReader.NODE_NAME_PROP,\n        props.get(ZkStateReader.NODE_NAME_PROP));\n    \n\n    joinElection(collection, coreZkNodeName, shardId, leaderProps);\n    \n    String leaderUrl = zkStateReader.getLeaderUrl(collection,\n        cloudDesc.getShardId(), 30000);\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    if (cc != null) { // CoreContainer only null in tests\n      try {\n        core = cc.getCore(desc.getName());\n\n        boolean startRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n            collection, coreZkNodeName, shardId, leaderProps, core, cc);\n        if (!startRecovery) {\n          publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n        }\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n    } else {\n      publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateCloudState(true);\n\n    return shardId;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c516f1fb8a8bbd4b308968d3f638304888ce86d1","date":1329014706,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","sourceNew":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return the shardId for the SolrCore\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n    props.put(ZkStateReader.ROLES_PROP, cloudDesc.getRoles());\n    props.put(ZkStateReader.STATE_PROP, ZkStateReader.DOWN);\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    // we only put a subset of props into the leader node\n    ZkNodeProps leaderProps = new ZkNodeProps(ZkStateReader.BASE_URL_PROP,\n        props.get(ZkStateReader.BASE_URL_PROP), ZkStateReader.CORE_NAME_PROP,\n        props.get(ZkStateReader.CORE_NAME_PROP), ZkStateReader.NODE_NAME_PROP,\n        props.get(ZkStateReader.NODE_NAME_PROP));\n    \n\n    joinElection(collection, coreZkNodeName, shardId, leaderProps);\n    \n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl = getLeaderUrl(collection, cloudDesc.getShardId());\n    \n    // now wait until our currently cloud state contains the latest leader\n    String cloudStateLeader = zkStateReader.getLeaderUrl(collection, cloudDesc.getShardId(), 30000);\n    int tries = 0;\n    while (!leaderUrl.equals(cloudStateLeader)) {\n      if (tries == 60) {\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"There is conflicting information about the leader of shard: \"\n                + cloudDesc.getShardId());\n      }\n      Thread.sleep(1000);\n      tries++;\n      cloudStateLeader = zkStateReader.getLeaderUrl(collection,\n          cloudDesc.getShardId(), 30000);\n    }\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    if (cc != null) { // CoreContainer only null in tests\n      try {\n        core = cc.getCore(desc.getName());\n\n        if (isLeader) {\n          // recover from local transaction log and wait for it to complete before\n          // going active\n          // TODO: should this be moved to another thread? To recoveryStrat?\n          // TODO: should this actually be done earlier, before (or as part of)\n          // leader election perhaps?\n          // TODO: ensure that a replica that is trying to recover waits until I'm\n          // active (or don't make me the\n          // leader until my local replay is done. But this replay is only needed\n          // on the leader - replicas\n          // will do recovery anyway\n          \n          UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n          if (!core.isReloaded() && ulog != null) {\n            Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()\n                .getUpdateLog().recoverFromLog();\n            if (recoveryFuture != null) {\n              recoveryFuture.get(); // NOTE: this could potentially block for\n                                    // minutes or more!\n              // TODO: public as recovering in the mean time?\n            }\n          }\n        }\n        \n        boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n            collection, coreZkNodeName, shardId, leaderProps, core, cc);\n        if (!didRecovery) {\n          publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n        }\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n    } else {\n      publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateCloudState(true);\n\n    return shardId;\n  }\n\n","sourceOld":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return the shardId for the SolrCore\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n    \n\n    log.info(\"Attempting to update \" + ZkStateReader.CLUSTER_STATE + \" version \"\n        + null);\n    CloudState state = CloudState.load(zkClient, zkStateReader.getCloudState().getLiveNodes());\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n    props.put(ZkStateReader.ROLES_PROP, cloudDesc.getRoles());\n    props.put(ZkStateReader.STATE_PROP, ZkStateReader.DOWN);\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    // we only put a subset of props into the leader node\n    ZkNodeProps leaderProps = new ZkNodeProps(ZkStateReader.BASE_URL_PROP,\n        props.get(ZkStateReader.BASE_URL_PROP), ZkStateReader.CORE_NAME_PROP,\n        props.get(ZkStateReader.CORE_NAME_PROP), ZkStateReader.NODE_NAME_PROP,\n        props.get(ZkStateReader.NODE_NAME_PROP));\n    \n\n    joinElection(collection, coreZkNodeName, shardId, leaderProps);\n    \n    String leaderUrl = zkStateReader.getLeaderUrl(collection,\n        cloudDesc.getShardId(), 30000);\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    if (cc != null) { // CoreContainer only null in tests\n      try {\n        core = cc.getCore(desc.getName());\n\n        if (isLeader) {\n          // recover from local transaction log and wait for it to complete before\n          // going active\n          // TODO: should this be moved to another thread? To recoveryStrat?\n          // TODO: should this actually be done earlier, before (or as part of)\n          // leader election perhaps?\n          // TODO: ensure that a replica that is trying to recover waits until I'm\n          // active (or don't make me the\n          // leader until my local replay is done. But this replay is only needed\n          // on the leader - replicas\n          // will do recovery anyway\n          \n          UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n          if (!core.isReloaded() && ulog != null) {\n            Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()\n                .getUpdateLog().recoverFromLog();\n            if (recoveryFuture != null) {\n              recoveryFuture.get(); // NOTE: this could potentially block for\n                                    // minutes or more!\n              // TODO: public as recovering in the mean time?\n            }\n          }\n        }\n        \n        boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n            collection, coreZkNodeName, shardId, leaderProps, core, cc);\n        if (!didRecovery) {\n          publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n        }\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n    } else {\n      publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateCloudState(true);\n\n    return shardId;\n  }\n\n","bugFix":["2c007e7c4cf8c55bc2a5884e315123afaaeec87f"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a80994db3380cd78c6f65b84515e2e931b6b3da","date":1329530403,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","sourceNew":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return the shardId for the SolrCore\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n // we only put a subset of props into the leader node\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    ZkNodeProps leaderProps = new ZkNodeProps(props);\n    \n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();\n    \n    // now wait until our currently cloud state contains the latest leader\n    String cloudStateLeader = zkStateReader.getLeaderUrl(collection, cloudDesc.getShardId(), 30000);\n    int tries = 0;\n    while (!leaderUrl.equals(cloudStateLeader)) {\n      if (tries == 60) {\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"There is conflicting information about the leader of shard: \"\n                + cloudDesc.getShardId());\n      }\n      Thread.sleep(1000);\n      tries++;\n      cloudStateLeader = zkStateReader.getLeaderUrl(collection,\n          cloudDesc.getShardId(), 30000);\n    }\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    if (cc != null) { // CoreContainer only null in tests\n      try {\n        core = cc.getCore(desc.getName());\n\n        if (isLeader) {\n          // recover from local transaction log and wait for it to complete before\n          // going active\n          // TODO: should this be moved to another thread? To recoveryStrat?\n          // TODO: should this actually be done earlier, before (or as part of)\n          // leader election perhaps?\n          // TODO: ensure that a replica that is trying to recover waits until I'm\n          // active (or don't make me the\n          // leader until my local replay is done. But this replay is only needed\n          // on the leader - replicas\n          // will do recovery anyway\n          \n          UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n          if (!core.isReloaded() && ulog != null) {\n            Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()\n                .getUpdateLog().recoverFromLog();\n            if (recoveryFuture != null) {\n              recoveryFuture.get(); // NOTE: this could potentially block for\n                                    // minutes or more!\n              // TODO: public as recovering in the mean time?\n            }\n          }\n        }\n        \n        boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n            collection, coreZkNodeName, shardId, leaderProps, core, cc);\n        if (!didRecovery) {\n          publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n        }\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n    } else {\n      publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateCloudState(true);\n\n    return shardId;\n  }\n\n","sourceOld":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return the shardId for the SolrCore\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n    props.put(ZkStateReader.ROLES_PROP, cloudDesc.getRoles());\n    props.put(ZkStateReader.STATE_PROP, ZkStateReader.DOWN);\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    // we only put a subset of props into the leader node\n    ZkNodeProps leaderProps = new ZkNodeProps(ZkStateReader.BASE_URL_PROP,\n        props.get(ZkStateReader.BASE_URL_PROP), ZkStateReader.CORE_NAME_PROP,\n        props.get(ZkStateReader.CORE_NAME_PROP), ZkStateReader.NODE_NAME_PROP,\n        props.get(ZkStateReader.NODE_NAME_PROP));\n    \n\n    joinElection(collection, coreZkNodeName, shardId, leaderProps);\n    \n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl = getLeaderUrl(collection, cloudDesc.getShardId());\n    \n    // now wait until our currently cloud state contains the latest leader\n    String cloudStateLeader = zkStateReader.getLeaderUrl(collection, cloudDesc.getShardId(), 30000);\n    int tries = 0;\n    while (!leaderUrl.equals(cloudStateLeader)) {\n      if (tries == 60) {\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"There is conflicting information about the leader of shard: \"\n                + cloudDesc.getShardId());\n      }\n      Thread.sleep(1000);\n      tries++;\n      cloudStateLeader = zkStateReader.getLeaderUrl(collection,\n          cloudDesc.getShardId(), 30000);\n    }\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    if (cc != null) { // CoreContainer only null in tests\n      try {\n        core = cc.getCore(desc.getName());\n\n        if (isLeader) {\n          // recover from local transaction log and wait for it to complete before\n          // going active\n          // TODO: should this be moved to another thread? To recoveryStrat?\n          // TODO: should this actually be done earlier, before (or as part of)\n          // leader election perhaps?\n          // TODO: ensure that a replica that is trying to recover waits until I'm\n          // active (or don't make me the\n          // leader until my local replay is done. But this replay is only needed\n          // on the leader - replicas\n          // will do recovery anyway\n          \n          UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n          if (!core.isReloaded() && ulog != null) {\n            Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()\n                .getUpdateLog().recoverFromLog();\n            if (recoveryFuture != null) {\n              recoveryFuture.get(); // NOTE: this could potentially block for\n                                    // minutes or more!\n              // TODO: public as recovering in the mean time?\n            }\n          }\n        }\n        \n        boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n            collection, coreZkNodeName, shardId, leaderProps, core, cc);\n        if (!didRecovery) {\n          publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n        }\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n    } else {\n      publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateCloudState(true);\n\n    return shardId;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7273051a9674cdb58df8b00d87e9821d9ddf5744","date":1330273532,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","sourceNew":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return the shardId for the SolrCore\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n // we only put a subset of props into the leader node\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    ZkNodeProps leaderProps = new ZkNodeProps(props);\n    \n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();\n    \n    // now wait until our currently cloud state contains the latest leader\n    String cloudStateLeader = zkStateReader.getLeaderUrl(collection, cloudDesc.getShardId(), 30000);\n    int tries = 0;\n    while (!leaderUrl.equals(cloudStateLeader)) {\n      if (tries == 60) {\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"There is conflicting information about the leader of shard: \"\n                + cloudDesc.getShardId());\n      }\n      Thread.sleep(1000);\n      tries++;\n      cloudStateLeader = zkStateReader.getLeaderUrl(collection,\n          cloudDesc.getShardId(), 30000);\n    }\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    if (cc != null) { // CoreContainer only null in tests\n      try {\n        core = cc.getCore(desc.getName());\n\n\n        // recover from local transaction log and wait for it to complete before\n        // going active\n        // TODO: should this be moved to another thread? To recoveryStrat?\n        // TODO: should this actually be done earlier, before (or as part of)\n        // leader election perhaps?\n        // TODO: if I'm the leader, ensure that a replica that is trying to recover waits until I'm\n        // active (or don't make me the\n        // leader until my local replay is done.\n\n        UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n        if (!core.isReloaded() && ulog != null) {\n          Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()\n              .getUpdateLog().recoverFromLog();\n          if (recoveryFuture != null) {\n            recoveryFuture.get(); // NOTE: this could potentially block for\n            // minutes or more!\n            // TODO: public as recovering in the mean time?\n            // TODO: in the future we could do peerync in parallel with recoverFromLog\n          }\n        }\n\n        \n        boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n            collection, coreZkNodeName, shardId, leaderProps, core, cc);\n        if (!didRecovery) {\n          publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n        }\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n    } else {\n      publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateCloudState(true);\n\n    return shardId;\n  }\n\n","sourceOld":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return the shardId for the SolrCore\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n // we only put a subset of props into the leader node\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    ZkNodeProps leaderProps = new ZkNodeProps(props);\n    \n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();\n    \n    // now wait until our currently cloud state contains the latest leader\n    String cloudStateLeader = zkStateReader.getLeaderUrl(collection, cloudDesc.getShardId(), 30000);\n    int tries = 0;\n    while (!leaderUrl.equals(cloudStateLeader)) {\n      if (tries == 60) {\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"There is conflicting information about the leader of shard: \"\n                + cloudDesc.getShardId());\n      }\n      Thread.sleep(1000);\n      tries++;\n      cloudStateLeader = zkStateReader.getLeaderUrl(collection,\n          cloudDesc.getShardId(), 30000);\n    }\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    if (cc != null) { // CoreContainer only null in tests\n      try {\n        core = cc.getCore(desc.getName());\n\n        if (isLeader) {\n          // recover from local transaction log and wait for it to complete before\n          // going active\n          // TODO: should this be moved to another thread? To recoveryStrat?\n          // TODO: should this actually be done earlier, before (or as part of)\n          // leader election perhaps?\n          // TODO: ensure that a replica that is trying to recover waits until I'm\n          // active (or don't make me the\n          // leader until my local replay is done. But this replay is only needed\n          // on the leader - replicas\n          // will do recovery anyway\n          \n          UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n          if (!core.isReloaded() && ulog != null) {\n            Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()\n                .getUpdateLog().recoverFromLog();\n            if (recoveryFuture != null) {\n              recoveryFuture.get(); // NOTE: this could potentially block for\n                                    // minutes or more!\n              // TODO: public as recovering in the mean time?\n            }\n          }\n        }\n        \n        boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n            collection, coreZkNodeName, shardId, leaderProps, core, cc);\n        if (!didRecovery) {\n          publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n        }\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n    } else {\n      publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateCloudState(true);\n\n    return shardId;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9813dd0748537c429b7c0a9b4723ea1ba496c047","date":1330304954,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","sourceNew":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return the shardId for the SolrCore\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n // we only put a subset of props into the leader node\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    ZkNodeProps leaderProps = new ZkNodeProps(props);\n    \n    try {\n      joinElection(desc);\n    } catch (InterruptedException e) {\n      // Restore the interrupted status\n      Thread.currentThread().interrupt();\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (KeeperException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (IOException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    }\n    \n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();\n    \n    // now wait until our currently cloud state contains the latest leader\n    String cloudStateLeader = zkStateReader.getLeaderUrl(collection, cloudDesc.getShardId(), 30000);\n    int tries = 0;\n    while (!leaderUrl.equals(cloudStateLeader)) {\n      if (tries == 60) {\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"There is conflicting information about the leader of shard: \"\n                + cloudDesc.getShardId());\n      }\n      Thread.sleep(1000);\n      tries++;\n      cloudStateLeader = zkStateReader.getLeaderUrl(collection,\n          cloudDesc.getShardId(), 30000);\n    }\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    if (cc != null) { // CoreContainer only null in tests\n      try {\n        core = cc.getCore(desc.getName());\n\n \n        // recover from local transaction log and wait for it to complete before\n        // going active\n        // TODO: should this be moved to another thread? To recoveryStrat?\n        // TODO: should this actually be done earlier, before (or as part of)\n        // leader election perhaps?\n        // TODO: if I'm the leader, ensure that a replica that is trying to recover waits until I'm\n        // active (or don't make me the\n        // leader until my local replay is done.\n\n        UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n        if (!core.isReloaded() && ulog != null) {\n          Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()\n              .getUpdateLog().recoverFromLog();\n          if (recoveryFuture != null) {\n            recoveryFuture.get(); // NOTE: this could potentially block for\n            // minutes or more!\n            // TODO: public as recovering in the mean time?\n            // TODO: in the future we could do peerync in parallel with recoverFromLog\n          }\n        }\n        \n        boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n            collection, coreZkNodeName, shardId, leaderProps, core, cc);\n        if (!didRecovery) {\n          publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n        }\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n    } else {\n      publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateCloudState(true);\n\n    return shardId;\n  }\n\n","sourceOld":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return the shardId for the SolrCore\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n // we only put a subset of props into the leader node\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    ZkNodeProps leaderProps = new ZkNodeProps(props);\n    \n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();\n    \n    // now wait until our currently cloud state contains the latest leader\n    String cloudStateLeader = zkStateReader.getLeaderUrl(collection, cloudDesc.getShardId(), 30000);\n    int tries = 0;\n    while (!leaderUrl.equals(cloudStateLeader)) {\n      if (tries == 60) {\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"There is conflicting information about the leader of shard: \"\n                + cloudDesc.getShardId());\n      }\n      Thread.sleep(1000);\n      tries++;\n      cloudStateLeader = zkStateReader.getLeaderUrl(collection,\n          cloudDesc.getShardId(), 30000);\n    }\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    if (cc != null) { // CoreContainer only null in tests\n      try {\n        core = cc.getCore(desc.getName());\n\n\n        // recover from local transaction log and wait for it to complete before\n        // going active\n        // TODO: should this be moved to another thread? To recoveryStrat?\n        // TODO: should this actually be done earlier, before (or as part of)\n        // leader election perhaps?\n        // TODO: if I'm the leader, ensure that a replica that is trying to recover waits until I'm\n        // active (or don't make me the\n        // leader until my local replay is done.\n\n        UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n        if (!core.isReloaded() && ulog != null) {\n          Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()\n              .getUpdateLog().recoverFromLog();\n          if (recoveryFuture != null) {\n            recoveryFuture.get(); // NOTE: this could potentially block for\n            // minutes or more!\n            // TODO: public as recovering in the mean time?\n            // TODO: in the future we could do peerync in parallel with recoverFromLog\n          }\n        }\n\n        \n        boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n            collection, coreZkNodeName, shardId, leaderProps, core, cc);\n        if (!didRecovery) {\n          publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n        }\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n    } else {\n      publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateCloudState(true);\n\n    return shardId;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ff2d7326b1f013c8da9bad45b1e98a3d16c38575","date":1330406992,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","sourceNew":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return the shardId for the SolrCore\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n // we only put a subset of props into the leader node\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    ZkNodeProps leaderProps = new ZkNodeProps(props);\n    \n    try {\n      joinElection(desc);\n    } catch (InterruptedException e) {\n      // Restore the interrupted status\n      Thread.currentThread().interrupt();\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (KeeperException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (IOException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    }\n    \n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();\n    \n    // now wait until our currently cloud state contains the latest leader\n    String cloudStateLeader = zkStateReader.getLeaderUrl(collection, cloudDesc.getShardId(), 30000);\n    int tries = 0;\n    while (!leaderUrl.equals(cloudStateLeader)) {\n      if (tries == 60) {\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"There is conflicting information about the leader of shard: \"\n                + cloudDesc.getShardId());\n      }\n      Thread.sleep(1000);\n      tries++;\n      cloudStateLeader = zkStateReader.getLeaderUrl(collection,\n          cloudDesc.getShardId(), 30000);\n    }\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    if (cc != null) { // CoreContainer only null in tests\n      try {\n        core = cc.getCore(desc.getName());\n\n \n        // recover from local transaction log and wait for it to complete before\n        // going active\n        // TODO: should this be moved to another thread? To recoveryStrat?\n        // TODO: should this actually be done earlier, before (or as part of)\n        // leader election perhaps?\n        // TODO: if I'm the leader, ensure that a replica that is trying to recover waits until I'm\n        // active (or don't make me the\n        // leader until my local replay is done.\n\n        UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n        if (!core.isReloaded() && ulog != null) {\n          Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()\n              .getUpdateLog().recoverFromLog();\n          if (recoveryFuture != null) {\n            recoveryFuture.get(); // NOTE: this could potentially block for\n            // minutes or more!\n            // TODO: public as recovering in the mean time?\n            // TODO: in the future we could do peerync in parallel with recoverFromLog\n          } else {\n            log.info(\"No LogReplay needed for core=\"+core.getName() + \" baseURL=\" + baseUrl);\n          }\n        }\n        \n        boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n            collection, coreZkNodeName, shardId, leaderProps, core, cc);\n        if (!didRecovery) {\n          publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n        }\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n    } else {\n      publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateCloudState(true);\n\n    return shardId;\n  }\n\n","sourceOld":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return the shardId for the SolrCore\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n // we only put a subset of props into the leader node\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    ZkNodeProps leaderProps = new ZkNodeProps(props);\n    \n    try {\n      joinElection(desc);\n    } catch (InterruptedException e) {\n      // Restore the interrupted status\n      Thread.currentThread().interrupt();\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (KeeperException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (IOException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    }\n    \n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();\n    \n    // now wait until our currently cloud state contains the latest leader\n    String cloudStateLeader = zkStateReader.getLeaderUrl(collection, cloudDesc.getShardId(), 30000);\n    int tries = 0;\n    while (!leaderUrl.equals(cloudStateLeader)) {\n      if (tries == 60) {\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"There is conflicting information about the leader of shard: \"\n                + cloudDesc.getShardId());\n      }\n      Thread.sleep(1000);\n      tries++;\n      cloudStateLeader = zkStateReader.getLeaderUrl(collection,\n          cloudDesc.getShardId(), 30000);\n    }\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    if (cc != null) { // CoreContainer only null in tests\n      try {\n        core = cc.getCore(desc.getName());\n\n \n        // recover from local transaction log and wait for it to complete before\n        // going active\n        // TODO: should this be moved to another thread? To recoveryStrat?\n        // TODO: should this actually be done earlier, before (or as part of)\n        // leader election perhaps?\n        // TODO: if I'm the leader, ensure that a replica that is trying to recover waits until I'm\n        // active (or don't make me the\n        // leader until my local replay is done.\n\n        UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n        if (!core.isReloaded() && ulog != null) {\n          Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()\n              .getUpdateLog().recoverFromLog();\n          if (recoveryFuture != null) {\n            recoveryFuture.get(); // NOTE: this could potentially block for\n            // minutes or more!\n            // TODO: public as recovering in the mean time?\n            // TODO: in the future we could do peerync in parallel with recoverFromLog\n          }\n        }\n        \n        boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n            collection, coreZkNodeName, shardId, leaderProps, core, cc);\n        if (!didRecovery) {\n          publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n        }\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n    } else {\n      publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateCloudState(true);\n\n    return shardId;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","date":1331075828,"type":3,"author":"Ryan McKinley","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","sourceNew":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return the shardId for the SolrCore\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n // we only put a subset of props into the leader node\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    ZkNodeProps leaderProps = new ZkNodeProps(props);\n    \n    try {\n      joinElection(desc);\n    } catch (InterruptedException e) {\n      // Restore the interrupted status\n      Thread.currentThread().interrupt();\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (KeeperException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (IOException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    }\n    \n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();\n    \n    // now wait until our currently cloud state contains the latest leader\n    String cloudStateLeader = zkStateReader.getLeaderUrl(collection, cloudDesc.getShardId(), 30000);\n    int tries = 0;\n    while (!leaderUrl.equals(cloudStateLeader)) {\n      if (tries == 60) {\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"There is conflicting information about the leader of shard: \"\n                + cloudDesc.getShardId());\n      }\n      Thread.sleep(1000);\n      tries++;\n      cloudStateLeader = zkStateReader.getLeaderUrl(collection,\n          cloudDesc.getShardId(), 30000);\n    }\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    if (cc != null) { // CoreContainer only null in tests\n      try {\n        core = cc.getCore(desc.getName());\n\n \n        // recover from local transaction log and wait for it to complete before\n        // going active\n        // TODO: should this be moved to another thread? To recoveryStrat?\n        // TODO: should this actually be done earlier, before (or as part of)\n        // leader election perhaps?\n        // TODO: if I'm the leader, ensure that a replica that is trying to recover waits until I'm\n        // active (or don't make me the\n        // leader until my local replay is done.\n\n        UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n        if (!core.isReloaded() && ulog != null) {\n          Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()\n              .getUpdateLog().recoverFromLog();\n          if (recoveryFuture != null) {\n            recoveryFuture.get(); // NOTE: this could potentially block for\n            // minutes or more!\n            // TODO: public as recovering in the mean time?\n            // TODO: in the future we could do peerync in parallel with recoverFromLog\n          } else {\n            log.info(\"No LogReplay needed for core=\"+core.getName() + \" baseURL=\" + baseUrl);\n          }\n        }\n        \n        boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n            collection, coreZkNodeName, shardId, leaderProps, core, cc);\n        if (!didRecovery) {\n          publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n        }\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n    } else {\n      publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateCloudState(true);\n\n    return shardId;\n  }\n\n","sourceOld":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return the shardId for the SolrCore\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n // we only put a subset of props into the leader node\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    ZkNodeProps leaderProps = new ZkNodeProps(props);\n    \n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();\n    \n    // now wait until our currently cloud state contains the latest leader\n    String cloudStateLeader = zkStateReader.getLeaderUrl(collection, cloudDesc.getShardId(), 30000);\n    int tries = 0;\n    while (!leaderUrl.equals(cloudStateLeader)) {\n      if (tries == 60) {\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"There is conflicting information about the leader of shard: \"\n                + cloudDesc.getShardId());\n      }\n      Thread.sleep(1000);\n      tries++;\n      cloudStateLeader = zkStateReader.getLeaderUrl(collection,\n          cloudDesc.getShardId(), 30000);\n    }\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    if (cc != null) { // CoreContainer only null in tests\n      try {\n        core = cc.getCore(desc.getName());\n\n        if (isLeader) {\n          // recover from local transaction log and wait for it to complete before\n          // going active\n          // TODO: should this be moved to another thread? To recoveryStrat?\n          // TODO: should this actually be done earlier, before (or as part of)\n          // leader election perhaps?\n          // TODO: ensure that a replica that is trying to recover waits until I'm\n          // active (or don't make me the\n          // leader until my local replay is done. But this replay is only needed\n          // on the leader - replicas\n          // will do recovery anyway\n          \n          UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n          if (!core.isReloaded() && ulog != null) {\n            Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()\n                .getUpdateLog().recoverFromLog();\n            if (recoveryFuture != null) {\n              recoveryFuture.get(); // NOTE: this could potentially block for\n                                    // minutes or more!\n              // TODO: public as recovering in the mean time?\n            }\n          }\n        }\n        \n        boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n            collection, coreZkNodeName, shardId, leaderProps, core, cc);\n        if (!didRecovery) {\n          publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n        }\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n    } else {\n      publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateCloudState(true);\n\n    return shardId;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a5093a9e893633cc091cf2f729d7863671c2b715","date":1339132888,"type":3,"author":"Sami Siren","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","sourceNew":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return the shardId for the SolrCore\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n // we only put a subset of props into the leader node\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    ZkNodeProps leaderProps = new ZkNodeProps(props);\n    \n    try {\n      joinElection(desc);\n    } catch (InterruptedException e) {\n      // Restore the interrupted status\n      Thread.currentThread().interrupt();\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (KeeperException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (IOException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    }\n    \n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();\n    \n    // now wait until our currently cloud state contains the latest leader\n    String cloudStateLeader = zkStateReader.getLeaderUrl(collection, cloudDesc.getShardId(), 30000);\n    int tries = 0;\n    while (!leaderUrl.equals(cloudStateLeader)) {\n      if (tries == 60) {\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"There is conflicting information about the leader of shard: \"\n                + cloudDesc.getShardId());\n      }\n      Thread.sleep(1000);\n      tries++;\n      cloudStateLeader = zkStateReader.getLeaderUrl(collection,\n          cloudDesc.getShardId(), 30000);\n    }\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    if (cc != null) { // CoreContainer only null in tests\n      try {\n        core = cc.getCore(desc.getName());\n\n \n        // recover from local transaction log and wait for it to complete before\n        // going active\n        // TODO: should this be moved to another thread? To recoveryStrat?\n        // TODO: should this actually be done earlier, before (or as part of)\n        // leader election perhaps?\n        // TODO: if I'm the leader, ensure that a replica that is trying to recover waits until I'm\n        // active (or don't make me the\n        // leader until my local replay is done.\n\n        UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n        if (!core.isReloaded() && ulog != null) {\n          Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()\n              .getUpdateLog().recoverFromLog();\n          if (recoveryFuture != null) {\n            recoveryFuture.get(); // NOTE: this could potentially block for\n            // minutes or more!\n            // TODO: public as recovering in the mean time?\n            // TODO: in the future we could do peerync in parallel with recoverFromLog\n          } else {\n            log.info(\"No LogReplay needed for core=\"+core.getName() + \" baseURL=\" + baseUrl);\n          }\n        }\n        \n        boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n            collection, coreZkNodeName, shardId, leaderProps, core, cc);\n        if (!didRecovery) {\n          publish(desc, ZkStateReader.ACTIVE);\n        }\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n    } else {\n      publish(desc, ZkStateReader.ACTIVE);\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateCloudState(true);\n\n    return shardId;\n  }\n\n","sourceOld":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return the shardId for the SolrCore\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n // we only put a subset of props into the leader node\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    ZkNodeProps leaderProps = new ZkNodeProps(props);\n    \n    try {\n      joinElection(desc);\n    } catch (InterruptedException e) {\n      // Restore the interrupted status\n      Thread.currentThread().interrupt();\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (KeeperException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (IOException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    }\n    \n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();\n    \n    // now wait until our currently cloud state contains the latest leader\n    String cloudStateLeader = zkStateReader.getLeaderUrl(collection, cloudDesc.getShardId(), 30000);\n    int tries = 0;\n    while (!leaderUrl.equals(cloudStateLeader)) {\n      if (tries == 60) {\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"There is conflicting information about the leader of shard: \"\n                + cloudDesc.getShardId());\n      }\n      Thread.sleep(1000);\n      tries++;\n      cloudStateLeader = zkStateReader.getLeaderUrl(collection,\n          cloudDesc.getShardId(), 30000);\n    }\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    if (cc != null) { // CoreContainer only null in tests\n      try {\n        core = cc.getCore(desc.getName());\n\n \n        // recover from local transaction log and wait for it to complete before\n        // going active\n        // TODO: should this be moved to another thread? To recoveryStrat?\n        // TODO: should this actually be done earlier, before (or as part of)\n        // leader election perhaps?\n        // TODO: if I'm the leader, ensure that a replica that is trying to recover waits until I'm\n        // active (or don't make me the\n        // leader until my local replay is done.\n\n        UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n        if (!core.isReloaded() && ulog != null) {\n          Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()\n              .getUpdateLog().recoverFromLog();\n          if (recoveryFuture != null) {\n            recoveryFuture.get(); // NOTE: this could potentially block for\n            // minutes or more!\n            // TODO: public as recovering in the mean time?\n            // TODO: in the future we could do peerync in parallel with recoverFromLog\n          } else {\n            log.info(\"No LogReplay needed for core=\"+core.getName() + \" baseURL=\" + baseUrl);\n          }\n        }\n        \n        boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n            collection, coreZkNodeName, shardId, leaderProps, core, cc);\n        if (!didRecovery) {\n          publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n        }\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n    } else {\n      publishAsActive(baseUrl, desc, coreZkNodeName, coreName);\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateCloudState(true);\n\n    return shardId;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"049f7138250089cf12d36414ed76f8078e18fb5b","date":1344014280,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","sourceNew":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return the shardId for the SolrCore\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n // we only put a subset of props into the leader node\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    ZkNodeProps leaderProps = new ZkNodeProps(props);\n    \n    try {\n      joinElection(desc);\n    } catch (InterruptedException e) {\n      // Restore the interrupted status\n      Thread.currentThread().interrupt();\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (KeeperException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (IOException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    }\n    \n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();\n    \n    // now wait until our currently cloud state contains the latest leader\n    String cloudStateLeader = zkStateReader.getLeaderUrl(collection, cloudDesc.getShardId(), 30000);\n    int tries = 0;\n    while (!leaderUrl.equals(cloudStateLeader)) {\n      if (tries == 60) {\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"There is conflicting information about the leader of shard: \"\n                + cloudDesc.getShardId() + \" our state says:\" + leaderUrl + \" but zookeeper says:\" + cloudStateLeader);\n      }\n      Thread.sleep(1000);\n      tries++;\n      cloudStateLeader = zkStateReader.getLeaderUrl(collection,\n          cloudDesc.getShardId(), 30000);\n    }\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    if (cc != null) { // CoreContainer only null in tests\n      try {\n        core = cc.getCore(desc.getName());\n\n \n        // recover from local transaction log and wait for it to complete before\n        // going active\n        // TODO: should this be moved to another thread? To recoveryStrat?\n        // TODO: should this actually be done earlier, before (or as part of)\n        // leader election perhaps?\n        // TODO: if I'm the leader, ensure that a replica that is trying to recover waits until I'm\n        // active (or don't make me the\n        // leader until my local replay is done.\n\n        UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n        if (!core.isReloaded() && ulog != null) {\n          Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()\n              .getUpdateLog().recoverFromLog();\n          if (recoveryFuture != null) {\n            recoveryFuture.get(); // NOTE: this could potentially block for\n            // minutes or more!\n            // TODO: public as recovering in the mean time?\n            // TODO: in the future we could do peerync in parallel with recoverFromLog\n          } else {\n            log.info(\"No LogReplay needed for core=\"+core.getName() + \" baseURL=\" + baseUrl);\n          }\n        }\n        \n        boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n            collection, coreZkNodeName, shardId, leaderProps, core, cc);\n        if (!didRecovery) {\n          publish(desc, ZkStateReader.ACTIVE);\n        }\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n    } else {\n      publish(desc, ZkStateReader.ACTIVE);\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateCloudState(true);\n\n    return shardId;\n  }\n\n","sourceOld":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return the shardId for the SolrCore\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n // we only put a subset of props into the leader node\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    ZkNodeProps leaderProps = new ZkNodeProps(props);\n    \n    try {\n      joinElection(desc);\n    } catch (InterruptedException e) {\n      // Restore the interrupted status\n      Thread.currentThread().interrupt();\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (KeeperException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (IOException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    }\n    \n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();\n    \n    // now wait until our currently cloud state contains the latest leader\n    String cloudStateLeader = zkStateReader.getLeaderUrl(collection, cloudDesc.getShardId(), 30000);\n    int tries = 0;\n    while (!leaderUrl.equals(cloudStateLeader)) {\n      if (tries == 60) {\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"There is conflicting information about the leader of shard: \"\n                + cloudDesc.getShardId());\n      }\n      Thread.sleep(1000);\n      tries++;\n      cloudStateLeader = zkStateReader.getLeaderUrl(collection,\n          cloudDesc.getShardId(), 30000);\n    }\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    if (cc != null) { // CoreContainer only null in tests\n      try {\n        core = cc.getCore(desc.getName());\n\n \n        // recover from local transaction log and wait for it to complete before\n        // going active\n        // TODO: should this be moved to another thread? To recoveryStrat?\n        // TODO: should this actually be done earlier, before (or as part of)\n        // leader election perhaps?\n        // TODO: if I'm the leader, ensure that a replica that is trying to recover waits until I'm\n        // active (or don't make me the\n        // leader until my local replay is done.\n\n        UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n        if (!core.isReloaded() && ulog != null) {\n          Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()\n              .getUpdateLog().recoverFromLog();\n          if (recoveryFuture != null) {\n            recoveryFuture.get(); // NOTE: this could potentially block for\n            // minutes or more!\n            // TODO: public as recovering in the mean time?\n            // TODO: in the future we could do peerync in parallel with recoverFromLog\n          } else {\n            log.info(\"No LogReplay needed for core=\"+core.getName() + \" baseURL=\" + baseUrl);\n          }\n        }\n        \n        boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n            collection, coreZkNodeName, shardId, leaderProps, core, cc);\n        if (!didRecovery) {\n          publish(desc, ZkStateReader.ACTIVE);\n        }\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n    } else {\n      publish(desc, ZkStateReader.ACTIVE);\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateCloudState(true);\n\n    return shardId;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c824b5854f7ad30cd53f0634fc7cb533df74590b","date":1344024196,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","sourceNew":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return the shardId for the SolrCore\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n // we only put a subset of props into the leader node\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    ZkNodeProps leaderProps = new ZkNodeProps(props);\n    \n    try {\n      joinElection(desc);\n    } catch (InterruptedException e) {\n      // Restore the interrupted status\n      Thread.currentThread().interrupt();\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (KeeperException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (IOException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    }\n    \n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();\n    \n    // now wait until our currently cloud state contains the latest leader\n    String cloudStateLeader = zkStateReader.getLeaderUrl(collection, shardId, 30000);\n    int tries = 0;\n    while (!leaderUrl.equals(cloudStateLeader)) {\n      if (tries == 60) {\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"There is conflicting information about the leader of shard: \"\n                + cloudDesc.getShardId() + \" our state says:\" + cloudStateLeader + \" but zookeeper says:\" + leaderUrl);\n      }\n      Thread.sleep(1000);\n      tries++;\n      cloudStateLeader = zkStateReader.getLeaderUrl(collection, shardId, 30000);\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();\n    }\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    if (cc != null) { // CoreContainer only null in tests\n      try {\n        core = cc.getCore(desc.getName());\n\n \n        // recover from local transaction log and wait for it to complete before\n        // going active\n        // TODO: should this be moved to another thread? To recoveryStrat?\n        // TODO: should this actually be done earlier, before (or as part of)\n        // leader election perhaps?\n        // TODO: if I'm the leader, ensure that a replica that is trying to recover waits until I'm\n        // active (or don't make me the\n        // leader until my local replay is done.\n\n        UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n        if (!core.isReloaded() && ulog != null) {\n          Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()\n              .getUpdateLog().recoverFromLog();\n          if (recoveryFuture != null) {\n            recoveryFuture.get(); // NOTE: this could potentially block for\n            // minutes or more!\n            // TODO: public as recovering in the mean time?\n            // TODO: in the future we could do peerync in parallel with recoverFromLog\n          } else {\n            log.info(\"No LogReplay needed for core=\"+core.getName() + \" baseURL=\" + baseUrl);\n          }\n        }\n        \n        boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n            collection, coreZkNodeName, shardId, leaderProps, core, cc);\n        if (!didRecovery) {\n          publish(desc, ZkStateReader.ACTIVE);\n        }\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n    } else {\n      publish(desc, ZkStateReader.ACTIVE);\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateCloudState(true);\n\n    return shardId;\n  }\n\n","sourceOld":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return the shardId for the SolrCore\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n // we only put a subset of props into the leader node\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    ZkNodeProps leaderProps = new ZkNodeProps(props);\n    \n    try {\n      joinElection(desc);\n    } catch (InterruptedException e) {\n      // Restore the interrupted status\n      Thread.currentThread().interrupt();\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (KeeperException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (IOException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    }\n    \n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();\n    \n    // now wait until our currently cloud state contains the latest leader\n    String cloudStateLeader = zkStateReader.getLeaderUrl(collection, cloudDesc.getShardId(), 30000);\n    int tries = 0;\n    while (!leaderUrl.equals(cloudStateLeader)) {\n      if (tries == 60) {\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"There is conflicting information about the leader of shard: \"\n                + cloudDesc.getShardId() + \" our state says:\" + leaderUrl + \" but zookeeper says:\" + cloudStateLeader);\n      }\n      Thread.sleep(1000);\n      tries++;\n      cloudStateLeader = zkStateReader.getLeaderUrl(collection,\n          cloudDesc.getShardId(), 30000);\n    }\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    if (cc != null) { // CoreContainer only null in tests\n      try {\n        core = cc.getCore(desc.getName());\n\n \n        // recover from local transaction log and wait for it to complete before\n        // going active\n        // TODO: should this be moved to another thread? To recoveryStrat?\n        // TODO: should this actually be done earlier, before (or as part of)\n        // leader election perhaps?\n        // TODO: if I'm the leader, ensure that a replica that is trying to recover waits until I'm\n        // active (or don't make me the\n        // leader until my local replay is done.\n\n        UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n        if (!core.isReloaded() && ulog != null) {\n          Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()\n              .getUpdateLog().recoverFromLog();\n          if (recoveryFuture != null) {\n            recoveryFuture.get(); // NOTE: this could potentially block for\n            // minutes or more!\n            // TODO: public as recovering in the mean time?\n            // TODO: in the future we could do peerync in parallel with recoverFromLog\n          } else {\n            log.info(\"No LogReplay needed for core=\"+core.getName() + \" baseURL=\" + baseUrl);\n          }\n        }\n        \n        boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n            collection, coreZkNodeName, shardId, leaderProps, core, cc);\n        if (!didRecovery) {\n          publish(desc, ZkStateReader.ACTIVE);\n        }\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n    } else {\n      publish(desc, ZkStateReader.ACTIVE);\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateCloudState(true);\n\n    return shardId;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3f767f8c99eaedb984df754fe61f21c5de260f94","date":1344105153,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","sourceNew":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return the shardId for the SolrCore\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n // we only put a subset of props into the leader node\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    ZkNodeProps leaderProps = new ZkNodeProps(props);\n    \n    try {\n      joinElection(desc);\n    } catch (InterruptedException e) {\n      // Restore the interrupted status\n      Thread.currentThread().interrupt();\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (KeeperException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (IOException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    }\n    \n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();\n    \n    // now wait until our currently cloud state contains the latest leader\n    String clusterStateLeader = zkStateReader.getLeaderUrl(collection, shardId, 30000);\n    int tries = 0;\n    while (!leaderUrl.equals(clusterStateLeader)) {\n      if (tries == 60) {\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"There is conflicting information about the leader of shard: \"\n                + cloudDesc.getShardId() + \" our state says:\" + clusterStateLeader + \" but zookeeper says:\" + leaderUrl);\n      }\n      Thread.sleep(1000);\n      tries++;\n      clusterStateLeader = zkStateReader.getLeaderUrl(collection, shardId, 30000);\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();\n    }\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    if (cc != null) { // CoreContainer only null in tests\n      try {\n        core = cc.getCore(desc.getName());\n\n \n        // recover from local transaction log and wait for it to complete before\n        // going active\n        // TODO: should this be moved to another thread? To recoveryStrat?\n        // TODO: should this actually be done earlier, before (or as part of)\n        // leader election perhaps?\n        // TODO: if I'm the leader, ensure that a replica that is trying to recover waits until I'm\n        // active (or don't make me the\n        // leader until my local replay is done.\n\n        UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n        if (!core.isReloaded() && ulog != null) {\n          Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()\n              .getUpdateLog().recoverFromLog();\n          if (recoveryFuture != null) {\n            recoveryFuture.get(); // NOTE: this could potentially block for\n            // minutes or more!\n            // TODO: public as recovering in the mean time?\n            // TODO: in the future we could do peerync in parallel with recoverFromLog\n          } else {\n            log.info(\"No LogReplay needed for core=\"+core.getName() + \" baseURL=\" + baseUrl);\n          }\n        }\n        \n        boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n            collection, coreZkNodeName, shardId, leaderProps, core, cc);\n        if (!didRecovery) {\n          publish(desc, ZkStateReader.ACTIVE);\n        }\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n    } else {\n      publish(desc, ZkStateReader.ACTIVE);\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateClusterState(true);\n\n    return shardId;\n  }\n\n","sourceOld":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return the shardId for the SolrCore\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n // we only put a subset of props into the leader node\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    ZkNodeProps leaderProps = new ZkNodeProps(props);\n    \n    try {\n      joinElection(desc);\n    } catch (InterruptedException e) {\n      // Restore the interrupted status\n      Thread.currentThread().interrupt();\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (KeeperException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (IOException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    }\n    \n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();\n    \n    // now wait until our currently cloud state contains the latest leader\n    String cloudStateLeader = zkStateReader.getLeaderUrl(collection, shardId, 30000);\n    int tries = 0;\n    while (!leaderUrl.equals(cloudStateLeader)) {\n      if (tries == 60) {\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"There is conflicting information about the leader of shard: \"\n                + cloudDesc.getShardId() + \" our state says:\" + cloudStateLeader + \" but zookeeper says:\" + leaderUrl);\n      }\n      Thread.sleep(1000);\n      tries++;\n      cloudStateLeader = zkStateReader.getLeaderUrl(collection, shardId, 30000);\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();\n    }\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    if (cc != null) { // CoreContainer only null in tests\n      try {\n        core = cc.getCore(desc.getName());\n\n \n        // recover from local transaction log and wait for it to complete before\n        // going active\n        // TODO: should this be moved to another thread? To recoveryStrat?\n        // TODO: should this actually be done earlier, before (or as part of)\n        // leader election perhaps?\n        // TODO: if I'm the leader, ensure that a replica that is trying to recover waits until I'm\n        // active (or don't make me the\n        // leader until my local replay is done.\n\n        UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n        if (!core.isReloaded() && ulog != null) {\n          Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()\n              .getUpdateLog().recoverFromLog();\n          if (recoveryFuture != null) {\n            recoveryFuture.get(); // NOTE: this could potentially block for\n            // minutes or more!\n            // TODO: public as recovering in the mean time?\n            // TODO: in the future we could do peerync in parallel with recoverFromLog\n          } else {\n            log.info(\"No LogReplay needed for core=\"+core.getName() + \" baseURL=\" + baseUrl);\n          }\n        }\n        \n        boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n            collection, coreZkNodeName, shardId, leaderProps, core, cc);\n        if (!didRecovery) {\n          publish(desc, ZkStateReader.ACTIVE);\n        }\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n    } else {\n      publish(desc, ZkStateReader.ACTIVE);\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateCloudState(true);\n\n    return shardId;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8fd5be977c105554c6a7b68afcdbc511439723ab","date":1344115570,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","sourceNew":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return the shardId for the SolrCore\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n // we only put a subset of props into the leader node\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    ZkNodeProps leaderProps = new ZkNodeProps(props);\n    \n    try {\n      joinElection(desc);\n    } catch (InterruptedException e) {\n      // Restore the interrupted status\n      Thread.currentThread().interrupt();\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (KeeperException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (IOException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    }\n    \n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();\n    \n    // now wait until our currently cloud state contains the latest leader\n    String clusterStateLeader = zkStateReader.getLeaderUrl(collection, shardId, 30000);\n    int tries = 0;\n    while (!leaderUrl.equals(clusterStateLeader)) {\n      if (tries == 60) {\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"There is conflicting information about the leader of shard: \"\n                + cloudDesc.getShardId() + \" our state says:\" + clusterStateLeader + \" but zookeeper says:\" + leaderUrl);\n      }\n      Thread.sleep(1000);\n      tries++;\n      clusterStateLeader = zkStateReader.getLeaderUrl(collection, shardId, 30000);\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();\n    }\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    if (cc != null) { // CoreContainer only null in tests\n      try {\n        core = cc.getCore(desc.getName());\n\n \n        // recover from local transaction log and wait for it to complete before\n        // going active\n        // TODO: should this be moved to another thread? To recoveryStrat?\n        // TODO: should this actually be done earlier, before (or as part of)\n        // leader election perhaps?\n        // TODO: if I'm the leader, ensure that a replica that is trying to recover waits until I'm\n        // active (or don't make me the\n        // leader until my local replay is done.\n\n        UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n        if (!core.isReloaded() && ulog != null) {\n          Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()\n              .getUpdateLog().recoverFromLog();\n          if (recoveryFuture != null) {\n            recoveryFuture.get(); // NOTE: this could potentially block for\n            // minutes or more!\n            // TODO: public as recovering in the mean time?\n            // TODO: in the future we could do peerync in parallel with recoverFromLog\n          } else {\n            log.info(\"No LogReplay needed for core=\"+core.getName() + \" baseURL=\" + baseUrl);\n          }\n        }\n        \n        boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n            collection, coreZkNodeName, shardId, leaderProps, core, cc);\n        if (!didRecovery) {\n          publish(desc, ZkStateReader.ACTIVE);\n        }\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n    } else {\n      publish(desc, ZkStateReader.ACTIVE);\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateClusterState(true);\n\n    return shardId;\n  }\n\n","sourceOld":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return the shardId for the SolrCore\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n // we only put a subset of props into the leader node\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    ZkNodeProps leaderProps = new ZkNodeProps(props);\n    \n    try {\n      joinElection(desc);\n    } catch (InterruptedException e) {\n      // Restore the interrupted status\n      Thread.currentThread().interrupt();\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (KeeperException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (IOException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    }\n    \n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();\n    \n    // now wait until our currently cloud state contains the latest leader\n    String cloudStateLeader = zkStateReader.getLeaderUrl(collection, cloudDesc.getShardId(), 30000);\n    int tries = 0;\n    while (!leaderUrl.equals(cloudStateLeader)) {\n      if (tries == 60) {\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"There is conflicting information about the leader of shard: \"\n                + cloudDesc.getShardId());\n      }\n      Thread.sleep(1000);\n      tries++;\n      cloudStateLeader = zkStateReader.getLeaderUrl(collection,\n          cloudDesc.getShardId(), 30000);\n    }\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    if (cc != null) { // CoreContainer only null in tests\n      try {\n        core = cc.getCore(desc.getName());\n\n \n        // recover from local transaction log and wait for it to complete before\n        // going active\n        // TODO: should this be moved to another thread? To recoveryStrat?\n        // TODO: should this actually be done earlier, before (or as part of)\n        // leader election perhaps?\n        // TODO: if I'm the leader, ensure that a replica that is trying to recover waits until I'm\n        // active (or don't make me the\n        // leader until my local replay is done.\n\n        UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n        if (!core.isReloaded() && ulog != null) {\n          Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()\n              .getUpdateLog().recoverFromLog();\n          if (recoveryFuture != null) {\n            recoveryFuture.get(); // NOTE: this could potentially block for\n            // minutes or more!\n            // TODO: public as recovering in the mean time?\n            // TODO: in the future we could do peerync in parallel with recoverFromLog\n          } else {\n            log.info(\"No LogReplay needed for core=\"+core.getName() + \" baseURL=\" + baseUrl);\n          }\n        }\n        \n        boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n            collection, coreZkNodeName, shardId, leaderProps, core, cc);\n        if (!didRecovery) {\n          publish(desc, ZkStateReader.ACTIVE);\n        }\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n    } else {\n      publish(desc, ZkStateReader.ACTIVE);\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateCloudState(true);\n\n    return shardId;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d6f074e73200c07d54f242d3880a8da5a35ff97b","date":1344507653,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","sourceNew":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return the shardId for the SolrCore\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n // we only put a subset of props into the leader node\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    ZkNodeProps leaderProps = new ZkNodeProps(props);\n    \n    try {\n      joinElection(desc);\n    } catch (InterruptedException e) {\n      // Restore the interrupted status\n      Thread.currentThread().interrupt();\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (KeeperException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (IOException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    }\n    \n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();\n    \n    // now wait until our currently cloud state contains the latest leader\n    String clusterStateLeader = zkStateReader.getLeaderUrl(collection, shardId, 30000);\n    int tries = 0;\n    while (!leaderUrl.equals(clusterStateLeader)) {\n      if (tries == 60) {\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"There is conflicting information about the leader of shard: \"\n                + cloudDesc.getShardId() + \" our state says:\" + clusterStateLeader + \" but zookeeper says:\" + leaderUrl);\n      }\n      Thread.sleep(1000);\n      tries++;\n      clusterStateLeader = zkStateReader.getLeaderUrl(collection, shardId, 30000);\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();\n    }\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    if (cc != null) { // CoreContainer only null in tests\n      try {\n        core = cc.getCore(desc.getName());\n\n \n        // recover from local transaction log and wait for it to complete before\n        // going active\n        // TODO: should this be moved to another thread? To recoveryStrat?\n        // TODO: should this actually be done earlier, before (or as part of)\n        // leader election perhaps?\n        // TODO: if I'm the leader, ensure that a replica that is trying to recover waits until I'm\n        // active (or don't make me the\n        // leader until my local replay is done.\n\n        UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n        if (!core.isReloaded() && ulog != null) {\n          Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()\n              .getUpdateLog().recoverFromLog();\n          if (recoveryFuture != null) {\n            recoveryFuture.get(); // NOTE: this could potentially block for\n            // minutes or more!\n            // TODO: public as recovering in the mean time?\n            // TODO: in the future we could do peerync in parallel with recoverFromLog\n          } else {\n            log.info(\"No LogReplay needed for core=\"+core.getName() + \" baseURL=\" + baseUrl);\n          }\n        }\n        \n        boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n            collection, coreZkNodeName, shardId, leaderProps, core, cc);\n        if (!didRecovery) {\n          publish(desc, ZkStateReader.ACTIVE);\n        }\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n    } else {\n      publish(desc, ZkStateReader.ACTIVE);\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateClusterState(true);\n\n    return shardId;\n  }\n\n","sourceOld":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return the shardId for the SolrCore\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n // we only put a subset of props into the leader node\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    ZkNodeProps leaderProps = new ZkNodeProps(props);\n    \n    try {\n      joinElection(desc);\n    } catch (InterruptedException e) {\n      // Restore the interrupted status\n      Thread.currentThread().interrupt();\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (KeeperException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (IOException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    }\n    \n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();\n    \n    // now wait until our currently cloud state contains the latest leader\n    String cloudStateLeader = zkStateReader.getLeaderUrl(collection, cloudDesc.getShardId(), 30000);\n    int tries = 0;\n    while (!leaderUrl.equals(cloudStateLeader)) {\n      if (tries == 60) {\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"There is conflicting information about the leader of shard: \"\n                + cloudDesc.getShardId());\n      }\n      Thread.sleep(1000);\n      tries++;\n      cloudStateLeader = zkStateReader.getLeaderUrl(collection,\n          cloudDesc.getShardId(), 30000);\n    }\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    if (cc != null) { // CoreContainer only null in tests\n      try {\n        core = cc.getCore(desc.getName());\n\n \n        // recover from local transaction log and wait for it to complete before\n        // going active\n        // TODO: should this be moved to another thread? To recoveryStrat?\n        // TODO: should this actually be done earlier, before (or as part of)\n        // leader election perhaps?\n        // TODO: if I'm the leader, ensure that a replica that is trying to recover waits until I'm\n        // active (or don't make me the\n        // leader until my local replay is done.\n\n        UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n        if (!core.isReloaded() && ulog != null) {\n          Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()\n              .getUpdateLog().recoverFromLog();\n          if (recoveryFuture != null) {\n            recoveryFuture.get(); // NOTE: this could potentially block for\n            // minutes or more!\n            // TODO: public as recovering in the mean time?\n            // TODO: in the future we could do peerync in parallel with recoverFromLog\n          } else {\n            log.info(\"No LogReplay needed for core=\"+core.getName() + \" baseURL=\" + baseUrl);\n          }\n        }\n        \n        boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n            collection, coreZkNodeName, shardId, leaderProps, core, cc);\n        if (!didRecovery) {\n          publish(desc, ZkStateReader.ACTIVE);\n        }\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n    } else {\n      publish(desc, ZkStateReader.ACTIVE);\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateCloudState(true);\n\n    return shardId;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"adc2388a5005de25370273411bc713d0ff722805","date":1345719157,"type":3,"author":"Sami Siren","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","sourceNew":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return the shardId for the SolrCore\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n // we only put a subset of props into the leader node\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    ZkNodeProps leaderProps = new ZkNodeProps(props);\n    \n    try {\n      joinElection(desc);\n    } catch (InterruptedException e) {\n      // Restore the interrupted status\n      Thread.currentThread().interrupt();\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (KeeperException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (IOException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    }\n    \n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();\n    \n    // now wait until our currently cloud state contains the latest leader\n    String clusterStateLeader = zkStateReader.getLeaderUrl(collection, shardId, 30000);\n    int tries = 0;\n    while (!leaderUrl.equals(clusterStateLeader)) {\n      if (tries == 60) {\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"There is conflicting information about the leader of shard: \"\n                + cloudDesc.getShardId() + \" our state says:\" + clusterStateLeader + \" but zookeeper says:\" + leaderUrl);\n      }\n      Thread.sleep(1000);\n      tries++;\n      clusterStateLeader = zkStateReader.getLeaderUrl(collection, shardId, 30000);\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();\n    }\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    try {\n      core = cc.getCore(desc.getName());\n\n \n      // recover from local transaction log and wait for it to complete before\n      // going active\n      // TODO: should this be moved to another thread? To recoveryStrat?\n      // TODO: should this actually be done earlier, before (or as part of)\n      // leader election perhaps?\n      // TODO: if I'm the leader, ensure that a replica that is trying to recover waits until I'm\n      // active (or don't make me the\n      // leader until my local replay is done.\n\n      UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n      if (!core.isReloaded() && ulog != null) {\n        Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()\n            .getUpdateLog().recoverFromLog();\n        if (recoveryFuture != null) {\n          recoveryFuture.get(); // NOTE: this could potentially block for\n          // minutes or more!\n          // TODO: public as recovering in the mean time?\n          // TODO: in the future we could do peerync in parallel with recoverFromLog\n        } else {\n          log.info(\"No LogReplay needed for core=\"+core.getName() + \" baseURL=\" + baseUrl);\n        }\n      }\n      \n      boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n          collection, coreZkNodeName, shardId, leaderProps, core, cc);\n      if (!didRecovery) {\n        publish(desc, ZkStateReader.ACTIVE);\n      }\n    } finally {\n      if (core != null) {\n        core.close();\n      }\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateClusterState(true);\n\n    return shardId;\n  }\n\n","sourceOld":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return the shardId for the SolrCore\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n // we only put a subset of props into the leader node\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    ZkNodeProps leaderProps = new ZkNodeProps(props);\n    \n    try {\n      joinElection(desc);\n    } catch (InterruptedException e) {\n      // Restore the interrupted status\n      Thread.currentThread().interrupt();\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (KeeperException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (IOException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    }\n    \n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();\n    \n    // now wait until our currently cloud state contains the latest leader\n    String clusterStateLeader = zkStateReader.getLeaderUrl(collection, shardId, 30000);\n    int tries = 0;\n    while (!leaderUrl.equals(clusterStateLeader)) {\n      if (tries == 60) {\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"There is conflicting information about the leader of shard: \"\n                + cloudDesc.getShardId() + \" our state says:\" + clusterStateLeader + \" but zookeeper says:\" + leaderUrl);\n      }\n      Thread.sleep(1000);\n      tries++;\n      clusterStateLeader = zkStateReader.getLeaderUrl(collection, shardId, 30000);\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();\n    }\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    if (cc != null) { // CoreContainer only null in tests\n      try {\n        core = cc.getCore(desc.getName());\n\n \n        // recover from local transaction log and wait for it to complete before\n        // going active\n        // TODO: should this be moved to another thread? To recoveryStrat?\n        // TODO: should this actually be done earlier, before (or as part of)\n        // leader election perhaps?\n        // TODO: if I'm the leader, ensure that a replica that is trying to recover waits until I'm\n        // active (or don't make me the\n        // leader until my local replay is done.\n\n        UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n        if (!core.isReloaded() && ulog != null) {\n          Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()\n              .getUpdateLog().recoverFromLog();\n          if (recoveryFuture != null) {\n            recoveryFuture.get(); // NOTE: this could potentially block for\n            // minutes or more!\n            // TODO: public as recovering in the mean time?\n            // TODO: in the future we could do peerync in parallel with recoverFromLog\n          } else {\n            log.info(\"No LogReplay needed for core=\"+core.getName() + \" baseURL=\" + baseUrl);\n          }\n        }\n        \n        boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n            collection, coreZkNodeName, shardId, leaderProps, core, cc);\n        if (!didRecovery) {\n          publish(desc, ZkStateReader.ACTIVE);\n        }\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n    } else {\n      publish(desc, ZkStateReader.ACTIVE);\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateClusterState(true);\n\n    return shardId;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"001b25b42373b22a52f399dbf072f1224632e8e6","date":1345889167,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","sourceNew":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return the shardId for the SolrCore\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n // we only put a subset of props into the leader node\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    ZkNodeProps leaderProps = new ZkNodeProps(props);\n    \n    try {\n      joinElection(desc);\n    } catch (InterruptedException e) {\n      // Restore the interrupted status\n      Thread.currentThread().interrupt();\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (KeeperException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (IOException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    }\n    \n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();\n    \n    // now wait until our currently cloud state contains the latest leader\n    String clusterStateLeader = zkStateReader.getLeaderUrl(collection, shardId, 30000);\n    int tries = 0;\n    while (!leaderUrl.equals(clusterStateLeader)) {\n      if (tries == 60) {\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"There is conflicting information about the leader of shard: \"\n                + cloudDesc.getShardId() + \" our state says:\" + clusterStateLeader + \" but zookeeper says:\" + leaderUrl);\n      }\n      Thread.sleep(1000);\n      tries++;\n      clusterStateLeader = zkStateReader.getLeaderUrl(collection, shardId, 30000);\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();\n    }\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    try {\n      core = cc.getCore(desc.getName());\n\n \n      // recover from local transaction log and wait for it to complete before\n      // going active\n      // TODO: should this be moved to another thread? To recoveryStrat?\n      // TODO: should this actually be done earlier, before (or as part of)\n      // leader election perhaps?\n      // TODO: if I'm the leader, ensure that a replica that is trying to recover waits until I'm\n      // active (or don't make me the\n      // leader until my local replay is done.\n\n      UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n      if (!core.isReloaded() && ulog != null) {\n        Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()\n            .getUpdateLog().recoverFromLog();\n        if (recoveryFuture != null) {\n          recoveryFuture.get(); // NOTE: this could potentially block for\n          // minutes or more!\n          // TODO: public as recovering in the mean time?\n          // TODO: in the future we could do peerync in parallel with recoverFromLog\n        } else {\n          log.info(\"No LogReplay needed for core=\"+core.getName() + \" baseURL=\" + baseUrl);\n        }\n      }\n      \n      boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n          collection, coreZkNodeName, shardId, leaderProps, core, cc);\n      if (!didRecovery) {\n        publish(desc, ZkStateReader.ACTIVE);\n      }\n    } finally {\n      if (core != null) {\n        core.close();\n      }\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateClusterState(true);\n\n    return shardId;\n  }\n\n","sourceOld":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return the shardId for the SolrCore\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n // we only put a subset of props into the leader node\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    ZkNodeProps leaderProps = new ZkNodeProps(props);\n    \n    try {\n      joinElection(desc);\n    } catch (InterruptedException e) {\n      // Restore the interrupted status\n      Thread.currentThread().interrupt();\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (KeeperException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (IOException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    }\n    \n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();\n    \n    // now wait until our currently cloud state contains the latest leader\n    String clusterStateLeader = zkStateReader.getLeaderUrl(collection, shardId, 30000);\n    int tries = 0;\n    while (!leaderUrl.equals(clusterStateLeader)) {\n      if (tries == 60) {\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"There is conflicting information about the leader of shard: \"\n                + cloudDesc.getShardId() + \" our state says:\" + clusterStateLeader + \" but zookeeper says:\" + leaderUrl);\n      }\n      Thread.sleep(1000);\n      tries++;\n      clusterStateLeader = zkStateReader.getLeaderUrl(collection, shardId, 30000);\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();\n    }\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    if (cc != null) { // CoreContainer only null in tests\n      try {\n        core = cc.getCore(desc.getName());\n\n \n        // recover from local transaction log and wait for it to complete before\n        // going active\n        // TODO: should this be moved to another thread? To recoveryStrat?\n        // TODO: should this actually be done earlier, before (or as part of)\n        // leader election perhaps?\n        // TODO: if I'm the leader, ensure that a replica that is trying to recover waits until I'm\n        // active (or don't make me the\n        // leader until my local replay is done.\n\n        UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n        if (!core.isReloaded() && ulog != null) {\n          Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()\n              .getUpdateLog().recoverFromLog();\n          if (recoveryFuture != null) {\n            recoveryFuture.get(); // NOTE: this could potentially block for\n            // minutes or more!\n            // TODO: public as recovering in the mean time?\n            // TODO: in the future we could do peerync in parallel with recoverFromLog\n          } else {\n            log.info(\"No LogReplay needed for core=\"+core.getName() + \" baseURL=\" + baseUrl);\n          }\n        }\n        \n        boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n            collection, coreZkNodeName, shardId, leaderProps, core, cc);\n        if (!didRecovery) {\n          publish(desc, ZkStateReader.ACTIVE);\n        }\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n    } else {\n      publish(desc, ZkStateReader.ACTIVE);\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateClusterState(true);\n\n    return shardId;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6013b4c7388f1627659c8f96c44abd10a294d3a6","date":1346343796,"type":5,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","sourceNew":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @param afterExpiration\n   * @return the shardId for the SolrCore\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores, boolean afterExpiration) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n // we only put a subset of props into the leader node\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    ZkNodeProps leaderProps = new ZkNodeProps(props);\n    \n    try {\n      joinElection(desc, afterExpiration);\n    } catch (InterruptedException e) {\n      // Restore the interrupted status\n      Thread.currentThread().interrupt();\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (KeeperException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (IOException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    }\n    \n    String leaderUrl = getLeader(cloudDesc);\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    try {\n      core = cc.getCore(desc.getName());\n\n \n      // recover from local transaction log and wait for it to complete before\n      // going active\n      // TODO: should this be moved to another thread? To recoveryStrat?\n      // TODO: should this actually be done earlier, before (or as part of)\n      // leader election perhaps?\n      // TODO: if I'm the leader, ensure that a replica that is trying to recover waits until I'm\n      // active (or don't make me the\n      // leader until my local replay is done.\n\n      UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n      if (!core.isReloaded() && ulog != null) {\n        Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()\n            .getUpdateLog().recoverFromLog();\n        if (recoveryFuture != null) {\n          recoveryFuture.get(); // NOTE: this could potentially block for\n          // minutes or more!\n          // TODO: public as recovering in the mean time?\n          // TODO: in the future we could do peerync in parallel with recoverFromLog\n        } else {\n          log.info(\"No LogReplay needed for core=\"+core.getName() + \" baseURL=\" + baseUrl);\n        }\n      }      \n      boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n          collection, coreZkNodeName, shardId, leaderProps, core, cc);\n      if (!didRecovery) {\n        publish(desc, ZkStateReader.ACTIVE);\n      }\n    } finally {\n      if (core != null) {\n        core.close();\n      }\n    }\n\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateClusterState(true);\n\n    return shardId;\n  }\n\n","sourceOld":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return the shardId for the SolrCore\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n // we only put a subset of props into the leader node\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    ZkNodeProps leaderProps = new ZkNodeProps(props);\n    \n    try {\n      joinElection(desc);\n    } catch (InterruptedException e) {\n      // Restore the interrupted status\n      Thread.currentThread().interrupt();\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (KeeperException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (IOException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    }\n    \n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();\n    \n    // now wait until our currently cloud state contains the latest leader\n    String clusterStateLeader = zkStateReader.getLeaderUrl(collection, shardId, 30000);\n    int tries = 0;\n    while (!leaderUrl.equals(clusterStateLeader)) {\n      if (tries == 60) {\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"There is conflicting information about the leader of shard: \"\n                + cloudDesc.getShardId() + \" our state says:\" + clusterStateLeader + \" but zookeeper says:\" + leaderUrl);\n      }\n      Thread.sleep(1000);\n      tries++;\n      clusterStateLeader = zkStateReader.getLeaderUrl(collection, shardId, 30000);\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();\n    }\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    try {\n      core = cc.getCore(desc.getName());\n\n \n      // recover from local transaction log and wait for it to complete before\n      // going active\n      // TODO: should this be moved to another thread? To recoveryStrat?\n      // TODO: should this actually be done earlier, before (or as part of)\n      // leader election perhaps?\n      // TODO: if I'm the leader, ensure that a replica that is trying to recover waits until I'm\n      // active (or don't make me the\n      // leader until my local replay is done.\n\n      UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n      if (!core.isReloaded() && ulog != null) {\n        Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()\n            .getUpdateLog().recoverFromLog();\n        if (recoveryFuture != null) {\n          recoveryFuture.get(); // NOTE: this could potentially block for\n          // minutes or more!\n          // TODO: public as recovering in the mean time?\n          // TODO: in the future we could do peerync in parallel with recoverFromLog\n        } else {\n          log.info(\"No LogReplay needed for core=\"+core.getName() + \" baseURL=\" + baseUrl);\n        }\n      }\n      \n      boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n          collection, coreZkNodeName, shardId, leaderProps, core, cc);\n      if (!didRecovery) {\n        publish(desc, ZkStateReader.ACTIVE);\n      }\n    } finally {\n      if (core != null) {\n        core.close();\n      }\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateClusterState(true);\n\n    return shardId;\n  }\n\n","bugFix":null,"bugIntro":["f9a98541130dbb2dd570f39bd89ced65760cad80"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"05a14b2611ead08655a2b2bdc61632eb31316e57","date":1346366621,"type":4,"author":"Uwe Schindler","isMerge":true,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","sourceNew":null,"sourceOld":"  /**\n   * Register shard with ZooKeeper.\n   * \n   * @param coreName\n   * @param desc\n   * @param recoverReloadedCores\n   * @return the shardId for the SolrCore\n   * @throws Exception\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  \n    final String baseUrl = getBaseUrl();\n    \n    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();\n    final String collection = cloudDesc.getCollectionName();\n\n    final String coreZkNodeName = getNodeName() + \"_\" + coreName;\n    \n    String shardId = cloudDesc.getShardId();\n\n    Map<String,String> props = new HashMap<String,String>();\n // we only put a subset of props into the leader node\n    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);\n    props.put(ZkStateReader.CORE_NAME_PROP, coreName);\n    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());\n\n\n    if (log.isInfoEnabled()) {\n        log.info(\"Register shard - core:\" + coreName + \" address:\"\n            + baseUrl + \" shardId:\" + shardId);\n    }\n\n    ZkNodeProps leaderProps = new ZkNodeProps(props);\n    \n    try {\n      joinElection(desc);\n    } catch (InterruptedException e) {\n      // Restore the interrupted status\n      Thread.currentThread().interrupt();\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (KeeperException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    } catch (IOException e) {\n      throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n    }\n    \n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();\n    \n    // now wait until our currently cloud state contains the latest leader\n    String clusterStateLeader = zkStateReader.getLeaderUrl(collection, shardId, 30000);\n    int tries = 0;\n    while (!leaderUrl.equals(clusterStateLeader)) {\n      if (tries == 60) {\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"There is conflicting information about the leader of shard: \"\n                + cloudDesc.getShardId() + \" our state says:\" + clusterStateLeader + \" but zookeeper says:\" + leaderUrl);\n      }\n      Thread.sleep(1000);\n      tries++;\n      clusterStateLeader = zkStateReader.getLeaderUrl(collection, shardId, 30000);\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();\n    }\n    \n    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    log.info(\"We are \" + ourUrl + \" and leader is \" + leaderUrl);\n    boolean isLeader = leaderUrl.equals(ourUrl);\n    \n\n    SolrCore core = null;\n    try {\n      core = cc.getCore(desc.getName());\n\n \n      // recover from local transaction log and wait for it to complete before\n      // going active\n      // TODO: should this be moved to another thread? To recoveryStrat?\n      // TODO: should this actually be done earlier, before (or as part of)\n      // leader election perhaps?\n      // TODO: if I'm the leader, ensure that a replica that is trying to recover waits until I'm\n      // active (or don't make me the\n      // leader until my local replay is done.\n\n      UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n      if (!core.isReloaded() && ulog != null) {\n        Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()\n            .getUpdateLog().recoverFromLog();\n        if (recoveryFuture != null) {\n          recoveryFuture.get(); // NOTE: this could potentially block for\n          // minutes or more!\n          // TODO: public as recovering in the mean time?\n          // TODO: in the future we could do peerync in parallel with recoverFromLog\n        } else {\n          log.info(\"No LogReplay needed for core=\"+core.getName() + \" baseURL=\" + baseUrl);\n        }\n      }\n      \n      boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n          collection, coreZkNodeName, shardId, leaderProps, core, cc);\n      if (!didRecovery) {\n        publish(desc, ZkStateReader.ACTIVE);\n      }\n    } finally {\n      if (core != null) {\n        core.close();\n      }\n    }\n    \n    // make sure we have an update cluster state right away\n    zkStateReader.updateClusterState(true);\n\n    return shardId;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d528fd7ae22865015b756e0a03832e2051de2a9c","date":1476721105,"type":1,"author":"Alan Woodward","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor).mjava","sourceNew":"  /**\n   * Register shard with ZooKeeper.\n   *\n   * @return the shardId for the SolrCore\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean skipRecovery) throws Exception {\n    return register(coreName, desc, false, false, skipRecovery);\n  }\n\n","sourceOld":"  /**\n   * Register shard with ZooKeeper.\n   *\n   * @return the shardId for the SolrCore\n   */\n  public String register(String coreName, final CoreDescriptor desc) throws Exception {\n    return register(coreName, desc, false, false);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#register(String,CoreDescriptor,boolean).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Register shard with ZooKeeper.\n   *\n   * @return the shardId for the SolrCore\n   */\n  public String register(String coreName, final CoreDescriptor desc, boolean skipRecovery) throws Exception {\n    return register(coreName, desc, false, false, skipRecovery);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"001b25b42373b22a52f399dbf072f1224632e8e6":["d6f074e73200c07d54f242d3880a8da5a35ff97b","adc2388a5005de25370273411bc713d0ff722805"],"adc2388a5005de25370273411bc713d0ff722805":["3f767f8c99eaedb984df754fe61f21c5de260f94"],"ff2d7326b1f013c8da9bad45b1e98a3d16c38575":["9813dd0748537c429b7c0a9b4723ea1ba496c047"],"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab":["3a80994db3380cd78c6f65b84515e2e931b6b3da","ff2d7326b1f013c8da9bad45b1e98a3d16c38575"],"fd92b8bcc88e969302510acf77bd6970da3994c4":["0d22ac6a4146774c1bc8400160fc0b6150294e92","5dd811388ff9e790b1bf628567bdf6da4ca24cb8"],"c516f1fb8a8bbd4b308968d3f638304888ce86d1":["5dd811388ff9e790b1bf628567bdf6da4ca24cb8"],"9981f0cb0e2c6b6512a483e09fa27c840ad08e36":["2c007e7c4cf8c55bc2a5884e315123afaaeec87f"],"6013b4c7388f1627659c8f96c44abd10a294d3a6":["adc2388a5005de25370273411bc713d0ff722805"],"c824b5854f7ad30cd53f0634fc7cb533df74590b":["049f7138250089cf12d36414ed76f8078e18fb5b"],"0d22ac6a4146774c1bc8400160fc0b6150294e92":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","2c007e7c4cf8c55bc2a5884e315123afaaeec87f"],"5dd811388ff9e790b1bf628567bdf6da4ca24cb8":["f97bf7e60e8b1531f7e992482be43dd563dde04e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"9813dd0748537c429b7c0a9b4723ea1ba496c047":["7273051a9674cdb58df8b00d87e9821d9ddf5744"],"78a55f24d9b493c2a1cecf79f1d78279062b545b":["a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d","5dd811388ff9e790b1bf628567bdf6da4ca24cb8"],"049f7138250089cf12d36414ed76f8078e18fb5b":["a5093a9e893633cc091cf2f729d7863671c2b715"],"f97bf7e60e8b1531f7e992482be43dd563dde04e":["9981f0cb0e2c6b6512a483e09fa27c840ad08e36"],"a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","2c007e7c4cf8c55bc2a5884e315123afaaeec87f"],"7273051a9674cdb58df8b00d87e9821d9ddf5744":["3a80994db3380cd78c6f65b84515e2e931b6b3da"],"d528fd7ae22865015b756e0a03832e2051de2a9c":["6013b4c7388f1627659c8f96c44abd10a294d3a6"],"8fd5be977c105554c6a7b68afcdbc511439723ab":["a5093a9e893633cc091cf2f729d7863671c2b715","3f767f8c99eaedb984df754fe61f21c5de260f94"],"05a14b2611ead08655a2b2bdc61632eb31316e57":["001b25b42373b22a52f399dbf072f1224632e8e6","6013b4c7388f1627659c8f96c44abd10a294d3a6"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["6013b4c7388f1627659c8f96c44abd10a294d3a6","d528fd7ae22865015b756e0a03832e2051de2a9c"],"3f767f8c99eaedb984df754fe61f21c5de260f94":["c824b5854f7ad30cd53f0634fc7cb533df74590b"],"a5093a9e893633cc091cf2f729d7863671c2b715":["ff2d7326b1f013c8da9bad45b1e98a3d16c38575"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["a5093a9e893633cc091cf2f729d7863671c2b715","3f767f8c99eaedb984df754fe61f21c5de260f94"],"2c007e7c4cf8c55bc2a5884e315123afaaeec87f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"3a80994db3380cd78c6f65b84515e2e931b6b3da":["c516f1fb8a8bbd4b308968d3f638304888ce86d1"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d528fd7ae22865015b756e0a03832e2051de2a9c"]},"commit2Childs":{"001b25b42373b22a52f399dbf072f1224632e8e6":["05a14b2611ead08655a2b2bdc61632eb31316e57"],"adc2388a5005de25370273411bc713d0ff722805":["001b25b42373b22a52f399dbf072f1224632e8e6","6013b4c7388f1627659c8f96c44abd10a294d3a6"],"ff2d7326b1f013c8da9bad45b1e98a3d16c38575":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","a5093a9e893633cc091cf2f729d7863671c2b715"],"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab":[],"fd92b8bcc88e969302510acf77bd6970da3994c4":[],"c516f1fb8a8bbd4b308968d3f638304888ce86d1":["3a80994db3380cd78c6f65b84515e2e931b6b3da"],"9981f0cb0e2c6b6512a483e09fa27c840ad08e36":["f97bf7e60e8b1531f7e992482be43dd563dde04e"],"6013b4c7388f1627659c8f96c44abd10a294d3a6":["d528fd7ae22865015b756e0a03832e2051de2a9c","05a14b2611ead08655a2b2bdc61632eb31316e57","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"0d22ac6a4146774c1bc8400160fc0b6150294e92":["fd92b8bcc88e969302510acf77bd6970da3994c4"],"c824b5854f7ad30cd53f0634fc7cb533df74590b":["3f767f8c99eaedb984df754fe61f21c5de260f94"],"5dd811388ff9e790b1bf628567bdf6da4ca24cb8":["fd92b8bcc88e969302510acf77bd6970da3994c4","c516f1fb8a8bbd4b308968d3f638304888ce86d1","78a55f24d9b493c2a1cecf79f1d78279062b545b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["0d22ac6a4146774c1bc8400160fc0b6150294e92","a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d","2c007e7c4cf8c55bc2a5884e315123afaaeec87f"],"9813dd0748537c429b7c0a9b4723ea1ba496c047":["ff2d7326b1f013c8da9bad45b1e98a3d16c38575"],"78a55f24d9b493c2a1cecf79f1d78279062b545b":[],"049f7138250089cf12d36414ed76f8078e18fb5b":["c824b5854f7ad30cd53f0634fc7cb533df74590b"],"f97bf7e60e8b1531f7e992482be43dd563dde04e":["5dd811388ff9e790b1bf628567bdf6da4ca24cb8"],"a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d":["78a55f24d9b493c2a1cecf79f1d78279062b545b"],"7273051a9674cdb58df8b00d87e9821d9ddf5744":["9813dd0748537c429b7c0a9b4723ea1ba496c047"],"d528fd7ae22865015b756e0a03832e2051de2a9c":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"8fd5be977c105554c6a7b68afcdbc511439723ab":[],"05a14b2611ead08655a2b2bdc61632eb31316e57":[],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"3f767f8c99eaedb984df754fe61f21c5de260f94":["adc2388a5005de25370273411bc713d0ff722805","8fd5be977c105554c6a7b68afcdbc511439723ab","d6f074e73200c07d54f242d3880a8da5a35ff97b"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["001b25b42373b22a52f399dbf072f1224632e8e6"],"a5093a9e893633cc091cf2f729d7863671c2b715":["049f7138250089cf12d36414ed76f8078e18fb5b","8fd5be977c105554c6a7b68afcdbc511439723ab","d6f074e73200c07d54f242d3880a8da5a35ff97b"],"2c007e7c4cf8c55bc2a5884e315123afaaeec87f":["9981f0cb0e2c6b6512a483e09fa27c840ad08e36","0d22ac6a4146774c1bc8400160fc0b6150294e92","a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d"],"3a80994db3380cd78c6f65b84515e2e931b6b3da":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","7273051a9674cdb58df8b00d87e9821d9ddf5744"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","fd92b8bcc88e969302510acf77bd6970da3994c4","78a55f24d9b493c2a1cecf79f1d78279062b545b","8fd5be977c105554c6a7b68afcdbc511439723ab","05a14b2611ead08655a2b2bdc61632eb31316e57","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}