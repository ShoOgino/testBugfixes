{"path":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#flushOffsets(int[]).mjava","commits":[{"id":"eda61b1e90b490cc5837200e04c02639a0d272c7","date":1358795519,"type":0,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#flushOffsets(int[]).mjava","pathOld":"/dev/null","sourceNew":"  private void flushOffsets(int[] fieldNums) throws IOException {\n    boolean hasOffsets = false;\n    long[] sumPos = new long[fieldNums.length];\n    long[] sumOffsets = new long[fieldNums.length];\n    for (DocData dd : pendingDocs) {\n      for (FieldData fd : dd.fields) {\n        hasOffsets |= fd.hasOffsets;\n        if (fd.hasOffsets && fd.hasPositions) {\n          final int fieldNumOff = Arrays.binarySearch(fieldNums, fd.fieldNum);\n          int pos = 0;\n          for (int i = 0; i < fd.numTerms; ++i) {\n            int previousPos = 0;\n            int previousOff = 0;\n            for (int j = 0; j < fd.freqs[i]; ++j) {\n              final int position = positionsBuf[fd.posStart + pos];\n              final int startOffset = startOffsetsBuf[fd.offStart + pos];\n              sumPos[fieldNumOff] += position - previousPos;\n              sumOffsets[fieldNumOff] += startOffset - previousOff;\n              previousPos = position;\n              previousOff = startOffset;\n              ++pos;\n            }\n          }\n          assert pos == fd.totalPositions;\n        }\n      }\n    }\n\n    if (!hasOffsets) {\n      // nothing to do\n      return;\n    }\n\n    final float[] charsPerTerm = new float[fieldNums.length];\n    for (int i = 0; i < fieldNums.length; ++i) {\n      charsPerTerm[i] = (sumPos[i] <= 0 || sumOffsets[i] <= 0) ? 0 : (float) ((double) sumOffsets[i] / sumPos[i]);\n    }\n\n    // start offsets\n    for (int i = 0; i < fieldNums.length; ++i) {\n      vectorsStream.writeInt(Float.floatToRawIntBits(charsPerTerm[i]));\n    }\n\n    writer.reset(vectorsStream);\n    for (DocData dd : pendingDocs) {\n      for (FieldData fd : dd.fields) {\n        if ((fd.flags & OFFSETS) != 0) {\n          final int fieldNumOff = Arrays.binarySearch(fieldNums, fd.fieldNum);\n          final float cpt = charsPerTerm[fieldNumOff];\n          int pos = 0;\n          for (int i = 0; i < fd.numTerms; ++i) {\n            int previousPos = 0;\n            int previousOff = 0;\n            for (int j = 0; j < fd.freqs[i]; ++j) {\n              final int position = fd.hasPositions ? positionsBuf[fd.posStart + pos] : 0;\n              final int startOffset = startOffsetsBuf[fd.offStart + pos];\n              writer.add(startOffset - previousOff - (int) (cpt * (position - previousPos)));\n              previousPos = position;\n              previousOff = startOffset;\n              ++pos;\n            }\n          }\n        }\n      }\n    }\n    writer.finish();\n\n    // lengths\n    writer.reset(vectorsStream);\n    for (DocData dd : pendingDocs) {\n      for (FieldData fd : dd.fields) {\n        if ((fd.flags & OFFSETS) != 0) {\n          int pos = 0;\n          for (int i = 0; i < fd.numTerms; ++i) {\n            for (int j = 0; j < fd.freqs[i]; ++j) {\n              writer.add(lengthsBuf[fd.offStart + pos++] - fd.prefixLengths[i] - fd.suffixLengths[i]);\n            }\n          }\n          assert pos == fd.totalPositions;\n        }\n      }\n    }\n    writer.finish();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"07155cdd910937cdf6877e48884d5782845c8b8b","date":1358796205,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#flushOffsets(int[]).mjava","pathOld":"/dev/null","sourceNew":"  private void flushOffsets(int[] fieldNums) throws IOException {\n    boolean hasOffsets = false;\n    long[] sumPos = new long[fieldNums.length];\n    long[] sumOffsets = new long[fieldNums.length];\n    for (DocData dd : pendingDocs) {\n      for (FieldData fd : dd.fields) {\n        hasOffsets |= fd.hasOffsets;\n        if (fd.hasOffsets && fd.hasPositions) {\n          final int fieldNumOff = Arrays.binarySearch(fieldNums, fd.fieldNum);\n          int pos = 0;\n          for (int i = 0; i < fd.numTerms; ++i) {\n            int previousPos = 0;\n            int previousOff = 0;\n            for (int j = 0; j < fd.freqs[i]; ++j) {\n              final int position = positionsBuf[fd.posStart + pos];\n              final int startOffset = startOffsetsBuf[fd.offStart + pos];\n              sumPos[fieldNumOff] += position - previousPos;\n              sumOffsets[fieldNumOff] += startOffset - previousOff;\n              previousPos = position;\n              previousOff = startOffset;\n              ++pos;\n            }\n          }\n          assert pos == fd.totalPositions;\n        }\n      }\n    }\n\n    if (!hasOffsets) {\n      // nothing to do\n      return;\n    }\n\n    final float[] charsPerTerm = new float[fieldNums.length];\n    for (int i = 0; i < fieldNums.length; ++i) {\n      charsPerTerm[i] = (sumPos[i] <= 0 || sumOffsets[i] <= 0) ? 0 : (float) ((double) sumOffsets[i] / sumPos[i]);\n    }\n\n    // start offsets\n    for (int i = 0; i < fieldNums.length; ++i) {\n      vectorsStream.writeInt(Float.floatToRawIntBits(charsPerTerm[i]));\n    }\n\n    writer.reset(vectorsStream);\n    for (DocData dd : pendingDocs) {\n      for (FieldData fd : dd.fields) {\n        if ((fd.flags & OFFSETS) != 0) {\n          final int fieldNumOff = Arrays.binarySearch(fieldNums, fd.fieldNum);\n          final float cpt = charsPerTerm[fieldNumOff];\n          int pos = 0;\n          for (int i = 0; i < fd.numTerms; ++i) {\n            int previousPos = 0;\n            int previousOff = 0;\n            for (int j = 0; j < fd.freqs[i]; ++j) {\n              final int position = fd.hasPositions ? positionsBuf[fd.posStart + pos] : 0;\n              final int startOffset = startOffsetsBuf[fd.offStart + pos];\n              writer.add(startOffset - previousOff - (int) (cpt * (position - previousPos)));\n              previousPos = position;\n              previousOff = startOffset;\n              ++pos;\n            }\n          }\n        }\n      }\n    }\n    writer.finish();\n\n    // lengths\n    writer.reset(vectorsStream);\n    for (DocData dd : pendingDocs) {\n      for (FieldData fd : dd.fields) {\n        if ((fd.flags & OFFSETS) != 0) {\n          int pos = 0;\n          for (int i = 0; i < fd.numTerms; ++i) {\n            for (int j = 0; j < fd.freqs[i]; ++j) {\n              writer.add(lengthsBuf[fd.offStart + pos++] - fd.prefixLengths[i] - fd.suffixLengths[i]);\n            }\n          }\n          assert pos == fd.totalPositions;\n        }\n      }\n    }\n    writer.finish();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a213f48d389d9553328ba1ae94fa937cf55b5a63","date":1578298748,"type":3,"author":"kkewwei","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#flushOffsets(int[]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#flushOffsets(int[]).mjava","sourceNew":"  private void flushOffsets(int[] fieldNums) throws IOException {\n    boolean hasOffsets = false;\n    long[] sumPos = new long[fieldNums.length];\n    long[] sumOffsets = new long[fieldNums.length];\n    for (DocData dd : pendingDocs) {\n      for (FieldData fd : dd.fields) {\n        hasOffsets |= fd.hasOffsets;\n        if (fd.hasOffsets && fd.hasPositions) {\n          final int fieldNumOff = Arrays.binarySearch(fieldNums, fd.fieldNum);\n          int pos = 0;\n          for (int i = 0; i < fd.numTerms; ++i) {\n            sumPos[fieldNumOff] += positionsBuf[fd.posStart + fd.freqs[i]-1 + pos];\n            sumOffsets[fieldNumOff] += startOffsetsBuf[fd.offStart + fd.freqs[i]-1 + pos];\n            pos += fd.freqs[i];\n          }\n          assert pos == fd.totalPositions;\n        }\n      }\n    }\n\n    if (!hasOffsets) {\n      // nothing to do\n      return;\n    }\n\n    final float[] charsPerTerm = new float[fieldNums.length];\n    for (int i = 0; i < fieldNums.length; ++i) {\n      charsPerTerm[i] = (sumPos[i] <= 0 || sumOffsets[i] <= 0) ? 0 : (float) ((double) sumOffsets[i] / sumPos[i]);\n    }\n\n    // start offsets\n    for (int i = 0; i < fieldNums.length; ++i) {\n      vectorsStream.writeInt(Float.floatToRawIntBits(charsPerTerm[i]));\n    }\n\n    writer.reset(vectorsStream);\n    for (DocData dd : pendingDocs) {\n      for (FieldData fd : dd.fields) {\n        if ((fd.flags & OFFSETS) != 0) {\n          final int fieldNumOff = Arrays.binarySearch(fieldNums, fd.fieldNum);\n          final float cpt = charsPerTerm[fieldNumOff];\n          int pos = 0;\n          for (int i = 0; i < fd.numTerms; ++i) {\n            int previousPos = 0;\n            int previousOff = 0;\n            for (int j = 0; j < fd.freqs[i]; ++j) {\n              final int position = fd.hasPositions ? positionsBuf[fd.posStart + pos] : 0;\n              final int startOffset = startOffsetsBuf[fd.offStart + pos];\n              writer.add(startOffset - previousOff - (int) (cpt * (position - previousPos)));\n              previousPos = position;\n              previousOff = startOffset;\n              ++pos;\n            }\n          }\n        }\n      }\n    }\n    writer.finish();\n\n    // lengths\n    writer.reset(vectorsStream);\n    for (DocData dd : pendingDocs) {\n      for (FieldData fd : dd.fields) {\n        if ((fd.flags & OFFSETS) != 0) {\n          int pos = 0;\n          for (int i = 0; i < fd.numTerms; ++i) {\n            for (int j = 0; j < fd.freqs[i]; ++j) {\n              writer.add(lengthsBuf[fd.offStart + pos++] - fd.prefixLengths[i] - fd.suffixLengths[i]);\n            }\n          }\n          assert pos == fd.totalPositions;\n        }\n      }\n    }\n    writer.finish();\n  }\n\n","sourceOld":"  private void flushOffsets(int[] fieldNums) throws IOException {\n    boolean hasOffsets = false;\n    long[] sumPos = new long[fieldNums.length];\n    long[] sumOffsets = new long[fieldNums.length];\n    for (DocData dd : pendingDocs) {\n      for (FieldData fd : dd.fields) {\n        hasOffsets |= fd.hasOffsets;\n        if (fd.hasOffsets && fd.hasPositions) {\n          final int fieldNumOff = Arrays.binarySearch(fieldNums, fd.fieldNum);\n          int pos = 0;\n          for (int i = 0; i < fd.numTerms; ++i) {\n            int previousPos = 0;\n            int previousOff = 0;\n            for (int j = 0; j < fd.freqs[i]; ++j) {\n              final int position = positionsBuf[fd.posStart + pos];\n              final int startOffset = startOffsetsBuf[fd.offStart + pos];\n              sumPos[fieldNumOff] += position - previousPos;\n              sumOffsets[fieldNumOff] += startOffset - previousOff;\n              previousPos = position;\n              previousOff = startOffset;\n              ++pos;\n            }\n          }\n          assert pos == fd.totalPositions;\n        }\n      }\n    }\n\n    if (!hasOffsets) {\n      // nothing to do\n      return;\n    }\n\n    final float[] charsPerTerm = new float[fieldNums.length];\n    for (int i = 0; i < fieldNums.length; ++i) {\n      charsPerTerm[i] = (sumPos[i] <= 0 || sumOffsets[i] <= 0) ? 0 : (float) ((double) sumOffsets[i] / sumPos[i]);\n    }\n\n    // start offsets\n    for (int i = 0; i < fieldNums.length; ++i) {\n      vectorsStream.writeInt(Float.floatToRawIntBits(charsPerTerm[i]));\n    }\n\n    writer.reset(vectorsStream);\n    for (DocData dd : pendingDocs) {\n      for (FieldData fd : dd.fields) {\n        if ((fd.flags & OFFSETS) != 0) {\n          final int fieldNumOff = Arrays.binarySearch(fieldNums, fd.fieldNum);\n          final float cpt = charsPerTerm[fieldNumOff];\n          int pos = 0;\n          for (int i = 0; i < fd.numTerms; ++i) {\n            int previousPos = 0;\n            int previousOff = 0;\n            for (int j = 0; j < fd.freqs[i]; ++j) {\n              final int position = fd.hasPositions ? positionsBuf[fd.posStart + pos] : 0;\n              final int startOffset = startOffsetsBuf[fd.offStart + pos];\n              writer.add(startOffset - previousOff - (int) (cpt * (position - previousPos)));\n              previousPos = position;\n              previousOff = startOffset;\n              ++pos;\n            }\n          }\n        }\n      }\n    }\n    writer.finish();\n\n    // lengths\n    writer.reset(vectorsStream);\n    for (DocData dd : pendingDocs) {\n      for (FieldData fd : dd.fields) {\n        if ((fd.flags & OFFSETS) != 0) {\n          int pos = 0;\n          for (int i = 0; i < fd.numTerms; ++i) {\n            for (int j = 0; j < fd.freqs[i]; ++j) {\n              writer.add(lengthsBuf[fd.offStart + pos++] - fd.prefixLengths[i] - fd.suffixLengths[i]);\n            }\n          }\n          assert pos == fd.totalPositions;\n        }\n      }\n    }\n    writer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ca62564055241632cd20d65b5ecb8c8e93bd60c4","date":1578383112,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#flushOffsets(int[]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#flushOffsets(int[]).mjava","sourceNew":"  private void flushOffsets(int[] fieldNums) throws IOException {\n    boolean hasOffsets = false;\n    long[] sumPos = new long[fieldNums.length];\n    long[] sumOffsets = new long[fieldNums.length];\n    for (DocData dd : pendingDocs) {\n      for (FieldData fd : dd.fields) {\n        hasOffsets |= fd.hasOffsets;\n        if (fd.hasOffsets && fd.hasPositions) {\n          final int fieldNumOff = Arrays.binarySearch(fieldNums, fd.fieldNum);\n          int pos = 0;\n          for (int i = 0; i < fd.numTerms; ++i) {\n            sumPos[fieldNumOff] += positionsBuf[fd.posStart + fd.freqs[i]-1 + pos];\n            sumOffsets[fieldNumOff] += startOffsetsBuf[fd.offStart + fd.freqs[i]-1 + pos];\n            pos += fd.freqs[i];\n          }\n          assert pos == fd.totalPositions;\n        }\n      }\n    }\n\n    if (!hasOffsets) {\n      // nothing to do\n      return;\n    }\n\n    final float[] charsPerTerm = new float[fieldNums.length];\n    for (int i = 0; i < fieldNums.length; ++i) {\n      charsPerTerm[i] = (sumPos[i] <= 0 || sumOffsets[i] <= 0) ? 0 : (float) ((double) sumOffsets[i] / sumPos[i]);\n    }\n\n    // start offsets\n    for (int i = 0; i < fieldNums.length; ++i) {\n      vectorsStream.writeInt(Float.floatToRawIntBits(charsPerTerm[i]));\n    }\n\n    writer.reset(vectorsStream);\n    for (DocData dd : pendingDocs) {\n      for (FieldData fd : dd.fields) {\n        if ((fd.flags & OFFSETS) != 0) {\n          final int fieldNumOff = Arrays.binarySearch(fieldNums, fd.fieldNum);\n          final float cpt = charsPerTerm[fieldNumOff];\n          int pos = 0;\n          for (int i = 0; i < fd.numTerms; ++i) {\n            int previousPos = 0;\n            int previousOff = 0;\n            for (int j = 0; j < fd.freqs[i]; ++j) {\n              final int position = fd.hasPositions ? positionsBuf[fd.posStart + pos] : 0;\n              final int startOffset = startOffsetsBuf[fd.offStart + pos];\n              writer.add(startOffset - previousOff - (int) (cpt * (position - previousPos)));\n              previousPos = position;\n              previousOff = startOffset;\n              ++pos;\n            }\n          }\n        }\n      }\n    }\n    writer.finish();\n\n    // lengths\n    writer.reset(vectorsStream);\n    for (DocData dd : pendingDocs) {\n      for (FieldData fd : dd.fields) {\n        if ((fd.flags & OFFSETS) != 0) {\n          int pos = 0;\n          for (int i = 0; i < fd.numTerms; ++i) {\n            for (int j = 0; j < fd.freqs[i]; ++j) {\n              writer.add(lengthsBuf[fd.offStart + pos++] - fd.prefixLengths[i] - fd.suffixLengths[i]);\n            }\n          }\n          assert pos == fd.totalPositions;\n        }\n      }\n    }\n    writer.finish();\n  }\n\n","sourceOld":"  private void flushOffsets(int[] fieldNums) throws IOException {\n    boolean hasOffsets = false;\n    long[] sumPos = new long[fieldNums.length];\n    long[] sumOffsets = new long[fieldNums.length];\n    for (DocData dd : pendingDocs) {\n      for (FieldData fd : dd.fields) {\n        hasOffsets |= fd.hasOffsets;\n        if (fd.hasOffsets && fd.hasPositions) {\n          final int fieldNumOff = Arrays.binarySearch(fieldNums, fd.fieldNum);\n          int pos = 0;\n          for (int i = 0; i < fd.numTerms; ++i) {\n            int previousPos = 0;\n            int previousOff = 0;\n            for (int j = 0; j < fd.freqs[i]; ++j) {\n              final int position = positionsBuf[fd.posStart + pos];\n              final int startOffset = startOffsetsBuf[fd.offStart + pos];\n              sumPos[fieldNumOff] += position - previousPos;\n              sumOffsets[fieldNumOff] += startOffset - previousOff;\n              previousPos = position;\n              previousOff = startOffset;\n              ++pos;\n            }\n          }\n          assert pos == fd.totalPositions;\n        }\n      }\n    }\n\n    if (!hasOffsets) {\n      // nothing to do\n      return;\n    }\n\n    final float[] charsPerTerm = new float[fieldNums.length];\n    for (int i = 0; i < fieldNums.length; ++i) {\n      charsPerTerm[i] = (sumPos[i] <= 0 || sumOffsets[i] <= 0) ? 0 : (float) ((double) sumOffsets[i] / sumPos[i]);\n    }\n\n    // start offsets\n    for (int i = 0; i < fieldNums.length; ++i) {\n      vectorsStream.writeInt(Float.floatToRawIntBits(charsPerTerm[i]));\n    }\n\n    writer.reset(vectorsStream);\n    for (DocData dd : pendingDocs) {\n      for (FieldData fd : dd.fields) {\n        if ((fd.flags & OFFSETS) != 0) {\n          final int fieldNumOff = Arrays.binarySearch(fieldNums, fd.fieldNum);\n          final float cpt = charsPerTerm[fieldNumOff];\n          int pos = 0;\n          for (int i = 0; i < fd.numTerms; ++i) {\n            int previousPos = 0;\n            int previousOff = 0;\n            for (int j = 0; j < fd.freqs[i]; ++j) {\n              final int position = fd.hasPositions ? positionsBuf[fd.posStart + pos] : 0;\n              final int startOffset = startOffsetsBuf[fd.offStart + pos];\n              writer.add(startOffset - previousOff - (int) (cpt * (position - previousPos)));\n              previousPos = position;\n              previousOff = startOffset;\n              ++pos;\n            }\n          }\n        }\n      }\n    }\n    writer.finish();\n\n    // lengths\n    writer.reset(vectorsStream);\n    for (DocData dd : pendingDocs) {\n      for (FieldData fd : dd.fields) {\n        if ((fd.flags & OFFSETS) != 0) {\n          int pos = 0;\n          for (int i = 0; i < fd.numTerms; ++i) {\n            for (int j = 0; j < fd.freqs[i]; ++j) {\n              writer.add(lengthsBuf[fd.offStart + pos++] - fd.prefixLengths[i] - fd.suffixLengths[i]);\n            }\n          }\n          assert pos == fd.totalPositions;\n        }\n      }\n    }\n    writer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"eda61b1e90b490cc5837200e04c02639a0d272c7":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a213f48d389d9553328ba1ae94fa937cf55b5a63":["eda61b1e90b490cc5837200e04c02639a0d272c7"],"07155cdd910937cdf6877e48884d5782845c8b8b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","eda61b1e90b490cc5837200e04c02639a0d272c7"],"ca62564055241632cd20d65b5ecb8c8e93bd60c4":["eda61b1e90b490cc5837200e04c02639a0d272c7","a213f48d389d9553328ba1ae94fa937cf55b5a63"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a213f48d389d9553328ba1ae94fa937cf55b5a63"]},"commit2Childs":{"eda61b1e90b490cc5837200e04c02639a0d272c7":["a213f48d389d9553328ba1ae94fa937cf55b5a63","07155cdd910937cdf6877e48884d5782845c8b8b","ca62564055241632cd20d65b5ecb8c8e93bd60c4"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["eda61b1e90b490cc5837200e04c02639a0d272c7","07155cdd910937cdf6877e48884d5782845c8b8b"],"a213f48d389d9553328ba1ae94fa937cf55b5a63":["ca62564055241632cd20d65b5ecb8c8e93bd60c4","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"07155cdd910937cdf6877e48884d5782845c8b8b":[],"ca62564055241632cd20d65b5ecb8c8e93bd60c4":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["07155cdd910937cdf6877e48884d5782845c8b8b","ca62564055241632cd20d65b5ecb8c8e93bd60c4","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}