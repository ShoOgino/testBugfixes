{"path":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#writeFieldNDims(IndexOutput,IndexOutput,IndexOutput,String,MutablePointValues).mjava","commits":[{"id":"78e689a3b60e84c75dc6dd7b181a71fc19ef8482","date":1591689554,"type":1,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#writeFieldNDims(IndexOutput,IndexOutput,IndexOutput,String,MutablePointValues).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#writeFieldNDims(IndexOutput,String,MutablePointValues).mjava","sourceNew":"  /* In the 2+D case, we recursively pick the split dimension, compute the\n   * median value and partition other values around it. */\n  private Runnable writeFieldNDims(IndexOutput metaOut, IndexOutput indexOut, IndexOutput dataOut, String fieldName, MutablePointValues values) throws IOException {\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and writeField\");\n    }\n\n    // Catch user silliness:\n    if (finished == true) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    finished = true;\n\n    pointCount = values.size();\n\n    final int numLeaves = Math.toIntExact((pointCount + maxPointsInLeafNode - 1) / maxPointsInLeafNode);\n    final int numSplits = numLeaves - 1;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    final byte[] splitPackedValues = new byte[numSplits * bytesPerDim];\n    final byte[] splitDimensionValues = new byte[numSplits];\n    final long[] leafBlockFPs = new long[numLeaves];\n\n    // compute the min/max for this slice\n    computePackedValueBounds(values, 0, Math.toIntExact(pointCount), minPackedValue, maxPackedValue, scratchBytesRef1);\n    for (int i = 0; i < Math.toIntExact(pointCount); ++i) {\n      docsSeen.set(values.getDocID(i));\n    }\n\n    final long dataStartFP = dataOut.getFilePointer();\n    final int[] parentSplits = new int[numIndexDims];\n    build(0, numLeaves, values, 0, Math.toIntExact(pointCount), dataOut,\n          minPackedValue.clone(), maxPackedValue.clone(), parentSplits,\n          splitPackedValues, splitDimensionValues, leafBlockFPs,\n          new int[maxPointsInLeafNode]);\n    assert Arrays.equals(parentSplits, new int[numIndexDims]);\n\n    scratchBytesRef1.length = bytesPerDim;\n    scratchBytesRef1.bytes = splitPackedValues;\n\n    BKDTreeLeafNodes leafNodes  = new BKDTreeLeafNodes() {\n      @Override\n      public long getLeafLP(int index) {\n        return leafBlockFPs[index];\n      }\n\n      @Override\n      public BytesRef getSplitValue(int index) {\n        scratchBytesRef1.offset = index * bytesPerDim;\n        return scratchBytesRef1;\n      }\n\n      @Override\n      public int getSplitDimension(int index) {\n        return splitDimensionValues[index] & 0xff;\n      }\n\n      @Override\n      public int numLeaves() {\n        return leafBlockFPs.length;\n      }\n    };\n\n    return () -> {\n      try {\n        writeIndex(metaOut, indexOut, maxPointsInLeafNode, leafNodes, dataStartFP);\n      } catch (IOException e) {\n        throw new UncheckedIOException(e);\n      }\n    };\n  }\n\n","sourceOld":"  /* In the 2+D case, we recursively pick the split dimension, compute the\n   * median value and partition other values around it. */\n  private long writeFieldNDims(IndexOutput out, String fieldName, MutablePointValues values) throws IOException {\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and writeField\");\n    }\n\n    // Catch user silliness:\n    if (finished == true) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    finished = true;\n\n    pointCount = values.size();\n\n    final int numLeaves = Math.toIntExact((pointCount + maxPointsInLeafNode - 1) / maxPointsInLeafNode);\n    final int numSplits = numLeaves - 1;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    final byte[] splitPackedValues = new byte[numSplits * bytesPerDim];\n    final byte[] splitDimensionValues = new byte[numSplits];\n    final long[] leafBlockFPs = new long[numLeaves];\n\n    // compute the min/max for this slice\n    computePackedValueBounds(values, 0, Math.toIntExact(pointCount), minPackedValue, maxPackedValue, scratchBytesRef1);\n    for (int i = 0; i < Math.toIntExact(pointCount); ++i) {\n      docsSeen.set(values.getDocID(i));\n    }\n\n    final int[] parentSplits = new int[numIndexDims];\n    build(0, numLeaves, values, 0, Math.toIntExact(pointCount), out,\n          minPackedValue.clone(), maxPackedValue.clone(), parentSplits,\n          splitPackedValues, splitDimensionValues, leafBlockFPs,\n          new int[maxPointsInLeafNode]);\n    assert Arrays.equals(parentSplits, new int[numIndexDims]);\n\n    scratchBytesRef1.length = bytesPerDim;\n    scratchBytesRef1.bytes = splitPackedValues;\n\n    BKDTreeLeafNodes leafNodes  = new BKDTreeLeafNodes() {\n      @Override\n      public long getLeafLP(int index) {\n        return leafBlockFPs[index];\n      }\n\n      @Override\n      public BytesRef getSplitValue(int index) {\n        scratchBytesRef1.offset = index * bytesPerDim;\n        return scratchBytesRef1;\n      }\n\n      @Override\n      public int getSplitDimension(int index) {\n        return splitDimensionValues[index] & 0xff;\n      }\n\n      @Override\n      public int numLeaves() {\n        return leafBlockFPs.length;\n      }\n    };\n\n    long indexFP = out.getFilePointer();\n    writeIndex(out, maxPointsInLeafNode, leafNodes);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bb94bf667d51f9c390c99d97afb36b7caab6b6e9","date":1599548621,"type":3,"author":"Ignacio Vera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#writeFieldNDims(IndexOutput,IndexOutput,IndexOutput,String,MutablePointValues).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#writeFieldNDims(IndexOutput,IndexOutput,IndexOutput,String,MutablePointValues).mjava","sourceNew":"  /* In the 2+D case, we recursively pick the split dimension, compute the\n   * median value and partition other values around it. */\n  private Runnable writeFieldNDims(IndexOutput metaOut, IndexOutput indexOut, IndexOutput dataOut, String fieldName, MutablePointValues values) throws IOException {\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and writeField\");\n    }\n\n    // Catch user silliness:\n    if (finished == true) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    finished = true;\n\n    pointCount = values.size();\n\n    final int numLeaves = Math.toIntExact((pointCount + config.maxPointsInLeafNode - 1) / config.maxPointsInLeafNode);\n    final int numSplits = numLeaves - 1;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    final byte[] splitPackedValues = new byte[numSplits * config.bytesPerDim];\n    final byte[] splitDimensionValues = new byte[numSplits];\n    final long[] leafBlockFPs = new long[numLeaves];\n\n    // compute the min/max for this slice\n    computePackedValueBounds(values, 0, Math.toIntExact(pointCount), minPackedValue, maxPackedValue, scratchBytesRef1);\n    for (int i = 0; i < Math.toIntExact(pointCount); ++i) {\n      docsSeen.set(values.getDocID(i));\n    }\n\n    final long dataStartFP = dataOut.getFilePointer();\n    final int[] parentSplits = new int[config.numIndexDims];\n    build(0, numLeaves, values, 0, Math.toIntExact(pointCount), dataOut,\n          minPackedValue.clone(), maxPackedValue.clone(), parentSplits,\n          splitPackedValues, splitDimensionValues, leafBlockFPs,\n          new int[config.maxPointsInLeafNode]);\n    assert Arrays.equals(parentSplits, new int[config.numIndexDims]);\n\n    scratchBytesRef1.length = config.bytesPerDim;\n    scratchBytesRef1.bytes = splitPackedValues;\n\n    BKDTreeLeafNodes leafNodes  = new BKDTreeLeafNodes() {\n      @Override\n      public long getLeafLP(int index) {\n        return leafBlockFPs[index];\n      }\n\n      @Override\n      public BytesRef getSplitValue(int index) {\n        scratchBytesRef1.offset = index * config.bytesPerDim;\n        return scratchBytesRef1;\n      }\n\n      @Override\n      public int getSplitDimension(int index) {\n        return splitDimensionValues[index] & 0xff;\n      }\n\n      @Override\n      public int numLeaves() {\n        return leafBlockFPs.length;\n      }\n    };\n\n    return () -> {\n      try {\n        writeIndex(metaOut, indexOut, config.maxPointsInLeafNode, leafNodes, dataStartFP);\n      } catch (IOException e) {\n        throw new UncheckedIOException(e);\n      }\n    };\n  }\n\n","sourceOld":"  /* In the 2+D case, we recursively pick the split dimension, compute the\n   * median value and partition other values around it. */\n  private Runnable writeFieldNDims(IndexOutput metaOut, IndexOutput indexOut, IndexOutput dataOut, String fieldName, MutablePointValues values) throws IOException {\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and writeField\");\n    }\n\n    // Catch user silliness:\n    if (finished == true) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    finished = true;\n\n    pointCount = values.size();\n\n    final int numLeaves = Math.toIntExact((pointCount + maxPointsInLeafNode - 1) / maxPointsInLeafNode);\n    final int numSplits = numLeaves - 1;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    final byte[] splitPackedValues = new byte[numSplits * bytesPerDim];\n    final byte[] splitDimensionValues = new byte[numSplits];\n    final long[] leafBlockFPs = new long[numLeaves];\n\n    // compute the min/max for this slice\n    computePackedValueBounds(values, 0, Math.toIntExact(pointCount), minPackedValue, maxPackedValue, scratchBytesRef1);\n    for (int i = 0; i < Math.toIntExact(pointCount); ++i) {\n      docsSeen.set(values.getDocID(i));\n    }\n\n    final long dataStartFP = dataOut.getFilePointer();\n    final int[] parentSplits = new int[numIndexDims];\n    build(0, numLeaves, values, 0, Math.toIntExact(pointCount), dataOut,\n          minPackedValue.clone(), maxPackedValue.clone(), parentSplits,\n          splitPackedValues, splitDimensionValues, leafBlockFPs,\n          new int[maxPointsInLeafNode]);\n    assert Arrays.equals(parentSplits, new int[numIndexDims]);\n\n    scratchBytesRef1.length = bytesPerDim;\n    scratchBytesRef1.bytes = splitPackedValues;\n\n    BKDTreeLeafNodes leafNodes  = new BKDTreeLeafNodes() {\n      @Override\n      public long getLeafLP(int index) {\n        return leafBlockFPs[index];\n      }\n\n      @Override\n      public BytesRef getSplitValue(int index) {\n        scratchBytesRef1.offset = index * bytesPerDim;\n        return scratchBytesRef1;\n      }\n\n      @Override\n      public int getSplitDimension(int index) {\n        return splitDimensionValues[index] & 0xff;\n      }\n\n      @Override\n      public int numLeaves() {\n        return leafBlockFPs.length;\n      }\n    };\n\n    return () -> {\n      try {\n        writeIndex(metaOut, indexOut, maxPointsInLeafNode, leafNodes, dataStartFP);\n      } catch (IOException e) {\n        throw new UncheckedIOException(e);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"bb94bf667d51f9c390c99d97afb36b7caab6b6e9":["78e689a3b60e84c75dc6dd7b181a71fc19ef8482"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"78e689a3b60e84c75dc6dd7b181a71fc19ef8482":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["bb94bf667d51f9c390c99d97afb36b7caab6b6e9"]},"commit2Childs":{"bb94bf667d51f9c390c99d97afb36b7caab6b6e9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["78e689a3b60e84c75dc6dd7b181a71fc19ef8482"],"78e689a3b60e84c75dc6dd7b181a71fc19ef8482":["bb94bf667d51f9c390c99d97afb36b7caab6b6e9"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}