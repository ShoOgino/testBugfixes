{"path":"modules/analysis/common/src/test/org/apache/lucene/analysis/util/TestSegmentingTokenizerBase.SentenceAndWordTokenizer#incrementWord().mjava","commits":[{"id":"6ae8b8ec55786d06eb9b03fc7bc86a907e1a3ae2","date":1326399048,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/util/TestSegmentingTokenizerBase.SentenceAndWordTokenizer#incrementWord().mjava","pathOld":"/dev/null","sourceNew":"    @Override\n    protected boolean incrementWord() {\n      wordStart = wordEnd;\n      while (wordStart < sentenceEnd) {\n        if (Character.isLetterOrDigit(buffer[wordStart]))\n          break;\n        wordStart++;\n      }\n      \n      if (wordStart == sentenceEnd) return false;\n      \n      wordEnd = wordStart+1;\n      while (wordEnd < sentenceEnd && Character.isLetterOrDigit(buffer[wordEnd]))\n        wordEnd++;\n      \n      clearAttributes();\n      termAtt.copyBuffer(buffer, wordStart, wordEnd-wordStart);\n      offsetAtt.setOffset(offset+wordStart, offset+wordEnd);\n      posIncAtt.setPositionIncrement(posIncAtt.getPositionIncrement() + posBoost);\n      posBoost = 0;\n      return true;\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7e477c2108982ba9974f73aa8800270c75cb4971","date":1327277332,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/util/TestSegmentingTokenizerBase.SentenceAndWordTokenizer#incrementWord().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/util/TestSegmentingTokenizerBase.SentenceAndWordTokenizer#incrementWord().mjava","sourceNew":"    @Override\n    protected boolean incrementWord() {\n      wordStart = wordEnd;\n      while (wordStart < sentenceEnd) {\n        if (Character.isLetterOrDigit(buffer[wordStart]))\n          break;\n        wordStart++;\n      }\n      \n      if (wordStart == sentenceEnd) return false;\n      \n      wordEnd = wordStart+1;\n      while (wordEnd < sentenceEnd && Character.isLetterOrDigit(buffer[wordEnd]))\n        wordEnd++;\n      \n      clearAttributes();\n      termAtt.copyBuffer(buffer, wordStart, wordEnd-wordStart);\n      offsetAtt.setOffset(correctOffset(offset+wordStart), correctOffset(offset+wordEnd));\n      posIncAtt.setPositionIncrement(posIncAtt.getPositionIncrement() + posBoost);\n      posBoost = 0;\n      return true;\n    }\n\n","sourceOld":"    @Override\n    protected boolean incrementWord() {\n      wordStart = wordEnd;\n      while (wordStart < sentenceEnd) {\n        if (Character.isLetterOrDigit(buffer[wordStart]))\n          break;\n        wordStart++;\n      }\n      \n      if (wordStart == sentenceEnd) return false;\n      \n      wordEnd = wordStart+1;\n      while (wordEnd < sentenceEnd && Character.isLetterOrDigit(buffer[wordEnd]))\n        wordEnd++;\n      \n      clearAttributes();\n      termAtt.copyBuffer(buffer, wordStart, wordEnd-wordStart);\n      offsetAtt.setOffset(offset+wordStart, offset+wordEnd);\n      posIncAtt.setPositionIncrement(posIncAtt.getPositionIncrement() + posBoost);\n      posBoost = 0;\n      return true;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5a89676536a5d3e2e875a9eed6b3f22a63cca643","date":1327356915,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/util/TestSegmentingTokenizerBase.SentenceAndWordTokenizer#incrementWord().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/util/TestSegmentingTokenizerBase.SentenceAndWordTokenizer#incrementWord().mjava","sourceNew":"    @Override\n    protected boolean incrementWord() {\n      wordStart = wordEnd;\n      while (wordStart < sentenceEnd) {\n        if (Character.isLetterOrDigit(buffer[wordStart]))\n          break;\n        wordStart++;\n      }\n      \n      if (wordStart == sentenceEnd) return false;\n      \n      wordEnd = wordStart+1;\n      while (wordEnd < sentenceEnd && Character.isLetterOrDigit(buffer[wordEnd]))\n        wordEnd++;\n      \n      clearAttributes();\n      termAtt.copyBuffer(buffer, wordStart, wordEnd-wordStart);\n      offsetAtt.setOffset(correctOffset(offset+wordStart), correctOffset(offset+wordEnd));\n      posIncAtt.setPositionIncrement(posIncAtt.getPositionIncrement() + posBoost);\n      posBoost = 0;\n      return true;\n    }\n\n","sourceOld":"    @Override\n    protected boolean incrementWord() {\n      wordStart = wordEnd;\n      while (wordStart < sentenceEnd) {\n        if (Character.isLetterOrDigit(buffer[wordStart]))\n          break;\n        wordStart++;\n      }\n      \n      if (wordStart == sentenceEnd) return false;\n      \n      wordEnd = wordStart+1;\n      while (wordEnd < sentenceEnd && Character.isLetterOrDigit(buffer[wordEnd]))\n        wordEnd++;\n      \n      clearAttributes();\n      termAtt.copyBuffer(buffer, wordStart, wordEnd-wordStart);\n      offsetAtt.setOffset(offset+wordStart, offset+wordEnd);\n      posIncAtt.setPositionIncrement(posIncAtt.getPositionIncrement() + posBoost);\n      posBoost = 0;\n      return true;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d","date":1327523564,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/util/TestSegmentingTokenizerBase.SentenceAndWordTokenizer#incrementWord().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/util/TestSegmentingTokenizerBase.SentenceAndWordTokenizer#incrementWord().mjava","sourceNew":"    @Override\n    protected boolean incrementWord() {\n      wordStart = wordEnd;\n      while (wordStart < sentenceEnd) {\n        if (Character.isLetterOrDigit(buffer[wordStart]))\n          break;\n        wordStart++;\n      }\n      \n      if (wordStart == sentenceEnd) return false;\n      \n      wordEnd = wordStart+1;\n      while (wordEnd < sentenceEnd && Character.isLetterOrDigit(buffer[wordEnd]))\n        wordEnd++;\n      \n      clearAttributes();\n      termAtt.copyBuffer(buffer, wordStart, wordEnd-wordStart);\n      offsetAtt.setOffset(correctOffset(offset+wordStart), correctOffset(offset+wordEnd));\n      posIncAtt.setPositionIncrement(posIncAtt.getPositionIncrement() + posBoost);\n      posBoost = 0;\n      return true;\n    }\n\n","sourceOld":"    @Override\n    protected boolean incrementWord() {\n      wordStart = wordEnd;\n      while (wordStart < sentenceEnd) {\n        if (Character.isLetterOrDigit(buffer[wordStart]))\n          break;\n        wordStart++;\n      }\n      \n      if (wordStart == sentenceEnd) return false;\n      \n      wordEnd = wordStart+1;\n      while (wordEnd < sentenceEnd && Character.isLetterOrDigit(buffer[wordEnd]))\n        wordEnd++;\n      \n      clearAttributes();\n      termAtt.copyBuffer(buffer, wordStart, wordEnd-wordStart);\n      offsetAtt.setOffset(offset+wordStart, offset+wordEnd);\n      posIncAtt.setPositionIncrement(posIncAtt.getPositionIncrement() + posBoost);\n      posBoost = 0;\n      return true;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1387d416b7beb69f15a682854da8ec4c153f5bce","date":1332264506,"type":4,"author":"Robert Muir","isMerge":false,"pathNew":"/dev/null","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/util/TestSegmentingTokenizerBase.SentenceAndWordTokenizer#incrementWord().mjava","sourceNew":null,"sourceOld":"    @Override\n    protected boolean incrementWord() {\n      wordStart = wordEnd;\n      while (wordStart < sentenceEnd) {\n        if (Character.isLetterOrDigit(buffer[wordStart]))\n          break;\n        wordStart++;\n      }\n      \n      if (wordStart == sentenceEnd) return false;\n      \n      wordEnd = wordStart+1;\n      while (wordEnd < sentenceEnd && Character.isLetterOrDigit(buffer[wordEnd]))\n        wordEnd++;\n      \n      clearAttributes();\n      termAtt.copyBuffer(buffer, wordStart, wordEnd-wordStart);\n      offsetAtt.setOffset(correctOffset(offset+wordStart), correctOffset(offset+wordEnd));\n      posIncAtt.setPositionIncrement(posIncAtt.getPositionIncrement() + posBoost);\n      posBoost = 0;\n      return true;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"5a89676536a5d3e2e875a9eed6b3f22a63cca643":["6ae8b8ec55786d06eb9b03fc7bc86a907e1a3ae2","7e477c2108982ba9974f73aa8800270c75cb4971"],"1387d416b7beb69f15a682854da8ec4c153f5bce":["7e477c2108982ba9974f73aa8800270c75cb4971"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d":["6ae8b8ec55786d06eb9b03fc7bc86a907e1a3ae2","7e477c2108982ba9974f73aa8800270c75cb4971"],"6ae8b8ec55786d06eb9b03fc7bc86a907e1a3ae2":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"7e477c2108982ba9974f73aa8800270c75cb4971":["6ae8b8ec55786d06eb9b03fc7bc86a907e1a3ae2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["1387d416b7beb69f15a682854da8ec4c153f5bce"]},"commit2Childs":{"5a89676536a5d3e2e875a9eed6b3f22a63cca643":[],"1387d416b7beb69f15a682854da8ec4c153f5bce":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["6ae8b8ec55786d06eb9b03fc7bc86a907e1a3ae2"],"a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d":[],"6ae8b8ec55786d06eb9b03fc7bc86a907e1a3ae2":["5a89676536a5d3e2e875a9eed6b3f22a63cca643","a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d","7e477c2108982ba9974f73aa8800270c75cb4971"],"7e477c2108982ba9974f73aa8800270c75cb4971":["5a89676536a5d3e2e875a9eed6b3f22a63cca643","1387d416b7beb69f15a682854da8ec4c153f5bce","a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["5a89676536a5d3e2e875a9eed6b3f22a63cca643","a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}