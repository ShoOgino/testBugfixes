{"path":"lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","commits":[{"id":"4edc984f0f4ac77c37e48ace2932f780f888453c","date":1388475218,"type":1,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","sourceNew":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n    //System.out.println(\"ssdv count\");\n\n    MultiDocValues.OrdinalMap ordinalMap;\n\n    // TODO: is this right?  really, we need a way to\n    // verify that this ordinalMap \"matches\" the leaves in\n    // matchingDocs...\n    if (dv instanceof MultiSortedSetDocValues && matchingDocs.size() > 1) {\n      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n    } else {\n      ordinalMap = null;\n    }\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      AtomicReader reader = hits.context.reader();\n      //System.out.println(\"  reader=\" + reader);\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != state.origReader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n      \n      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);\n      if (segValues == null) {\n        continue;\n      }\n\n      final int maxDoc = reader.maxDoc();\n      assert maxDoc == hits.bits.length();\n      //System.out.println(\"  dv=\" + dv);\n\n      // TODO: yet another option is to count all segs\n      // first, only in seg-ord space, and then do a\n      // merge-sort-PQ in the end to only \"resolve to\n      // global\" those seg ords that can compete, if we know\n      // we just want top K?  ie, this is the same algo\n      // that'd be used for merging facets across shards\n      // (distributed faceting).  but this has much higher\n      // temp ram req'ts (sum of number of ords across all\n      // segs)\n      if (ordinalMap != null) {\n        int segOrd = hits.context.ord;\n\n        int numSegOrds = (int) segValues.getValueCount();\n\n        if (hits.totalHits < numSegOrds/10) {\n          //System.out.println(\"    remap as-we-go\");\n          // Remap every ord to global ord as we iterate:\n          int doc = 0;\n          while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {\n            //System.out.println(\"    doc=\" + doc);\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      segOrd=\" + segOrd + \" ord=\" + term + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, term));\n              counts[(int) ordinalMap.getGlobalOrd(segOrd, term)]++;\n              term = (int) segValues.nextOrd();\n            }\n            ++doc;\n          }\n        } else {\n          //System.out.println(\"    count in seg ord first\");\n\n          // First count in seg-ord space:\n          final int[] segCounts = new int[numSegOrds];\n          int doc = 0;\n          while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {\n            //System.out.println(\"    doc=\" + doc);\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      ord=\" + term);\n              segCounts[term]++;\n              term = (int) segValues.nextOrd();\n            }\n            ++doc;\n          }\n\n          // Then, migrate to global ords:\n          for(int ord=0;ord<numSegOrds;ord++) {\n            int count = segCounts[ord];\n            if (count != 0) {\n              //System.out.println(\"    migrate segOrd=\" + segOrd + \" ord=\" + ord + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, ord));\n              counts[(int) ordinalMap.getGlobalOrd(segOrd, ord)] += count;\n            }\n          }\n        }\n      } else {\n        // No ord mapping (e.g., single segment index):\n        // just aggregate directly into counts:\n\n        int doc = 0;\n        while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {\n          segValues.setDocument(doc);\n          int term = (int) segValues.nextOrd();\n          while (term != SortedSetDocValues.NO_MORE_ORDS) {\n            counts[term]++;\n            term = (int) segValues.nextOrd();\n          }\n          ++doc;\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n    //System.out.println(\"ssdv count\");\n\n    MultiDocValues.OrdinalMap ordinalMap;\n\n    // TODO: is this right?  really, we need a way to\n    // verify that this ordinalMap \"matches\" the leaves in\n    // matchingDocs...\n    if (dv instanceof MultiSortedSetDocValues && matchingDocs.size() > 1) {\n      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n    } else {\n      ordinalMap = null;\n    }\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      AtomicReader reader = hits.context.reader();\n      //System.out.println(\"  reader=\" + reader);\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != state.origReader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n      \n      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);\n      if (segValues == null) {\n        continue;\n      }\n\n      final int maxDoc = reader.maxDoc();\n      assert maxDoc == hits.bits.length();\n      //System.out.println(\"  dv=\" + dv);\n\n      // TODO: yet another option is to count all segs\n      // first, only in seg-ord space, and then do a\n      // merge-sort-PQ in the end to only \"resolve to\n      // global\" those seg ords that can compete, if we know\n      // we just want top K?  ie, this is the same algo\n      // that'd be used for merging facets across shards\n      // (distributed faceting).  but this has much higher\n      // temp ram req'ts (sum of number of ords across all\n      // segs)\n      if (ordinalMap != null) {\n        int segOrd = hits.context.ord;\n\n        int numSegOrds = (int) segValues.getValueCount();\n\n        if (hits.totalHits < numSegOrds/10) {\n          //System.out.println(\"    remap as-we-go\");\n          // Remap every ord to global ord as we iterate:\n          int doc = 0;\n          while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {\n            //System.out.println(\"    doc=\" + doc);\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      segOrd=\" + segOrd + \" ord=\" + term + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, term));\n              counts[(int) ordinalMap.getGlobalOrd(segOrd, term)]++;\n              term = (int) segValues.nextOrd();\n            }\n            ++doc;\n          }\n        } else {\n          //System.out.println(\"    count in seg ord first\");\n\n          // First count in seg-ord space:\n          final int[] segCounts = new int[numSegOrds];\n          int doc = 0;\n          while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {\n            //System.out.println(\"    doc=\" + doc);\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      ord=\" + term);\n              segCounts[term]++;\n              term = (int) segValues.nextOrd();\n            }\n            ++doc;\n          }\n\n          // Then, migrate to global ords:\n          for(int ord=0;ord<numSegOrds;ord++) {\n            int count = segCounts[ord];\n            if (count != 0) {\n              //System.out.println(\"    migrate segOrd=\" + segOrd + \" ord=\" + ord + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, ord));\n              counts[(int) ordinalMap.getGlobalOrd(segOrd, ord)] += count;\n            }\n          }\n        }\n      } else {\n        // No ord mapping (e.g., single segment index):\n        // just aggregate directly into counts:\n\n        int doc = 0;\n        while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {\n          segValues.setDocument(doc);\n          int term = (int) segValues.nextOrd();\n          while (term != SortedSetDocValues.NO_MORE_ORDS) {\n            counts[term]++;\n            term = (int) segValues.nextOrd();\n          }\n          ++doc;\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3cc728b07df73b197e6d940d27f9b08b63918f13","date":1388834348,"type":0,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","pathOld":"/dev/null","sourceNew":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n    //System.out.println(\"ssdv count\");\n\n    MultiDocValues.OrdinalMap ordinalMap;\n\n    // TODO: is this right?  really, we need a way to\n    // verify that this ordinalMap \"matches\" the leaves in\n    // matchingDocs...\n    if (dv instanceof MultiSortedSetDocValues && matchingDocs.size() > 1) {\n      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n    } else {\n      ordinalMap = null;\n    }\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      AtomicReader reader = hits.context.reader();\n      //System.out.println(\"  reader=\" + reader);\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != state.origReader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n      \n      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);\n      if (segValues == null) {\n        continue;\n      }\n\n      final int maxDoc = reader.maxDoc();\n      assert maxDoc == hits.bits.length();\n      //System.out.println(\"  dv=\" + dv);\n\n      // TODO: yet another option is to count all segs\n      // first, only in seg-ord space, and then do a\n      // merge-sort-PQ in the end to only \"resolve to\n      // global\" those seg ords that can compete, if we know\n      // we just want top K?  ie, this is the same algo\n      // that'd be used for merging facets across shards\n      // (distributed faceting).  but this has much higher\n      // temp ram req'ts (sum of number of ords across all\n      // segs)\n      if (ordinalMap != null) {\n        int segOrd = hits.context.ord;\n\n        int numSegOrds = (int) segValues.getValueCount();\n\n        if (hits.totalHits < numSegOrds/10) {\n          //System.out.println(\"    remap as-we-go\");\n          // Remap every ord to global ord as we iterate:\n          int doc = 0;\n          while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {\n            //System.out.println(\"    doc=\" + doc);\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      segOrd=\" + segOrd + \" ord=\" + term + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, term));\n              counts[(int) ordinalMap.getGlobalOrd(segOrd, term)]++;\n              term = (int) segValues.nextOrd();\n            }\n            ++doc;\n          }\n        } else {\n          //System.out.println(\"    count in seg ord first\");\n\n          // First count in seg-ord space:\n          final int[] segCounts = new int[numSegOrds];\n          int doc = 0;\n          while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {\n            //System.out.println(\"    doc=\" + doc);\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      ord=\" + term);\n              segCounts[term]++;\n              term = (int) segValues.nextOrd();\n            }\n            ++doc;\n          }\n\n          // Then, migrate to global ords:\n          for(int ord=0;ord<numSegOrds;ord++) {\n            int count = segCounts[ord];\n            if (count != 0) {\n              //System.out.println(\"    migrate segOrd=\" + segOrd + \" ord=\" + ord + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, ord));\n              counts[(int) ordinalMap.getGlobalOrd(segOrd, ord)] += count;\n            }\n          }\n        }\n      } else {\n        // No ord mapping (e.g., single segment index):\n        // just aggregate directly into counts:\n\n        int doc = 0;\n        while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {\n          segValues.setDocument(doc);\n          int term = (int) segValues.nextOrd();\n          while (term != SortedSetDocValues.NO_MORE_ORDS) {\n            counts[term]++;\n            term = (int) segValues.nextOrd();\n          }\n          ++doc;\n        }\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0732a1e488deedeceef7f601e066085e7ac655a","date":1391629546,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","sourceNew":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n    //System.out.println(\"ssdv count\");\n\n    MultiDocValues.OrdinalMap ordinalMap;\n\n    // TODO: is this right?  really, we need a way to\n    // verify that this ordinalMap \"matches\" the leaves in\n    // matchingDocs...\n    if (dv instanceof MultiSortedSetDocValues && matchingDocs.size() > 1) {\n      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n    } else {\n      ordinalMap = null;\n    }\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      AtomicReader reader = hits.context.reader();\n      //System.out.println(\"  reader=\" + reader);\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != state.origReader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n      \n      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);\n      if (segValues == null) {\n        continue;\n      }\n\n      DocIdSetIterator docs = hits.bits.iterator();\n\n      // TODO: yet another option is to count all segs\n      // first, only in seg-ord space, and then do a\n      // merge-sort-PQ in the end to only \"resolve to\n      // global\" those seg ords that can compete, if we know\n      // we just want top K?  ie, this is the same algo\n      // that'd be used for merging facets across shards\n      // (distributed faceting).  but this has much higher\n      // temp ram req'ts (sum of number of ords across all\n      // segs)\n      if (ordinalMap != null) {\n        int segOrd = hits.context.ord;\n\n        int numSegOrds = (int) segValues.getValueCount();\n\n        if (hits.totalHits < numSegOrds/10) {\n          //System.out.println(\"    remap as-we-go\");\n          // Remap every ord to global ord as we iterate:\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      segOrd=\" + segOrd + \" ord=\" + term + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, term));\n              counts[(int) ordinalMap.getGlobalOrd(segOrd, term)]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n        } else {\n          //System.out.println(\"    count in seg ord first\");\n\n          // First count in seg-ord space:\n          final int[] segCounts = new int[numSegOrds];\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      ord=\" + term);\n              segCounts[term]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n\n          // Then, migrate to global ords:\n          for(int ord=0;ord<numSegOrds;ord++) {\n            int count = segCounts[ord];\n            if (count != 0) {\n              //System.out.println(\"    migrate segOrd=\" + segOrd + \" ord=\" + ord + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, ord));\n              counts[(int) ordinalMap.getGlobalOrd(segOrd, ord)] += count;\n            }\n          }\n        }\n      } else {\n        // No ord mapping (e.g., single segment index):\n        // just aggregate directly into counts:\n        int doc;\n        while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n          segValues.setDocument(doc);\n          int term = (int) segValues.nextOrd();\n          while (term != SortedSetDocValues.NO_MORE_ORDS) {\n            counts[term]++;\n            term = (int) segValues.nextOrd();\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n    //System.out.println(\"ssdv count\");\n\n    MultiDocValues.OrdinalMap ordinalMap;\n\n    // TODO: is this right?  really, we need a way to\n    // verify that this ordinalMap \"matches\" the leaves in\n    // matchingDocs...\n    if (dv instanceof MultiSortedSetDocValues && matchingDocs.size() > 1) {\n      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n    } else {\n      ordinalMap = null;\n    }\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      AtomicReader reader = hits.context.reader();\n      //System.out.println(\"  reader=\" + reader);\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != state.origReader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n      \n      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);\n      if (segValues == null) {\n        continue;\n      }\n\n      final int maxDoc = reader.maxDoc();\n      assert maxDoc == hits.bits.length();\n      //System.out.println(\"  dv=\" + dv);\n\n      // TODO: yet another option is to count all segs\n      // first, only in seg-ord space, and then do a\n      // merge-sort-PQ in the end to only \"resolve to\n      // global\" those seg ords that can compete, if we know\n      // we just want top K?  ie, this is the same algo\n      // that'd be used for merging facets across shards\n      // (distributed faceting).  but this has much higher\n      // temp ram req'ts (sum of number of ords across all\n      // segs)\n      if (ordinalMap != null) {\n        int segOrd = hits.context.ord;\n\n        int numSegOrds = (int) segValues.getValueCount();\n\n        if (hits.totalHits < numSegOrds/10) {\n          //System.out.println(\"    remap as-we-go\");\n          // Remap every ord to global ord as we iterate:\n          int doc = 0;\n          while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {\n            //System.out.println(\"    doc=\" + doc);\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      segOrd=\" + segOrd + \" ord=\" + term + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, term));\n              counts[(int) ordinalMap.getGlobalOrd(segOrd, term)]++;\n              term = (int) segValues.nextOrd();\n            }\n            ++doc;\n          }\n        } else {\n          //System.out.println(\"    count in seg ord first\");\n\n          // First count in seg-ord space:\n          final int[] segCounts = new int[numSegOrds];\n          int doc = 0;\n          while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {\n            //System.out.println(\"    doc=\" + doc);\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      ord=\" + term);\n              segCounts[term]++;\n              term = (int) segValues.nextOrd();\n            }\n            ++doc;\n          }\n\n          // Then, migrate to global ords:\n          for(int ord=0;ord<numSegOrds;ord++) {\n            int count = segCounts[ord];\n            if (count != 0) {\n              //System.out.println(\"    migrate segOrd=\" + segOrd + \" ord=\" + ord + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, ord));\n              counts[(int) ordinalMap.getGlobalOrd(segOrd, ord)] += count;\n            }\n          }\n        }\n      } else {\n        // No ord mapping (e.g., single segment index):\n        // just aggregate directly into counts:\n\n        int doc = 0;\n        while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {\n          segValues.setDocument(doc);\n          int term = (int) segValues.nextOrd();\n          while (term != SortedSetDocValues.NO_MORE_ORDS) {\n            counts[term]++;\n            term = (int) segValues.nextOrd();\n          }\n          ++doc;\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"30fc8c22124fdcf9d76449bd2fa04decbe74e2e2","date":1391685460,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","sourceNew":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n    //System.out.println(\"ssdv count\");\n\n    MultiDocValues.OrdinalMap ordinalMap;\n\n    // TODO: is this right?  really, we need a way to\n    // verify that this ordinalMap \"matches\" the leaves in\n    // matchingDocs...\n    if (dv instanceof MultiSortedSetDocValues && matchingDocs.size() > 1) {\n      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n    } else {\n      ordinalMap = null;\n    }\n    \n    IndexReader origReader = state.getOrigReader();\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      AtomicReader reader = hits.context.reader();\n      //System.out.println(\"  reader=\" + reader);\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != origReader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n      \n      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);\n      if (segValues == null) {\n        continue;\n      }\n\n      DocIdSetIterator docs = hits.bits.iterator();\n\n      // TODO: yet another option is to count all segs\n      // first, only in seg-ord space, and then do a\n      // merge-sort-PQ in the end to only \"resolve to\n      // global\" those seg ords that can compete, if we know\n      // we just want top K?  ie, this is the same algo\n      // that'd be used for merging facets across shards\n      // (distributed faceting).  but this has much higher\n      // temp ram req'ts (sum of number of ords across all\n      // segs)\n      if (ordinalMap != null) {\n        int segOrd = hits.context.ord;\n\n        int numSegOrds = (int) segValues.getValueCount();\n\n        if (hits.totalHits < numSegOrds/10) {\n          //System.out.println(\"    remap as-we-go\");\n          // Remap every ord to global ord as we iterate:\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      segOrd=\" + segOrd + \" ord=\" + term + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, term));\n              counts[(int) ordinalMap.getGlobalOrd(segOrd, term)]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n        } else {\n          //System.out.println(\"    count in seg ord first\");\n\n          // First count in seg-ord space:\n          final int[] segCounts = new int[numSegOrds];\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      ord=\" + term);\n              segCounts[term]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n\n          // Then, migrate to global ords:\n          for(int ord=0;ord<numSegOrds;ord++) {\n            int count = segCounts[ord];\n            if (count != 0) {\n              //System.out.println(\"    migrate segOrd=\" + segOrd + \" ord=\" + ord + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, ord));\n              counts[(int) ordinalMap.getGlobalOrd(segOrd, ord)] += count;\n            }\n          }\n        }\n      } else {\n        // No ord mapping (e.g., single segment index):\n        // just aggregate directly into counts:\n        int doc;\n        while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n          segValues.setDocument(doc);\n          int term = (int) segValues.nextOrd();\n          while (term != SortedSetDocValues.NO_MORE_ORDS) {\n            counts[term]++;\n            term = (int) segValues.nextOrd();\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n    //System.out.println(\"ssdv count\");\n\n    MultiDocValues.OrdinalMap ordinalMap;\n\n    // TODO: is this right?  really, we need a way to\n    // verify that this ordinalMap \"matches\" the leaves in\n    // matchingDocs...\n    if (dv instanceof MultiSortedSetDocValues && matchingDocs.size() > 1) {\n      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n    } else {\n      ordinalMap = null;\n    }\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      AtomicReader reader = hits.context.reader();\n      //System.out.println(\"  reader=\" + reader);\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != state.origReader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n      \n      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);\n      if (segValues == null) {\n        continue;\n      }\n\n      DocIdSetIterator docs = hits.bits.iterator();\n\n      // TODO: yet another option is to count all segs\n      // first, only in seg-ord space, and then do a\n      // merge-sort-PQ in the end to only \"resolve to\n      // global\" those seg ords that can compete, if we know\n      // we just want top K?  ie, this is the same algo\n      // that'd be used for merging facets across shards\n      // (distributed faceting).  but this has much higher\n      // temp ram req'ts (sum of number of ords across all\n      // segs)\n      if (ordinalMap != null) {\n        int segOrd = hits.context.ord;\n\n        int numSegOrds = (int) segValues.getValueCount();\n\n        if (hits.totalHits < numSegOrds/10) {\n          //System.out.println(\"    remap as-we-go\");\n          // Remap every ord to global ord as we iterate:\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      segOrd=\" + segOrd + \" ord=\" + term + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, term));\n              counts[(int) ordinalMap.getGlobalOrd(segOrd, term)]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n        } else {\n          //System.out.println(\"    count in seg ord first\");\n\n          // First count in seg-ord space:\n          final int[] segCounts = new int[numSegOrds];\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      ord=\" + term);\n              segCounts[term]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n\n          // Then, migrate to global ords:\n          for(int ord=0;ord<numSegOrds;ord++) {\n            int count = segCounts[ord];\n            if (count != 0) {\n              //System.out.println(\"    migrate segOrd=\" + segOrd + \" ord=\" + ord + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, ord));\n              counts[(int) ordinalMap.getGlobalOrd(segOrd, ord)] += count;\n            }\n          }\n        }\n      } else {\n        // No ord mapping (e.g., single segment index):\n        // just aggregate directly into counts:\n        int doc;\n        while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n          segValues.setDocument(doc);\n          int term = (int) segValues.nextOrd();\n          while (term != SortedSetDocValues.NO_MORE_ORDS) {\n            counts[term]++;\n            term = (int) segValues.nextOrd();\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9b25d26ad3e8f824d95db88ecc5e5d9d71d3c595","date":1402950824,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","sourceNew":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n    //System.out.println(\"ssdv count\");\n\n    MultiDocValues.OrdinalMap ordinalMap;\n\n    // TODO: is this right?  really, we need a way to\n    // verify that this ordinalMap \"matches\" the leaves in\n    // matchingDocs...\n    if (dv instanceof MultiSortedSetDocValues && matchingDocs.size() > 1) {\n      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n    } else {\n      ordinalMap = null;\n    }\n    \n    IndexReader origReader = state.getOrigReader();\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      AtomicReader reader = hits.context.reader();\n      //System.out.println(\"  reader=\" + reader);\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != origReader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n      \n      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);\n      if (segValues == null) {\n        continue;\n      }\n\n      DocIdSetIterator docs = hits.bits.iterator();\n\n      // TODO: yet another option is to count all segs\n      // first, only in seg-ord space, and then do a\n      // merge-sort-PQ in the end to only \"resolve to\n      // global\" those seg ords that can compete, if we know\n      // we just want top K?  ie, this is the same algo\n      // that'd be used for merging facets across shards\n      // (distributed faceting).  but this has much higher\n      // temp ram req'ts (sum of number of ords across all\n      // segs)\n      if (ordinalMap != null) {\n        final int segOrd = hits.context.ord;\n        final LongValues ordMap = ordinalMap.getGlobalOrds(segOrd);\n\n        int numSegOrds = (int) segValues.getValueCount();\n\n        if (hits.totalHits < numSegOrds/10) {\n          //System.out.println(\"    remap as-we-go\");\n          // Remap every ord to global ord as we iterate:\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      segOrd=\" + segOrd + \" ord=\" + term + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, term));\n              counts[(int) ordMap.get(term)]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n        } else {\n          //System.out.println(\"    count in seg ord first\");\n\n          // First count in seg-ord space:\n          final int[] segCounts = new int[numSegOrds];\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      ord=\" + term);\n              segCounts[term]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n\n          // Then, migrate to global ords:\n          for(int ord=0;ord<numSegOrds;ord++) {\n            int count = segCounts[ord];\n            if (count != 0) {\n              //System.out.println(\"    migrate segOrd=\" + segOrd + \" ord=\" + ord + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, ord));\n              counts[(int) ordMap.get(ord)] += count;\n            }\n          }\n        }\n      } else {\n        // No ord mapping (e.g., single segment index):\n        // just aggregate directly into counts:\n        int doc;\n        while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n          segValues.setDocument(doc);\n          int term = (int) segValues.nextOrd();\n          while (term != SortedSetDocValues.NO_MORE_ORDS) {\n            counts[term]++;\n            term = (int) segValues.nextOrd();\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n    //System.out.println(\"ssdv count\");\n\n    MultiDocValues.OrdinalMap ordinalMap;\n\n    // TODO: is this right?  really, we need a way to\n    // verify that this ordinalMap \"matches\" the leaves in\n    // matchingDocs...\n    if (dv instanceof MultiSortedSetDocValues && matchingDocs.size() > 1) {\n      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n    } else {\n      ordinalMap = null;\n    }\n    \n    IndexReader origReader = state.getOrigReader();\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      AtomicReader reader = hits.context.reader();\n      //System.out.println(\"  reader=\" + reader);\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != origReader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n      \n      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);\n      if (segValues == null) {\n        continue;\n      }\n\n      DocIdSetIterator docs = hits.bits.iterator();\n\n      // TODO: yet another option is to count all segs\n      // first, only in seg-ord space, and then do a\n      // merge-sort-PQ in the end to only \"resolve to\n      // global\" those seg ords that can compete, if we know\n      // we just want top K?  ie, this is the same algo\n      // that'd be used for merging facets across shards\n      // (distributed faceting).  but this has much higher\n      // temp ram req'ts (sum of number of ords across all\n      // segs)\n      if (ordinalMap != null) {\n        int segOrd = hits.context.ord;\n\n        int numSegOrds = (int) segValues.getValueCount();\n\n        if (hits.totalHits < numSegOrds/10) {\n          //System.out.println(\"    remap as-we-go\");\n          // Remap every ord to global ord as we iterate:\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      segOrd=\" + segOrd + \" ord=\" + term + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, term));\n              counts[(int) ordinalMap.getGlobalOrd(segOrd, term)]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n        } else {\n          //System.out.println(\"    count in seg ord first\");\n\n          // First count in seg-ord space:\n          final int[] segCounts = new int[numSegOrds];\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      ord=\" + term);\n              segCounts[term]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n\n          // Then, migrate to global ords:\n          for(int ord=0;ord<numSegOrds;ord++) {\n            int count = segCounts[ord];\n            if (count != 0) {\n              //System.out.println(\"    migrate segOrd=\" + segOrd + \" ord=\" + ord + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, ord));\n              counts[(int) ordinalMap.getGlobalOrd(segOrd, ord)] += count;\n            }\n          }\n        }\n      } else {\n        // No ord mapping (e.g., single segment index):\n        // just aggregate directly into counts:\n        int doc;\n        while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n          segValues.setDocument(doc);\n          int term = (int) segValues.nextOrd();\n          while (term != SortedSetDocValues.NO_MORE_ORDS) {\n            counts[term]++;\n            term = (int) segValues.nextOrd();\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f6d0aee18c1653f7ee634fa8830abdb001dcfe1b","date":1402998114,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","sourceNew":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n    //System.out.println(\"ssdv count\");\n\n    MultiDocValues.OrdinalMap ordinalMap;\n\n    // TODO: is this right?  really, we need a way to\n    // verify that this ordinalMap \"matches\" the leaves in\n    // matchingDocs...\n    if (dv instanceof MultiSortedSetDocValues && matchingDocs.size() > 1) {\n      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n    } else {\n      ordinalMap = null;\n    }\n    \n    IndexReader origReader = state.getOrigReader();\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      AtomicReader reader = hits.context.reader();\n      //System.out.println(\"  reader=\" + reader);\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != origReader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n      \n      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);\n      if (segValues == null) {\n        continue;\n      }\n\n      DocIdSetIterator docs = hits.bits.iterator();\n\n      // TODO: yet another option is to count all segs\n      // first, only in seg-ord space, and then do a\n      // merge-sort-PQ in the end to only \"resolve to\n      // global\" those seg ords that can compete, if we know\n      // we just want top K?  ie, this is the same algo\n      // that'd be used for merging facets across shards\n      // (distributed faceting).  but this has much higher\n      // temp ram req'ts (sum of number of ords across all\n      // segs)\n      if (ordinalMap != null) {\n        final int segOrd = hits.context.ord;\n        final LongValues ordMap = ordinalMap.getGlobalOrds(segOrd);\n\n        int numSegOrds = (int) segValues.getValueCount();\n\n        if (hits.totalHits < numSegOrds/10) {\n          //System.out.println(\"    remap as-we-go\");\n          // Remap every ord to global ord as we iterate:\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      segOrd=\" + segOrd + \" ord=\" + term + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, term));\n              counts[(int) ordMap.get(term)]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n        } else {\n          //System.out.println(\"    count in seg ord first\");\n\n          // First count in seg-ord space:\n          final int[] segCounts = new int[numSegOrds];\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      ord=\" + term);\n              segCounts[term]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n\n          // Then, migrate to global ords:\n          for(int ord=0;ord<numSegOrds;ord++) {\n            int count = segCounts[ord];\n            if (count != 0) {\n              //System.out.println(\"    migrate segOrd=\" + segOrd + \" ord=\" + ord + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, ord));\n              counts[(int) ordMap.get(ord)] += count;\n            }\n          }\n        }\n      } else {\n        // No ord mapping (e.g., single segment index):\n        // just aggregate directly into counts:\n        int doc;\n        while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n          segValues.setDocument(doc);\n          int term = (int) segValues.nextOrd();\n          while (term != SortedSetDocValues.NO_MORE_ORDS) {\n            counts[term]++;\n            term = (int) segValues.nextOrd();\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n    //System.out.println(\"ssdv count\");\n\n    MultiDocValues.OrdinalMap ordinalMap;\n\n    // TODO: is this right?  really, we need a way to\n    // verify that this ordinalMap \"matches\" the leaves in\n    // matchingDocs...\n    if (dv instanceof MultiSortedSetDocValues && matchingDocs.size() > 1) {\n      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n    } else {\n      ordinalMap = null;\n    }\n    \n    IndexReader origReader = state.getOrigReader();\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      AtomicReader reader = hits.context.reader();\n      //System.out.println(\"  reader=\" + reader);\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != origReader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n      \n      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);\n      if (segValues == null) {\n        continue;\n      }\n\n      DocIdSetIterator docs = hits.bits.iterator();\n\n      // TODO: yet another option is to count all segs\n      // first, only in seg-ord space, and then do a\n      // merge-sort-PQ in the end to only \"resolve to\n      // global\" those seg ords that can compete, if we know\n      // we just want top K?  ie, this is the same algo\n      // that'd be used for merging facets across shards\n      // (distributed faceting).  but this has much higher\n      // temp ram req'ts (sum of number of ords across all\n      // segs)\n      if (ordinalMap != null) {\n        int segOrd = hits.context.ord;\n\n        int numSegOrds = (int) segValues.getValueCount();\n\n        if (hits.totalHits < numSegOrds/10) {\n          //System.out.println(\"    remap as-we-go\");\n          // Remap every ord to global ord as we iterate:\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      segOrd=\" + segOrd + \" ord=\" + term + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, term));\n              counts[(int) ordinalMap.getGlobalOrd(segOrd, term)]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n        } else {\n          //System.out.println(\"    count in seg ord first\");\n\n          // First count in seg-ord space:\n          final int[] segCounts = new int[numSegOrds];\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      ord=\" + term);\n              segCounts[term]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n\n          // Then, migrate to global ords:\n          for(int ord=0;ord<numSegOrds;ord++) {\n            int count = segCounts[ord];\n            if (count != 0) {\n              //System.out.println(\"    migrate segOrd=\" + segOrd + \" ord=\" + ord + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, ord));\n              counts[(int) ordinalMap.getGlobalOrd(segOrd, ord)] += count;\n            }\n          }\n        }\n      } else {\n        // No ord mapping (e.g., single segment index):\n        // just aggregate directly into counts:\n        int doc;\n        while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n          segValues.setDocument(doc);\n          int term = (int) segValues.nextOrd();\n          while (term != SortedSetDocValues.NO_MORE_ORDS) {\n            counts[term]++;\n            term = (int) segValues.nextOrd();\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","sourceNew":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n    //System.out.println(\"ssdv count\");\n\n    MultiDocValues.OrdinalMap ordinalMap;\n\n    // TODO: is this right?  really, we need a way to\n    // verify that this ordinalMap \"matches\" the leaves in\n    // matchingDocs...\n    if (dv instanceof MultiSortedSetDocValues && matchingDocs.size() > 1) {\n      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n    } else {\n      ordinalMap = null;\n    }\n    \n    IndexReader origReader = state.getOrigReader();\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      LeafReader reader = hits.context.reader();\n      //System.out.println(\"  reader=\" + reader);\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != origReader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n      \n      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);\n      if (segValues == null) {\n        continue;\n      }\n\n      DocIdSetIterator docs = hits.bits.iterator();\n\n      // TODO: yet another option is to count all segs\n      // first, only in seg-ord space, and then do a\n      // merge-sort-PQ in the end to only \"resolve to\n      // global\" those seg ords that can compete, if we know\n      // we just want top K?  ie, this is the same algo\n      // that'd be used for merging facets across shards\n      // (distributed faceting).  but this has much higher\n      // temp ram req'ts (sum of number of ords across all\n      // segs)\n      if (ordinalMap != null) {\n        final int segOrd = hits.context.ord;\n        final LongValues ordMap = ordinalMap.getGlobalOrds(segOrd);\n\n        int numSegOrds = (int) segValues.getValueCount();\n\n        if (hits.totalHits < numSegOrds/10) {\n          //System.out.println(\"    remap as-we-go\");\n          // Remap every ord to global ord as we iterate:\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      segOrd=\" + segOrd + \" ord=\" + term + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, term));\n              counts[(int) ordMap.get(term)]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n        } else {\n          //System.out.println(\"    count in seg ord first\");\n\n          // First count in seg-ord space:\n          final int[] segCounts = new int[numSegOrds];\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      ord=\" + term);\n              segCounts[term]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n\n          // Then, migrate to global ords:\n          for(int ord=0;ord<numSegOrds;ord++) {\n            int count = segCounts[ord];\n            if (count != 0) {\n              //System.out.println(\"    migrate segOrd=\" + segOrd + \" ord=\" + ord + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, ord));\n              counts[(int) ordMap.get(ord)] += count;\n            }\n          }\n        }\n      } else {\n        // No ord mapping (e.g., single segment index):\n        // just aggregate directly into counts:\n        int doc;\n        while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n          segValues.setDocument(doc);\n          int term = (int) segValues.nextOrd();\n          while (term != SortedSetDocValues.NO_MORE_ORDS) {\n            counts[term]++;\n            term = (int) segValues.nextOrd();\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n    //System.out.println(\"ssdv count\");\n\n    MultiDocValues.OrdinalMap ordinalMap;\n\n    // TODO: is this right?  really, we need a way to\n    // verify that this ordinalMap \"matches\" the leaves in\n    // matchingDocs...\n    if (dv instanceof MultiSortedSetDocValues && matchingDocs.size() > 1) {\n      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n    } else {\n      ordinalMap = null;\n    }\n    \n    IndexReader origReader = state.getOrigReader();\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      AtomicReader reader = hits.context.reader();\n      //System.out.println(\"  reader=\" + reader);\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != origReader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n      \n      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);\n      if (segValues == null) {\n        continue;\n      }\n\n      DocIdSetIterator docs = hits.bits.iterator();\n\n      // TODO: yet another option is to count all segs\n      // first, only in seg-ord space, and then do a\n      // merge-sort-PQ in the end to only \"resolve to\n      // global\" those seg ords that can compete, if we know\n      // we just want top K?  ie, this is the same algo\n      // that'd be used for merging facets across shards\n      // (distributed faceting).  but this has much higher\n      // temp ram req'ts (sum of number of ords across all\n      // segs)\n      if (ordinalMap != null) {\n        final int segOrd = hits.context.ord;\n        final LongValues ordMap = ordinalMap.getGlobalOrds(segOrd);\n\n        int numSegOrds = (int) segValues.getValueCount();\n\n        if (hits.totalHits < numSegOrds/10) {\n          //System.out.println(\"    remap as-we-go\");\n          // Remap every ord to global ord as we iterate:\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      segOrd=\" + segOrd + \" ord=\" + term + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, term));\n              counts[(int) ordMap.get(term)]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n        } else {\n          //System.out.println(\"    count in seg ord first\");\n\n          // First count in seg-ord space:\n          final int[] segCounts = new int[numSegOrds];\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      ord=\" + term);\n              segCounts[term]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n\n          // Then, migrate to global ords:\n          for(int ord=0;ord<numSegOrds;ord++) {\n            int count = segCounts[ord];\n            if (count != 0) {\n              //System.out.println(\"    migrate segOrd=\" + segOrd + \" ord=\" + ord + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, ord));\n              counts[(int) ordMap.get(ord)] += count;\n            }\n          }\n        }\n      } else {\n        // No ord mapping (e.g., single segment index):\n        // just aggregate directly into counts:\n        int doc;\n        while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n          segValues.setDocument(doc);\n          int term = (int) segValues.nextOrd();\n          while (term != SortedSetDocValues.NO_MORE_ORDS) {\n            counts[term]++;\n            term = (int) segValues.nextOrd();\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6652c74b2358a0b13223817a6a793bf1c9d0749d","date":1474465301,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","sourceNew":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n    //System.out.println(\"ssdv count\");\n\n    MultiDocValues.OrdinalMap ordinalMap;\n\n    // TODO: is this right?  really, we need a way to\n    // verify that this ordinalMap \"matches\" the leaves in\n    // matchingDocs...\n    if (dv instanceof MultiDocValues.MultiSortedSetDocValues && matchingDocs.size() > 1) {\n      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n    } else {\n      ordinalMap = null;\n    }\n    \n    IndexReader origReader = state.getOrigReader();\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      LeafReader reader = hits.context.reader();\n      //System.out.println(\"  reader=\" + reader);\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != origReader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n      \n      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);\n      if (segValues == null) {\n        continue;\n      }\n\n      DocIdSetIterator docs = hits.bits.iterator();\n\n      // TODO: yet another option is to count all segs\n      // first, only in seg-ord space, and then do a\n      // merge-sort-PQ in the end to only \"resolve to\n      // global\" those seg ords that can compete, if we know\n      // we just want top K?  ie, this is the same algo\n      // that'd be used for merging facets across shards\n      // (distributed faceting).  but this has much higher\n      // temp ram req'ts (sum of number of ords across all\n      // segs)\n      if (ordinalMap != null) {\n        final int segOrd = hits.context.ord;\n        final LongValues ordMap = ordinalMap.getGlobalOrds(segOrd);\n\n        int numSegOrds = (int) segValues.getValueCount();\n\n        if (hits.totalHits < numSegOrds/10) {\n          //System.out.println(\"    remap as-we-go\");\n          // Remap every ord to global ord as we iterate:\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            if (doc > segValues.docID()) {\n              segValues.advance(doc);\n            }\n            if (doc == segValues.docID()) {\n              int term = (int) segValues.nextOrd();\n              while (term != SortedSetDocValues.NO_MORE_ORDS) {\n                //System.out.println(\"      segOrd=\" + segOrd + \" ord=\" + term + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, term));\n                counts[(int) ordMap.get(term)]++;\n                term = (int) segValues.nextOrd();\n              }\n            }\n          }\n        } else {\n          //System.out.println(\"    count in seg ord first\");\n\n          // First count in seg-ord space:\n          final int[] segCounts = new int[numSegOrds];\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            if (doc > segValues.docID()) {\n              segValues.advance(doc);\n            }\n            if (doc == segValues.docID()) {\n              int term = (int) segValues.nextOrd();\n              while (term != SortedSetDocValues.NO_MORE_ORDS) {\n                //System.out.println(\"      ord=\" + term);\n                segCounts[term]++;\n                term = (int) segValues.nextOrd();\n              }\n            }\n          }\n\n          // Then, migrate to global ords:\n          for(int ord=0;ord<numSegOrds;ord++) {\n            int count = segCounts[ord];\n            if (count != 0) {\n              //System.out.println(\"    migrate segOrd=\" + segOrd + \" ord=\" + ord + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, ord));\n              counts[(int) ordMap.get(ord)] += count;\n            }\n          }\n        }\n      } else {\n        // No ord mapping (e.g., single segment index):\n        // just aggregate directly into counts:\n        int doc;\n        while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n          if (doc > segValues.docID()) {\n            segValues.advance(doc);\n          }\n          if (doc == segValues.docID()) {\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              counts[term]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n    //System.out.println(\"ssdv count\");\n\n    MultiDocValues.OrdinalMap ordinalMap;\n\n    // TODO: is this right?  really, we need a way to\n    // verify that this ordinalMap \"matches\" the leaves in\n    // matchingDocs...\n    if (dv instanceof MultiSortedSetDocValues && matchingDocs.size() > 1) {\n      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n    } else {\n      ordinalMap = null;\n    }\n    \n    IndexReader origReader = state.getOrigReader();\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      LeafReader reader = hits.context.reader();\n      //System.out.println(\"  reader=\" + reader);\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != origReader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n      \n      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);\n      if (segValues == null) {\n        continue;\n      }\n\n      DocIdSetIterator docs = hits.bits.iterator();\n\n      // TODO: yet another option is to count all segs\n      // first, only in seg-ord space, and then do a\n      // merge-sort-PQ in the end to only \"resolve to\n      // global\" those seg ords that can compete, if we know\n      // we just want top K?  ie, this is the same algo\n      // that'd be used for merging facets across shards\n      // (distributed faceting).  but this has much higher\n      // temp ram req'ts (sum of number of ords across all\n      // segs)\n      if (ordinalMap != null) {\n        final int segOrd = hits.context.ord;\n        final LongValues ordMap = ordinalMap.getGlobalOrds(segOrd);\n\n        int numSegOrds = (int) segValues.getValueCount();\n\n        if (hits.totalHits < numSegOrds/10) {\n          //System.out.println(\"    remap as-we-go\");\n          // Remap every ord to global ord as we iterate:\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      segOrd=\" + segOrd + \" ord=\" + term + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, term));\n              counts[(int) ordMap.get(term)]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n        } else {\n          //System.out.println(\"    count in seg ord first\");\n\n          // First count in seg-ord space:\n          final int[] segCounts = new int[numSegOrds];\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      ord=\" + term);\n              segCounts[term]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n\n          // Then, migrate to global ords:\n          for(int ord=0;ord<numSegOrds;ord++) {\n            int count = segCounts[ord];\n            if (count != 0) {\n              //System.out.println(\"    migrate segOrd=\" + segOrd + \" ord=\" + ord + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, ord));\n              counts[(int) ordMap.get(ord)] += count;\n            }\n          }\n        }\n      } else {\n        // No ord mapping (e.g., single segment index):\n        // just aggregate directly into counts:\n        int doc;\n        while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n          segValues.setDocument(doc);\n          int term = (int) segValues.nextOrd();\n          while (term != SortedSetDocValues.NO_MORE_ORDS) {\n            counts[term]++;\n            term = (int) segValues.nextOrd();\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"17e5da53e4e5bd659e22add9bba1cfa222e7e30d","date":1475435902,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","sourceNew":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n    //System.out.println(\"ssdv count\");\n\n    MultiDocValues.OrdinalMap ordinalMap;\n\n    // TODO: is this right?  really, we need a way to\n    // verify that this ordinalMap \"matches\" the leaves in\n    // matchingDocs...\n    if (dv instanceof MultiDocValues.MultiSortedSetDocValues && matchingDocs.size() > 1) {\n      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n    } else {\n      ordinalMap = null;\n    }\n    \n    IndexReader origReader = state.getOrigReader();\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      LeafReader reader = hits.context.reader();\n      //System.out.println(\"  reader=\" + reader);\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != origReader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n      \n      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);\n      if (segValues == null) {\n        continue;\n      }\n\n      DocIdSetIterator docs = hits.bits.iterator();\n\n      // TODO: yet another option is to count all segs\n      // first, only in seg-ord space, and then do a\n      // merge-sort-PQ in the end to only \"resolve to\n      // global\" those seg ords that can compete, if we know\n      // we just want top K?  ie, this is the same algo\n      // that'd be used for merging facets across shards\n      // (distributed faceting).  but this has much higher\n      // temp ram req'ts (sum of number of ords across all\n      // segs)\n      if (ordinalMap != null) {\n        final int segOrd = hits.context.ord;\n        final LongValues ordMap = ordinalMap.getGlobalOrds(segOrd);\n\n        int numSegOrds = (int) segValues.getValueCount();\n\n        if (hits.totalHits < numSegOrds/10) {\n          //System.out.println(\"    remap as-we-go\");\n          // Remap every ord to global ord as we iterate:\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            if (doc > segValues.docID()) {\n              segValues.advance(doc);\n            }\n            if (doc == segValues.docID()) {\n              int term = (int) segValues.nextOrd();\n              while (term != SortedSetDocValues.NO_MORE_ORDS) {\n                //System.out.println(\"      segOrd=\" + segOrd + \" ord=\" + term + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, term));\n                counts[(int) ordMap.get(term)]++;\n                term = (int) segValues.nextOrd();\n              }\n            }\n          }\n        } else {\n          //System.out.println(\"    count in seg ord first\");\n\n          // First count in seg-ord space:\n          final int[] segCounts = new int[numSegOrds];\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            if (doc > segValues.docID()) {\n              segValues.advance(doc);\n            }\n            if (doc == segValues.docID()) {\n              int term = (int) segValues.nextOrd();\n              while (term != SortedSetDocValues.NO_MORE_ORDS) {\n                //System.out.println(\"      ord=\" + term);\n                segCounts[term]++;\n                term = (int) segValues.nextOrd();\n              }\n            }\n          }\n\n          // Then, migrate to global ords:\n          for(int ord=0;ord<numSegOrds;ord++) {\n            int count = segCounts[ord];\n            if (count != 0) {\n              //System.out.println(\"    migrate segOrd=\" + segOrd + \" ord=\" + ord + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, ord));\n              counts[(int) ordMap.get(ord)] += count;\n            }\n          }\n        }\n      } else {\n        // No ord mapping (e.g., single segment index):\n        // just aggregate directly into counts:\n        int doc;\n        while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n          if (doc > segValues.docID()) {\n            segValues.advance(doc);\n          }\n          if (doc == segValues.docID()) {\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              counts[term]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n    //System.out.println(\"ssdv count\");\n\n    MultiDocValues.OrdinalMap ordinalMap;\n\n    // TODO: is this right?  really, we need a way to\n    // verify that this ordinalMap \"matches\" the leaves in\n    // matchingDocs...\n    if (dv instanceof MultiSortedSetDocValues && matchingDocs.size() > 1) {\n      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n    } else {\n      ordinalMap = null;\n    }\n    \n    IndexReader origReader = state.getOrigReader();\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      LeafReader reader = hits.context.reader();\n      //System.out.println(\"  reader=\" + reader);\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != origReader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n      \n      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);\n      if (segValues == null) {\n        continue;\n      }\n\n      DocIdSetIterator docs = hits.bits.iterator();\n\n      // TODO: yet another option is to count all segs\n      // first, only in seg-ord space, and then do a\n      // merge-sort-PQ in the end to only \"resolve to\n      // global\" those seg ords that can compete, if we know\n      // we just want top K?  ie, this is the same algo\n      // that'd be used for merging facets across shards\n      // (distributed faceting).  but this has much higher\n      // temp ram req'ts (sum of number of ords across all\n      // segs)\n      if (ordinalMap != null) {\n        final int segOrd = hits.context.ord;\n        final LongValues ordMap = ordinalMap.getGlobalOrds(segOrd);\n\n        int numSegOrds = (int) segValues.getValueCount();\n\n        if (hits.totalHits < numSegOrds/10) {\n          //System.out.println(\"    remap as-we-go\");\n          // Remap every ord to global ord as we iterate:\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      segOrd=\" + segOrd + \" ord=\" + term + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, term));\n              counts[(int) ordMap.get(term)]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n        } else {\n          //System.out.println(\"    count in seg ord first\");\n\n          // First count in seg-ord space:\n          final int[] segCounts = new int[numSegOrds];\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      ord=\" + term);\n              segCounts[term]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n\n          // Then, migrate to global ords:\n          for(int ord=0;ord<numSegOrds;ord++) {\n            int count = segCounts[ord];\n            if (count != 0) {\n              //System.out.println(\"    migrate segOrd=\" + segOrd + \" ord=\" + ord + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, ord));\n              counts[(int) ordMap.get(ord)] += count;\n            }\n          }\n        }\n      } else {\n        // No ord mapping (e.g., single segment index):\n        // just aggregate directly into counts:\n        int doc;\n        while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n          segValues.setDocument(doc);\n          int term = (int) segValues.nextOrd();\n          while (term != SortedSetDocValues.NO_MORE_ORDS) {\n            counts[term]++;\n            term = (int) segValues.nextOrd();\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","sourceNew":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n    //System.out.println(\"ssdv count\");\n\n    MultiDocValues.OrdinalMap ordinalMap;\n\n    // TODO: is this right?  really, we need a way to\n    // verify that this ordinalMap \"matches\" the leaves in\n    // matchingDocs...\n    if (dv instanceof MultiDocValues.MultiSortedSetDocValues && matchingDocs.size() > 1) {\n      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n    } else {\n      ordinalMap = null;\n    }\n    \n    IndexReader origReader = state.getOrigReader();\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      LeafReader reader = hits.context.reader();\n      //System.out.println(\"  reader=\" + reader);\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != origReader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n      \n      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);\n      if (segValues == null) {\n        continue;\n      }\n\n      DocIdSetIterator docs = hits.bits.iterator();\n\n      // TODO: yet another option is to count all segs\n      // first, only in seg-ord space, and then do a\n      // merge-sort-PQ in the end to only \"resolve to\n      // global\" those seg ords that can compete, if we know\n      // we just want top K?  ie, this is the same algo\n      // that'd be used for merging facets across shards\n      // (distributed faceting).  but this has much higher\n      // temp ram req'ts (sum of number of ords across all\n      // segs)\n      if (ordinalMap != null) {\n        final int segOrd = hits.context.ord;\n        final LongValues ordMap = ordinalMap.getGlobalOrds(segOrd);\n\n        int numSegOrds = (int) segValues.getValueCount();\n\n        if (hits.totalHits < numSegOrds/10) {\n          //System.out.println(\"    remap as-we-go\");\n          // Remap every ord to global ord as we iterate:\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            if (doc > segValues.docID()) {\n              segValues.advance(doc);\n            }\n            if (doc == segValues.docID()) {\n              int term = (int) segValues.nextOrd();\n              while (term != SortedSetDocValues.NO_MORE_ORDS) {\n                //System.out.println(\"      segOrd=\" + segOrd + \" ord=\" + term + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, term));\n                counts[(int) ordMap.get(term)]++;\n                term = (int) segValues.nextOrd();\n              }\n            }\n          }\n        } else {\n          //System.out.println(\"    count in seg ord first\");\n\n          // First count in seg-ord space:\n          final int[] segCounts = new int[numSegOrds];\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            if (doc > segValues.docID()) {\n              segValues.advance(doc);\n            }\n            if (doc == segValues.docID()) {\n              int term = (int) segValues.nextOrd();\n              while (term != SortedSetDocValues.NO_MORE_ORDS) {\n                //System.out.println(\"      ord=\" + term);\n                segCounts[term]++;\n                term = (int) segValues.nextOrd();\n              }\n            }\n          }\n\n          // Then, migrate to global ords:\n          for(int ord=0;ord<numSegOrds;ord++) {\n            int count = segCounts[ord];\n            if (count != 0) {\n              //System.out.println(\"    migrate segOrd=\" + segOrd + \" ord=\" + ord + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, ord));\n              counts[(int) ordMap.get(ord)] += count;\n            }\n          }\n        }\n      } else {\n        // No ord mapping (e.g., single segment index):\n        // just aggregate directly into counts:\n        int doc;\n        while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n          if (doc > segValues.docID()) {\n            segValues.advance(doc);\n          }\n          if (doc == segValues.docID()) {\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              counts[term]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n    //System.out.println(\"ssdv count\");\n\n    MultiDocValues.OrdinalMap ordinalMap;\n\n    // TODO: is this right?  really, we need a way to\n    // verify that this ordinalMap \"matches\" the leaves in\n    // matchingDocs...\n    if (dv instanceof MultiSortedSetDocValues && matchingDocs.size() > 1) {\n      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n    } else {\n      ordinalMap = null;\n    }\n    \n    IndexReader origReader = state.getOrigReader();\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      LeafReader reader = hits.context.reader();\n      //System.out.println(\"  reader=\" + reader);\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != origReader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n      \n      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);\n      if (segValues == null) {\n        continue;\n      }\n\n      DocIdSetIterator docs = hits.bits.iterator();\n\n      // TODO: yet another option is to count all segs\n      // first, only in seg-ord space, and then do a\n      // merge-sort-PQ in the end to only \"resolve to\n      // global\" those seg ords that can compete, if we know\n      // we just want top K?  ie, this is the same algo\n      // that'd be used for merging facets across shards\n      // (distributed faceting).  but this has much higher\n      // temp ram req'ts (sum of number of ords across all\n      // segs)\n      if (ordinalMap != null) {\n        final int segOrd = hits.context.ord;\n        final LongValues ordMap = ordinalMap.getGlobalOrds(segOrd);\n\n        int numSegOrds = (int) segValues.getValueCount();\n\n        if (hits.totalHits < numSegOrds/10) {\n          //System.out.println(\"    remap as-we-go\");\n          // Remap every ord to global ord as we iterate:\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      segOrd=\" + segOrd + \" ord=\" + term + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, term));\n              counts[(int) ordMap.get(term)]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n        } else {\n          //System.out.println(\"    count in seg ord first\");\n\n          // First count in seg-ord space:\n          final int[] segCounts = new int[numSegOrds];\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      ord=\" + term);\n              segCounts[term]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n\n          // Then, migrate to global ords:\n          for(int ord=0;ord<numSegOrds;ord++) {\n            int count = segCounts[ord];\n            if (count != 0) {\n              //System.out.println(\"    migrate segOrd=\" + segOrd + \" ord=\" + ord + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, ord));\n              counts[(int) ordMap.get(ord)] += count;\n            }\n          }\n        }\n      } else {\n        // No ord mapping (e.g., single segment index):\n        // just aggregate directly into counts:\n        int doc;\n        while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n          segValues.setDocument(doc);\n          int term = (int) segValues.nextOrd();\n          while (term != SortedSetDocValues.NO_MORE_ORDS) {\n            counts[term]++;\n            term = (int) segValues.nextOrd();\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"11134e449dabe11d6d0ff6a564d84b82cbe93722","date":1477299083,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","sourceNew":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n    //System.out.println(\"ssdv count\");\n\n    MultiDocValues.OrdinalMap ordinalMap;\n\n    // TODO: is this right?  really, we need a way to\n    // verify that this ordinalMap \"matches\" the leaves in\n    // matchingDocs...\n    if (dv instanceof MultiDocValues.MultiSortedSetDocValues && matchingDocs.size() > 1) {\n      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n    } else {\n      ordinalMap = null;\n    }\n    \n    IndexReader origReader = state.getOrigReader();\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      LeafReader reader = hits.context.reader();\n      //System.out.println(\"  reader=\" + reader);\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != origReader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n      \n      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);\n      if (segValues == null) {\n        continue;\n      }\n\n      DocIdSetIterator docs = hits.bits.iterator();\n\n      // TODO: yet another option is to count all segs\n      // first, only in seg-ord space, and then do a\n      // merge-sort-PQ in the end to only \"resolve to\n      // global\" those seg ords that can compete, if we know\n      // we just want top K?  ie, this is the same algo\n      // that'd be used for merging facets across shards\n      // (distributed faceting).  but this has much higher\n      // temp ram req'ts (sum of number of ords across all\n      // segs)\n      if (ordinalMap != null) {\n        final int segOrd = hits.context.ord;\n        final LongValues ordMap = ordinalMap.getGlobalOrds(segOrd);\n\n        int numSegOrds = (int) segValues.getValueCount();\n\n        if (hits.totalHits < numSegOrds/10) {\n          //System.out.println(\"    remap as-we-go\");\n          // Remap every ord to global ord as we iterate:\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            if (segValues.advanceExact(doc)) {\n              int term = (int) segValues.nextOrd();\n              while (term != SortedSetDocValues.NO_MORE_ORDS) {\n                //System.out.println(\"      segOrd=\" + segOrd + \" ord=\" + term + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, term));\n                counts[(int) ordMap.get(term)]++;\n                term = (int) segValues.nextOrd();\n              }\n            }\n          }\n        } else {\n          //System.out.println(\"    count in seg ord first\");\n\n          // First count in seg-ord space:\n          final int[] segCounts = new int[numSegOrds];\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            if (segValues.advanceExact(doc)) {\n              int term = (int) segValues.nextOrd();\n              while (term != SortedSetDocValues.NO_MORE_ORDS) {\n                //System.out.println(\"      ord=\" + term);\n                segCounts[term]++;\n                term = (int) segValues.nextOrd();\n              }\n            }\n          }\n\n          // Then, migrate to global ords:\n          for(int ord=0;ord<numSegOrds;ord++) {\n            int count = segCounts[ord];\n            if (count != 0) {\n              //System.out.println(\"    migrate segOrd=\" + segOrd + \" ord=\" + ord + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, ord));\n              counts[(int) ordMap.get(ord)] += count;\n            }\n          }\n        }\n      } else {\n        // No ord mapping (e.g., single segment index):\n        // just aggregate directly into counts:\n        int doc;\n        while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n          if (segValues.advanceExact(doc)) {\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              counts[term]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n    //System.out.println(\"ssdv count\");\n\n    MultiDocValues.OrdinalMap ordinalMap;\n\n    // TODO: is this right?  really, we need a way to\n    // verify that this ordinalMap \"matches\" the leaves in\n    // matchingDocs...\n    if (dv instanceof MultiDocValues.MultiSortedSetDocValues && matchingDocs.size() > 1) {\n      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n    } else {\n      ordinalMap = null;\n    }\n    \n    IndexReader origReader = state.getOrigReader();\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      LeafReader reader = hits.context.reader();\n      //System.out.println(\"  reader=\" + reader);\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != origReader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n      \n      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);\n      if (segValues == null) {\n        continue;\n      }\n\n      DocIdSetIterator docs = hits.bits.iterator();\n\n      // TODO: yet another option is to count all segs\n      // first, only in seg-ord space, and then do a\n      // merge-sort-PQ in the end to only \"resolve to\n      // global\" those seg ords that can compete, if we know\n      // we just want top K?  ie, this is the same algo\n      // that'd be used for merging facets across shards\n      // (distributed faceting).  but this has much higher\n      // temp ram req'ts (sum of number of ords across all\n      // segs)\n      if (ordinalMap != null) {\n        final int segOrd = hits.context.ord;\n        final LongValues ordMap = ordinalMap.getGlobalOrds(segOrd);\n\n        int numSegOrds = (int) segValues.getValueCount();\n\n        if (hits.totalHits < numSegOrds/10) {\n          //System.out.println(\"    remap as-we-go\");\n          // Remap every ord to global ord as we iterate:\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            if (doc > segValues.docID()) {\n              segValues.advance(doc);\n            }\n            if (doc == segValues.docID()) {\n              int term = (int) segValues.nextOrd();\n              while (term != SortedSetDocValues.NO_MORE_ORDS) {\n                //System.out.println(\"      segOrd=\" + segOrd + \" ord=\" + term + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, term));\n                counts[(int) ordMap.get(term)]++;\n                term = (int) segValues.nextOrd();\n              }\n            }\n          }\n        } else {\n          //System.out.println(\"    count in seg ord first\");\n\n          // First count in seg-ord space:\n          final int[] segCounts = new int[numSegOrds];\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            if (doc > segValues.docID()) {\n              segValues.advance(doc);\n            }\n            if (doc == segValues.docID()) {\n              int term = (int) segValues.nextOrd();\n              while (term != SortedSetDocValues.NO_MORE_ORDS) {\n                //System.out.println(\"      ord=\" + term);\n                segCounts[term]++;\n                term = (int) segValues.nextOrd();\n              }\n            }\n          }\n\n          // Then, migrate to global ords:\n          for(int ord=0;ord<numSegOrds;ord++) {\n            int count = segCounts[ord];\n            if (count != 0) {\n              //System.out.println(\"    migrate segOrd=\" + segOrd + \" ord=\" + ord + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, ord));\n              counts[(int) ordMap.get(ord)] += count;\n            }\n          }\n        }\n      } else {\n        // No ord mapping (e.g., single segment index):\n        // just aggregate directly into counts:\n        int doc;\n        while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n          if (doc > segValues.docID()) {\n            segValues.advance(doc);\n          }\n          if (doc == segValues.docID()) {\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              counts[term]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d2714c85633b642b29871cf5ff8d17d3ba7bfd76","date":1477307753,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","sourceNew":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n    //System.out.println(\"ssdv count\");\n\n    MultiDocValues.OrdinalMap ordinalMap;\n\n    // TODO: is this right?  really, we need a way to\n    // verify that this ordinalMap \"matches\" the leaves in\n    // matchingDocs...\n    if (dv instanceof MultiDocValues.MultiSortedSetDocValues && matchingDocs.size() > 1) {\n      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n    } else {\n      ordinalMap = null;\n    }\n    \n    IndexReader origReader = state.getOrigReader();\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      LeafReader reader = hits.context.reader();\n      //System.out.println(\"  reader=\" + reader);\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != origReader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n      \n      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);\n      if (segValues == null) {\n        continue;\n      }\n\n      DocIdSetIterator docs = hits.bits.iterator();\n\n      // TODO: yet another option is to count all segs\n      // first, only in seg-ord space, and then do a\n      // merge-sort-PQ in the end to only \"resolve to\n      // global\" those seg ords that can compete, if we know\n      // we just want top K?  ie, this is the same algo\n      // that'd be used for merging facets across shards\n      // (distributed faceting).  but this has much higher\n      // temp ram req'ts (sum of number of ords across all\n      // segs)\n      if (ordinalMap != null) {\n        final int segOrd = hits.context.ord;\n        final LongValues ordMap = ordinalMap.getGlobalOrds(segOrd);\n\n        int numSegOrds = (int) segValues.getValueCount();\n\n        if (hits.totalHits < numSegOrds/10) {\n          //System.out.println(\"    remap as-we-go\");\n          // Remap every ord to global ord as we iterate:\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            if (segValues.advanceExact(doc)) {\n              int term = (int) segValues.nextOrd();\n              while (term != SortedSetDocValues.NO_MORE_ORDS) {\n                //System.out.println(\"      segOrd=\" + segOrd + \" ord=\" + term + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, term));\n                counts[(int) ordMap.get(term)]++;\n                term = (int) segValues.nextOrd();\n              }\n            }\n          }\n        } else {\n          //System.out.println(\"    count in seg ord first\");\n\n          // First count in seg-ord space:\n          final int[] segCounts = new int[numSegOrds];\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            if (segValues.advanceExact(doc)) {\n              int term = (int) segValues.nextOrd();\n              while (term != SortedSetDocValues.NO_MORE_ORDS) {\n                //System.out.println(\"      ord=\" + term);\n                segCounts[term]++;\n                term = (int) segValues.nextOrd();\n              }\n            }\n          }\n\n          // Then, migrate to global ords:\n          for(int ord=0;ord<numSegOrds;ord++) {\n            int count = segCounts[ord];\n            if (count != 0) {\n              //System.out.println(\"    migrate segOrd=\" + segOrd + \" ord=\" + ord + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, ord));\n              counts[(int) ordMap.get(ord)] += count;\n            }\n          }\n        }\n      } else {\n        // No ord mapping (e.g., single segment index):\n        // just aggregate directly into counts:\n        int doc;\n        while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n          if (segValues.advanceExact(doc)) {\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              counts[term]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n    //System.out.println(\"ssdv count\");\n\n    MultiDocValues.OrdinalMap ordinalMap;\n\n    // TODO: is this right?  really, we need a way to\n    // verify that this ordinalMap \"matches\" the leaves in\n    // matchingDocs...\n    if (dv instanceof MultiDocValues.MultiSortedSetDocValues && matchingDocs.size() > 1) {\n      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n    } else {\n      ordinalMap = null;\n    }\n    \n    IndexReader origReader = state.getOrigReader();\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      LeafReader reader = hits.context.reader();\n      //System.out.println(\"  reader=\" + reader);\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != origReader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n      \n      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);\n      if (segValues == null) {\n        continue;\n      }\n\n      DocIdSetIterator docs = hits.bits.iterator();\n\n      // TODO: yet another option is to count all segs\n      // first, only in seg-ord space, and then do a\n      // merge-sort-PQ in the end to only \"resolve to\n      // global\" those seg ords that can compete, if we know\n      // we just want top K?  ie, this is the same algo\n      // that'd be used for merging facets across shards\n      // (distributed faceting).  but this has much higher\n      // temp ram req'ts (sum of number of ords across all\n      // segs)\n      if (ordinalMap != null) {\n        final int segOrd = hits.context.ord;\n        final LongValues ordMap = ordinalMap.getGlobalOrds(segOrd);\n\n        int numSegOrds = (int) segValues.getValueCount();\n\n        if (hits.totalHits < numSegOrds/10) {\n          //System.out.println(\"    remap as-we-go\");\n          // Remap every ord to global ord as we iterate:\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            if (doc > segValues.docID()) {\n              segValues.advance(doc);\n            }\n            if (doc == segValues.docID()) {\n              int term = (int) segValues.nextOrd();\n              while (term != SortedSetDocValues.NO_MORE_ORDS) {\n                //System.out.println(\"      segOrd=\" + segOrd + \" ord=\" + term + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, term));\n                counts[(int) ordMap.get(term)]++;\n                term = (int) segValues.nextOrd();\n              }\n            }\n          }\n        } else {\n          //System.out.println(\"    count in seg ord first\");\n\n          // First count in seg-ord space:\n          final int[] segCounts = new int[numSegOrds];\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            if (doc > segValues.docID()) {\n              segValues.advance(doc);\n            }\n            if (doc == segValues.docID()) {\n              int term = (int) segValues.nextOrd();\n              while (term != SortedSetDocValues.NO_MORE_ORDS) {\n                //System.out.println(\"      ord=\" + term);\n                segCounts[term]++;\n                term = (int) segValues.nextOrd();\n              }\n            }\n          }\n\n          // Then, migrate to global ords:\n          for(int ord=0;ord<numSegOrds;ord++) {\n            int count = segCounts[ord];\n            if (count != 0) {\n              //System.out.println(\"    migrate segOrd=\" + segOrd + \" ord=\" + ord + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, ord));\n              counts[(int) ordMap.get(ord)] += count;\n            }\n          }\n        }\n      } else {\n        // No ord mapping (e.g., single segment index):\n        // just aggregate directly into counts:\n        int doc;\n        while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n          if (doc > segValues.docID()) {\n            segValues.advance(doc);\n          }\n          if (doc == segValues.docID()) {\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              counts[term]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4b8b2bc4b8b503cc0b5743b19445798c62069e4d","date":1477390943,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","sourceNew":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n    //System.out.println(\"ssdv count\");\n\n    MultiDocValues.OrdinalMap ordinalMap;\n\n    // TODO: is this right?  really, we need a way to\n    // verify that this ordinalMap \"matches\" the leaves in\n    // matchingDocs...\n    if (dv instanceof MultiDocValues.MultiSortedSetDocValues && matchingDocs.size() > 1) {\n      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n    } else {\n      ordinalMap = null;\n    }\n    \n    IndexReader origReader = state.getOrigReader();\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      LeafReader reader = hits.context.reader();\n      //System.out.println(\"  reader=\" + reader);\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != origReader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n      \n      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);\n      if (segValues == null) {\n        continue;\n      }\n\n      DocIdSetIterator it = ConjunctionDISI.intersectIterators(Arrays.asList(\n                                  hits.bits.iterator(), segValues));\n\n      // TODO: yet another option is to count all segs\n      // first, only in seg-ord space, and then do a\n      // merge-sort-PQ in the end to only \"resolve to\n      // global\" those seg ords that can compete, if we know\n      // we just want top K?  ie, this is the same algo\n      // that'd be used for merging facets across shards\n      // (distributed faceting).  but this has much higher\n      // temp ram req'ts (sum of number of ords across all\n      // segs)\n      if (ordinalMap != null) {\n        final int segOrd = hits.context.ord;\n        final LongValues ordMap = ordinalMap.getGlobalOrds(segOrd);\n\n        int numSegOrds = (int) segValues.getValueCount();\n\n        if (hits.totalHits < numSegOrds/10) {\n          //System.out.println(\"    remap as-we-go\");\n          // Remap every ord to global ord as we iterate:\n          for (int doc = it.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = it.nextDoc()) {\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      segOrd=\" + segOrd + \" ord=\" + term + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, term));\n              counts[(int) ordMap.get(term)]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n        } else {\n          //System.out.println(\"    count in seg ord first\");\n\n          // First count in seg-ord space:\n          final int[] segCounts = new int[numSegOrds];\n          for (int doc = it.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = it.nextDoc()) {\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      ord=\" + term);\n              segCounts[term]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n\n          // Then, migrate to global ords:\n          for(int ord=0;ord<numSegOrds;ord++) {\n            int count = segCounts[ord];\n            if (count != 0) {\n              //System.out.println(\"    migrate segOrd=\" + segOrd + \" ord=\" + ord + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, ord));\n              counts[(int) ordMap.get(ord)] += count;\n            }\n          }\n        }\n      } else {\n        // No ord mapping (e.g., single segment index):\n        // just aggregate directly into counts:\n        for (int doc = it.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = it.nextDoc()) {\n          int term = (int) segValues.nextOrd();\n          while (term != SortedSetDocValues.NO_MORE_ORDS) {\n            counts[term]++;\n            term = (int) segValues.nextOrd();\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n    //System.out.println(\"ssdv count\");\n\n    MultiDocValues.OrdinalMap ordinalMap;\n\n    // TODO: is this right?  really, we need a way to\n    // verify that this ordinalMap \"matches\" the leaves in\n    // matchingDocs...\n    if (dv instanceof MultiDocValues.MultiSortedSetDocValues && matchingDocs.size() > 1) {\n      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n    } else {\n      ordinalMap = null;\n    }\n    \n    IndexReader origReader = state.getOrigReader();\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      LeafReader reader = hits.context.reader();\n      //System.out.println(\"  reader=\" + reader);\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != origReader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n      \n      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);\n      if (segValues == null) {\n        continue;\n      }\n\n      DocIdSetIterator docs = hits.bits.iterator();\n\n      // TODO: yet another option is to count all segs\n      // first, only in seg-ord space, and then do a\n      // merge-sort-PQ in the end to only \"resolve to\n      // global\" those seg ords that can compete, if we know\n      // we just want top K?  ie, this is the same algo\n      // that'd be used for merging facets across shards\n      // (distributed faceting).  but this has much higher\n      // temp ram req'ts (sum of number of ords across all\n      // segs)\n      if (ordinalMap != null) {\n        final int segOrd = hits.context.ord;\n        final LongValues ordMap = ordinalMap.getGlobalOrds(segOrd);\n\n        int numSegOrds = (int) segValues.getValueCount();\n\n        if (hits.totalHits < numSegOrds/10) {\n          //System.out.println(\"    remap as-we-go\");\n          // Remap every ord to global ord as we iterate:\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            if (segValues.advanceExact(doc)) {\n              int term = (int) segValues.nextOrd();\n              while (term != SortedSetDocValues.NO_MORE_ORDS) {\n                //System.out.println(\"      segOrd=\" + segOrd + \" ord=\" + term + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, term));\n                counts[(int) ordMap.get(term)]++;\n                term = (int) segValues.nextOrd();\n              }\n            }\n          }\n        } else {\n          //System.out.println(\"    count in seg ord first\");\n\n          // First count in seg-ord space:\n          final int[] segCounts = new int[numSegOrds];\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            if (segValues.advanceExact(doc)) {\n              int term = (int) segValues.nextOrd();\n              while (term != SortedSetDocValues.NO_MORE_ORDS) {\n                //System.out.println(\"      ord=\" + term);\n                segCounts[term]++;\n                term = (int) segValues.nextOrd();\n              }\n            }\n          }\n\n          // Then, migrate to global ords:\n          for(int ord=0;ord<numSegOrds;ord++) {\n            int count = segCounts[ord];\n            if (count != 0) {\n              //System.out.println(\"    migrate segOrd=\" + segOrd + \" ord=\" + ord + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, ord));\n              counts[(int) ordMap.get(ord)] += count;\n            }\n          }\n        }\n      } else {\n        // No ord mapping (e.g., single segment index):\n        // just aggregate directly into counts:\n        int doc;\n        while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n          if (segValues.advanceExact(doc)) {\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              counts[term]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"60b61628d1912768f51eccaa8ead5a5a32ab34c6","date":1477427681,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","sourceNew":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n    //System.out.println(\"ssdv count\");\n\n    MultiDocValues.OrdinalMap ordinalMap;\n\n    // TODO: is this right?  really, we need a way to\n    // verify that this ordinalMap \"matches\" the leaves in\n    // matchingDocs...\n    if (dv instanceof MultiDocValues.MultiSortedSetDocValues && matchingDocs.size() > 1) {\n      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n    } else {\n      ordinalMap = null;\n    }\n    \n    IndexReader origReader = state.getOrigReader();\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      LeafReader reader = hits.context.reader();\n      //System.out.println(\"  reader=\" + reader);\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != origReader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n      \n      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);\n      if (segValues == null) {\n        continue;\n      }\n\n      DocIdSetIterator it = ConjunctionDISI.intersectIterators(Arrays.asList(\n                                  hits.bits.iterator(), segValues));\n\n      // TODO: yet another option is to count all segs\n      // first, only in seg-ord space, and then do a\n      // merge-sort-PQ in the end to only \"resolve to\n      // global\" those seg ords that can compete, if we know\n      // we just want top K?  ie, this is the same algo\n      // that'd be used for merging facets across shards\n      // (distributed faceting).  but this has much higher\n      // temp ram req'ts (sum of number of ords across all\n      // segs)\n      if (ordinalMap != null) {\n        final int segOrd = hits.context.ord;\n        final LongValues ordMap = ordinalMap.getGlobalOrds(segOrd);\n\n        int numSegOrds = (int) segValues.getValueCount();\n\n        if (hits.totalHits < numSegOrds/10) {\n          //System.out.println(\"    remap as-we-go\");\n          // Remap every ord to global ord as we iterate:\n          for (int doc = it.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = it.nextDoc()) {\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      segOrd=\" + segOrd + \" ord=\" + term + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, term));\n              counts[(int) ordMap.get(term)]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n        } else {\n          //System.out.println(\"    count in seg ord first\");\n\n          // First count in seg-ord space:\n          final int[] segCounts = new int[numSegOrds];\n          for (int doc = it.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = it.nextDoc()) {\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      ord=\" + term);\n              segCounts[term]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n\n          // Then, migrate to global ords:\n          for(int ord=0;ord<numSegOrds;ord++) {\n            int count = segCounts[ord];\n            if (count != 0) {\n              //System.out.println(\"    migrate segOrd=\" + segOrd + \" ord=\" + ord + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, ord));\n              counts[(int) ordMap.get(ord)] += count;\n            }\n          }\n        }\n      } else {\n        // No ord mapping (e.g., single segment index):\n        // just aggregate directly into counts:\n        for (int doc = it.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = it.nextDoc()) {\n          int term = (int) segValues.nextOrd();\n          while (term != SortedSetDocValues.NO_MORE_ORDS) {\n            counts[term]++;\n            term = (int) segValues.nextOrd();\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n    //System.out.println(\"ssdv count\");\n\n    MultiDocValues.OrdinalMap ordinalMap;\n\n    // TODO: is this right?  really, we need a way to\n    // verify that this ordinalMap \"matches\" the leaves in\n    // matchingDocs...\n    if (dv instanceof MultiDocValues.MultiSortedSetDocValues && matchingDocs.size() > 1) {\n      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n    } else {\n      ordinalMap = null;\n    }\n    \n    IndexReader origReader = state.getOrigReader();\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      LeafReader reader = hits.context.reader();\n      //System.out.println(\"  reader=\" + reader);\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != origReader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n      \n      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);\n      if (segValues == null) {\n        continue;\n      }\n\n      DocIdSetIterator docs = hits.bits.iterator();\n\n      // TODO: yet another option is to count all segs\n      // first, only in seg-ord space, and then do a\n      // merge-sort-PQ in the end to only \"resolve to\n      // global\" those seg ords that can compete, if we know\n      // we just want top K?  ie, this is the same algo\n      // that'd be used for merging facets across shards\n      // (distributed faceting).  but this has much higher\n      // temp ram req'ts (sum of number of ords across all\n      // segs)\n      if (ordinalMap != null) {\n        final int segOrd = hits.context.ord;\n        final LongValues ordMap = ordinalMap.getGlobalOrds(segOrd);\n\n        int numSegOrds = (int) segValues.getValueCount();\n\n        if (hits.totalHits < numSegOrds/10) {\n          //System.out.println(\"    remap as-we-go\");\n          // Remap every ord to global ord as we iterate:\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            if (segValues.advanceExact(doc)) {\n              int term = (int) segValues.nextOrd();\n              while (term != SortedSetDocValues.NO_MORE_ORDS) {\n                //System.out.println(\"      segOrd=\" + segOrd + \" ord=\" + term + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, term));\n                counts[(int) ordMap.get(term)]++;\n                term = (int) segValues.nextOrd();\n              }\n            }\n          }\n        } else {\n          //System.out.println(\"    count in seg ord first\");\n\n          // First count in seg-ord space:\n          final int[] segCounts = new int[numSegOrds];\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            if (segValues.advanceExact(doc)) {\n              int term = (int) segValues.nextOrd();\n              while (term != SortedSetDocValues.NO_MORE_ORDS) {\n                //System.out.println(\"      ord=\" + term);\n                segCounts[term]++;\n                term = (int) segValues.nextOrd();\n              }\n            }\n          }\n\n          // Then, migrate to global ords:\n          for(int ord=0;ord<numSegOrds;ord++) {\n            int count = segCounts[ord];\n            if (count != 0) {\n              //System.out.println(\"    migrate segOrd=\" + segOrd + \" ord=\" + ord + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, ord));\n              counts[(int) ordMap.get(ord)] += count;\n            }\n          }\n        }\n      } else {\n        // No ord mapping (e.g., single segment index):\n        // just aggregate directly into counts:\n        int doc;\n        while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n          if (segValues.advanceExact(doc)) {\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              counts[term]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"80d0e6d59ae23f4a6f30eaf40bfb40742300287f","date":1477598926,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","sourceNew":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n    //System.out.println(\"ssdv count\");\n\n    MultiDocValues.OrdinalMap ordinalMap;\n\n    // TODO: is this right?  really, we need a way to\n    // verify that this ordinalMap \"matches\" the leaves in\n    // matchingDocs...\n    if (dv instanceof MultiDocValues.MultiSortedSetDocValues && matchingDocs.size() > 1) {\n      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n    } else {\n      ordinalMap = null;\n    }\n    \n    IndexReader origReader = state.getOrigReader();\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      LeafReader reader = hits.context.reader();\n      //System.out.println(\"  reader=\" + reader);\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != origReader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n      \n      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);\n      if (segValues == null) {\n        continue;\n      }\n\n      DocIdSetIterator it = ConjunctionDISI.intersectIterators(Arrays.asList(\n                                  hits.bits.iterator(), segValues));\n\n      // TODO: yet another option is to count all segs\n      // first, only in seg-ord space, and then do a\n      // merge-sort-PQ in the end to only \"resolve to\n      // global\" those seg ords that can compete, if we know\n      // we just want top K?  ie, this is the same algo\n      // that'd be used for merging facets across shards\n      // (distributed faceting).  but this has much higher\n      // temp ram req'ts (sum of number of ords across all\n      // segs)\n      if (ordinalMap != null) {\n        final int segOrd = hits.context.ord;\n        final LongValues ordMap = ordinalMap.getGlobalOrds(segOrd);\n\n        int numSegOrds = (int) segValues.getValueCount();\n\n        if (hits.totalHits < numSegOrds/10) {\n          //System.out.println(\"    remap as-we-go\");\n          // Remap every ord to global ord as we iterate:\n          for (int doc = it.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = it.nextDoc()) {\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      segOrd=\" + segOrd + \" ord=\" + term + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, term));\n              counts[(int) ordMap.get(term)]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n        } else {\n          //System.out.println(\"    count in seg ord first\");\n\n          // First count in seg-ord space:\n          final int[] segCounts = new int[numSegOrds];\n          for (int doc = it.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = it.nextDoc()) {\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      ord=\" + term);\n              segCounts[term]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n\n          // Then, migrate to global ords:\n          for(int ord=0;ord<numSegOrds;ord++) {\n            int count = segCounts[ord];\n            if (count != 0) {\n              //System.out.println(\"    migrate segOrd=\" + segOrd + \" ord=\" + ord + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, ord));\n              counts[(int) ordMap.get(ord)] += count;\n            }\n          }\n        }\n      } else {\n        // No ord mapping (e.g., single segment index):\n        // just aggregate directly into counts:\n        for (int doc = it.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = it.nextDoc()) {\n          int term = (int) segValues.nextOrd();\n          while (term != SortedSetDocValues.NO_MORE_ORDS) {\n            counts[term]++;\n            term = (int) segValues.nextOrd();\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n    //System.out.println(\"ssdv count\");\n\n    MultiDocValues.OrdinalMap ordinalMap;\n\n    // TODO: is this right?  really, we need a way to\n    // verify that this ordinalMap \"matches\" the leaves in\n    // matchingDocs...\n    if (dv instanceof MultiDocValues.MultiSortedSetDocValues && matchingDocs.size() > 1) {\n      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n    } else {\n      ordinalMap = null;\n    }\n    \n    IndexReader origReader = state.getOrigReader();\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      LeafReader reader = hits.context.reader();\n      //System.out.println(\"  reader=\" + reader);\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != origReader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n      \n      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);\n      if (segValues == null) {\n        continue;\n      }\n\n      DocIdSetIterator docs = hits.bits.iterator();\n\n      // TODO: yet another option is to count all segs\n      // first, only in seg-ord space, and then do a\n      // merge-sort-PQ in the end to only \"resolve to\n      // global\" those seg ords that can compete, if we know\n      // we just want top K?  ie, this is the same algo\n      // that'd be used for merging facets across shards\n      // (distributed faceting).  but this has much higher\n      // temp ram req'ts (sum of number of ords across all\n      // segs)\n      if (ordinalMap != null) {\n        final int segOrd = hits.context.ord;\n        final LongValues ordMap = ordinalMap.getGlobalOrds(segOrd);\n\n        int numSegOrds = (int) segValues.getValueCount();\n\n        if (hits.totalHits < numSegOrds/10) {\n          //System.out.println(\"    remap as-we-go\");\n          // Remap every ord to global ord as we iterate:\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            if (doc > segValues.docID()) {\n              segValues.advance(doc);\n            }\n            if (doc == segValues.docID()) {\n              int term = (int) segValues.nextOrd();\n              while (term != SortedSetDocValues.NO_MORE_ORDS) {\n                //System.out.println(\"      segOrd=\" + segOrd + \" ord=\" + term + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, term));\n                counts[(int) ordMap.get(term)]++;\n                term = (int) segValues.nextOrd();\n              }\n            }\n          }\n        } else {\n          //System.out.println(\"    count in seg ord first\");\n\n          // First count in seg-ord space:\n          final int[] segCounts = new int[numSegOrds];\n          int doc;\n          while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            //System.out.println(\"    doc=\" + doc);\n            if (doc > segValues.docID()) {\n              segValues.advance(doc);\n            }\n            if (doc == segValues.docID()) {\n              int term = (int) segValues.nextOrd();\n              while (term != SortedSetDocValues.NO_MORE_ORDS) {\n                //System.out.println(\"      ord=\" + term);\n                segCounts[term]++;\n                term = (int) segValues.nextOrd();\n              }\n            }\n          }\n\n          // Then, migrate to global ords:\n          for(int ord=0;ord<numSegOrds;ord++) {\n            int count = segCounts[ord];\n            if (count != 0) {\n              //System.out.println(\"    migrate segOrd=\" + segOrd + \" ord=\" + ord + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, ord));\n              counts[(int) ordMap.get(ord)] += count;\n            }\n          }\n        }\n      } else {\n        // No ord mapping (e.g., single segment index):\n        // just aggregate directly into counts:\n        int doc;\n        while ((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n          if (doc > segValues.docID()) {\n            segValues.advance(doc);\n          }\n          if (doc == segValues.docID()) {\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              counts[term]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8cfd1df435f04d4287925cca73cf22120f723892","date":1493925365,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","sourceNew":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n    //System.out.println(\"ssdv count\");\n\n    MultiDocValues.OrdinalMap ordinalMap;\n\n    // TODO: is this right?  really, we need a way to\n    // verify that this ordinalMap \"matches\" the leaves in\n    // matchingDocs...\n    if (dv instanceof MultiDocValues.MultiSortedSetDocValues && matchingDocs.size() > 1) {\n      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n    } else {\n      ordinalMap = null;\n    }\n    \n    IndexReader reader = state.getReader();\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != reader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n\n      countOneSegment(ordinalMap, hits.context.reader(), hits.context.ord, hits);\n    }\n  }\n\n","sourceOld":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n    //System.out.println(\"ssdv count\");\n\n    MultiDocValues.OrdinalMap ordinalMap;\n\n    // TODO: is this right?  really, we need a way to\n    // verify that this ordinalMap \"matches\" the leaves in\n    // matchingDocs...\n    if (dv instanceof MultiDocValues.MultiSortedSetDocValues && matchingDocs.size() > 1) {\n      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n    } else {\n      ordinalMap = null;\n    }\n    \n    IndexReader origReader = state.getOrigReader();\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      LeafReader reader = hits.context.reader();\n      //System.out.println(\"  reader=\" + reader);\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != origReader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n      \n      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);\n      if (segValues == null) {\n        continue;\n      }\n\n      DocIdSetIterator it = ConjunctionDISI.intersectIterators(Arrays.asList(\n                                  hits.bits.iterator(), segValues));\n\n      // TODO: yet another option is to count all segs\n      // first, only in seg-ord space, and then do a\n      // merge-sort-PQ in the end to only \"resolve to\n      // global\" those seg ords that can compete, if we know\n      // we just want top K?  ie, this is the same algo\n      // that'd be used for merging facets across shards\n      // (distributed faceting).  but this has much higher\n      // temp ram req'ts (sum of number of ords across all\n      // segs)\n      if (ordinalMap != null) {\n        final int segOrd = hits.context.ord;\n        final LongValues ordMap = ordinalMap.getGlobalOrds(segOrd);\n\n        int numSegOrds = (int) segValues.getValueCount();\n\n        if (hits.totalHits < numSegOrds/10) {\n          //System.out.println(\"    remap as-we-go\");\n          // Remap every ord to global ord as we iterate:\n          for (int doc = it.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = it.nextDoc()) {\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      segOrd=\" + segOrd + \" ord=\" + term + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, term));\n              counts[(int) ordMap.get(term)]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n        } else {\n          //System.out.println(\"    count in seg ord first\");\n\n          // First count in seg-ord space:\n          final int[] segCounts = new int[numSegOrds];\n          for (int doc = it.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = it.nextDoc()) {\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      ord=\" + term);\n              segCounts[term]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n\n          // Then, migrate to global ords:\n          for(int ord=0;ord<numSegOrds;ord++) {\n            int count = segCounts[ord];\n            if (count != 0) {\n              //System.out.println(\"    migrate segOrd=\" + segOrd + \" ord=\" + ord + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, ord));\n              counts[(int) ordMap.get(ord)] += count;\n            }\n          }\n        }\n      } else {\n        // No ord mapping (e.g., single segment index):\n        // just aggregate directly into counts:\n        for (int doc = it.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = it.nextDoc()) {\n          int term = (int) segValues.nextOrd();\n          while (term != SortedSetDocValues.NO_MORE_ORDS) {\n            counts[term]++;\n            term = (int) segValues.nextOrd();\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e9017cf144952056066919f1ebc7897ff9bd71b1","date":1496757600,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","sourceNew":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n    //System.out.println(\"ssdv count\");\n\n    MultiDocValues.OrdinalMap ordinalMap;\n\n    // TODO: is this right?  really, we need a way to\n    // verify that this ordinalMap \"matches\" the leaves in\n    // matchingDocs...\n    if (dv instanceof MultiDocValues.MultiSortedSetDocValues && matchingDocs.size() > 1) {\n      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n    } else {\n      ordinalMap = null;\n    }\n    \n    IndexReader reader = state.getReader();\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != reader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n\n      countOneSegment(ordinalMap, hits.context.reader(), hits.context.ord, hits);\n    }\n  }\n\n","sourceOld":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n    //System.out.println(\"ssdv count\");\n\n    MultiDocValues.OrdinalMap ordinalMap;\n\n    // TODO: is this right?  really, we need a way to\n    // verify that this ordinalMap \"matches\" the leaves in\n    // matchingDocs...\n    if (dv instanceof MultiDocValues.MultiSortedSetDocValues && matchingDocs.size() > 1) {\n      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n    } else {\n      ordinalMap = null;\n    }\n    \n    IndexReader origReader = state.getOrigReader();\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      LeafReader reader = hits.context.reader();\n      //System.out.println(\"  reader=\" + reader);\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != origReader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n      \n      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);\n      if (segValues == null) {\n        continue;\n      }\n\n      DocIdSetIterator it = ConjunctionDISI.intersectIterators(Arrays.asList(\n                                  hits.bits.iterator(), segValues));\n\n      // TODO: yet another option is to count all segs\n      // first, only in seg-ord space, and then do a\n      // merge-sort-PQ in the end to only \"resolve to\n      // global\" those seg ords that can compete, if we know\n      // we just want top K?  ie, this is the same algo\n      // that'd be used for merging facets across shards\n      // (distributed faceting).  but this has much higher\n      // temp ram req'ts (sum of number of ords across all\n      // segs)\n      if (ordinalMap != null) {\n        final int segOrd = hits.context.ord;\n        final LongValues ordMap = ordinalMap.getGlobalOrds(segOrd);\n\n        int numSegOrds = (int) segValues.getValueCount();\n\n        if (hits.totalHits < numSegOrds/10) {\n          //System.out.println(\"    remap as-we-go\");\n          // Remap every ord to global ord as we iterate:\n          for (int doc = it.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = it.nextDoc()) {\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      segOrd=\" + segOrd + \" ord=\" + term + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, term));\n              counts[(int) ordMap.get(term)]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n        } else {\n          //System.out.println(\"    count in seg ord first\");\n\n          // First count in seg-ord space:\n          final int[] segCounts = new int[numSegOrds];\n          for (int doc = it.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = it.nextDoc()) {\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      ord=\" + term);\n              segCounts[term]++;\n              term = (int) segValues.nextOrd();\n            }\n          }\n\n          // Then, migrate to global ords:\n          for(int ord=0;ord<numSegOrds;ord++) {\n            int count = segCounts[ord];\n            if (count != 0) {\n              //System.out.println(\"    migrate segOrd=\" + segOrd + \" ord=\" + ord + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, ord));\n              counts[(int) ordMap.get(ord)] += count;\n            }\n          }\n        }\n      } else {\n        // No ord mapping (e.g., single segment index):\n        // just aggregate directly into counts:\n        for (int doc = it.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = it.nextDoc()) {\n          int term = (int) segValues.nextOrd();\n          while (term != SortedSetDocValues.NO_MORE_ORDS) {\n            counts[term]++;\n            term = (int) segValues.nextOrd();\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"957c610636f393a85a38f1af670540028db13e6b","date":1500044517,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","sourceNew":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n    //System.out.println(\"ssdv count\");\n\n    OrdinalMap ordinalMap;\n\n    // TODO: is this right?  really, we need a way to\n    // verify that this ordinalMap \"matches\" the leaves in\n    // matchingDocs...\n    if (dv instanceof MultiDocValues.MultiSortedSetDocValues && matchingDocs.size() > 1) {\n      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n    } else {\n      ordinalMap = null;\n    }\n    \n    IndexReader reader = state.getReader();\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != reader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n\n      countOneSegment(ordinalMap, hits.context.reader(), hits.context.ord, hits);\n    }\n  }\n\n","sourceOld":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n    //System.out.println(\"ssdv count\");\n\n    MultiDocValues.OrdinalMap ordinalMap;\n\n    // TODO: is this right?  really, we need a way to\n    // verify that this ordinalMap \"matches\" the leaves in\n    // matchingDocs...\n    if (dv instanceof MultiDocValues.MultiSortedSetDocValues && matchingDocs.size() > 1) {\n      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n    } else {\n      ordinalMap = null;\n    }\n    \n    IndexReader reader = state.getReader();\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != reader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n\n      countOneSegment(ordinalMap, hits.context.reader(), hits.context.ord, hits);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"aaf90fc29510e72665ac7934f34c3d1c25efad64","date":1500354819,"type":3,"author":"Cao Manh Dat","isMerge":true,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","sourceNew":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n    //System.out.println(\"ssdv count\");\n\n    OrdinalMap ordinalMap;\n\n    // TODO: is this right?  really, we need a way to\n    // verify that this ordinalMap \"matches\" the leaves in\n    // matchingDocs...\n    if (dv instanceof MultiDocValues.MultiSortedSetDocValues && matchingDocs.size() > 1) {\n      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n    } else {\n      ordinalMap = null;\n    }\n    \n    IndexReader reader = state.getReader();\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != reader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n\n      countOneSegment(ordinalMap, hits.context.reader(), hits.context.ord, hits);\n    }\n  }\n\n","sourceOld":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n    //System.out.println(\"ssdv count\");\n\n    MultiDocValues.OrdinalMap ordinalMap;\n\n    // TODO: is this right?  really, we need a way to\n    // verify that this ordinalMap \"matches\" the leaves in\n    // matchingDocs...\n    if (dv instanceof MultiDocValues.MultiSortedSetDocValues && matchingDocs.size() > 1) {\n      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n    } else {\n      ordinalMap = null;\n    }\n    \n    IndexReader reader = state.getReader();\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != reader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n\n      countOneSegment(ordinalMap, hits.context.reader(), hits.context.ord, hits);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"8cfd1df435f04d4287925cca73cf22120f723892":["60b61628d1912768f51eccaa8ead5a5a32ab34c6"],"957c610636f393a85a38f1af670540028db13e6b":["8cfd1df435f04d4287925cca73cf22120f723892"],"4edc984f0f4ac77c37e48ace2932f780f888453c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"30fc8c22124fdcf9d76449bd2fa04decbe74e2e2":["a0732a1e488deedeceef7f601e066085e7ac655a"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["9b25d26ad3e8f824d95db88ecc5e5d9d71d3c595"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["c9fb5f46e264daf5ba3860defe623a89d202dd87","17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"60b61628d1912768f51eccaa8ead5a5a32ab34c6":["d2714c85633b642b29871cf5ff8d17d3ba7bfd76","4b8b2bc4b8b503cc0b5743b19445798c62069e4d"],"e9017cf144952056066919f1ebc7897ff9bd71b1":["60b61628d1912768f51eccaa8ead5a5a32ab34c6","8cfd1df435f04d4287925cca73cf22120f723892"],"f6d0aee18c1653f7ee634fa8830abdb001dcfe1b":["30fc8c22124fdcf9d76449bd2fa04decbe74e2e2","9b25d26ad3e8f824d95db88ecc5e5d9d71d3c595"],"aaf90fc29510e72665ac7934f34c3d1c25efad64":["e9017cf144952056066919f1ebc7897ff9bd71b1","957c610636f393a85a38f1af670540028db13e6b"],"a0732a1e488deedeceef7f601e066085e7ac655a":["3cc728b07df73b197e6d940d27f9b08b63918f13"],"11134e449dabe11d6d0ff6a564d84b82cbe93722":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["c9fb5f46e264daf5ba3860defe623a89d202dd87","6652c74b2358a0b13223817a6a793bf1c9d0749d"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"80d0e6d59ae23f4a6f30eaf40bfb40742300287f":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","60b61628d1912768f51eccaa8ead5a5a32ab34c6"],"4b8b2bc4b8b503cc0b5743b19445798c62069e4d":["d2714c85633b642b29871cf5ff8d17d3ba7bfd76"],"3cc728b07df73b197e6d940d27f9b08b63918f13":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","4edc984f0f4ac77c37e48ace2932f780f888453c"],"9b25d26ad3e8f824d95db88ecc5e5d9d71d3c595":["30fc8c22124fdcf9d76449bd2fa04decbe74e2e2"],"d2714c85633b642b29871cf5ff8d17d3ba7bfd76":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","11134e449dabe11d6d0ff6a564d84b82cbe93722"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["957c610636f393a85a38f1af670540028db13e6b"]},"commit2Childs":{"8cfd1df435f04d4287925cca73cf22120f723892":["957c610636f393a85a38f1af670540028db13e6b","e9017cf144952056066919f1ebc7897ff9bd71b1"],"957c610636f393a85a38f1af670540028db13e6b":["aaf90fc29510e72665ac7934f34c3d1c25efad64","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"4edc984f0f4ac77c37e48ace2932f780f888453c":["3cc728b07df73b197e6d940d27f9b08b63918f13"],"30fc8c22124fdcf9d76449bd2fa04decbe74e2e2":["f6d0aee18c1653f7ee634fa8830abdb001dcfe1b","9b25d26ad3e8f824d95db88ecc5e5d9d71d3c595"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","17e5da53e4e5bd659e22add9bba1cfa222e7e30d","6652c74b2358a0b13223817a6a793bf1c9d0749d"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["80d0e6d59ae23f4a6f30eaf40bfb40742300287f"],"60b61628d1912768f51eccaa8ead5a5a32ab34c6":["8cfd1df435f04d4287925cca73cf22120f723892","e9017cf144952056066919f1ebc7897ff9bd71b1","80d0e6d59ae23f4a6f30eaf40bfb40742300287f"],"e9017cf144952056066919f1ebc7897ff9bd71b1":["aaf90fc29510e72665ac7934f34c3d1c25efad64"],"f6d0aee18c1653f7ee634fa8830abdb001dcfe1b":[],"aaf90fc29510e72665ac7934f34c3d1c25efad64":[],"a0732a1e488deedeceef7f601e066085e7ac655a":["30fc8c22124fdcf9d76449bd2fa04decbe74e2e2"],"11134e449dabe11d6d0ff6a564d84b82cbe93722":["d2714c85633b642b29871cf5ff8d17d3ba7bfd76"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["4edc984f0f4ac77c37e48ace2932f780f888453c","3cc728b07df73b197e6d940d27f9b08b63918f13"],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","11134e449dabe11d6d0ff6a564d84b82cbe93722","d2714c85633b642b29871cf5ff8d17d3ba7bfd76"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"80d0e6d59ae23f4a6f30eaf40bfb40742300287f":[],"4b8b2bc4b8b503cc0b5743b19445798c62069e4d":["60b61628d1912768f51eccaa8ead5a5a32ab34c6"],"3cc728b07df73b197e6d940d27f9b08b63918f13":["a0732a1e488deedeceef7f601e066085e7ac655a"],"9b25d26ad3e8f824d95db88ecc5e5d9d71d3c595":["c9fb5f46e264daf5ba3860defe623a89d202dd87","f6d0aee18c1653f7ee634fa8830abdb001dcfe1b"],"d2714c85633b642b29871cf5ff8d17d3ba7bfd76":["60b61628d1912768f51eccaa8ead5a5a32ab34c6","4b8b2bc4b8b503cc0b5743b19445798c62069e4d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["f6d0aee18c1653f7ee634fa8830abdb001dcfe1b","aaf90fc29510e72665ac7934f34c3d1c25efad64","80d0e6d59ae23f4a6f30eaf40bfb40742300287f","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}