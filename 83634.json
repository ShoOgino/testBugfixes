{"path":"lucene/backwards/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptions().mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/backwards/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptions().mjava","pathOld":"backwards/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptions().mjava","sourceNew":"  public void testRandomExceptions() throws Throwable {\n    MockRAMDirectory dir = new MockRAMDirectory();\n\n    MockIndexWriter writer  = new MockIndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n    ((ConcurrentMergeScheduler) writer.getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.setRAMBufferSizeMB(0.1);\n\n    if (DEBUG)\n      writer.setInfoStream(System.out);\n\n    IndexerThread thread = new IndexerThread(0, writer);\n    thread.run();\n    if (thread.failure != null) {\n      thread.failure.printStackTrace(System.out);\n      fail(\"thread \" + thread.getName() + \": hit unexpected failure\");\n    }\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n  }\n\n","sourceOld":"  public void testRandomExceptions() throws Throwable {\n    MockRAMDirectory dir = new MockRAMDirectory();\n\n    MockIndexWriter writer  = new MockIndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n    ((ConcurrentMergeScheduler) writer.getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.setRAMBufferSizeMB(0.1);\n\n    if (DEBUG)\n      writer.setInfoStream(System.out);\n\n    IndexerThread thread = new IndexerThread(0, writer);\n    thread.run();\n    if (thread.failure != null) {\n      thread.failure.printStackTrace(System.out);\n      fail(\"thread \" + thread.getName() + \": hit unexpected failure\");\n    }\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b48e4082e2f39f1eb6f935ea9a1203c5e8d830a9","date":1270985469,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/backwards/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptions().mjava","pathOld":"lucene/backwards/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptions().mjava","sourceNew":"  public void testRandomExceptions() throws Throwable {\n    MockRAMDirectory dir = new MockRAMDirectory();\n\n    MockIndexWriter writer  = new MockIndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n    ((ConcurrentMergeScheduler) writer.getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.setRAMBufferSizeMB(0.1);\n    writer.commit();\n    \n    if (DEBUG)\n      writer.setInfoStream(System.out);\n\n    IndexerThread thread = new IndexerThread(0, writer);\n    thread.run();\n    if (thread.failure != null) {\n      thread.failure.printStackTrace(System.out);\n      fail(\"thread \" + thread.getName() + \": hit unexpected failure\");\n    }\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n  }\n\n","sourceOld":"  public void testRandomExceptions() throws Throwable {\n    MockRAMDirectory dir = new MockRAMDirectory();\n\n    MockIndexWriter writer  = new MockIndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n    ((ConcurrentMergeScheduler) writer.getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.setRAMBufferSizeMB(0.1);\n\n    if (DEBUG)\n      writer.setInfoStream(System.out);\n\n    IndexerThread thread = new IndexerThread(0, writer);\n    thread.run();\n    if (thread.failure != null) {\n      thread.failure.printStackTrace(System.out);\n      fail(\"thread \" + thread.getName() + \": hit unexpected failure\");\n    }\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d8d3f45cdd3ff689aaf7a3aab99e2df31305ac10","date":1270996866,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/backwards/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptions().mjava","pathOld":"lucene/backwards/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptions().mjava","sourceNew":"  public void testRandomExceptions() throws Throwable {\n    MockRAMDirectory dir = new MockRAMDirectory();\n\n    MockIndexWriter writer  = new MockIndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n    ((ConcurrentMergeScheduler) writer.getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.setRAMBufferSizeMB(0.1);\n\n    if (DEBUG)\n      writer.setInfoStream(System.out);\n\n    IndexerThread thread = new IndexerThread(0, writer);\n    thread.run();\n    if (thread.failure != null) {\n      thread.failure.printStackTrace(System.out);\n      fail(\"thread \" + thread.getName() + \": hit unexpected failure\");\n    }\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n  }\n\n","sourceOld":"  public void testRandomExceptions() throws Throwable {\n    MockRAMDirectory dir = new MockRAMDirectory();\n\n    MockIndexWriter writer  = new MockIndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n    ((ConcurrentMergeScheduler) writer.getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.setRAMBufferSizeMB(0.1);\n    writer.commit();\n    \n    if (DEBUG)\n      writer.setInfoStream(System.out);\n\n    IndexerThread thread = new IndexerThread(0, writer);\n    thread.run();\n    if (thread.failure != null) {\n      thread.failure.printStackTrace(System.out);\n      fail(\"thread \" + thread.getName() + \": hit unexpected failure\");\n    }\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n  }\n\n","bugFix":null,"bugIntro":["69a923a22517eb7ff0bad9c6d1a7d45cc0696bd4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"69a923a22517eb7ff0bad9c6d1a7d45cc0696bd4","date":1271167458,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/backwards/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptions().mjava","pathOld":"lucene/backwards/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptions().mjava","sourceNew":"  public void testRandomExceptions() throws Throwable {\n    MockRAMDirectory dir = new MockRAMDirectory();\n\n    MockIndexWriter writer  = new MockIndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n    ((ConcurrentMergeScheduler) writer.getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.setRAMBufferSizeMB(0.1);\n    writer.commit();\n    \n    if (DEBUG)\n      writer.setInfoStream(System.out);\n\n    IndexerThread thread = new IndexerThread(0, writer);\n    thread.run();\n    if (thread.failure != null) {\n      thread.failure.printStackTrace(System.out);\n      fail(\"thread \" + thread.getName() + \": hit unexpected failure\");\n    }\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n  }\n\n","sourceOld":"  public void testRandomExceptions() throws Throwable {\n    MockRAMDirectory dir = new MockRAMDirectory();\n\n    MockIndexWriter writer  = new MockIndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n    ((ConcurrentMergeScheduler) writer.getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.setRAMBufferSizeMB(0.1);\n\n    if (DEBUG)\n      writer.setInfoStream(System.out);\n\n    IndexerThread thread = new IndexerThread(0, writer);\n    thread.run();\n    if (thread.failure != null) {\n      thread.failure.printStackTrace(System.out);\n      fail(\"thread \" + thread.getName() + \": hit unexpected failure\");\n    }\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n  }\n\n","bugFix":["d8d3f45cdd3ff689aaf7a3aab99e2df31305ac10"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6","date":1272983566,"type":4,"author":"Uwe Schindler","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/backwards/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptions().mjava","sourceNew":null,"sourceOld":"  public void testRandomExceptions() throws Throwable {\n    MockRAMDirectory dir = new MockRAMDirectory();\n\n    MockIndexWriter writer  = new MockIndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n    ((ConcurrentMergeScheduler) writer.getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.setRAMBufferSizeMB(0.1);\n    writer.commit();\n    \n    if (DEBUG)\n      writer.setInfoStream(System.out);\n\n    IndexerThread thread = new IndexerThread(0, writer);\n    thread.run();\n    if (thread.failure != null) {\n      thread.failure.printStackTrace(System.out);\n      fail(\"thread \" + thread.getName() + \": hit unexpected failure\");\n    }\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6":["69a923a22517eb7ff0bad9c6d1a7d45cc0696bd4"],"69a923a22517eb7ff0bad9c6d1a7d45cc0696bd4":["d8d3f45cdd3ff689aaf7a3aab99e2df31305ac10"],"b48e4082e2f39f1eb6f935ea9a1203c5e8d830a9":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6"],"d8d3f45cdd3ff689aaf7a3aab99e2df31305ac10":["b48e4082e2f39f1eb6f935ea9a1203c5e8d830a9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"69a923a22517eb7ff0bad9c6d1a7d45cc0696bd4":["90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6"],"b48e4082e2f39f1eb6f935ea9a1203c5e8d830a9":["d8d3f45cdd3ff689aaf7a3aab99e2df31305ac10"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"d8d3f45cdd3ff689aaf7a3aab99e2df31305ac10":["69a923a22517eb7ff0bad9c6d1a7d45cc0696bd4"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["b48e4082e2f39f1eb6f935ea9a1203c5e8d830a9"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}