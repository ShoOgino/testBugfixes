{"path":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","commits":[{"id":"1509f151d7692d84fae414b2b799ac06ba60fcb4","date":1314451621,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","sourceNew":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.indexed() && doInvert) {\n        \n        if (i > 0)\n          fieldState.position += docState.analyzer == null ? 0 : docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        // TODO (LUCENE-2309): this analysis logic should be\n        // outside of indexer -- field should simply give us\n        // a TokenStream, even for multi-valued fields\n\n        if (!field.tokenized()) {\t\t  // un-tokenized field\n          final String stringValue = field.stringValue();\n          assert stringValue != null;\n          final int valueLength = stringValue.length();\n          parent.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = parent.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success) {\n              docState.docWriter.setAborting();\n            }\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) {\n            stream = streamValue;\n          } else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null) {\n              reader = readerValue;\n            } else {\n              String stringValue = field.stringValue();\n              if (stringValue == null) {\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              }\n              parent.stringReader.init(stringValue);\n              reader = parent.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success) {\n                  docState.docWriter.setAborting();\n                }\n              }\n              fieldState.length++;\n              fieldState.position++;\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += docState.analyzer == null ? 0 : docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n        \n        if (i > 0)\n          fieldState.position += docState.analyzer == null ? 0 : docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          parent.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = parent.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success) {\n              docState.docWriter.setAborting();\n            }\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null) {\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              }\n              parent.stringReader.init(stringValue);\n              reader = parent.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success) {\n                  docState.docWriter.setAborting();\n                }\n              }\n              fieldState.length++;\n              fieldState.position++;\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += docState.analyzer == null ? 0 : docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6eb141f80638abdb6ffaa5149877f36ea39b6ad5","date":1315714072,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","sourceNew":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.fieldType().indexed() && doInvert) {\n        \n        if (i > 0)\n          fieldState.position += docState.analyzer == null ? 0 : docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        // TODO (LUCENE-2309): this analysis logic should be\n        // outside of indexer -- field should simply give us\n        // a TokenStream, even for multi-valued fields\n\n        if (!field.fieldType().tokenized()) {\t\t  // un-tokenized field\n          final String stringValue = field.stringValue();\n          assert stringValue != null;\n          final int valueLength = stringValue.length();\n          parent.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = parent.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success) {\n              docState.docWriter.setAborting();\n            }\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) {\n            stream = streamValue;\n          } else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null) {\n              reader = readerValue;\n            } else {\n              String stringValue = field.stringValue();\n              if (stringValue == null) {\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              }\n              parent.stringReader.init(stringValue);\n              reader = parent.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success) {\n                  docState.docWriter.setAborting();\n                }\n              }\n              fieldState.length++;\n              fieldState.position++;\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += docState.analyzer == null ? 0 : docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.indexed() && doInvert) {\n        \n        if (i > 0)\n          fieldState.position += docState.analyzer == null ? 0 : docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        // TODO (LUCENE-2309): this analysis logic should be\n        // outside of indexer -- field should simply give us\n        // a TokenStream, even for multi-valued fields\n\n        if (!field.tokenized()) {\t\t  // un-tokenized field\n          final String stringValue = field.stringValue();\n          assert stringValue != null;\n          final int valueLength = stringValue.length();\n          parent.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = parent.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success) {\n              docState.docWriter.setAborting();\n            }\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) {\n            stream = streamValue;\n          } else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null) {\n              reader = readerValue;\n            } else {\n              String stringValue = field.stringValue();\n              if (stringValue == null) {\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              }\n              parent.stringReader.init(stringValue);\n              reader = parent.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success) {\n                  docState.docWriter.setAborting();\n                }\n              }\n              fieldState.length++;\n              fieldState.position++;\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += docState.analyzer == null ? 0 : docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":["d1336abe0899b2984e5652903556c1925fbdca9f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8b3bdb938a073ccc28d7ed813f6e8c4cb58e04c5","date":1316747797,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","sourceNew":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.fieldType().indexed() && doInvert) {\n\n        if (i > 0)\n          fieldState.position += docState.analyzer == null ? 0 : docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        final TokenStream stream = field.tokenStream(docState.analyzer);\n        // reset the TokenStream to the first token\n        stream.reset();\n\n        try {\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          consumer.start(field);\n\n          for (;;) {\n\n            // If we hit an exception in stream.next below\n            // (which is fairly common, eg if analyzer\n            // chokes on a given document), then it's\n            // non-aborting and (above) this one document\n            // will be marked as deleted, but still\n            // consume a docID\n\n            if (!hasMoreTokens) break;\n\n            final int posIncr = posIncrAttribute.getPositionIncrement();\n            fieldState.position += posIncr;\n            if (fieldState.position > 0) {\n              fieldState.position--;\n            }\n\n            if (posIncr == 0)\n              fieldState.numOverlap++;\n\n            boolean success = false;\n            try {\n              // If we hit an exception in here, we abort\n              // all buffered documents since the last\n              // flush, on the likelihood that the\n              // internal state of the consumer is now\n              // corrupt and should not be flushed to a\n              // new segment:\n              consumer.add();\n              success = true;\n            } finally {\n              if (!success) {\n                docState.docWriter.setAborting();\n              }\n            }\n            fieldState.length++;\n            fieldState.position++;\n\n            hasMoreTokens = stream.incrementToken();\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n\n          fieldState.offset += offsetAttribute.endOffset();\n        } finally {\n          stream.close();\n        }\n\n        fieldState.offset += docState.analyzer == null ? 0 : docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.fieldType().indexed() && doInvert) {\n        \n        if (i > 0)\n          fieldState.position += docState.analyzer == null ? 0 : docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        // TODO (LUCENE-2309): this analysis logic should be\n        // outside of indexer -- field should simply give us\n        // a TokenStream, even for multi-valued fields\n\n        if (!field.fieldType().tokenized()) {\t\t  // un-tokenized field\n          final String stringValue = field.stringValue();\n          assert stringValue != null;\n          final int valueLength = stringValue.length();\n          parent.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = parent.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success) {\n              docState.docWriter.setAborting();\n            }\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) {\n            stream = streamValue;\n          } else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null) {\n              reader = readerValue;\n            } else {\n              String stringValue = field.stringValue();\n              if (stringValue == null) {\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              }\n              parent.stringReader.init(stringValue);\n              reader = parent.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success) {\n                  docState.docWriter.setAborting();\n                }\n              }\n              fieldState.length++;\n              fieldState.position++;\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += docState.analyzer == null ? 0 : docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":["a7c7a5405c388fd86e5962126be8ad09283eb5cc","0e748132a1ca480bd503ee795aee58f5765809a6","782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"31f025ae60076ae95274433f3fe8e6ace2857a87","date":1326669465,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","sourceNew":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.fieldType().indexed() && doInvert) {\n\n        if (i > 0) {\n          fieldState.position += docState.analyzer == null ? 0 : docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        }\n\n        final TokenStream stream = field.tokenStream(docState.analyzer);\n        // reset the TokenStream to the first token\n        stream.reset();\n\n        try {\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          consumer.start(field);\n\n          for (;;) {\n\n            // If we hit an exception in stream.next below\n            // (which is fairly common, eg if analyzer\n            // chokes on a given document), then it's\n            // non-aborting and (above) this one document\n            // will be marked as deleted, but still\n            // consume a docID\n\n            if (!hasMoreTokens) break;\n\n            final int posIncr = posIncrAttribute.getPositionIncrement();\n            fieldState.position += posIncr;\n            if (fieldState.position > 0) {\n              fieldState.position--;\n            }\n\n            if (posIncr == 0)\n              fieldState.numOverlap++;\n\n            boolean success = false;\n            try {\n              // If we hit an exception in here, we abort\n              // all buffered documents since the last\n              // flush, on the likelihood that the\n              // internal state of the consumer is now\n              // corrupt and should not be flushed to a\n              // new segment:\n              consumer.add();\n              success = true;\n            } finally {\n              if (!success) {\n                docState.docWriter.setAborting();\n              }\n            }\n            fieldState.length++;\n            fieldState.position++;\n\n            hasMoreTokens = stream.incrementToken();\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n\n          fieldState.offset += offsetAttribute.endOffset();\n        } finally {\n          stream.close();\n        }\n\n        fieldState.offset += docState.analyzer == null ? 0 : docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.fieldType().indexed() && doInvert) {\n\n        if (i > 0)\n          fieldState.position += docState.analyzer == null ? 0 : docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        final TokenStream stream = field.tokenStream(docState.analyzer);\n        // reset the TokenStream to the first token\n        stream.reset();\n\n        try {\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          consumer.start(field);\n\n          for (;;) {\n\n            // If we hit an exception in stream.next below\n            // (which is fairly common, eg if analyzer\n            // chokes on a given document), then it's\n            // non-aborting and (above) this one document\n            // will be marked as deleted, but still\n            // consume a docID\n\n            if (!hasMoreTokens) break;\n\n            final int posIncr = posIncrAttribute.getPositionIncrement();\n            fieldState.position += posIncr;\n            if (fieldState.position > 0) {\n              fieldState.position--;\n            }\n\n            if (posIncr == 0)\n              fieldState.numOverlap++;\n\n            boolean success = false;\n            try {\n              // If we hit an exception in here, we abort\n              // all buffered documents since the last\n              // flush, on the likelihood that the\n              // internal state of the consumer is now\n              // corrupt and should not be flushed to a\n              // new segment:\n              consumer.add();\n              success = true;\n            } finally {\n              if (!success) {\n                docState.docWriter.setAborting();\n              }\n            }\n            fieldState.length++;\n            fieldState.position++;\n\n            hasMoreTokens = stream.incrementToken();\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n\n          fieldState.offset += offsetAttribute.endOffset();\n        } finally {\n          stream.close();\n        }\n\n        fieldState.offset += docState.analyzer == null ? 0 : docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","sourceNew":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.fieldType().indexed() && doInvert) {\n\n        if (i > 0) {\n          fieldState.position += docState.analyzer == null ? 0 : docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        }\n\n        final TokenStream stream = field.tokenStream(docState.analyzer);\n        // reset the TokenStream to the first token\n        stream.reset();\n\n        try {\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          consumer.start(field);\n\n          for (;;) {\n\n            // If we hit an exception in stream.next below\n            // (which is fairly common, eg if analyzer\n            // chokes on a given document), then it's\n            // non-aborting and (above) this one document\n            // will be marked as deleted, but still\n            // consume a docID\n\n            if (!hasMoreTokens) break;\n\n            final int posIncr = posIncrAttribute.getPositionIncrement();\n            fieldState.position += posIncr;\n            if (fieldState.position > 0) {\n              fieldState.position--;\n            }\n\n            if (posIncr == 0)\n              fieldState.numOverlap++;\n\n            boolean success = false;\n            try {\n              // If we hit an exception in here, we abort\n              // all buffered documents since the last\n              // flush, on the likelihood that the\n              // internal state of the consumer is now\n              // corrupt and should not be flushed to a\n              // new segment:\n              consumer.add();\n              success = true;\n            } finally {\n              if (!success) {\n                docState.docWriter.setAborting();\n              }\n            }\n            fieldState.length++;\n            fieldState.position++;\n\n            hasMoreTokens = stream.incrementToken();\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n\n          fieldState.offset += offsetAttribute.endOffset();\n        } finally {\n          stream.close();\n        }\n\n        fieldState.offset += docState.analyzer == null ? 0 : docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.fieldType().indexed() && doInvert) {\n\n        if (i > 0) {\n          fieldState.position += docState.analyzer == null ? 0 : docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        }\n\n        final TokenStream stream = field.tokenStream(docState.analyzer);\n        // reset the TokenStream to the first token\n        stream.reset();\n\n        try {\n          boolean hasMoreTokens = stream.incrementToken();\n\n          fieldState.attributeSource = stream;\n\n          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n\n          consumer.start(field);\n\n          for (;;) {\n\n            // If we hit an exception in stream.next below\n            // (which is fairly common, eg if analyzer\n            // chokes on a given document), then it's\n            // non-aborting and (above) this one document\n            // will be marked as deleted, but still\n            // consume a docID\n\n            if (!hasMoreTokens) break;\n\n            final int posIncr = posIncrAttribute.getPositionIncrement();\n            fieldState.position += posIncr;\n            if (fieldState.position > 0) {\n              fieldState.position--;\n            }\n\n            if (posIncr == 0)\n              fieldState.numOverlap++;\n\n            boolean success = false;\n            try {\n              // If we hit an exception in here, we abort\n              // all buffered documents since the last\n              // flush, on the likelihood that the\n              // internal state of the consumer is now\n              // corrupt and should not be flushed to a\n              // new segment:\n              consumer.add();\n              success = true;\n            } finally {\n              if (!success) {\n                docState.docWriter.setAborting();\n              }\n            }\n            fieldState.length++;\n            fieldState.position++;\n\n            hasMoreTokens = stream.incrementToken();\n          }\n          // trigger streams to perform end-of-stream operations\n          stream.end();\n\n          fieldState.offset += offsetAttribute.endOffset();\n        } finally {\n          stream.close();\n        }\n\n        fieldState.offset += docState.analyzer == null ? 0 : docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["31f025ae60076ae95274433f3fe8e6ace2857a87"],"6eb141f80638abdb6ffaa5149877f36ea39b6ad5":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"8b3bdb938a073ccc28d7ed813f6e8c4cb58e04c5":["6eb141f80638abdb6ffaa5149877f36ea39b6ad5"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"31f025ae60076ae95274433f3fe8e6ace2857a87":["8b3bdb938a073ccc28d7ed813f6e8c4cb58e04c5"]},"commit2Childs":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"6eb141f80638abdb6ffaa5149877f36ea39b6ad5":["8b3bdb938a073ccc28d7ed813f6e8c4cb58e04c5"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["6eb141f80638abdb6ffaa5149877f36ea39b6ad5"],"8b3bdb938a073ccc28d7ed813f6e8c4cb58e04c5":["31f025ae60076ae95274433f3fe8e6ace2857a87"],"31f025ae60076ae95274433f3fe8e6ace2857a87":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}