{"path":"solr/contrib/ltr/src/java/org/apache/solr/ltr/LTRRescorer#scoreFeatures(SolrIndexSearcher,TopDocs,int,LTRScoringQuery.ModelWeight,ScoreDoc[],List[LeafReaderContext],ScoreDoc[]).mjava","commits":[{"id":"58c36d634c9789cb739fbd175c1a8d50b3303f6b","date":1478022614,"type":0,"author":"Christine Poerschke","isMerge":false,"pathNew":"solr/contrib/ltr/src/java/org/apache/solr/ltr/LTRRescorer#scoreFeatures(SolrIndexSearcher,TopDocs,int,LTRScoringQuery.ModelWeight,ScoreDoc[],List[LeafReaderContext],ScoreDoc[]).mjava","pathOld":"/dev/null","sourceNew":"  public void scoreFeatures(SolrIndexSearcher solrIndexSearch, TopDocs firstPassTopDocs,\n      int topN, LTRScoringQuery.ModelWeight modelWeight, ScoreDoc[] hits, List<LeafReaderContext> leaves,\n      ScoreDoc[] reranked) throws IOException {\n\n    int readerUpto = -1;\n    int endDoc = 0;\n    int docBase = 0;\n\n    LTRScoringQuery.ModelWeight.ModelScorer scorer = null;\n    int hitUpto = 0;\n    final FeatureLogger<?> featureLogger = scoringQuery.getFeatureLogger();\n\n    while (hitUpto < hits.length) {\n      final ScoreDoc hit = hits[hitUpto];\n      final int docID = hit.doc;\n      LeafReaderContext readerContext = null;\n      while (docID >= endDoc) {\n        readerUpto++;\n        readerContext = leaves.get(readerUpto);\n        endDoc = readerContext.docBase + readerContext.reader().maxDoc();\n      }\n      // We advanced to another segment\n      if (readerContext != null) {\n        docBase = readerContext.docBase;\n        scorer = modelWeight.scorer(readerContext);\n      }\n      // Scorer for a LTRScoringQuery.ModelWeight should never be null since we always have to\n      // call score\n      // even if no feature scorers match, since a model might use that info to\n      // return a\n      // non-zero score. Same applies for the case of advancing a LTRScoringQuery.ModelWeight.ModelScorer\n      // past the target\n      // doc since the model algorithm still needs to compute a potentially\n      // non-zero score from blank features.\n      assert (scorer != null);\n      final int targetDoc = docID - docBase;\n      scorer.docID();\n      scorer.iterator().advance(targetDoc);\n\n      scorer.getDocInfo().setOriginalDocScore(new Float(hit.score));\n      hit.score = scorer.score();\n      if (hitUpto < topN) {\n        reranked[hitUpto] = hit;\n        // if the heap is not full, maybe I want to log the features for this\n        // document\n        if (featureLogger != null) {\n          featureLogger.log(hit.doc, scoringQuery, solrIndexSearch,\n              modelWeight.getFeaturesInfo());\n        }\n      } else if (hitUpto == topN) {\n        // collected topN document, I create the heap\n        heapify(reranked, topN);\n      }\n      if (hitUpto >= topN) {\n        // once that heap is ready, if the score of this document is lower that\n        // the minimum\n        // i don't want to log the feature. Otherwise I replace it with the\n        // minimum and fix the\n        // heap.\n        if (hit.score > reranked[0].score) {\n          reranked[0] = hit;\n          heapAdjust(reranked, topN, 0);\n          if (featureLogger != null) {\n            featureLogger.log(hit.doc, scoringQuery, solrIndexSearch,\n                modelWeight.getFeaturesInfo());\n          }\n        }\n      }\n      hitUpto++;\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9720b151fde2073f4e401450f4574e5f31c2d0ff","date":1478184029,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/contrib/ltr/src/java/org/apache/solr/ltr/LTRRescorer#scoreFeatures(SolrIndexSearcher,TopDocs,int,LTRScoringQuery.ModelWeight,ScoreDoc[],List[LeafReaderContext],ScoreDoc[]).mjava","pathOld":"/dev/null","sourceNew":"  public void scoreFeatures(SolrIndexSearcher solrIndexSearch, TopDocs firstPassTopDocs,\n      int topN, LTRScoringQuery.ModelWeight modelWeight, ScoreDoc[] hits, List<LeafReaderContext> leaves,\n      ScoreDoc[] reranked) throws IOException {\n\n    int readerUpto = -1;\n    int endDoc = 0;\n    int docBase = 0;\n\n    LTRScoringQuery.ModelWeight.ModelScorer scorer = null;\n    int hitUpto = 0;\n    final FeatureLogger<?> featureLogger = scoringQuery.getFeatureLogger();\n\n    while (hitUpto < hits.length) {\n      final ScoreDoc hit = hits[hitUpto];\n      final int docID = hit.doc;\n      LeafReaderContext readerContext = null;\n      while (docID >= endDoc) {\n        readerUpto++;\n        readerContext = leaves.get(readerUpto);\n        endDoc = readerContext.docBase + readerContext.reader().maxDoc();\n      }\n      // We advanced to another segment\n      if (readerContext != null) {\n        docBase = readerContext.docBase;\n        scorer = modelWeight.scorer(readerContext);\n      }\n      // Scorer for a LTRScoringQuery.ModelWeight should never be null since we always have to\n      // call score\n      // even if no feature scorers match, since a model might use that info to\n      // return a\n      // non-zero score. Same applies for the case of advancing a LTRScoringQuery.ModelWeight.ModelScorer\n      // past the target\n      // doc since the model algorithm still needs to compute a potentially\n      // non-zero score from blank features.\n      assert (scorer != null);\n      final int targetDoc = docID - docBase;\n      scorer.docID();\n      scorer.iterator().advance(targetDoc);\n\n      scorer.getDocInfo().setOriginalDocScore(new Float(hit.score));\n      hit.score = scorer.score();\n      if (hitUpto < topN) {\n        reranked[hitUpto] = hit;\n        // if the heap is not full, maybe I want to log the features for this\n        // document\n        if (featureLogger != null) {\n          featureLogger.log(hit.doc, scoringQuery, solrIndexSearch,\n              modelWeight.getFeaturesInfo());\n        }\n      } else if (hitUpto == topN) {\n        // collected topN document, I create the heap\n        heapify(reranked, topN);\n      }\n      if (hitUpto >= topN) {\n        // once that heap is ready, if the score of this document is lower that\n        // the minimum\n        // i don't want to log the feature. Otherwise I replace it with the\n        // minimum and fix the\n        // heap.\n        if (hit.score > reranked[0].score) {\n          reranked[0] = hit;\n          heapAdjust(reranked, topN, 0);\n          if (featureLogger != null) {\n            featureLogger.log(hit.doc, scoringQuery, solrIndexSearch,\n                modelWeight.getFeaturesInfo());\n          }\n        }\n      }\n      hitUpto++;\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8fa1f8f009c5147b03311c7e6e6469fa7b677186","date":1482164570,"type":3,"author":"Christine Poerschke","isMerge":false,"pathNew":"solr/contrib/ltr/src/java/org/apache/solr/ltr/LTRRescorer#scoreFeatures(SolrIndexSearcher,TopDocs,int,LTRScoringQuery.ModelWeight,ScoreDoc[],List[LeafReaderContext],ScoreDoc[]).mjava","pathOld":"solr/contrib/ltr/src/java/org/apache/solr/ltr/LTRRescorer#scoreFeatures(SolrIndexSearcher,TopDocs,int,LTRScoringQuery.ModelWeight,ScoreDoc[],List[LeafReaderContext],ScoreDoc[]).mjava","sourceNew":"  public void scoreFeatures(SolrIndexSearcher solrIndexSearch, TopDocs firstPassTopDocs,\n      int topN, LTRScoringQuery.ModelWeight modelWeight, ScoreDoc[] hits, List<LeafReaderContext> leaves,\n      ScoreDoc[] reranked) throws IOException {\n\n    int readerUpto = -1;\n    int endDoc = 0;\n    int docBase = 0;\n\n    LTRScoringQuery.ModelWeight.ModelScorer scorer = null;\n    int hitUpto = 0;\n    final FeatureLogger featureLogger = scoringQuery.getFeatureLogger();\n\n    while (hitUpto < hits.length) {\n      final ScoreDoc hit = hits[hitUpto];\n      final int docID = hit.doc;\n      LeafReaderContext readerContext = null;\n      while (docID >= endDoc) {\n        readerUpto++;\n        readerContext = leaves.get(readerUpto);\n        endDoc = readerContext.docBase + readerContext.reader().maxDoc();\n      }\n      // We advanced to another segment\n      if (readerContext != null) {\n        docBase = readerContext.docBase;\n        scorer = modelWeight.scorer(readerContext);\n      }\n      // Scorer for a LTRScoringQuery.ModelWeight should never be null since we always have to\n      // call score\n      // even if no feature scorers match, since a model might use that info to\n      // return a\n      // non-zero score. Same applies for the case of advancing a LTRScoringQuery.ModelWeight.ModelScorer\n      // past the target\n      // doc since the model algorithm still needs to compute a potentially\n      // non-zero score from blank features.\n      assert (scorer != null);\n      final int targetDoc = docID - docBase;\n      scorer.docID();\n      scorer.iterator().advance(targetDoc);\n\n      scorer.getDocInfo().setOriginalDocScore(new Float(hit.score));\n      hit.score = scorer.score();\n      if (hitUpto < topN) {\n        reranked[hitUpto] = hit;\n        // if the heap is not full, maybe I want to log the features for this\n        // document\n        if (featureLogger != null) {\n          featureLogger.log(hit.doc, scoringQuery, solrIndexSearch,\n              modelWeight.getFeaturesInfo());\n        }\n      } else if (hitUpto == topN) {\n        // collected topN document, I create the heap\n        heapify(reranked, topN);\n      }\n      if (hitUpto >= topN) {\n        // once that heap is ready, if the score of this document is lower that\n        // the minimum\n        // i don't want to log the feature. Otherwise I replace it with the\n        // minimum and fix the\n        // heap.\n        if (hit.score > reranked[0].score) {\n          reranked[0] = hit;\n          heapAdjust(reranked, topN, 0);\n          if (featureLogger != null) {\n            featureLogger.log(hit.doc, scoringQuery, solrIndexSearch,\n                modelWeight.getFeaturesInfo());\n          }\n        }\n      }\n      hitUpto++;\n    }\n  }\n\n","sourceOld":"  public void scoreFeatures(SolrIndexSearcher solrIndexSearch, TopDocs firstPassTopDocs,\n      int topN, LTRScoringQuery.ModelWeight modelWeight, ScoreDoc[] hits, List<LeafReaderContext> leaves,\n      ScoreDoc[] reranked) throws IOException {\n\n    int readerUpto = -1;\n    int endDoc = 0;\n    int docBase = 0;\n\n    LTRScoringQuery.ModelWeight.ModelScorer scorer = null;\n    int hitUpto = 0;\n    final FeatureLogger<?> featureLogger = scoringQuery.getFeatureLogger();\n\n    while (hitUpto < hits.length) {\n      final ScoreDoc hit = hits[hitUpto];\n      final int docID = hit.doc;\n      LeafReaderContext readerContext = null;\n      while (docID >= endDoc) {\n        readerUpto++;\n        readerContext = leaves.get(readerUpto);\n        endDoc = readerContext.docBase + readerContext.reader().maxDoc();\n      }\n      // We advanced to another segment\n      if (readerContext != null) {\n        docBase = readerContext.docBase;\n        scorer = modelWeight.scorer(readerContext);\n      }\n      // Scorer for a LTRScoringQuery.ModelWeight should never be null since we always have to\n      // call score\n      // even if no feature scorers match, since a model might use that info to\n      // return a\n      // non-zero score. Same applies for the case of advancing a LTRScoringQuery.ModelWeight.ModelScorer\n      // past the target\n      // doc since the model algorithm still needs to compute a potentially\n      // non-zero score from blank features.\n      assert (scorer != null);\n      final int targetDoc = docID - docBase;\n      scorer.docID();\n      scorer.iterator().advance(targetDoc);\n\n      scorer.getDocInfo().setOriginalDocScore(new Float(hit.score));\n      hit.score = scorer.score();\n      if (hitUpto < topN) {\n        reranked[hitUpto] = hit;\n        // if the heap is not full, maybe I want to log the features for this\n        // document\n        if (featureLogger != null) {\n          featureLogger.log(hit.doc, scoringQuery, solrIndexSearch,\n              modelWeight.getFeaturesInfo());\n        }\n      } else if (hitUpto == topN) {\n        // collected topN document, I create the heap\n        heapify(reranked, topN);\n      }\n      if (hitUpto >= topN) {\n        // once that heap is ready, if the score of this document is lower that\n        // the minimum\n        // i don't want to log the feature. Otherwise I replace it with the\n        // minimum and fix the\n        // heap.\n        if (hit.score > reranked[0].score) {\n          reranked[0] = hit;\n          heapAdjust(reranked, topN, 0);\n          if (featureLogger != null) {\n            featureLogger.log(hit.doc, scoringQuery, solrIndexSearch,\n                modelWeight.getFeaturesInfo());\n          }\n        }\n      }\n      hitUpto++;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf","date":1482251961,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/contrib/ltr/src/java/org/apache/solr/ltr/LTRRescorer#scoreFeatures(SolrIndexSearcher,TopDocs,int,LTRScoringQuery.ModelWeight,ScoreDoc[],List[LeafReaderContext],ScoreDoc[]).mjava","pathOld":"solr/contrib/ltr/src/java/org/apache/solr/ltr/LTRRescorer#scoreFeatures(SolrIndexSearcher,TopDocs,int,LTRScoringQuery.ModelWeight,ScoreDoc[],List[LeafReaderContext],ScoreDoc[]).mjava","sourceNew":"  public void scoreFeatures(SolrIndexSearcher solrIndexSearch, TopDocs firstPassTopDocs,\n      int topN, LTRScoringQuery.ModelWeight modelWeight, ScoreDoc[] hits, List<LeafReaderContext> leaves,\n      ScoreDoc[] reranked) throws IOException {\n\n    int readerUpto = -1;\n    int endDoc = 0;\n    int docBase = 0;\n\n    LTRScoringQuery.ModelWeight.ModelScorer scorer = null;\n    int hitUpto = 0;\n    final FeatureLogger featureLogger = scoringQuery.getFeatureLogger();\n\n    while (hitUpto < hits.length) {\n      final ScoreDoc hit = hits[hitUpto];\n      final int docID = hit.doc;\n      LeafReaderContext readerContext = null;\n      while (docID >= endDoc) {\n        readerUpto++;\n        readerContext = leaves.get(readerUpto);\n        endDoc = readerContext.docBase + readerContext.reader().maxDoc();\n      }\n      // We advanced to another segment\n      if (readerContext != null) {\n        docBase = readerContext.docBase;\n        scorer = modelWeight.scorer(readerContext);\n      }\n      // Scorer for a LTRScoringQuery.ModelWeight should never be null since we always have to\n      // call score\n      // even if no feature scorers match, since a model might use that info to\n      // return a\n      // non-zero score. Same applies for the case of advancing a LTRScoringQuery.ModelWeight.ModelScorer\n      // past the target\n      // doc since the model algorithm still needs to compute a potentially\n      // non-zero score from blank features.\n      assert (scorer != null);\n      final int targetDoc = docID - docBase;\n      scorer.docID();\n      scorer.iterator().advance(targetDoc);\n\n      scorer.getDocInfo().setOriginalDocScore(new Float(hit.score));\n      hit.score = scorer.score();\n      if (hitUpto < topN) {\n        reranked[hitUpto] = hit;\n        // if the heap is not full, maybe I want to log the features for this\n        // document\n        if (featureLogger != null) {\n          featureLogger.log(hit.doc, scoringQuery, solrIndexSearch,\n              modelWeight.getFeaturesInfo());\n        }\n      } else if (hitUpto == topN) {\n        // collected topN document, I create the heap\n        heapify(reranked, topN);\n      }\n      if (hitUpto >= topN) {\n        // once that heap is ready, if the score of this document is lower that\n        // the minimum\n        // i don't want to log the feature. Otherwise I replace it with the\n        // minimum and fix the\n        // heap.\n        if (hit.score > reranked[0].score) {\n          reranked[0] = hit;\n          heapAdjust(reranked, topN, 0);\n          if (featureLogger != null) {\n            featureLogger.log(hit.doc, scoringQuery, solrIndexSearch,\n                modelWeight.getFeaturesInfo());\n          }\n        }\n      }\n      hitUpto++;\n    }\n  }\n\n","sourceOld":"  public void scoreFeatures(SolrIndexSearcher solrIndexSearch, TopDocs firstPassTopDocs,\n      int topN, LTRScoringQuery.ModelWeight modelWeight, ScoreDoc[] hits, List<LeafReaderContext> leaves,\n      ScoreDoc[] reranked) throws IOException {\n\n    int readerUpto = -1;\n    int endDoc = 0;\n    int docBase = 0;\n\n    LTRScoringQuery.ModelWeight.ModelScorer scorer = null;\n    int hitUpto = 0;\n    final FeatureLogger<?> featureLogger = scoringQuery.getFeatureLogger();\n\n    while (hitUpto < hits.length) {\n      final ScoreDoc hit = hits[hitUpto];\n      final int docID = hit.doc;\n      LeafReaderContext readerContext = null;\n      while (docID >= endDoc) {\n        readerUpto++;\n        readerContext = leaves.get(readerUpto);\n        endDoc = readerContext.docBase + readerContext.reader().maxDoc();\n      }\n      // We advanced to another segment\n      if (readerContext != null) {\n        docBase = readerContext.docBase;\n        scorer = modelWeight.scorer(readerContext);\n      }\n      // Scorer for a LTRScoringQuery.ModelWeight should never be null since we always have to\n      // call score\n      // even if no feature scorers match, since a model might use that info to\n      // return a\n      // non-zero score. Same applies for the case of advancing a LTRScoringQuery.ModelWeight.ModelScorer\n      // past the target\n      // doc since the model algorithm still needs to compute a potentially\n      // non-zero score from blank features.\n      assert (scorer != null);\n      final int targetDoc = docID - docBase;\n      scorer.docID();\n      scorer.iterator().advance(targetDoc);\n\n      scorer.getDocInfo().setOriginalDocScore(new Float(hit.score));\n      hit.score = scorer.score();\n      if (hitUpto < topN) {\n        reranked[hitUpto] = hit;\n        // if the heap is not full, maybe I want to log the features for this\n        // document\n        if (featureLogger != null) {\n          featureLogger.log(hit.doc, scoringQuery, solrIndexSearch,\n              modelWeight.getFeaturesInfo());\n        }\n      } else if (hitUpto == topN) {\n        // collected topN document, I create the heap\n        heapify(reranked, topN);\n      }\n      if (hitUpto >= topN) {\n        // once that heap is ready, if the score of this document is lower that\n        // the minimum\n        // i don't want to log the feature. Otherwise I replace it with the\n        // minimum and fix the\n        // heap.\n        if (hit.score > reranked[0].score) {\n          reranked[0] = hit;\n          heapAdjust(reranked, topN, 0);\n          if (featureLogger != null) {\n            featureLogger.log(hit.doc, scoringQuery, solrIndexSearch,\n                modelWeight.getFeaturesInfo());\n          }\n        }\n      }\n      hitUpto++;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2534504cc6a6ab3301865de897422111495e0aad","date":1499445439,"type":5,"author":"Christine Poerschke","isMerge":false,"pathNew":"solr/contrib/ltr/src/java/org/apache/solr/ltr/LTRRescorer#scoreFeatures(IndexSearcher,TopDocs,int,LTRScoringQuery.ModelWeight,ScoreDoc[],List[LeafReaderContext],ScoreDoc[]).mjava","pathOld":"solr/contrib/ltr/src/java/org/apache/solr/ltr/LTRRescorer#scoreFeatures(SolrIndexSearcher,TopDocs,int,LTRScoringQuery.ModelWeight,ScoreDoc[],List[LeafReaderContext],ScoreDoc[]).mjava","sourceNew":"  public void scoreFeatures(IndexSearcher indexSearcher, TopDocs firstPassTopDocs,\n      int topN, LTRScoringQuery.ModelWeight modelWeight, ScoreDoc[] hits, List<LeafReaderContext> leaves,\n      ScoreDoc[] reranked) throws IOException {\n\n    int readerUpto = -1;\n    int endDoc = 0;\n    int docBase = 0;\n\n    LTRScoringQuery.ModelWeight.ModelScorer scorer = null;\n    int hitUpto = 0;\n    final FeatureLogger featureLogger = scoringQuery.getFeatureLogger();\n\n    while (hitUpto < hits.length) {\n      final ScoreDoc hit = hits[hitUpto];\n      final int docID = hit.doc;\n      LeafReaderContext readerContext = null;\n      while (docID >= endDoc) {\n        readerUpto++;\n        readerContext = leaves.get(readerUpto);\n        endDoc = readerContext.docBase + readerContext.reader().maxDoc();\n      }\n      // We advanced to another segment\n      if (readerContext != null) {\n        docBase = readerContext.docBase;\n        scorer = modelWeight.scorer(readerContext);\n      }\n      // Scorer for a LTRScoringQuery.ModelWeight should never be null since we always have to\n      // call score\n      // even if no feature scorers match, since a model might use that info to\n      // return a\n      // non-zero score. Same applies for the case of advancing a LTRScoringQuery.ModelWeight.ModelScorer\n      // past the target\n      // doc since the model algorithm still needs to compute a potentially\n      // non-zero score from blank features.\n      assert (scorer != null);\n      final int targetDoc = docID - docBase;\n      scorer.docID();\n      scorer.iterator().advance(targetDoc);\n\n      scorer.getDocInfo().setOriginalDocScore(new Float(hit.score));\n      hit.score = scorer.score();\n      if (hitUpto < topN) {\n        reranked[hitUpto] = hit;\n        // if the heap is not full, maybe I want to log the features for this\n        // document\n        if (featureLogger != null && indexSearcher instanceof SolrIndexSearcher) {\n          featureLogger.log(hit.doc, scoringQuery, (SolrIndexSearcher)indexSearcher,\n              modelWeight.getFeaturesInfo());\n        }\n      } else if (hitUpto == topN) {\n        // collected topN document, I create the heap\n        heapify(reranked, topN);\n      }\n      if (hitUpto >= topN) {\n        // once that heap is ready, if the score of this document is lower that\n        // the minimum\n        // i don't want to log the feature. Otherwise I replace it with the\n        // minimum and fix the\n        // heap.\n        if (hit.score > reranked[0].score) {\n          reranked[0] = hit;\n          heapAdjust(reranked, topN, 0);\n          if (featureLogger != null && indexSearcher instanceof SolrIndexSearcher) {\n            featureLogger.log(hit.doc, scoringQuery, (SolrIndexSearcher)indexSearcher,\n                modelWeight.getFeaturesInfo());\n          }\n        }\n      }\n      hitUpto++;\n    }\n  }\n\n","sourceOld":"  public void scoreFeatures(SolrIndexSearcher solrIndexSearch, TopDocs firstPassTopDocs,\n      int topN, LTRScoringQuery.ModelWeight modelWeight, ScoreDoc[] hits, List<LeafReaderContext> leaves,\n      ScoreDoc[] reranked) throws IOException {\n\n    int readerUpto = -1;\n    int endDoc = 0;\n    int docBase = 0;\n\n    LTRScoringQuery.ModelWeight.ModelScorer scorer = null;\n    int hitUpto = 0;\n    final FeatureLogger featureLogger = scoringQuery.getFeatureLogger();\n\n    while (hitUpto < hits.length) {\n      final ScoreDoc hit = hits[hitUpto];\n      final int docID = hit.doc;\n      LeafReaderContext readerContext = null;\n      while (docID >= endDoc) {\n        readerUpto++;\n        readerContext = leaves.get(readerUpto);\n        endDoc = readerContext.docBase + readerContext.reader().maxDoc();\n      }\n      // We advanced to another segment\n      if (readerContext != null) {\n        docBase = readerContext.docBase;\n        scorer = modelWeight.scorer(readerContext);\n      }\n      // Scorer for a LTRScoringQuery.ModelWeight should never be null since we always have to\n      // call score\n      // even if no feature scorers match, since a model might use that info to\n      // return a\n      // non-zero score. Same applies for the case of advancing a LTRScoringQuery.ModelWeight.ModelScorer\n      // past the target\n      // doc since the model algorithm still needs to compute a potentially\n      // non-zero score from blank features.\n      assert (scorer != null);\n      final int targetDoc = docID - docBase;\n      scorer.docID();\n      scorer.iterator().advance(targetDoc);\n\n      scorer.getDocInfo().setOriginalDocScore(new Float(hit.score));\n      hit.score = scorer.score();\n      if (hitUpto < topN) {\n        reranked[hitUpto] = hit;\n        // if the heap is not full, maybe I want to log the features for this\n        // document\n        if (featureLogger != null) {\n          featureLogger.log(hit.doc, scoringQuery, solrIndexSearch,\n              modelWeight.getFeaturesInfo());\n        }\n      } else if (hitUpto == topN) {\n        // collected topN document, I create the heap\n        heapify(reranked, topN);\n      }\n      if (hitUpto >= topN) {\n        // once that heap is ready, if the score of this document is lower that\n        // the minimum\n        // i don't want to log the feature. Otherwise I replace it with the\n        // minimum and fix the\n        // heap.\n        if (hit.score > reranked[0].score) {\n          reranked[0] = hit;\n          heapAdjust(reranked, topN, 0);\n          if (featureLogger != null) {\n            featureLogger.log(hit.doc, scoringQuery, solrIndexSearch,\n                modelWeight.getFeaturesInfo());\n          }\n        }\n      }\n      hitUpto++;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f9a989a32a073c55e3aef6f807a3474184bbcf49","date":1499930209,"type":5,"author":"Cao Manh Dat","isMerge":true,"pathNew":"solr/contrib/ltr/src/java/org/apache/solr/ltr/LTRRescorer#scoreFeatures(IndexSearcher,TopDocs,int,LTRScoringQuery.ModelWeight,ScoreDoc[],List[LeafReaderContext],ScoreDoc[]).mjava","pathOld":"solr/contrib/ltr/src/java/org/apache/solr/ltr/LTRRescorer#scoreFeatures(SolrIndexSearcher,TopDocs,int,LTRScoringQuery.ModelWeight,ScoreDoc[],List[LeafReaderContext],ScoreDoc[]).mjava","sourceNew":"  public void scoreFeatures(IndexSearcher indexSearcher, TopDocs firstPassTopDocs,\n      int topN, LTRScoringQuery.ModelWeight modelWeight, ScoreDoc[] hits, List<LeafReaderContext> leaves,\n      ScoreDoc[] reranked) throws IOException {\n\n    int readerUpto = -1;\n    int endDoc = 0;\n    int docBase = 0;\n\n    LTRScoringQuery.ModelWeight.ModelScorer scorer = null;\n    int hitUpto = 0;\n    final FeatureLogger featureLogger = scoringQuery.getFeatureLogger();\n\n    while (hitUpto < hits.length) {\n      final ScoreDoc hit = hits[hitUpto];\n      final int docID = hit.doc;\n      LeafReaderContext readerContext = null;\n      while (docID >= endDoc) {\n        readerUpto++;\n        readerContext = leaves.get(readerUpto);\n        endDoc = readerContext.docBase + readerContext.reader().maxDoc();\n      }\n      // We advanced to another segment\n      if (readerContext != null) {\n        docBase = readerContext.docBase;\n        scorer = modelWeight.scorer(readerContext);\n      }\n      // Scorer for a LTRScoringQuery.ModelWeight should never be null since we always have to\n      // call score\n      // even if no feature scorers match, since a model might use that info to\n      // return a\n      // non-zero score. Same applies for the case of advancing a LTRScoringQuery.ModelWeight.ModelScorer\n      // past the target\n      // doc since the model algorithm still needs to compute a potentially\n      // non-zero score from blank features.\n      assert (scorer != null);\n      final int targetDoc = docID - docBase;\n      scorer.docID();\n      scorer.iterator().advance(targetDoc);\n\n      scorer.getDocInfo().setOriginalDocScore(new Float(hit.score));\n      hit.score = scorer.score();\n      if (hitUpto < topN) {\n        reranked[hitUpto] = hit;\n        // if the heap is not full, maybe I want to log the features for this\n        // document\n        if (featureLogger != null && indexSearcher instanceof SolrIndexSearcher) {\n          featureLogger.log(hit.doc, scoringQuery, (SolrIndexSearcher)indexSearcher,\n              modelWeight.getFeaturesInfo());\n        }\n      } else if (hitUpto == topN) {\n        // collected topN document, I create the heap\n        heapify(reranked, topN);\n      }\n      if (hitUpto >= topN) {\n        // once that heap is ready, if the score of this document is lower that\n        // the minimum\n        // i don't want to log the feature. Otherwise I replace it with the\n        // minimum and fix the\n        // heap.\n        if (hit.score > reranked[0].score) {\n          reranked[0] = hit;\n          heapAdjust(reranked, topN, 0);\n          if (featureLogger != null && indexSearcher instanceof SolrIndexSearcher) {\n            featureLogger.log(hit.doc, scoringQuery, (SolrIndexSearcher)indexSearcher,\n                modelWeight.getFeaturesInfo());\n          }\n        }\n      }\n      hitUpto++;\n    }\n  }\n\n","sourceOld":"  public void scoreFeatures(SolrIndexSearcher solrIndexSearch, TopDocs firstPassTopDocs,\n      int topN, LTRScoringQuery.ModelWeight modelWeight, ScoreDoc[] hits, List<LeafReaderContext> leaves,\n      ScoreDoc[] reranked) throws IOException {\n\n    int readerUpto = -1;\n    int endDoc = 0;\n    int docBase = 0;\n\n    LTRScoringQuery.ModelWeight.ModelScorer scorer = null;\n    int hitUpto = 0;\n    final FeatureLogger featureLogger = scoringQuery.getFeatureLogger();\n\n    while (hitUpto < hits.length) {\n      final ScoreDoc hit = hits[hitUpto];\n      final int docID = hit.doc;\n      LeafReaderContext readerContext = null;\n      while (docID >= endDoc) {\n        readerUpto++;\n        readerContext = leaves.get(readerUpto);\n        endDoc = readerContext.docBase + readerContext.reader().maxDoc();\n      }\n      // We advanced to another segment\n      if (readerContext != null) {\n        docBase = readerContext.docBase;\n        scorer = modelWeight.scorer(readerContext);\n      }\n      // Scorer for a LTRScoringQuery.ModelWeight should never be null since we always have to\n      // call score\n      // even if no feature scorers match, since a model might use that info to\n      // return a\n      // non-zero score. Same applies for the case of advancing a LTRScoringQuery.ModelWeight.ModelScorer\n      // past the target\n      // doc since the model algorithm still needs to compute a potentially\n      // non-zero score from blank features.\n      assert (scorer != null);\n      final int targetDoc = docID - docBase;\n      scorer.docID();\n      scorer.iterator().advance(targetDoc);\n\n      scorer.getDocInfo().setOriginalDocScore(new Float(hit.score));\n      hit.score = scorer.score();\n      if (hitUpto < topN) {\n        reranked[hitUpto] = hit;\n        // if the heap is not full, maybe I want to log the features for this\n        // document\n        if (featureLogger != null) {\n          featureLogger.log(hit.doc, scoringQuery, solrIndexSearch,\n              modelWeight.getFeaturesInfo());\n        }\n      } else if (hitUpto == topN) {\n        // collected topN document, I create the heap\n        heapify(reranked, topN);\n      }\n      if (hitUpto >= topN) {\n        // once that heap is ready, if the score of this document is lower that\n        // the minimum\n        // i don't want to log the feature. Otherwise I replace it with the\n        // minimum and fix the\n        // heap.\n        if (hit.score > reranked[0].score) {\n          reranked[0] = hit;\n          heapAdjust(reranked, topN, 0);\n          if (featureLogger != null) {\n            featureLogger.log(hit.doc, scoringQuery, solrIndexSearch,\n                modelWeight.getFeaturesInfo());\n          }\n        }\n      }\n      hitUpto++;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fb569fd721c41eafc2a2d788499a7df490c7f1a5","date":1499930871,"type":5,"author":"Noble Paul","isMerge":true,"pathNew":"solr/contrib/ltr/src/java/org/apache/solr/ltr/LTRRescorer#scoreFeatures(IndexSearcher,TopDocs,int,LTRScoringQuery.ModelWeight,ScoreDoc[],List[LeafReaderContext],ScoreDoc[]).mjava","pathOld":"solr/contrib/ltr/src/java/org/apache/solr/ltr/LTRRescorer#scoreFeatures(SolrIndexSearcher,TopDocs,int,LTRScoringQuery.ModelWeight,ScoreDoc[],List[LeafReaderContext],ScoreDoc[]).mjava","sourceNew":"  public void scoreFeatures(IndexSearcher indexSearcher, TopDocs firstPassTopDocs,\n      int topN, LTRScoringQuery.ModelWeight modelWeight, ScoreDoc[] hits, List<LeafReaderContext> leaves,\n      ScoreDoc[] reranked) throws IOException {\n\n    int readerUpto = -1;\n    int endDoc = 0;\n    int docBase = 0;\n\n    LTRScoringQuery.ModelWeight.ModelScorer scorer = null;\n    int hitUpto = 0;\n    final FeatureLogger featureLogger = scoringQuery.getFeatureLogger();\n\n    while (hitUpto < hits.length) {\n      final ScoreDoc hit = hits[hitUpto];\n      final int docID = hit.doc;\n      LeafReaderContext readerContext = null;\n      while (docID >= endDoc) {\n        readerUpto++;\n        readerContext = leaves.get(readerUpto);\n        endDoc = readerContext.docBase + readerContext.reader().maxDoc();\n      }\n      // We advanced to another segment\n      if (readerContext != null) {\n        docBase = readerContext.docBase;\n        scorer = modelWeight.scorer(readerContext);\n      }\n      // Scorer for a LTRScoringQuery.ModelWeight should never be null since we always have to\n      // call score\n      // even if no feature scorers match, since a model might use that info to\n      // return a\n      // non-zero score. Same applies for the case of advancing a LTRScoringQuery.ModelWeight.ModelScorer\n      // past the target\n      // doc since the model algorithm still needs to compute a potentially\n      // non-zero score from blank features.\n      assert (scorer != null);\n      final int targetDoc = docID - docBase;\n      scorer.docID();\n      scorer.iterator().advance(targetDoc);\n\n      scorer.getDocInfo().setOriginalDocScore(new Float(hit.score));\n      hit.score = scorer.score();\n      if (hitUpto < topN) {\n        reranked[hitUpto] = hit;\n        // if the heap is not full, maybe I want to log the features for this\n        // document\n        if (featureLogger != null && indexSearcher instanceof SolrIndexSearcher) {\n          featureLogger.log(hit.doc, scoringQuery, (SolrIndexSearcher)indexSearcher,\n              modelWeight.getFeaturesInfo());\n        }\n      } else if (hitUpto == topN) {\n        // collected topN document, I create the heap\n        heapify(reranked, topN);\n      }\n      if (hitUpto >= topN) {\n        // once that heap is ready, if the score of this document is lower that\n        // the minimum\n        // i don't want to log the feature. Otherwise I replace it with the\n        // minimum and fix the\n        // heap.\n        if (hit.score > reranked[0].score) {\n          reranked[0] = hit;\n          heapAdjust(reranked, topN, 0);\n          if (featureLogger != null && indexSearcher instanceof SolrIndexSearcher) {\n            featureLogger.log(hit.doc, scoringQuery, (SolrIndexSearcher)indexSearcher,\n                modelWeight.getFeaturesInfo());\n          }\n        }\n      }\n      hitUpto++;\n    }\n  }\n\n","sourceOld":"  public void scoreFeatures(SolrIndexSearcher solrIndexSearch, TopDocs firstPassTopDocs,\n      int topN, LTRScoringQuery.ModelWeight modelWeight, ScoreDoc[] hits, List<LeafReaderContext> leaves,\n      ScoreDoc[] reranked) throws IOException {\n\n    int readerUpto = -1;\n    int endDoc = 0;\n    int docBase = 0;\n\n    LTRScoringQuery.ModelWeight.ModelScorer scorer = null;\n    int hitUpto = 0;\n    final FeatureLogger featureLogger = scoringQuery.getFeatureLogger();\n\n    while (hitUpto < hits.length) {\n      final ScoreDoc hit = hits[hitUpto];\n      final int docID = hit.doc;\n      LeafReaderContext readerContext = null;\n      while (docID >= endDoc) {\n        readerUpto++;\n        readerContext = leaves.get(readerUpto);\n        endDoc = readerContext.docBase + readerContext.reader().maxDoc();\n      }\n      // We advanced to another segment\n      if (readerContext != null) {\n        docBase = readerContext.docBase;\n        scorer = modelWeight.scorer(readerContext);\n      }\n      // Scorer for a LTRScoringQuery.ModelWeight should never be null since we always have to\n      // call score\n      // even if no feature scorers match, since a model might use that info to\n      // return a\n      // non-zero score. Same applies for the case of advancing a LTRScoringQuery.ModelWeight.ModelScorer\n      // past the target\n      // doc since the model algorithm still needs to compute a potentially\n      // non-zero score from blank features.\n      assert (scorer != null);\n      final int targetDoc = docID - docBase;\n      scorer.docID();\n      scorer.iterator().advance(targetDoc);\n\n      scorer.getDocInfo().setOriginalDocScore(new Float(hit.score));\n      hit.score = scorer.score();\n      if (hitUpto < topN) {\n        reranked[hitUpto] = hit;\n        // if the heap is not full, maybe I want to log the features for this\n        // document\n        if (featureLogger != null) {\n          featureLogger.log(hit.doc, scoringQuery, solrIndexSearch,\n              modelWeight.getFeaturesInfo());\n        }\n      } else if (hitUpto == topN) {\n        // collected topN document, I create the heap\n        heapify(reranked, topN);\n      }\n      if (hitUpto >= topN) {\n        // once that heap is ready, if the score of this document is lower that\n        // the minimum\n        // i don't want to log the feature. Otherwise I replace it with the\n        // minimum and fix the\n        // heap.\n        if (hit.score > reranked[0].score) {\n          reranked[0] = hit;\n          heapAdjust(reranked, topN, 0);\n          if (featureLogger != null) {\n            featureLogger.log(hit.doc, scoringQuery, solrIndexSearch,\n                modelWeight.getFeaturesInfo());\n          }\n        }\n      }\n      hitUpto++;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"8fa1f8f009c5147b03311c7e6e6469fa7b677186":["58c36d634c9789cb739fbd175c1a8d50b3303f6b"],"58c36d634c9789cb739fbd175c1a8d50b3303f6b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf":["9720b151fde2073f4e401450f4574e5f31c2d0ff","8fa1f8f009c5147b03311c7e6e6469fa7b677186"],"9720b151fde2073f4e401450f4574e5f31c2d0ff":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","58c36d634c9789cb739fbd175c1a8d50b3303f6b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"fb569fd721c41eafc2a2d788499a7df490c7f1a5":["8fa1f8f009c5147b03311c7e6e6469fa7b677186","f9a989a32a073c55e3aef6f807a3474184bbcf49"],"2534504cc6a6ab3301865de897422111495e0aad":["8fa1f8f009c5147b03311c7e6e6469fa7b677186"],"f9a989a32a073c55e3aef6f807a3474184bbcf49":["8fa1f8f009c5147b03311c7e6e6469fa7b677186","2534504cc6a6ab3301865de897422111495e0aad"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["2534504cc6a6ab3301865de897422111495e0aad"]},"commit2Childs":{"8fa1f8f009c5147b03311c7e6e6469fa7b677186":["5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf","fb569fd721c41eafc2a2d788499a7df490c7f1a5","2534504cc6a6ab3301865de897422111495e0aad","f9a989a32a073c55e3aef6f807a3474184bbcf49"],"58c36d634c9789cb739fbd175c1a8d50b3303f6b":["8fa1f8f009c5147b03311c7e6e6469fa7b677186","9720b151fde2073f4e401450f4574e5f31c2d0ff"],"5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf":[],"9720b151fde2073f4e401450f4574e5f31c2d0ff":["5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["58c36d634c9789cb739fbd175c1a8d50b3303f6b","9720b151fde2073f4e401450f4574e5f31c2d0ff"],"fb569fd721c41eafc2a2d788499a7df490c7f1a5":[],"2534504cc6a6ab3301865de897422111495e0aad":["f9a989a32a073c55e3aef6f807a3474184bbcf49","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"f9a989a32a073c55e3aef6f807a3474184bbcf49":["fb569fd721c41eafc2a2d788499a7df490c7f1a5"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf","fb569fd721c41eafc2a2d788499a7df490c7f1a5","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}